<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1218" opendate="2010-3-9 00:00:00" fixdate="2010-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE t LIKE some_view should create a new empty base table, but instead creates a copy of view</summary>
      <description>I think it should copy only the column definitions from the view metadata. Currently it is copying the entire descriptor, resulting in a new view instead of a new base table.</description>
      <version>0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.create.like.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="12181" opendate="2015-10-15 00:00:00" fixdate="2015-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change hive.stats.fetch.column.stats value to true for MiniTezCliDriver</summary>
      <description>There was a performance concern earlier, but HIVE-7587 has fixed that. We can change the default to true now.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.nonmr.fetch.threshold.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.pruning.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.pruning.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.dynpart.hashjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.vector.dynpart.hashjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.unionDistinct.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.partition.pruning.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.5.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1264" opendate="2010-3-21 00:00:00" fixdate="2010-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive work with Hadoop security</summary>
      <description></description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.19.java.org.apache.hadoop.hive.shims.Hadoop19Shims.java</file>
      <file type="M">shims.src.0.18.java.org.apache.hadoop.hive.shims.Hadoop18Shims.java</file>
      <file type="M">shims.src.0.17.java.org.apache.hadoop.hive.shims.Hadoop17Shims.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">shims.build.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1304" opendate="2010-4-13 00:00:00" fixdate="2010-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add row_sequence UDF</summary>
      <description>This is a poor man's answer to the standard analytic function row_number(); it assigns a sequence of numbers to rows, starting from 1.I'm calling it row_sequence() to distinguish it from the real analytic function, so that once we add support for those, there won't be any conflict with the existing UDF.The problem with this UDF approach is that there are no guarantees about ordering in SQL processing internals, so use with caution.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1440" opendate="2010-6-26 00:00:00" fixdate="2010-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FetchOperator(mapjoin) does not work with RCFile</summary>
      <description>RCFile needs column prunning's results. But when initializing the mapjoin's fetch operator, the cp's result is not passed to record reader.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14444" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade qtest execution framework to junit4 - migrate most of them</summary>
      <description>this is the second step..migrating all exiting qtestgen generated tests to junit4it might be possible that not all will get migrated in this ticket...I will leave out the problematic ones...</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestPerfCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestBeeLineDriver.vm</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.antlib.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14445" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade maven surefire to 2.19.1</summary>
      <description>newer maven surefire has a great feature: it is possible to select testmethods by regular expressions...and there are also improvements in using '#' to address testmethodsi've looked into this earlier...the upgrade is "almost" seemless...i'm already using 2.19.1, but the spark modules don't really like the empty spark.home variable</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14480" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC ETLSplitStrategy should use thread pool when computing splits</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="1453" opendate="2010-7-7 00:00:00" fixdate="2010-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Eclipse launch templates auto-adjust to Hive version number changes</summary>
      <description>The changes to prepare for branching out 0.6.0 required changes to build configuration which caused the launch configurations to break as the jars they referred to were renamed automatically. As a result, none of the launch configurations are working at this point.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">eclipse-templates.TestTruncate.launchtemplate</file>
      <file type="M">eclipse-templates.TestMTQueries.launchtemplate</file>
      <file type="M">eclipse-templates.TestJdbc.launchtemplate</file>
      <file type="M">eclipse-templates.TestHive.launchtemplate</file>
      <file type="M">eclipse-templates.TestCliDriver.launchtemplate</file>
      <file type="M">eclipse-templates.HiveCLI.launchtemplate</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1465" opendate="2010-7-14 00:00:00" fixdate="2010-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-site.xml ${user.name} not replaced for local-file derby metastore connection URL</summary>
      <description>Seems that for this parameter&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:derby:;databaseName=/var/lib/hive/metastore/${user.name}_db;create=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;${user.name} is never replaced by the actual user name:$ ls -la /var/lib/hive/metastore/total 24drwxrwxrwt 3 root root 4096 Apr 30 12:37 .drwxr-xr-x 3 root root 4096 Apr 30 12:25 ..drwxrwxr-x 5 hadoop hadoop 4096 Apr 30 12:37 ${user.name}_db</description>
      <version>0.5.0,0.6.0,0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14781" opendate="2016-9-16 00:00:00" fixdate="2016-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ptest killall command does not work</summary>
      <description>killall -f is not a valid flag.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testPassingUnitTest.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testPassingQFileTest.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testFailingUnitTest.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testFailingQFile.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.Phase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14830" opendate="2016-9-23 00:00:00" fixdate="2016-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move a majority of the MiniLlapCliDriver tests to use an inline AM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1489" opendate="2010-7-27 00:00:00" fixdate="2010-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestCliDriver -Doverwrite=true does not put the file in the correct directory</summary>
      <description>When adding a new file in clientpositive with -Doverwrite=true, the output file was in ql/ rather than ql/src/test/results/clientpositive.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1494" opendate="2010-7-30 00:00:00" fixdate="2010-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Index followup: remove sort by clause and fix a bug in collect_set udaf</summary>
      <description></description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14953" opendate="2016-10-13 00:00:00" fixdate="2016-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t use globStatus on S3 in MM tables</summary>
      <description>Need to investigate if recursive get is faster. Also, normal listStatus might suffice because MM code handles directory structure in a more definite manner than old code; so it knows where the files of interest are to be found.</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="14954" opendate="2016-10-13 00:00:00" fixdate="2016-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>put FSOP manifests for the instances of the same vertex into a directory</summary>
      <description>Deleting 100s of manifests can be expensive.</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MmCleanerThread.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="1497" opendate="2010-7-31 00:00:00" fixdate="2010-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support COMMENT clause on CREATE INDEX, and add new command for SHOW INDEXES</summary>
      <description>We need to work out the syntax for SHOW/DESCRIBE, taking partitioning into account.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1498" opendate="2010-7-31 00:00:00" fixdate="2010-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support IDXPROPERTIES on CREATE INDEX</summary>
      <description>It's partially there in the grammar but not hooked in; should work pretty much the same as TBLPROPERTIES.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.index.creation.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15000" opendate="2016-10-18 00:00:00" fixdate="2016-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove addlocaldriverjar, and addlocaldrivername from command line help</summary>
      <description>As discussed with Ferd, the following commands are not working, and never were intended as a command line parameters, so they should be removed from the command line help.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15001" opendate="2016-10-18 00:00:00" fixdate="2016-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove showConnectedUrl from command line help</summary>
      <description>As discussed with nemon, the showConnectedUrl commandline parameter is not working since a erroneous merge. Instead beeline always prints the currently connected url. Since it is good for everyone, no extra parameter is needed to turn this feature on.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="15003" opendate="2016-10-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update &amp;#39;ALTER TABLE...UPDATE STATISTICS FOR COLUMN..&amp;#39; statement to support more data types</summary>
      <description>Currently ALTER TABLE...UDPATE STATISTICS FOR COLUMN... only support updating statistics for following data types: STRING DOUBLE BOOLEAN BINARY DECIMAL DATEWe are missing the following data types TINYINT SMALLINT INT BIGINT FLOAT VARCHAR CHAR TIMESTAMP</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.table.update.status.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15013" opendate="2016-10-19 00:00:00" fixdate="2016-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config dir generated for tests should not be under the test tmp directory</summary>
      <description>mvn is used to clean up tmp directories created for tests, and to setup the config directory. The current structure is target/tmptarget/tmp/configAll of this is setup when mvn test is executed.Tests generate data under tmp - warehouse, metastore, etc. Having the conf dir there (generated by mvn) makes it complicate to add per test cleanup - since the entire tmp directory cannot be removed.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15020" opendate="2016-10-19 00:00:00" fixdate="2016-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle truncate for MM tables (not atomic yet)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15021" opendate="2016-10-19 00:00:00" fixdate="2016-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle (or add a test for) multi-insert into MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
    </fixedFiles>
  </bug>
  <bug id="1511" opendate="2010-8-4 00:00:00" fixdate="2010-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive plan serialization is slow</summary>
      <description>As reported by Edward Capriolo:For reference I did this as a test case....SELECT * FROM src wherekey=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0OR key=0 OR key=0 OR key=0 ORkey=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0OR key=0 OR key=0 OR key=0 OR...(100 more of these)No OOM but I gave up after the test case did not go anywhere for about2 minutes.</description>
      <version>0.7.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.java</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RowSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1512" opendate="2010-8-5 00:00:00" fixdate="2010-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to get hive_hbase-handler to work with hbase versions 0.20.4 0.20.5 and cloudera CDH3 version</summary>
      <description>the current trunk hive_hbase-handler only works with hbase 0.20.3, we need to get it to work with hbase versions 0.20.4 0.20.5 and cloudera CDH3 version</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15120" opendate="2016-11-3 00:00:00" fixdate="2016-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Storage based auth: allow option to enforce write checks for external tables</summary>
      <description>Under storage based authorization, we don't require write permissions on table directory for external table create/drop.This is because external table contents are populated often from outside of hive and are not written into from hive. So write access is not needed. Also, we can't require write permissions to drop a table if we don't require them for creation (users who created them should be able to drop them).However, this difference in behavior of external tables is not well documented. So users get surprised to learn that drop table can be done by just any user who has read access to the directory. At that point changing the large number of scripts that use external tables is hard. It would be good to have a user config option to have external tables to be treated same as managed tables.The option should be off by default, so that the behavior is backward compatible by default.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15121" opendate="2016-11-3 00:00:00" fixdate="2016-1-3 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Last MR job in Hive should be able to write to a different scratch directory</summary>
      <description>Hive should be able to configure all intermediate MR jobs to write to HDFS, but the final MR job to write to S3.This will be useful for implementing parallel renames on S3. The idea is that for a multi-job query, all intermediate MR jobs write to HDFS, and then the final job writes to S3. Writing to HDFS should be faster than writing to S3, so it makes more sense to write intermediate data to HDFS.The advantage is that any copying of data that needs to be done from the scratch directory to the final table directory can be done server-side, within the blobstore. The MoveTask simply renames data from the scratch directory to the final table location, which should translate to a server-side COPY request. This way HiveServer2 doesn't have to actually copy any data, it just tells the blobstore to do all the work.</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.BlobStorageUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15122" opendate="2016-11-3 00:00:00" fixdate="2016-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: Upcasting types should not obscure stats (min/max/ndv)</summary>
      <description>A UDFToLong breaks PK/FK inferences and triggers mis-estimation of joins in LLAP.Snippet from the bad plan.| STAGE PLANS: || Stage: Stage-1 || Tez || DagId: hive_20161031222730_a700058f-78eb-40d6-a67d-43add60a50e2:6 || Edges: || Map 2 &lt;- Map 1 (BROADCAST_EDGE) || Map 3 &lt;- Map 2 (BROADCAST_EDGE) || Reducer 4 &lt;- Map 3 (CUSTOM_SIMPLE_EDGE), Map 7 (CUSTOM_SIMPLE_EDGE), Map 8 (BROADCAST_EDGE), Map 9 (BROADCAST_EDGE) || Reducer 5 &lt;- Reducer 4 (SIMPLE_EDGE) || Reducer 6 &lt;- Reducer 5 (SIMPLE_EDGE) || DagName: || Vertices: || Map 1 || Map Operator Tree: || TableScan || alias: supplier || filterExpr: (s_suppkey is not null and s_nationkey is not null) (type: boolean) || Statistics: Num rows: 10000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE || Filter Operator || predicate: (s_suppkey is not null and s_nationkey is not null) (type: boolean) || Statistics: Num rows: 10000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE || Select Operator || expressions: s_suppkey (type: bigint), s_nationkey (type: bigint) || outputColumnNames: _col0, _col1 || Statistics: Num rows: 10000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE || Reduce Output Operator || key expressions: _col0 (type: bigint) || sort order: + || Map-reduce partition columns: _col0 (type: bigint) || Statistics: Num rows: 10000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE || value expressions: _col1 (type: bigint) || Execution mode: vectorized, llap || LLAP IO: all inputs || Map 2 || Map Operator Tree: || TableScan || alias: lineitem || filterExpr: (l_suppkey is not null and l_orderkey is not null) (type: boolean) || Statistics: Num rows: 2285121364 Data size: 63983407882 Basic stats: COMPLETE Column stats: PARTIAL || Filter Operator || predicate: (l_suppkey is not null and l_orderkey is not null) (type: boolean) || Statistics: Num rows: 2285121364 Data size: 127966796384 Basic stats: COMPLETE Column stats: PARTIAL || Select Operator || expressions: l_orderkey (type: bigint), l_suppkey (type: int), l_extendedprice (type: double), l_discount (type: double), l_shipdate (type: date) || outputColumnNames: _col0, _col1, _col2, _col3, _col4 || Statistics: Num rows: 2285121364 Data size: 127966796384 Basic stats: COMPLETE Column stats: PARTIAL || Map Join Operator || condition map: || Inner Join 0 to 1 || keys: || 0 _col0 (type: bigint) || 1 UDFToLong(_col1) (type: bigint) || outputColumnNames: _col1, _col2, _col4, _col5, _col6 || input vertices: || 0 Map 1 || Statistics: Num rows: 10000000 Data size: 880000000 Basic stats: COMPLETE Column stats: PARTIAL || Reduce Output Operator || key expressions: _col2 (type: bigint) || sort order: + || Map-reduce partition columns: _col2 (type: bigint) || Statistics: Num rows: 10000000 Data size: 880000000 Basic stats: COMPLETE Column stats: PARTIAL || value expressions: _col1 (type: bigint), _col4 (type: double), _col5 (type: double), _col6 (type: date) || Execution mode: vectorized, llap || LLAP IO: all inputs || Map 3 || Map Operator Tree: || TableScan || alias: orders || filterExpr: (o_orderkey is not null and o_custkey is not null) (type: boolean) || Statistics: Num rows: 4318801126 Data size: 51825626753 Basic stats: COMPLETE Column stats: NONE || Filter Operator || predicate: (o_orderkey is not null and o_custkey is not null) (type: boolean) || Statistics: Num rows: 4318801126 Data size: 51825626753 Basic stats: COMPLETE Column stats: NONE || Select Operator || expressions: o_orderkey (type: int), o_custkey (type: bigint) || outputColumnNames: _col0, _col1 || Statistics: Num rows: 4318801126 Data size: 51825626753 Basic stats: COMPLETE Column stats: NONE || Map Join Operator || condition map: || Inner Join 0 to 1 || keys: || 0 _col2 (type: bigint) || 1 UDFToLong(_col0) (type: bigint) || outputColumnNames: _col1, _col4, _col5, _col6, _col8 || input vertices: || 0 Map 2 || Statistics: Num rows: 4750681341 Data size: 57008190663 Basic stats: COMPLETE Column stats: NONE || Reduce Output Operator || key expressions: _col8 (type: bigint) || sort order: + || Map-reduce partition columns: _col8 (type: bigint) || Statistics: Num rows: 4750681341 Data size: 57008190663 Basic stats: COMPLETE Column stats: NONE || value expressions: _col1 (type: bigint), _col4 (type: double), _col5 (type: double), _col6 (type: date) || Execution mode: vectorized, llap || LLAP IO: all inputs || Map 7 Note the Map2 to Map3 output.This causes a rather large join (120GB) to be categorized as a map-join.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.join.pkfk.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1529" opendate="2010-8-11 00:00:00" fixdate="2010-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ANSI SQL covariance aggregate functions: covar_pop and covar_samp.</summary>
      <description>Create new built-in aggregate functions covar_pop and covar_samp, functions commonly used in statistical data analyses.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15460" opendate="2016-12-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ptest2 test failures</summary>
      <description>I see these failures when I try to run tests on ptest2Failed tests: testBatch(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testAlternativeTestJVM(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepNone(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepGit(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepHadoop1(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepSvn(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15466" opendate="2016-12-19 00:00:00" fixdate="2016-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental DROP_TABLE/DROP_PTN</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="15470" opendate="2016-12-20 00:00:00" fixdate="2016-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Catch Throwable instead of Exception in driver.execute.</summary>
      <description>Catch Throwable instead of Exception in driver.execute. So the failed query with Throwable not Exception will also be logged and reported.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="1549" opendate="2010-8-17 00:00:00" fixdate="2010-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ANSI SQL correlation aggregate function CORR(X,Y).</summary>
      <description>Aggregate function that computes the Pearson's coefficient of correlation between a set of number pairs.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1556" opendate="2010-8-18 00:00:00" fixdate="2010-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestContribCliDriver test</summary>
      <description>Due to https://issues.apache.org/jira/browse/HIVE-1548, TestContribCliDriver is broken. Some test results need to be updated</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.null.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes6.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes4.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1563" opendate="2010-8-19 00:00:00" fixdate="2010-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase tests broken</summary>
      <description>Broken by HIVE-1548, which did not update all log files.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.joins.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.bulk.m.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15732" opendate="2017-1-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add the ability to restrict configuration for the queries submitted to HS2 (Tez pool)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15735" opendate="2017-1-26 00:00:00" fixdate="2017-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In some cases, view objects inside a view do not have parents</summary>
      <description>This cause Sentry throws "No valid privileges" error:Error: Error while compiling statement: FAILED: SemanticException No valid privileges.To reproduce:Enable sentry:create table t1( i int);create view v1 as select * from t1;create view v2 as select * from v1 union all select * from v1;If the user does not have read permission on t1 and v1, the queryselect * from v2; This will fail with:Error: Error while compiling statement: FAILED: SemanticException No valid privileges User foo does not have privileges for QUERY The required privileges: Server=server1-&gt;Db=database2-&gt;Table=v1-&gt;action=select; (state=42000,code=40000)Sentry should not check v1's permission, for v1 has at least one parent(v2).</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestViewEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15910" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvements in Hive Unit Test by using In-memory Derby DB</summary>
      <description>Hive UT currently uses Derby DB with storage on disk which have some practical problems.1. The run-time of Hive unit tests are high as need to operate on the disk quite often.2. It can cause conflict if multiple test cases operates on the same table name (such as table being created already exist).To solve these problems, we shall use an in-memory storage option of Derby DB which can be even persisted if the test case demands that.https://db.apache.org/derby/docs/10.8/devguide/cdevdvlpinmemdb.html</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15921" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-order the slider stop command to avoid a force if possible</summary>
      <description>A graceful stop is required for slider --service llapstatus to work properly</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="16012" opendate="2017-2-22 00:00:00" fixdate="2017-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BytesBytes hash table - better capacity exhaustion handling</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="16081" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow "0.23" shim creation for Hadoop 3</summary>
      <description>We only have one shim, so for now, we are going to use that. Hopefully we won't need shims and build profiles for Hadoop 3</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="1614" opendate="2010-9-3 00:00:00" fixdate="2010-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDTF json_tuple should return null row when input is not a valid JSON string</summary>
      <description>If the input column is not a valid JSON string, json_tuple will not return anything but this will prevent the downstream operators to access the left-hand side table. We should output a NULL row instead, similar to when the input column is a NULL value.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16140" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stabilize few randomly failing tests</summary>
      <description>Golden file update for vector_between_in test and sort_before_diff for couple of Perf tests.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query14.q</file>
    </fixedFiles>
  </bug>
  <bug id="16340" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Kerberos + SSL connections to HMS</summary>
      <description>It should be possible to connect to HMS with Kerberos authentication and SSL enabled, at the same time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
    </fixedFiles>
  </bug>
  <bug id="16341" opendate="2017-3-31 00:00:00" fixdate="2017-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez Task Execution Summary has incorrect input record counts on some operators</summary>
      <description>Task Execution Summary-------------------------------------------------------------------------------------------------------------------------------- VERTICES TOTAL_TASKS FAILED_ATTEMPTS KILLED_TASKS DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS-------------------------------------------------------------------------------------------------------------------------------- Map 1 167 0 0 17640.00 2,109,200 23,068 150,000,004 11,995,136 Map 11 5 0 0 10559.00 71,960 633 4,023,690 799,900 Map 13 1 0 0 2244.00 6,090 29 25 3 Map 3 1 0 0 2849.00 7,080 99 25 3 Map 5 271 0 0 55834.00 12,934,890 358,376 1,500,000,001 1,500,000,161 Map 7 241 0 0 91243.00 5,020,860 71,182 1,827,250,341 652,413,443Reducer 10 1 0 0 1010.00 1,900 0 4 0Reducer 12 1 0 0 3854.00 1,320 0 799,900 1Reducer 14 1 0 0 1420.00 3,790 45 3 1 Reducer 2 1 0 0 9720.00 6,220 122 11,995,136 1 Reducer 4 1 0 0 810.00 2,100 105 3 1 Reducer 6 1 0 0 24863.00 3,260 5 1,500,000,161 1 Reducer 8 412 0 0 88215.00 17,106,440 184,524 2,165,208,640 1,864 Reducer 9 2 0 0 29752.00 3,980 0 1,864 4--------------------------------------------------------------------------------------------------------------------Seeing this on queries using runtime filtering. Noticed the INPUT_RECORDS look incorrect for the reducers that are responsible for aggregating the min/max/bloomfilter (Reducers 12, 14, 2, 6). For example Reducer 2 shows 12M input records. However looking at the task logs for Reducer 2, there were only 167 input records.It looks like Map 1 has 2 different output vertices (Reducer 2 and Reducer 8), but the total output rows for Map 1 (rather than just the rows going to each specific vertex) is being counted in the input rows for both Reducer 2 and Reducer 8.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
    </fixedFiles>
  </bug>
  <bug id="1641" opendate="2010-9-15 00:00:00" fixdate="2010-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add map joined table to distributed cache</summary>
      <description>Currently, the mappers directly read the map-joined table from HDFS, which makes it difficult to scale.We end up getting lots of timeouts once the number of mappers are beyond a few thousand, due to concurrent mappers.It would be good idea to put the mapped file into distributed cache and read from there instead.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.gen-javabean.org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.htree.HashDirectory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.queries.clientpositive.join39.q</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1658" opendate="2010-9-21 00:00:00" fixdate="2010-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix describe [extended] column formatting</summary>
      <description>When displaying the column schema, the formatting should follow should be name&lt;TAB&gt;type&lt;TAB&gt;comment&lt;NEWLINE&gt;to be inline with the previous formatting style for backward compatibility.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">hbase-handler.src.test.queries.hbase.stats.q</file>
      <file type="M">hbase-handler.src.test.results.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.stats.q.out</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.create.view.q</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16601" opendate="2017-5-5 00:00:00" fixdate="2017-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display Session Id and Query Name / Id in Spark UI</summary>
      <description>We should display the session id for each HoS Application Launched, and the Query Name / Id and Dag Id for each Spark job launched. Hive-on-MR does something similar via the mapred.job.name parameter. The query name is displayed in the Job Name of the MR app.The changes here should also allow us to leverage the config hive.query.name for HoS.This should help with debuggability of HoS applications. The Hive-on-Tez UI does something similar.Related issues for Hive-on-Tez: HIVE-12357, HIVE-12523</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.parallel.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16731" opendate="2017-5-22 00:00:00" fixdate="2017-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Make "CASE WHEN (day_name=&amp;#39;Sunday&amp;#39;) THEN column1 ELSE null end" that involves a column name or expression THEN or ELSE vectorize</summary>
      <description>Currently, CASE WHEN statements like that become VectorUDFAdaptor expressions.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.adaptor.usage.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="17160" opendate="2017-7-24 00:00:00" fixdate="2017-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding kerberos Authorization to the Druid hive integration</summary>
      <description>This goal of this feature is to allow hive querying a secured druid cluster using kerberos credentials.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17321" opendate="2017-8-15 00:00:00" fixdate="2017-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: analyze ORC table doesn&amp;#39;t compute raw data size when noscan/partialscan is not specified</summary>
      <description>Need to implement HIVE-9560 for Spark.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.string.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="17471" opendate="2017-9-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Enable hive.vectorized.row.identifier.enabled to true by default</summary>
      <description>We set it disabled in https://issues.apache.org/jira/browse/HIVE-17116 "Vectorization: Add infrastructure for vectorization of ROW__ID struct"But forgot to turn it on to true by default in Teddy's ACID ROW__ID work...</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1753" opendate="2010-10-26 00:00:00" fixdate="2010-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE 1633 hit for Stage2 jobs with CombineHiveInputFormat</summary>
      <description>Errors are the same as HIVE-1633 but I see them for Stage-2 jobs.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1754" opendate="2010-10-27 00:00:00" fixdate="2010-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove JDBM component from Map Join</summary>
      <description>Right now, JDBM is the major performance bottleneck of performance.With the growth of the small table, the PUT and GET operation will take most of execution time.Map Join is designed to load the data of small table into memory. If the data is too large to hold in memory, then there is no need to use the map join strategy.</description>
      <version>0.6.0,0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.TranslationPage.java</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JDBMSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JDBMDummyDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JDBMSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JDBMDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.gen-javabean.org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerOptions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.ByteArrayComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.ByteArraySerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.CacheEvictionException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.CachePolicy.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.CachePolicyListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.Conversion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.DefaultSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.FastIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.IntegerComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.IntegerSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.IterationException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.LongComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.LongSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.MRU.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.ObjectBAComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.Serialization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.Serializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.SoftCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.StringComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.Tuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.TupleBrowser.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.helper.WrappedRuntimeException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.htree.HashBucket.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.htree.HashDirectory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.htree.HashNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.htree.HTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.BaseRecordManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.BlockIo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.BlockView.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.CacheRecordManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.DataPage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.FileHeader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.FreeLogicalRowIdPage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.FreeLogicalRowIdPageManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.FreePhysicalRowId.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.FreePhysicalRowIdPage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.FreePhysicalRowIdPageManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.Location.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.LogicalRowIdManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.Magic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.PageCursor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.PageHeader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.PageManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.PhysicalRowId.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.PhysicalRowIdManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.Provider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.RecordCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.RecordFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.RecordHeader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.TransactionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17542" opendate="2017-9-15 00:00:00" fixdate="2017-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make HoS CombineEquivalentWorkResolver Configurable</summary>
      <description>The CombineEquivalentWorkResolver is run by default. We should make it configurable so that users can disable it in case there are any issues. We can enable it by default to preserve backwards compatibility.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1760" opendate="2010-10-29 00:00:00" fixdate="2010-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mismatched open/commit transaction calls in case of connection retry</summary>
      <description>Consider the create table function (parts removed for simplicity): private void create_table_core(final RawStore ms, final Table tbl) throws AlreadyExistsException, MetaException, InvalidObjectException { Path tblPath = null; boolean success = false, madeDir = false; try { ms.openTransaction(); // get_table checks whether database exists, it should be moved here if (is_table_exists(tbl.getDbName(), tbl.getTableName())) { throw new AlreadyExistsException("Table " + tbl.getTableName() + " already exists"); } ms.createTable(tbl); success = ms.commitTransaction(); } finally { if (!success) { ms.rollbackTransaction(); if (madeDir) { wh.deleteDir(tblPath, true); } } } }A potential openTransaction() / commitTransaction() mismatch can occur if the is_table_exits() method call experiences a connection failure. Since get_table() in is_table_exists() uses executeWithRetry(), the transaction will be rolled back and get_table() will be called again if the is a connection problem. However, this rollback and retry will reset the global openTransactionCalls counter back to 0, effectively canceling out the openTransaction() call. Then later in the method when commitTransaction() is called, Hive will throw an error similar to the following:Caused by: java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that there are unbalanced calls to openTransaction/commitTransactionA similar problem exists with create_type_core()</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17750" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a flag to automatically create most tables as MM</summary>
      <description>After merge we are going to do another round of gap identification... similar to HIVE-14990.However the approach used there is a huge PITA. It'd be much better to make tables MM by default at create time, not pretend they are MM at check time, from the perspective of spurious error elimination.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17840" opendate="2017-10-19 00:00:00" fixdate="2017-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetaStore eats exception if transactionalListeners.notifyEvent fail</summary>
      <description>For example, in add_partitions_core, if there's exception in MetaStoreListenerNotifier.notifyEvent(transactionalListeners,....), transaction rollback but no exception thrown. Client will assume add partition is successful and take a positive path.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="17841" opendate="2017-10-19 00:00:00" fixdate="2017-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement applying the resource plan</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.SampleTezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.QueryAllocationManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.AmPluginNode.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17843" opendate="2017-10-19 00:00:00" fixdate="2017-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UINT32 Parquet columns are handled as signed INT32-s, silently reading incorrect data</summary>
      <description>An unsigned 32 bit Parquet column, such asoptional int32 uint_32_col (UINT_32)is read by Hive as if it were signed, leading to incorrect results.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="1785" opendate="2010-11-12 00:00:00" fixdate="2010-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change Pre/Post Query Hooks to take in 1 parameter: HookContext</summary>
      <description>This way, it would be possible to add new parameters to the hooks without changing the existing hooks.This will be a incompatible change, and all the hooks need to change to the new API</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PreExecute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1786" opendate="2010-11-12 00:00:00" fixdate="2010-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve documentation for str_to_map() UDF</summary>
      <description>Currently, desc and desc extended return the same info.There is no mention of defaults anywhere</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1792" opendate="2010-11-14 00:00:00" fixdate="2010-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>track the joins which are being converted to map-join automatically</summary>
      <description>We should be able to track how many queries (join) got converted tomap-join</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1797" opendate="2010-11-18 00:00:00" fixdate="2010-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compressed the hashtable dump file before put into distributed cache</summary>
      <description>Clearly, the size of small table is the performance bottleneck for map join.Because the size of the small table will affect the memory usage and dumped hashtable file.That means there are 2 boundaries of the map join performance.1) The memory usage for local task and mapred task2) The dumped hashtable file size for distributed cacheThe reason that test case in last email spends most of the execution time on initializing is because it hits the second boundary.Since we have already bound the memory usage, one thing we can do is to let the performance never hits the secondary bound before it hits the first boundary.Assuming the heap size is 1.6 G and the small table file size is 15M compressed (75M uncompressed),local task can roughly hold that 1.5M unique rows in memory. Roughly the dumped file size will be 150M, which is too large to put into the distributed cache.From experiments, we can basically conclude when the dumped file size is smaller than 30M. The distributed cache works well and all the mappers will be initialized in a short time (less than 30 secs).One easy implementation is to compress the hashtable file. I use the gzip to compress the hashtable file and the file size is compressed from 100M to 13M.After several tests, all the mappers will be initialized in less than 23 secs.But this solution adds some decompression overhead to each mapper.Mappers on the same machine will do the duplicated decompression work.Maybe in the future, we can let the distributed cache to support this.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PathUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1804" opendate="2010-11-22 00:00:00" fixdate="2010-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mapjoin will fail if there are no files associating with the join tables</summary>
      <description>If there are some empty tables without any file associated, the map join will fail.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1811" opendate="2010-11-24 00:00:00" fixdate="2010-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the time the local task takes</summary>
      <description>After the local tasks finished, show the how much time it takes</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1853" opendate="2010-12-16 00:00:00" fixdate="2010-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>downgrade JDO version</summary>
      <description>After HIVE-1609, we are seeing some table not found errors intermittently.We have a test case where 5 processes are concurrently issueing the same query - explain extended insert .. select from &lt;T&gt;and once in a while, we get a error &lt;T&gt; not found - When we revert back the JDO version, the error is gone.We can investigate later to find the JDO bug, but for now this is a show-stopper for facebook, and needsto be reverted back immediately.This also means, that the filters will not be pushed to mysql.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1854" opendate="2010-12-19 00:00:00" fixdate="2010-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily disable metastore tests for listPartitionsByFilter()</summary>
      <description>After the JDO downgrade in HIVE-1853, the tests for the disabled function listPartitionByFilter() should be disabled as well.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1856" opendate="2010-12-20 00:00:00" fixdate="2010-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DROP TABLE/VIEW ... IF EXISTS</summary>
      <description>This issue combines issues HIVE-1550/1165/1542/1551: augment DROP TABLE/VIEW with IF EXISTS signal an error if the table/view doesn't exist and IF EXISTS wasn't specified introduce a flag in the configuration that allows you to turn off the new behavior</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.drop.view.failure2.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.drop.view.failure2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1858" opendate="2010-12-21 00:00:00" fixdate="2010-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DROP {PARTITION, INDEX, TEMPORARY FUNCTION} IF EXISTS</summary>
      <description>Extend HIVE-1856 to support IF EXISTS for {DROP TABLE, VIEW} and ALTER TABLE DROP PARTITION signal an error if the to-be-dropped entity doesn't exist and IF EXISTS isn't specified this behavior can be disabled by setting hive.exec.drop.ignorenonexistent to true</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.drop.multi.partitions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.view.q</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.multi.partitions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1862" opendate="2010-12-22 00:00:00" fixdate="2010-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revive partition filtering in the Hive MetaStore</summary>
      <description>HIVE-1853 downgraded the JDO version. This makes the feature of partition filtering in the metastore unusable. This jira is to keep track of the lost feature and discussing approaches to bring it back.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="1874" opendate="2011-1-3 00:00:00" fixdate="2011-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix HBase filter pushdown broken by HIVE-1638</summary>
      <description>See comments at end of HIVE-1660 for what happened.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18990" opendate="2018-3-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive doesn&amp;#39;t close Tez session properly</summary>
      <description>Hive doesn't close Tez session properly if AM isn't ready for accepting DAG.STRThis can be easily reproduced using the following steps:1) configure cluster on Tez;2) create file test.hqlcat ~/test.hqlshow databases;3) run the job$ hive --hiveconf hive.root.logger=DEBUG,console --hiveconf hive.execution.engine=tez -f ~/test.hqlIf we login into Yarn UI, we will see that jobs status is failed even it finished successfully.It happens because hive creates tez session by default. And if query finished very quickly, we can't close tez session properly because AM isn't ready for accepting any requests.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="1956" opendate="2011-2-4 00:00:00" fixdate="2011-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Provide DFS initialization script for Hive</summary>
      <description>This script automates the creation of the Hive warehouse and scratch directories on DFS</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1969" opendate="2011-2-8 00:00:00" fixdate="2011-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMinimrCliDriver merge_dynamic_partition2 and 3 are failing on trunk</summary>
      <description>I haven't looked into it yet but saw this at the end of the .q.out:+Ended Job = job_201102071402_0020 with errors+FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1973" opendate="2011-2-8 00:00:00" fixdate="2011-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting error when join on tables where name of table has uppercase letters</summary>
      <description>When execute a join query on tables containing Uppercase letters in the table names hit an exception Ex: create table a(b int); create table tabForJoin(b int,c int); select * from a join tabForJoin on(a.b=tabForJoin.b); Got an exception like this FAILED: Error in semantic analysis: Invalid Table Alias tabForJoinBut if i give without capital letters ,It is working</description>
      <version>0.5.0,0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1976" opendate="2011-2-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception should be thrown when invalid jar,file,archive is given to add command</summary>
      <description>When executed add command with non existing jar it should throw exception through HiveStatementEx: add jar /root/invalidpath/testjar.jarHere testjar.jar is not exist so it should throw exception.</description>
      <version>0.5.0,0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="1988" opendate="2011-2-14 00:00:00" fixdate="2011-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the delegation token issued by the MetaStore owned by the right user</summary>
      <description>The 'owner' of any delegation token issued by the MetaStore is set to the requesting user. When a delegation token is asked by the user himself during a job submission, this is fine. However, in the case where the token is requested for by services (e.g., Oozie), on behalf of the user, the token's owner is set to the user the service is running as. Later on, when the token is used by a MapReduce task, the MetaStore treats the incoming request as coming from Oozie and does operations as Oozie. This means any new directory creations (e.g., create_table) on the hdfs by the MetaStore will end up with Oozie as the owner.Also, the MetaStore doesn't check whether a user asking for a token on behalf of some other user, is actually authorized to act on behalf of that other user. We should start using the ProxyUser authorization in the MetaStore (HADOOP-6510's APIs).</description>
      <version>0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="2007" opendate="2011-2-25 00:00:00" fixdate="2011-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Executing queries using Hive Server is not logging to the log file specified in hive-log4j.properties</summary>
      <description>Start Hive Server by specifying the log details ( filelocation , appender , loglevel ) in hive-log4j.properties, but logging is not happening as per the details provided in the hive-log4j.properties.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20311" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add txn stats checks to some more paths</summary>
      <description>These were set to false in the original patch for no reason as far as I see.I later added notes but not TODOs to switch them over, so they remained as non-txn.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats.partial.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.like.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.if.expr.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.nway.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20322" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlakyTest: TestMiniDruidCliDriver</summary>
      <description>TestMiniDruidCliDriver is failing intermittently but I'm seeing it fail a significant percentage of the time.druid_timestamptzdruidmini_joinsdruidmini_maskingdruidmini_test1</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="20420" opendate="2018-8-18 00:00:00" fixdate="2018-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a fallback authorizer when no other authorizer is in use</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.4,3.1.1,4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.SettableConfigUpdater.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20423" opendate="2018-8-20 00:00:00" fixdate="2018-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set NULLS LAST as the default null ordering</summary>
      <description>HIVE-20150 TopNKeyOperator pushdown can be more efficient if NULLSLAST becomes the default null ordering.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.where.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.tmp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.localtimezone.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.llap.text.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.llap.io.data.conversion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.like.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.trailing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.update.where.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.update.tmp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.update.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedworkext.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.ValueBoundaryScanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFEvaluator.java</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.delete.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.non.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.delete.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.with.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.non.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.acid.fast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20424" opendate="2018-8-20 00:00:00" fixdate="2018-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool shall not pollute beeline history</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.schematool.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="20425" opendate="2018-8-20 00:00:00" fixdate="2018-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use a custom range of port for embedded Derby used by Druid.</summary>
      <description>Seems like good amount of the flakiness of Druid Tests is due to port collision between Derby used by Hive and the one used by Druid. The goal of this Patch is to use a custom range 60_000 to 65535 and find the first available to be used by Druid Derby process.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.MiniDruidCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="20450" opendate="2018-8-23 00:00:00" fixdate="2018-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add replication test for LOAD command on ACID table.</summary>
      <description>Add replication test for LOAD command on ACID/MM table.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="20590" opendate="2018-9-19 00:00:00" fixdate="2018-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow merge statement to have column schema</summary>
      <description>Currently MERGE statement doesn't let user specify column schema with INSERT statements, therefore DEFAULT constraint are not applicable with it.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sqlmerge.stats.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCardinalityViolation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="20620" opendate="2018-9-21 00:00:00" fixdate="2018-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>manifest collisions when inserting into bucketed sorted MM tables with dynamic partitioning</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20741" opendate="2018-10-13 00:00:00" fixdate="2018-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable udaf_context_ngrams.q and udaf_corr.q tests</summary>
      <description>Two qfile tests for TestCliDriver, they may all relate to number precision issues:org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver&amp;#91;udaf_context_ngrams&amp;#93; (batchId=79)Error:Client Execution succeeded but contained differences (error code = 1) after executing udaf_context_ngrams.q 43c43&lt; [{"ngram":["travelling"],"estfrequency":1.0}]&amp;#8212;&gt; [{"ngram":["travelling"],"estfrequency":3.0}]org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver&amp;#91;udaf_corr&amp;#93; (batchId=84)Client Execution succeeded but contained differences (error code = 1) after executing udaf_corr.q 100c100&lt; 0.6633880657639324&amp;#8212;&gt; 0.6633880657639326</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="2080" opendate="2011-3-29 00:00:00" fixdate="2011-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Few code improvements in the ql and serde packages.</summary>
      <description>Few code improvements in the ql and serde packages.1) Little performance Improvements 2) Null checks to avoid NPEs3) Effective varaible management.</description>
      <version>0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFunction.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldType.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeField.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20860" opendate="2018-11-2 00:00:00" fixdate="2018-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix or disable TestMiniLlapLocalCliDriver.testCliDriver[cbo_limit]</summary>
      <description>Test failed in one of the precommit job. Looks like there is some case where there is additonal space in the diffError MessageClient Execution succeeded but contained differences (error code = 1) after executing cbo_limit.q 11c11&lt; 1 4 2---&gt; 1 4 2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="21030" opendate="2018-12-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add credential store env properties redaction in JobConf</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestHiveCredentialProviders.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21040" opendate="2018-12-13 00:00:00" fixdate="2018-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>msck does unnecessary file listing at last level of directory tree</summary>
      <description>Here is the code snippet which is run by msck to list directoriesfinal Path currentPath = pd.p; final int currentDepth = pd.depth; FileStatus[] fileStatuses = fs.listStatus(currentPath, FileUtils.HIDDEN_FILES_PATH_FILTER); // found no files under a sub-directory under table base path; it is possible that the table // is empty and hence there are no partition sub-directories created under base path if (fileStatuses.length == 0 &amp;&amp; currentDepth &gt; 0 &amp;&amp; currentDepth &lt; partColNames.size()) { // since maxDepth is not yet reached, we are missing partition // columns in currentPath logOrThrowExceptionWithMsg( "MSCK is missing partition columns under " + currentPath.toString()); } else { // found files under currentPath add them to the queue if it is a directory for (FileStatus fileStatus : fileStatuses) { if (!fileStatus.isDirectory() &amp;&amp; currentDepth &lt; partColNames.size()) { // found a file at depth which is less than number of partition keys logOrThrowExceptionWithMsg( "MSCK finds a file rather than a directory when it searches for " + fileStatus.getPath().toString()); } else if (fileStatus.isDirectory() &amp;&amp; currentDepth &lt; partColNames.size()) { // found a sub-directory at a depth less than number of partition keys // validate if the partition directory name matches with the corresponding // partition colName at currentDepth Path nextPath = fileStatus.getPath(); String[] parts = nextPath.getName().split("="); if (parts.length != 2) { logOrThrowExceptionWithMsg("Invalid partition name " + nextPath); } else if (!parts[0].equalsIgnoreCase(partColNames.get(currentDepth))) { logOrThrowExceptionWithMsg( "Unexpected partition key " + parts[0] + " found at " + nextPath); } else { // add sub-directory to the work queue if maxDepth is not yet reached pendingPaths.add(new PathDepthInfo(nextPath, currentDepth + 1)); } } } if (currentDepth == partColNames.size()) { return currentPath; } }You can see that when the currentDepth at the maxDepth it still does a unnecessary listing of the files. We can improve this call by checking the currentDepth and bailing out early.This can improve the performance of msck command significantly especially when there are lot of files in each partitions on remote filesystems like S3 or ADLS</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="21041" opendate="2018-12-14 00:00:00" fixdate="2018-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE, ParseException in getting schema from logical plan</summary>
      <description>HIVE-20552 makes getting schema from logical plan faster. But it throws ParseException when it has column alias, and NullPointerException when it has subqueries.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2140" opendate="2011-5-2 00:00:00" fixdate="2011-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return correct Major / Minor version numbers for Hive Driver</summary>
      <description>Click to add description</description>
      <version>0.6.0,0.7.0</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="21580" opendate="2019-4-4 00:00:00" fixdate="2019-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ISO 8601 week numbering SQL:2016 formats</summary>
      <description>Enable Hive to parse the following datetime formats when any combination/subset of these or previously implemented patterns is provided in one string. Also catch combinationsthat conflict. IYYY IYY IY I IWhttps://docs.google.com/document/d/1V7k6-lrPGW7_uhqM-FhKl3QsxwCRy69v2KIxPsGjc1k/edit</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="21762" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP to support new format for replication policy input to take included tables list.</summary>
      <description>REPL DUMP syntax:REPL DUMP &lt;repl_policy&gt; [FROM &lt;last_repl_id&gt; WITH &lt;key_values_list&gt;; New format for the Replication policy have 3 parts all separated with Dot (.).1. First part is DB name.2. Second part is included list. Comma separated table names/regex with in square brackets[]. If square brackets are not there, then it is treated as single table replication which skips DB level events.3. Third part is excluded list. Comma separated table names/regex with in square brackets[].&lt;db_name&gt; -- Full DB replication which is currently supported&lt;db_name&gt;.['.*?'] -- Full DB replication&lt;db_name&gt;.[] -- Replicate just functions and not include any tables.&lt;db_name&gt;.['t1', 't2'] -- DB replication with static list of tables t1 and t2 included.&lt;db_name&gt;.['t1*', 't2', '*t3'].['t100', '5t3', 't4'] -- DB replication with all tables having prefix t1, with suffix t3 and include table t2 and exclude t100 which has the prefix t1, 5t3 which suffix t3 and t4. Need to support regular expression of any format. A table is included in dump only if it matches the regular expressions in included list and doesn't match the excluded list.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.BasicFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.ReplConst.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplLastIdInfo.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.ReplicationTestUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.TableContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DeletePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DeleteTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.OpenTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplRemoveFirstIncLoadPendFlagDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21763" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental replication to allow changing include/exclude tables list in replication policy.</summary>
      <description>REPL DUMP takes 2 inputs along with existing FROM and WITH clause.- REPL DUMP &lt;current_repl_policy&gt; [REPLACE &lt;previous_repl_policy&gt; FROM &lt;last_repl_id&gt; WITH &lt;key_values_list&gt;;- current_repl_policy and previous_repl_policy can be any format mentioned in Point-4.- REPLACE clause to be supported to take previous repl policy as input. If REPLACE clause is not there, then the policy remains unchanged.- Rest of the format remains same. Now, REPL DUMP on this DB will replicate the tables based on current_repl_policy. Single table replication of format &lt;db_name&gt;.t1 doesnt allow changing the policy dynamically. So REPLACE clause is not allowed if previous_repl_policy of this format. If any table is added dynamically either due to change in regular expression or added to include list should be bootstrapped using independant table level replication policy.- Hive will automatically figure out the list of tables newly included in the list by comparing the current_repl_policy &amp; previous_repl_policy inputs and combine bootstrap dump for added tables as part of incremental dump. "_bootstrap" directory can be created in dump dir to accommodate all tables to be bootstrapped.- If any table is renamed, then it may gets dynamically added/removed for replication based on defined replication policy + include/exclude list. So, Hive will perform bootstrap for the table which is just included after rename. REPL LOAD should check for changes in repl policy and drop the tables/views excluded in the new policy compared to previous policy. It should be done before performing incremental and bootstrap load from the current dump. REPL LOAD on incremental dump should load events directories first and then check for "_bootstrap" directory and perform bootstrap load on them.Rename table is not in scope of this jira.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.repl.ReplScope.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="21764" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP should detect and bootstrap any rename table events where old table was excluded but renamed table is included.</summary>
      <description>REPL DUMP fetches the events from NOTIFICATION_LOG table based on regular expression + inclusion/exclusion list. So, in case of rename table event, the event will be ignored if old table doesn't match the pattern but the new table should be bootstrapped. REPL DUMP should have a mechanism to detect such tables and automatically bootstrap with incremental replication.Also, if renamed table is excluded from replication policy, then need to drop the old table at target as well.There are 4 scenarios that needs to be handled. Both new name and old name satisfies the table name pattern filter. No need to do anything. The incremental event for rename should take care of the replication. Both the names does not satisfy the table name pattern filter. Both the names are not in the scope of the policy and thus nothing needs to be done. New name satisfies the pattern but the old name does not. The table will not be present at the target. Rename event handler for dump should detect this case and add the new table name to the list of table for bootstrap. All the events related to the table (new name) should be ignored. If there is a drop event for the table (with new name), then remove the table from the list of tables to be bootstrapped. In case of rename (double rename) If the new name satisfies the table pattern, then add the new name to the list of tables to be bootstrapped and remove the old name from the list of tables to be bootstrapped. If the new name does not satisfies then just removed the table name from the list of tables to be bootstrapped. New name does not satisfies the pattern but the old name satisfies. Change the rename event to a drop event.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.ReplEventFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.OpenTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="22281" opendate="2019-10-2 00:00:00" fixdate="2019-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table statement fails with "not supported NULLS LAST for ORDER BY in ASC order"</summary>
      <description>CREATE TABLE table_core2c4ywq7yjx ( k1 STRING, f1 STRING, sequence_num BIGINT, create_bsk BIGINT, change_bsk BIGINT, op_code STRING ) PARTITIONED BY (run_id BIGINT) CLUSTERED BY (k1) SORTED BY (k1, change_bsk, sequence_num) INTO 4 BUCKETS STORED AS ORCError while compiling statement: FAILED: SemanticException create/alter table: not supported NULLS LAST for ORDER BY in ASC order</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22282" opendate="2019-10-2 00:00:00" fixdate="2019-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Obtain LLAP delegation token only when LLAP is configured for Kerberos authentication</summary>
      <description>Contains also Kerberos related Zookeeper configuration changes after refactor.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.registry.impl.TestZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2233" opendate="2011-6-21 00:00:00" fixdate="2011-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show current database in hive prompt</summary>
      <description>Currently the hive prompt doesn't show which database the user in. It would be nice if it were something along the lines of hive (prod_tracking)&gt; or such.</description>
      <version>0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22340" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent shaded imports</summary>
      <description>Make sure that hive developers don't import the shaded version of some class by accident.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FileUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFBinarySetFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.RelTreeSignature.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.metrics.ReadWriteLockMetrics.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduSerDe.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduPredicateHandler.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduOutputFormat.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduInputFormat.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.kudu.KuduTestSetup.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22391" opendate="2019-10-22 00:00:00" fixdate="2019-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE while checking Hive query results cache</summary>
      <description>NPE when results cache was enabled:2019-10-21T14:51:55,718 ERROR [b7d7bea8-eef0-4ea4-ae12-951cb5dc96e3 HiveServer2-Handler-Pool: Thread-210]: ql.Driver (:()) - FAILED: NullPointerException nulljava.lang.NullPointerException at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkResultsCache(SemanticAnalyzer.java:15061) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12320) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:360) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:289) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1869) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1816) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1811) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197) at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:262) at org.apache.hive.service.cli.operation.Operation.run(Operation.java:247) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:575) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:561) at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315) at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:566) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="22680" opendate="2019-12-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in druid-handler Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.DruidKerberosUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="22681" opendate="2019-12-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hcatalog-webhcat Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22890" opendate="2020-2-14 00:00:00" fixdate="2020-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl load fails if table name contains _function</summary>
      <description>Repl load tries to load function if table name contains _function. Similarly for the below contantspublic static final String FUNCTIONS_ROOT_DIR_NAME = "_functions";The code just checks for contains(FUNCTIONS_ROOT_DIR_NAME). So even if any table or db name contains _functions, it takes the Function Load flow and fails.org.apache.hadoop.hive.ql.parse.SemanticException: Invalid pathorg.apache.hadoop.hive.ql.parse.SemanticException: Invalid path at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.tasks(LoadFunction.java:94) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.executeBootStrapLoad(ReplLoadTask.java:238) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.execute(ReplLoadTask.java:110) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:103) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:82) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT]Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.isFunctionAlreadyLoaded(LoadFunction.java:105) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.tasks(LoadFunction.java:81) ~[hive-exec-3.1.0.3.1.5.1-2.jar:3.1.1000-SNAPSHOT] ... 5 more</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="2307" opendate="2011-7-26 00:00:00" fixdate="2011-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema creation scripts for PostgreSQL use bit(1) instead of boolean</summary>
      <description>The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.hive&gt; create table test (id int); FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7" using statement "INSERT INTO "SDS" ("SD_ID","INPUT_FORMAT","OUTPUT_FORMAT","LOCATION","SERDE_ID","NUM_BUCKETS","IS_COMPRESSED") VALUES (?,?,?,?,?,?,?)" failed : ERROR: column "IS_COMPRESSED" is of type bit but expression is of type boolean</description>
      <version>0.5.0,0.6.0,0.7.0,0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0-to-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.5.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.1.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.3.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="23073" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade netty and upgrade to netty 4.1.48.Final</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23280" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trigger compaction with old aborted txns</summary>
      <description>When a txn is aborted and the compaction threshold for number of aborted txns is not reached then the aborted transaction can remain forever in the RDBMS database. This could result in several serious performance degradations: getOpenTxns has to list this aborted txn forever TXN_TO_WRITE_ID table is not cleanedWe should add a threshold, so after a given time the compaction is started anyway.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionInfoStruct.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23283" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate random temp ID for lock enqueue and commitTxn</summary>
      <description>In order to optimize the S4U scope of enqueue lock and commitTxn, currently a hardcoded constant (-1) is used to first insert all the lock and ws entries with a temporary lockID/commitID. However, in a concurrent environment this seems to cause some performance degradation (and deadlock issues with some rdbms) as multiple concurrent transactions are trying to insert rows with the same primary key (e.g. (-1, 1), (-1, 2), (-1, 3), .. etc. for (extID/intID) in HIVE_LOCKS). The proposed solution is to replace the constant with a random generated negative number, which seems to resolve this issue.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23284" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on mariadb-java-client</summary>
      <description>It hasGNU Lesser General Public License which is Category X.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.Mysql.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.DEV-README</file>
    </fixedFiles>
  </bug>
  <bug id="23287" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on icu4j</summary>
      <description>Brought in transitively via druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23293" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locks: Implement zero-wait readers</summary>
      <description>With a new lock type (EXCL_WRITE) for INSERT_OVERWRITE, SHARED_READ does not have to wait for any lock - it can fails fast for a pending EXCLUSIVE, because even if there is an EXCL_WRITE or SHARED_WRITE pending, there's no semantic reason to wait for them.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.drop.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="2735" opendate="2012-1-22 00:00:00" fixdate="2012-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2748" opendate="2012-1-25 00:00:00" fixdate="2012-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2765" opendate="2012-1-31 00:00:00" fixdate="2012-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase handler uses ZooKeeperConnectionException which is not compatible with HBase versions other than 0.89</summary>
      <description>It cannot integrate with HBase0.21 and may not be able to integrate with hbase0.9x</description>
      <version>0.7.0,0.8.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2771" opendate="2012-2-1 00:00:00" fixdate="2012-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for filter pushdown for key ranges in hbase for keys of type string</summary>
      <description>This is a subtask of HIVE-1643</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2815" opendate="2012-2-22 00:00:00" fixdate="2012-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter pushdown in hbase for keys stored in binary format</summary>
      <description>This patch enables filter pushdown for keys stored in binary format in hbase</description>
      <version>0.6.0,0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="2822" opendate="2012-2-24 00:00:00" fixdate="2012-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add JSON output to the hive ddl commands</summary>
      <description>The goal is to have an option to produce JSON output of the DDL commands that is easily machine parseable.For example, "desc my_table" currently gives id bigint user string and we want to allow a json output: { "columns": [ {"name": "id", "type": "bigint"}, {"name": "user", "type": "string"} ] }</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.column.rename4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.database.drop.does.not.exist.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.database.create.already.exists.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2824" opendate="2012-2-25 00:00:00" fixdate="2012-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>typo in configuration parameter</summary>
      <description>hive.files.umask.vlaueshould be:hive.files.umask.value</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2825" opendate="2012-2-25 00:00:00" fixdate="2012-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concatenating a partition does not inherit location from table</summary>
      <description>When a table is created in one dfs, a partition is added to that table, the table's dfs is changed, and the partitioned is concatenated, the partitions location remains exactly the same. Instead, it should inherit its location from the table, and be updated accordingly.See https://issues.apache.org/jira/browse/HIVE-1707 for an analogous change to insert overwrite.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2905" opendate="2012-3-26 00:00:00" fixdate="2012-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Desc table can&amp;#39;t show non-ascii comments</summary>
      <description>When desc a table with command line or hive jdbc way, the table's comment can't be read.1. I have updated javax.jdo.option.ConnectionURL parameter in hive-site.xml file. jdbc:mysql://...:3306/hive?characterEncoding=UTF-82. In mysql database, the comment field of COLUMNS table can be read normally.</description>
      <version>0.7.0,0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="3149" opendate="2012-6-16 00:00:00" fixdate="2012-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamically generated paritions deleted by Block level merge</summary>
      <description>When creating partitions in a table using dynamic partitions and a Block level merge is executed at the end of the query, some partitions may be lost. Specifically if the values of two or more dynamic partition keys end in the same sequence of numbers, all but the largest will be dropped.I was not able to confirm it, but I suspect that if a map reduce job is speculated as part of the merge, the duplicate data will not be deleted either.E.g.insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)select key, value, if(key % 2 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';In this query, if a Block level merge is executed at the end, only one of the partitions ds=2008-04-08/hr=a1 and ds=2008-04-08/hr=b1 will appear in the final table.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="48" opendate="2008-9-7 00:00:00" fixdate="2008-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JDBC connections for interoperability between Hive and RDBMS</summary>
      <description>In many DW and BI systems, the data are stored in RDBMS for now such as oracle, mysql, postgresql ... for reporting, charting and etc.It would be useful to be able to import data from RDBMS and export data to RDBMS using JDBC connections.If Hive support JDBC connections, It wll be much easier to use 3rd party DW/BI tools.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
