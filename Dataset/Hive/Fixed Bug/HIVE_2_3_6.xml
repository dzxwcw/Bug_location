<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="22148" opendate="2019-8-27 00:00:00" fixdate="2019-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>S3A delegation tokens are not added in the job config of the Compactor.</summary>
      <description>Compactor job does not have the s3 delegation tokens, required to contact s3 and causes the job to fail.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
    </fixedFiles>
  </bug>
  <bug id="22149" opendate="2019-8-27 00:00:00" fixdate="2019-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore: Unify codahale metrics.log json structure between hiveserver2 and metastore services</summary>
      <description>While fixing HIVE-22140 I found some really annoying differences between the codahale metric file structures between hiveserver2 and metastore, e.g.open_connections: can be found in "counters" for hs2, but in "gauges" for msthreads count: it's a proper "threads.count" for hs2, but a really ambiguous "count" for msso I realized that "memory." and "threads." prefix is completely absent in ms metrics file, which is misleading</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.metrics.TestMetrics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.metrics.Metrics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="22195" opendate="2019-9-11 00:00:00" fixdate="2019-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure authentication type for Zookeeper when different from the default cluster wide</summary>
      <description>This could be useful in case when cluster is kerberized, but Zookeeper is not.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistryClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22217" opendate="2019-9-18 00:00:00" fixdate="2019-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better Logging for Hive JAR Reload</summary>
      <description>Troubleshooting Hive Reloadable Auxiliary JARs has always been difficult.Add logging to at least confirm which JAR files are being loaded.</description>
      <version>2.3.6,3.2.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="22225" opendate="2019-9-20 00:00:00" fixdate="2019-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add owner name for create database if missing</summary>
      <description>When Spark connects to HMS, the database owner name is not filled. This could happen to other clients as well. We shall fill this in HMS. Ownership info is useful for authorizer component in Ranger, etc.Â </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.client.builder.DatabaseBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="22230" opendate="2019-9-23 00:00:00" fixdate="2019-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for filtering partitions on temporary tables</summary>
      <description>We need support for filtering partitions on temporary tables. In order to achieve this, SessionHiveMetastoreClient must implement the following methods:public List&lt;Partition&gt; listPartitionsByFilter(String catName, String dbName, String tableName,String filter, int maxParts)public int getNumPartitionsByFilter(String catName, String dbName, String tableName, String filter)public PartitionSpecProxy listPartitionSpecsByFilter(String catName, String dbName, String tblName, String filter, int maxParts)public PartitionValuesResponse listPartitionValues(PartitionValuesRequest request)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestListPartitions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestSessionHiveMetastoreClientListPartitionsTempTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestSessionHiveMetastoreClientGetPartitionsTempTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.TempTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.PartitionTree.java</file>
    </fixedFiles>
  </bug>
  <bug id="22231" opendate="2019-9-23 00:00:00" fixdate="2019-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive query with big size via knox fails with Broken pipe Write failed</summary>
      <description>Posting large data through knox is causing the Broken pipe (Write failed) java.net.SocketException. Issue here that HS2 is responding with 401 even before complete query is transferred. HS2 has to wait until all the data is received and then respond with 401. That way knox can re-open the connection and send the negotiate header with the data.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="22236" opendate="2019-9-24 00:00:00" fixdate="2019-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail to create View selecting View containing NOT IN subquery</summary>
      <description>Given a complicated view with a select statement that has subquery containing "NOT IN" Hive fails to create a simple view as SELECT * FROM complicated_view (with CBO disabled).The unparse replacements of the complicated view will be applied to the text of the simple view, resulting in IllegalArgumentException: replace: range invalid exceptions from org.antlr.runtime.TokenRewriteStream.replace.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22238" opendate="2019-9-24 00:00:00" fixdate="2019-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PK/FK selectivity estimation underscales estimations</summary>
      <description>at this point the parent operators rownum is scaled according to pkfkselectivityhowever pkfkselectivity is computed on a whole subtree.Scaling it by that amount will count in estimation already used when parentstats was calculated...so depending on the number of upstream joins - this may lead to severe underestimationswhat happened was: optimization was able to push the filter to the other side of the join as a result the incoming data was already filtered scaling down by the PK selectiviy - was actually already there...but a new "scaling" happened</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.stat.estimate.drill.q</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="2224" opendate="2011-6-15 00:00:00" fixdate="2011-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to add partitions atomically</summary>
      <description>I'd like to see an atomic version of the add_partitions() call.Whether this is to be done by config to affect add_partitions() behaviour (not my preference) or just changing add_partitions() default behaviour (my preference, but likely to affect current behaviour, so will need others' input) or by making a new add_partitions_atomic() call depends on discussion.This looks relatively doable to implement (will need a dependent add_partition_core to not do a ms.commit_partition() early, and to cache list of directories created to remove on rollback, and a list of AddPartitionEvent to trigger in one shot later)Thoughts? This also seems like something to implement for allowing HIVE-1805.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="22241" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement UDF to interpret date/timestamp using its internal representation and Gregorian-Julian hybrid calendar</summary>
      <description>UDF that converts a date/timestamp to new proleptic Gregorian calendar (ISO 8601 standard), which is produced by extending the Gregorian calendar backward to dates preceding its official introduction in 1582, assuming that its internal days/milliseconds since epoch is calculated using legacy Gregorian-Julian hybrid calendar, i.e., calendar that supports both the Julian and Gregorian calendar systems with the support of a single discontinuity, which corresponds by default to the Gregorian date when the Gregorian calendar was instituted.</description>
      <version>None</version>
      <fixedVersion>3.1.3,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="22242" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move TempTable and PartitionTree out of SessionHiveMetastoreClient</summary>
      <description>SessionHiveMetastoreClient is getting too crowded and unreadable. TempTable and PartitionTree should moved into a separate class.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22243" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align Apache Thrift version to 0.9.3-1 in standalone-metastore as well</summary>
      <description>Thrift was bumped to 0.9.3-1 in HIVE-21173, but standalone-metastore was left out of this.Thanks for pointing this out Ashutosh Bapat!</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22244" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added default ACLs for znodes on a non-kerberized cluster</summary>
      <description>Set default ACLs for znodes on a non-kerberized cluster: Create/Read/Delete/Write/Admin to the world</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22245" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make qtest feature parser reuseable</summary>
      <description>right now we have a parser for --! qt:dataset ; to enable further addition of things (I would like to run scheduled query service for some qtests )</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.ql.dataset.TestDatasetParser.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.QTestDatasetHandler.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.DatasetParser.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.DatasetCollection.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22453" opendate="2019-11-4 00:00:00" fixdate="2019-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table unnecessarily fetches partitions</summary>
      <description>The simple describe table command without EXTENDED and FORMATTED (i.e., DESCRIBE table_name) fetches all partitions when no partition is specified, although it does not display partition statistics in nature.The command should not fetch partitions since it can take a long time for a large amount of partitions.For instance, in our environment, the command takes around 8 seconds for a table with 8760 (24 * 365) partitions.</description>
      <version>2.3.6,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="22458" opendate="2019-11-5 00:00:00" fixdate="2019-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more constraints on showing partitions</summary>
      <description>When we showing partitions of a table withÂ thousands of partitions,Â  all the partitions will be returned and it's not easy to catch the specified one from them, this make showing partitions hard to use. We can add where/limit/order by constraints to show partitions like:Â show partitions table_name &amp;#91;partition_specs&amp;#93; where partition_key &gt;= value order by partition_key desc limit n;Â </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.show.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.showparts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.show.ShowPartitionsOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.show.ShowPartitionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.show.ShowPartitionAnalyzer.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="22606" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AvroSerde logs avro.schema.literal under INFO level</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="22698" opendate="2020-1-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Statement#closeOnCompletion()</summary>
      <description>I am a member of MyBatisÂ team and a user reported that Hive does not support java.sql.Statement#closeOnCompletion() yet.</description>
      <version>2.3.6,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="22703" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction configuration check when starting HMS/HS2</summary>
      <description>Currently when starting HMS we can have bugous configuration which prevents compatction to work. We should find a way to inform the admin about the configuration error, or even prevent HMS to start in this case.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="22705" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cache is polluted by query-based compactor</summary>
      <description>One of the steps that query-based compaction does is the verification of ACID sort order by using theÂ validate_acid_sort_order UDF. This is a prerequisite before the actual compaction can happen, and is done by aÂ query that reads the whole table content.This results in the whole table content being populated into the cache. The problem is that this content is not useful and will rather pollute the cache space, as it can never be used again: cache content binds to files (file IDs) that obviously will be changed in this case by compaction.I propose we disable LLAP caching in the session of query-based compaction's queries.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez.java</file>
    </fixedFiles>
  </bug>
  <bug id="22736" opendate="2020-1-16 00:00:00" fixdate="2020-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replication across multiple encryption zones</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.storage.AlterTableUnarchiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.storage.AlterTableSetLocationOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.storage.AlterTableArchiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.storage.AlterTableArchiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.create.CreateTableOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.metadata.TestAlterTableMetadata.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStoreUpdateUsingEvents.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22842" opendate="2020-2-6 00:00:00" fixdate="2020-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timestamp/date vectors in Arrow serializer should use correct calendar for value representation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.CalendarUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22844" opendate="2020-2-7 00:00:00" fixdate="2020-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validate cm configs, add retries in fs apis for cm</summary>
      <description>Retry create cm root logic Rename encryptionZones to cmRootLocations to be more accurate Check cmRootEncrypted.isAbsolute() first before we go for creating anything Validate fallbackNonEncryptedCmRootDir if it's really not encrypted Refactor deleteTableData logic</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMultipleEncryptionZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.MetastoreHousekeepingLeaderTestBase.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22896" opendate="2020-2-17 00:00:00" fixdate="2020-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase fast hashtable size on detecting initial collision</summary>
      <description>This would help in avoiding collisions and helps in burning lesser CPU cycles during probing.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="22964" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM table split computation is very slow</summary>
      <description>Since for MM table we process the paths prior to inputFormat.getSplits() we end up doing listing on the whole table at once. This could be optimized.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22966" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Consider including waitTime for comparing attempts in same vertex</summary>
      <description>When attempts are compared within same vertex, it should pick up the attempt with longest wait time to avoid starvation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22967" opendate="2020-3-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support hive.reloadable.aux.jars.path for Hive on Tez</summary>
      <description>The jars in hive.reloadable.aux.jars.path are not localized in Tez containers. As a result, any query utilizing those reloadable jars fails for Hive on Tez due to ClassNotFoundException.Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1578856704640_0087_1_00, diagnostics=[Task failed, taskId=task_1578856704640_0087_1_00_000001, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure) : attempt_1578856704640_0087_1_00_000001_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:354) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266) ... 16 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.example.hive.udf.Lower at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:71) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.initializeOp(VectorFilterOperator.java:83) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:573) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:525) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:386) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.initializeMapOperator(VectorMapOperator.java:591) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:317) ... 17 moreCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.example.hive.udf.Lower at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:134) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(FunctionRegistry.java:1492) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.&lt;init&gt;(ExprNodeGenericFuncEvaluator.java:111) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.get(ExprNodeEvaluatorFactory.java:58) at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:63) ... 24 moreCaused by: java.lang.ClassNotFoundException: com.example.hive.udf.Lower at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClassInternal(GenericUDFBridge.java:142) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:132) ... 28 more], TaskAttempt 1 failed, (omitted)HIVE-14037 and HIVE-14142 resolved the similar issues for MapReduce and Hive on Spark, respectively, in the past.Â </description>
      <version>2.3.6,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22968" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set hive.parquet.timestamp.time.unit default to micros</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23015" opendate="2020-3-12 00:00:00" fixdate="2020-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLED definition</summary>
      <description>TheÂ HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLEDÂ is definied like this:HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLED("hive.vectorized.groupby.complex.types.enabled", true, "This flag should be set to true to enable group by vectorization\n" + "of aggregations that use complex types.\n", "For example, AVG uses a complex type (STRUCT) for partial aggregation results" + "The default value is true."),Notice that the end of the "of aggregations that use complex types.\n" line is a "," instead of "+".So this will end up using the following constructor using the second sentence as an "altName" ConfVars(String varname, Object defaultVal, String description, String altName) { this(varname, defaultVal, null, description, true, false, altName); }We should fix this.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
