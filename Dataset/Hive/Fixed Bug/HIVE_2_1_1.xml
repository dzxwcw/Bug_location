<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="14224" opendate="2016-7-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP rename query specific log files once a query is complete</summary>
      <description>Once a query is complete, rename the query specific log file so that YARN can aggregate the logs (once it's configured to do so).</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14311" opendate="2016-7-22 00:00:00" fixdate="2016-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to schedule Heartbeat task if the query doesn&amp;#39;t require locks</summary>
      <description>Otherwise the Heartbeat task will just stay there and not be cleaned up, which may cause OOM eventually.</description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14348" opendate="2016-7-26 00:00:00" fixdate="2016-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for alter table exchange partition</summary>
      <description></description>
      <version>1.2.1,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exchgpartition2lel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exchange.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14377" opendate="2016-7-28 00:00:00" fixdate="2016-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: issue with how estimate cache removes unneeded buffers</summary>
      <description>Results in NPE when reading</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileEstimateErrors.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14378" opendate="2016-7-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data size may be estimated as 0 if no columns are being projected after an operator</summary>
      <description>in those cases we still emit rows.. but they may not have any columns within it. We shouldn't estimate 0 data size in such cases.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partial.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14421" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FS.deleteOnExit holds references to _tmp_space.db files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14422" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IF: when using LLAP IF from multiple threads in secure cluster, tokens can get mixed up</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="14433" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor LLAP plan cache avoidance and fix issue in merge processor</summary>
      <description>Map and reduce processors do this: if (LlapProxy.isDaemon()) { cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache(); // do not cache plan...but merge processor just gets the plan. If it runs in LLAP, it can get a cached plan. Need to move this logic into ObjectCache itself, via a isPlan arg or something. That will also fix this issue for merge processor</description>
      <version>2.0.1,2.1.1,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14446" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add switch to control BloomFilter in Hybrid grace hash join and make the FPP adjustable</summary>
      <description>When row count exceeds certain limit, it doesn't make sense to generate a bloom filter, since its size will be a few hundred MB or even a few GB.</description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14447" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set HIVE_TRANSACTIONAL_TABLE_SCAN to the correct job conf for FetchOperator</summary>
      <description></description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14635" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>establish a separate path for FSOP to write into final path</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="14636" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pass information from FSOP/TezTask to commit to take care of speculative execution and failed tasks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveStatsUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14638" opendate="2016-8-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle unions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveStatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14639" opendate="2016-8-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle SKEWED BY for MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14641" opendate="2016-8-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle writing to dynamic partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14642" opendate="2016-8-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle insert overwrite for MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="14643" opendate="2016-8-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle ctas for the MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14645" opendate="2016-8-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>table conversion to and from MM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ImportCommitTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="14706" opendate="2016-9-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage information not set properly</summary>
      <description>I am trying to fetch column level lineage after a CTAS query in a Post Execution hook in Hive. Below are the queries:create table t1(id int, name string);create table t2 as select * from t1;The lineage information is retrieved using the following sample piece of code:lInfo = hookContext.getLinfo()for(Map.Entry&lt;LineageInfo.DependencyKey, LineageInfo.Dependency&gt; e : lInfo.entrySet()) { System.out.println("Col Lineage Key : " + e.getKey()); System.out.println("Col Lineage Value: " + e.getValue());}The Dependency field(i.e Col Lineage Value) is coming in as null.</description>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14865" opendate="2016-9-30 00:00:00" fixdate="2016-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix comments after HIVE-14350</summary>
      <description>there are still some comments in the code that should've been updated in HIVE-14350</description>
      <version>2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14913" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new unit tests</summary>
      <description>Moving bunch of tests from system test to hive unit tests to reduce testing overhead</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.acid.part.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.acid.non.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.part.update.q</file>
      <file type="M">ql.src.test.queries.clientpositive.lvj.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.acid.non.acid.q</file>
      <file type="M">ql.src.test.queries.clientpositive.current.date.timestamp.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cte.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cte.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.table.stats.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.basic.q</file>
    </fixedFiles>
  </bug>
  <bug id="14914" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the &amp;#39;TestClass&amp;#39; did not produce a TEST-*.xml file message</summary>
      <description>For timed out unit test batches - this report may not be generated correctly.Also, there's no differentiation between 0 tests in a batch vs an actual missing report.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestReportParser.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testPassingQFileTest.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testFailingQFile.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestQFileTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JUnitReportParser.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.UnitTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.QFileTestBatch.java</file>
    </fixedFiles>
  </bug>
  <bug id="14915" opendate="2016-10-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to skip log collection for successful tests</summary>
      <description>We generate multiple gigs of tests at the moment. An option to skip log collection for successful tests could be useful.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.AbstractTestPhase.java</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.server.TestExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15137" opendate="2016-11-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore add partitions background thread should use current username</summary>
      <description>The background thread used in HIVE-13901 for adding partitions needs to be reinitialized with current UGI for each invocation. Otherwise the user in context while thread was created would be the current UGI during the actions in the thread.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.external2.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="1514" opendate="2010-8-5 00:00:00" fixdate="2010-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Be able to modify a partition&amp;#39;s fileformat and file location information.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.mix.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.diff.part.input.formats.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterPartitionProtectModeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15434" opendate="2016-12-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDF to allow interrogation of uniontype values</summary>
      <description>OverviewAs stated in the documention:UNIONTYPE support is incomplete The UNIONTYPE datatype was introduced in Hive 0.7.0 (HIVE-537), but full support for this type in Hive remains incomplete. Queries that reference UNIONTYPE fields in JOIN (HIVE-2508), WHERE, and GROUP BY clauses will fail, and Hive does not define syntax to extract the tag or value fields of a UNIONTYPE. This means that UNIONTYPEs are effectively look-at-only.It is essential to have a usable uniontype. Until full support is added to Hive users should at least have the ability to inspect and extract values for further comparison or transformation.ProposalI propose to add a GenericUDF that has 2 modes of operation. Consider the following schema and data that contains a union:Schema:struct&lt;field1:uniontype&lt;int,string&gt;&gt;Query:hive&gt; select field1 from thing;{0:0}{1:"one"}Explode to StructThis method will recursively convert all unions within the type to structs with fields named tag_n, n being the tag number. Only the tag_* field that matches the tag of the union will be populated with the value. In the case above the schema of field1 will be converted to:struct&lt;tag_0:int,tag_1:string&gt;hive&gt; select extract_union(field1) from thing;{"tag_0":0,"tag_1":null}{"tag_0":null,"tag_1":one}hive&gt; select extract_union(field1).tag_0 from thing;0nullExtract the specified tagThis method will simply extract the value of the specified tag. If the tag number matches then the value is returned, if it does not, then null is returned.hive&gt; select extract_union(field1, 0) from thing;0null</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="15436" opendate="2016-12-15 00:00:00" fixdate="2016-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhancing metastore APIs to retrieve only materialized views</summary>
      <description>Enhancing metastore APIs such that, instead of returning all tables, it can return only: views materialized views</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="15437" opendate="2016-12-15 00:00:00" fixdate="2016-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>avro tables join fails when - tbl join tbl_postfix</summary>
      <description>The following queries return good results:select * from table1 where col1=key1; select * from table1_1 where col1=key1; When join them together, it gets following error:Caused by: java.io.IOException: org.apache.avro.AvroTypeException: Found long, expecting union at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121) ~[hive-shims-common-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77) ~[hive-shims-common-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:365) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:43) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:229) ~[hive-shims-common-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:141) ~[hive-shims-common-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]The two avro tables both is defined by using avro schema, and the first table's name is the second table name's prefix. Note that this happens when single map is reading input from both tables. If map-join is used with MR execution engine, or if Tez execution engine is used, this issue is not seen.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15550" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix arglist logging in schematool</summary>
      <description>In DEBUG mode schemaTool prints the password to log file.This is also seen if the user includes --verbose option.</description>
      <version>2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">errata.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15586" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Insert and Create statement Transactional</summary>
      <description>Currently insert/create will return the handle to user without waiting for the data been loaded by the druid cluster. In order to avoid that will add a passive wait till the segment are loaded by historical in case the coordinator is UP.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DruidStorageHandlerTest.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15616" opendate="2017-1-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve contents of qfile test output</summary>
      <description>The current output of the failed qtests has a less than ideal signal to noise ratio.We have duplicated stack traces and messages between the error message/stack trace/error out.For diff errors the actual difference is missing from the error message and can be found only in the standard out.I would like to simplify this output by removing duplications, moving relevant information to the top.</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestProcessExecResult.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreAccumuloCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
    </fixedFiles>
  </bug>
  <bug id="15679" opendate="2017-1-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some metastore metrics always contains column=null (write_column_statistics, write_partition_column_statistics)</summary>
      <description>HIVE-14764 refactored the metrics start functions outside the for loop. This caused that the colName is always null in this case.We should remove these extra elements, since they provide no extra information.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="15711" opendate="2017-1-24 00:00:00" fixdate="2017-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky TestEmbeddedThriftBinaryCLIService.testTaskStatus</summary>
      <description>the above test is flaky and on the local build environments it keeps failing. Fix to prevent it from failing intermittently.</description>
      <version>2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="15713" opendate="2017-1-24 00:00:00" fixdate="2017-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add ldap authentication related configuration to restricted list</summary>
      <description>Various ldap configuration parameters as below should be added to the restricted list of configuration parameters such that users cant change them per session. hive.server2.authentication.ldap.baseDNhive.server2.authentication.ldap.urlhive.server2.authentication.ldap.Domainhive.server2.authentication.ldap.groupDNPatternhive.server2.authentication.ldap.groupFilterhive.server2.authentication.ldap.userDNPatternhive.server2.authentication.ldap.userFilterhive.server2.authentication.ldap.groupMembershipKeyhive.server2.authentication.ldap.userMembershipKeyhive.server2.authentication.ldap.groupClassKeyhive.server2.authentication.ldap.customLDAPQuery</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15748" opendate="2017-1-30 00:00:00" fixdate="2017-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove cycles created due to semi join branch and map join Op on same operator pipeline</summary>
      <description>If a semi join branch and map join operator are on same operator pipeline, then there could be a cycle created. Where the other map feeding into the mapjoin operator is waiting for the semi join branch to finish causing a cycle.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="15765" opendate="2017-1-31 00:00:00" fixdate="2017-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bracketed comments</summary>
      <description>C-style comments are in the SQL spec as well as supported by all major DBs. The are useful for inline annotation of the SQL. We should have them too.Example:select/*+ MAPJOIN(a) */ /* mapjoin hint */a /* column */from foo join bar;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SelectClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15766" opendate="2017-1-31 00:00:00" fixdate="2017-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBNotificationlistener leaks JDOPersistenceManager</summary>
      <description></description>
      <version>2.0.0,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15779" opendate="2017-2-1 00:00:00" fixdate="2017-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: WaitQueue comparators should return 0 when tasks of the same DAG are of same priority</summary>
      <description>Observed cases, where in tasks within same vertex were competing with each and getting killed[IPC Server handler 3 on 44598 (1484282558103_4855_1_00_003877_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003179_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003877_7 because of lower priority[IPC Server handler 1 on 44598 (1484282558103_4855_1_00_002959_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003832_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_002959_7 because of lower priority[IPC Server handler 0 on 44598 (1484282558103_4855_1_00_003723_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003254_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003723_7 because of lower priority[IPC Server handler 4 on 44598 (1484282558103_4855_1_00_003560_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003076_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003560_7 because of lower priority[IPC Server handler 2 on 44598 (1484282558103_4855_1_00_003775_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_004011_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003775_7 because of lower priority[IPC Server handler 3 on 44598 (1484282558103_4855_1_00_003842_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_004045_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003842_7 because of lower priority[IPC Server handler 1 on 44598 (1484282558103_4855_1_00_003953_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003915_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003953_7 because of lower priority[IPC Server handler 0 on 44598 (1484282558103_4855_1_00_003819_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003919_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003819_7 because of lower priority[IPC Server handler 4 on 44598 (1484282558103_4855_1_00_002074_8)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003790_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_002074_8 because of lower priority[IPC Server handler 2 on 44598 (1484282558103_4855_1_00_003670_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003736_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003670_7 because of lower priority[IPC Server handler 1 on 44598 (1484282558103_4855_1_00_003153_8)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003877_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003153_8 because of lower priority[IPC Server handler 0 on 44598 (1484282558103_4855_1_00_003328_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003775_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003328_7 because of lower priority[IPC Server handler 4 on 44598 (1484282558103_4855_1_00_003817_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003842_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003817_7 because of lower priority[IPC Server handler 2 on 44598 (1484282558103_4855_1_00_004065_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003723_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_004065_7 because of lower priority[IPC Server handler 3 on 44598 (1484282558103_4855_1_00_003902_7)] impl.TaskExecutorService: attempt_1484282558103_4855_1_00_003560_7 evicted from wait queue in favor of attempt_1484282558103_4855_1_00_003902_7 because of lower priority</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.FirstInFirstOutComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15853" opendate="2017-2-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin removed in case of dynamically partitioned hash join</summary>
      <description>Incase of dynamically partitioned hash join, the semijoin branch is removed to avoid a cycle even though it does not create a cycle.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="15867" opendate="2017-2-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add blobstore tests for import/export</summary>
      <description>This patch covers ten separate tests testing import and export operations running against blobstore filesystems: Import addpartition blobstore -&gt; file file -&gt; blobstore blobstore -&gt; blobstore blobstore -&gt; hdfs import/export blobstore -&gt; file file -&gt; blobstore blobstore -&gt; blobstore (partitioned and non-partitioned) blobstore -&gt; HDFS (partitioned and non-partitioned)</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-blobstore.src.test.resources.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15874" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid position alias in Group By when CBO failed</summary>
      <description>for examplecreate table alias_test_01(a INT, b STRING) ;create table alias_test_02(a INT, b STRING) ;create table alias_test_03(a INT, b STRING) ;set hive.groupby.position.alias = true;set hive.cbo.enable=true;explain select * from alias_test_01 alias01 left join (select 2017 as a, b from alias_test_02 group by 1, 2) alias02 on alias01.a = alias02.a left join alias_test_03 alias03on alias01.a = alias03.a;error info:FAILED: SemanticException &amp;#91;Error 10220&amp;#93;: Invalid position alias in Group ByPosition alias: 2017 does not existThe Select List is indexed from 1 to 2the first process Position Alias result:when CBO optimize failed and reAnalyzeAST is true, position alias will be processed twice.1. 'group by 1, 2' convert to 'group by 2017, b'2. 'group by 2017, b' 2017 column does not exist</description>
      <version>1.2.1,2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15903" opendate="2017-2-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute table stats when user computes column stats</summary>
      <description></description>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.remove.26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.special.character.in.tabnames.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.drop.partition.with.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.table.invalidate.column.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15916" opendate="2017-2-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add blobstore tests for CTAS</summary>
      <description>This patch covers 3 tests checking CTAS operations against blobstore filesystems. The tests check we can create a table with a CTAS statement from another table, for the source-target combinations blobtore-blobstore, blobstore-hdfs, hdfs-blobstore, and for two target tables, one in the same default database as the source, and another in a new database.</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.ctas.q</file>
    </fixedFiles>
  </bug>
  <bug id="15917" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect error handling from BackgroundWork can cause beeline query to hang</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="15918" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some debug messages to identify an issue getting runtimeInfo from tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="15964" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16064" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ALL set quantifier with aggregate functions</summary>
      <description>SQL:2011 allows &lt;set quantifier&gt; ALL with aggregate functions which is equivalent to aggregate function without ALL.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16067" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: send out container complete messages after a fragment completes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapContainerLauncher.java</file>
    </fixedFiles>
  </bug>
  <bug id="16068" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BloomFilter expectedEntries not always using NDV when it&amp;#39;s available during runtime filtering</summary>
      <description>The current logic only uses NDV if it's the only ColumnStat available, but it looks like there can sometimes be other ColStats in the semijoin Select operator.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16084" opendate="2017-3-2 00:00:00" fixdate="2017-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW COMPACTIONS should display CompactionID</summary>
      <description>together with HIVE-13353 it will let users search for specific job</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.showlocks.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16106" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Datanucleus 4.2.12</summary>
      <description>As described in HIVE-14698, the datanucleus-rdbms package that we have currently (4.1.7) has a bug which generates incorrect synatx for MS SQL Server. The bug has been fixed in later releases. HIVE-14698 was a workaround for hive, but since DN has the fix in its 4.2.x line, we should pick it from there. Link to DN releases: https://sourceforge.net/projects/datanucleus/files/datanucleus-accessplatform/. We'll be upgrading the relevant jars in hive to their corresponding versions present in datanucleus-accessplatform v4.2.12</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16107" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HttpClient should retry one more time on NoHttpResponseException</summary>
      <description>Hive's JDBC client in HTTP transport mode doesn't retry on NoHttpResponseException. We've seen the exception being thrown to the JDBC end user when used with Knox as the proxy, when Knox upgraded its jetty version, which has a smaller value for jetty connector idletimeout, and as a result closes the HTTP connection on server side. The next jdbc query on the client, throws a NoHttpResponseException. However, subsequent queries reconnect, but the JDBC driver should ideally handle this by retrying.</description>
      <version>2.0.1,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="16154" opendate="2017-3-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Determine when dynamic runtime filtering should be disabled</summary>
      <description>Currently dynamic min/max/bloom optimization is always enabled. However there are times where it may not be beneficial, such as if the semijoin has a PK-FK relation and there are no filters on the semijoin table. Try to devise a way to do a cost/benefit calculation to see if there is enough benefit to adding the runtime filter.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.semijoin.reduction2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16155" opendate="2017-3-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need for ConditionalTask if no conditional map join is created</summary>
      <description></description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="1624" opendate="2010-9-8 00:00:00" fixdate="2010-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Patch to allows scripts in S3 location</summary>
      <description>I want to submit a patch which allows user to run scripts located in S3.This patch enables Hive to download the hive scripts located in S3 buckets and execute them. This saves users the effort of copying scripts to HDFS before executing them.ThanksVaibhav</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16293" opendate="2017-3-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column pruner should continue to work when SEL has more than 1 child</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.varchar.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.add.part.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.add.part.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.udfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16294" opendate="2017-3-24 00:00:00" fixdate="2017-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support snapshot for truncate table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16297" opendate="2017-3-25 00:00:00" fixdate="2017-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improving hive logging configuration variables</summary>
      <description>There are a few places in the source-code where we use Configuration.dumpConfiguration(). We should preprocess the configuration properties before dumping it in the logs.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">common.src.java.org.apache.hive.http.ConfServlet.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="16298" opendate="2017-3-25 00:00:00" fixdate="2017-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add config to specify multi-column joins have correlated columns</summary>
      <description>The default row estimation for multi-key joins divides the row estimate by the product of the NDVs for each join column, which can cause the row estimate to be low. Try adding a config to assume the columns are correlated, where we only divide the row estimate by the largest NDV.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16335" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline user HS2 connection file should use /etc/hive/conf instead of /etc/conf/hive</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clientssays: BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/conf/hive in that order.shouldn't it be?BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/hive/conf in that order?Most distributions I've used have a /etc/hive/conf dir.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="16336" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename hive.spark.use.file.size.for.mapjoin to hive.spark.use.ts.stats.for.mapjoin</summary>
      <description>The name hive.spark.use.file.size.for.mapjoin is confusing. It indicates that HoS uses file size for mapjoin but in fact it still uses (in-memory) data size. We should change it.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16413" opendate="2017-4-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table as select does not check ownership of the location</summary>
      <description>1. following statement failed: create table foo(id int) location 'hdfs:///tmp/foo';Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=userx, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/tmp/foo]] (state=42000,code=40000)2. but when use create table as select, it successed:0: jdbc:hive2://189.39.151.44:21066/&gt; create table foo location 'hdfs:///tmp/foo' as select * from xxx2;INFO : Number of reduce tasks is set to 0 since there's no reduce operatorINFO : number of splits:1INFO : Submitting tokens for job: job_1491449632882_0094INFO : Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:haclusterINFO : The url to track the job: https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Starting Job = job_1491449632882_0094, Tracking URL = https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Kill Command = /opt/hive-1.3.0/bin/..//../hadoop/bin/hadoop job -kill job_1491449632882_0094INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0INFO : 2017-04-10 09:44:49,185 Stage-1 map = 0%, reduce = 0%INFO : 2017-04-10 09:44:57,202 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.98 secINFO : MapReduce Total cumulative CPU time: 1 seconds 980 msecINFO : Ended Job = job_1491449632882_0094INFO : Stage-3 is selected by condition resolver.INFO : Stage-2 is filtered out by condition resolver.INFO : Stage-4 is filtered out by condition resolver.INFO : Moving data to directory hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001 from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10003INFO : Moving data to directory hdfs:/tmp/foo from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001No rows affected (26.969 seconds)3. and the table location is hdfs://hacluster/tmp/foo :0: jdbc:hive2://189.39.151.44:21066/&gt; desc formatted foo;+-------------------------------+-------------------------------------------------------+-----------------------+--+| col_name | data_type | comment |+-------------------------------+-------------------------------------------------------+-----------------------+--+| # col_name | data_type | comment || | NULL | NULL || id | int | || | NULL | NULL || # Detailed Table Information | NULL | NULL || Database: | default | NULL || Owner: | userx | NULL || CreateTime: | Mon Apr 10 09:44:59 CST 2017 | NULL || LastAccessTime: | UNKNOWN | NULL || Protect Mode: | None | NULL || Retention: | 0 | NULL || Location: | hdfs://hacluster/tmp/foo | NULL || Table Type: | MANAGED_TABLE | NULL || Table Parameters: | NULL | NULL || | COLUMN_STATS_ACCURATE | false || | numFiles | 1 || | numRows | -1 || | rawDataSize | -1 || | totalSize | 56 || | transient_lastDdlTime | 1491788699 || | NULL | NULL || # Storage Information | NULL | NULL || SerDe Library: | org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe | NULL || InputFormat: | org.apache.hadoop.hive.ql.io.RCFileInputFormat | NULL || OutputFormat: | org.apache.hadoop.hive.ql.io.RCFileOutputFormat | NULL || Compressed: | No | NULL || Num Buckets: | -1 | NULL || Bucket Columns: | [] | NULL || Sort Columns: | [] | NULL || Storage Desc Params: | NULL | NULL || | serialization.format | 1 |+-------------------------------+-------------------------------------------------------+-----------------------+--+</description>
      <version>1.2.2,1.3.0,2.1.1</version>
      <fixedVersion>1.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug id="16419" opendate="2017-4-11 00:00:00" fixdate="2017-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude hadoop related classes for JDBC stabdalone jar</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16425" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: unload old hashtables before reloadHashTable</summary>
      <description>@Override protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException { // The super method will reload a hash table partition of one of the small tables. // Currently, for native vector map join it will only be one small table. super.reloadHashTable(pos, partitionId); MapJoinTableContainer smallTable = spilledMapJoinTables[pos]; vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable); needHashTableSetup = true; LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName()); if (isLogDebugEnabled) { LOG.debug(CLASS_NAME + " reloadHashTable!"); } }The super call causes an OOM because of existing memory usage by vectorMapJoinHashTable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16429" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</summary>
      <description>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16456" opendate="2017-4-14 00:00:00" fixdate="2017-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kill spark job when InterruptedException happens or driverContext.isShutdown is true.</summary>
      <description>Kill spark job when InterruptedException happens or driverContext.isShutdown is true. If the InterruptedException happened in RemoteSparkJobMonitor and LocalSparkJobMonitor, it will be better to kill the job. Also there is a race condition between submit the spark job and query/operation cancellation, it will be better to check driverContext.isShutdown right after submit the spark job. This will guarantee the job being killed no matter when shutdown is called. It is similar as HIVE-15997.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16482" opendate="2017-4-19 00:00:00" fixdate="2017-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Ser/Des need to use dimension output name</summary>
      <description>Druid Ser/Desr need to use dimension output name in order to function with Extraction function.Some part of the Ser/Desr code uses the method DimensionSpec.getDimension() although when extraction function are in game the name of the dimension will be defined by DimensionSpec.getOutputName()</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16483" opendate="2017-4-19 00:00:00" fixdate="2017-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS should populate split related configurations to HiveConf</summary>
      <description>There are several split related configurations, such as MAPREDMINSPLITSIZE, MAPREDMINSPLITSIZEPERNODE, MAPREDMINSPLITSIZEPERRACK, etc., that should be populated to HiveConf. Currently we only do this for MAPREDMINSPLITSIZE.All the others, if not set, will be using the default value, which is 1.Without these, Spark sometimes will not merge small files for file formats such as text.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16485" opendate="2017-4-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable outputName for RS operator in explain formatted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.formatted.oid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explain.formatted.oid.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AnnotateReduceSinkOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Vertex.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Stage.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="16554" opendate="2017-4-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Make HouseKeeperService threads daemon</summary>
      <description></description>
      <version>2.0.1,2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16556" opendate="2017-4-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify schematool scripts to initialize and create METASTORE_DB_PROPERTIES table</summary>
      <description>sub-task to modify schema tool and its related changes so that the new table is added to the schema when schematool initializes or upgrades the schema.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16557" opendate="2017-4-28 00:00:00" fixdate="2017-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Specialize ReduceSink empty key case</summary>
      <description>Gopal pointed out that native Vectorization of ReduceSink is missing the empty key case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.vector.nohybridgrace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1661" opendate="2010-9-21 00:00:00" fixdate="2010-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default values for parameters</summary>
      <description>It would be good to have a default value for some hive parameters:say RETENTION to be 30 days.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16712" opendate="2017-5-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StringBuffer v.s. StringBuilder</summary>
      <description>Where appropriate, replaced StringBuffer with StringBuilder to remove superfluous synchronization.</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastValueStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseFilterPlanUtil.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.Schema.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.GroupingValidator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParserUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16759" opendate="2017-5-25 00:00:00" fixdate="2017-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add table type information to HMS log notifications</summary>
      <description>The DB notifications used by HiveMetaStore should include the table type for all notifications that include table events, such as create, drop and alter table.This would be useful for consumers to identify views vs tables.</description>
      <version>2.1.1</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.DropTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.DropPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.api.TestHCatClientNotification.java</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONInsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.InsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="16801" opendate="2017-6-1 00:00:00" fixdate="2017-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: throwExpandError should be an immediate fatal</summary>
      <description>Vectorized hashtable throwExpandError() should not be retried before failing query - it is a deterministic failure (&amp; immediate query failure).</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="16803" opendate="2017-6-1 00:00:00" fixdate="2017-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table change column comment should not try to get column stats for update</summary>
      <description>When running command like "alter table .. change .." (e.g. ALTER TABLE testtbl CHANGE col col string COMMENT 'change column comment' to change a column's comment, Hive should not go to fetch the column stats for update since the comment change does not affect table/partition column stats.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16821" opendate="2017-6-3 00:00:00" fixdate="2017-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: support Explain Analyze in vectorized mode</summary>
      <description>Currently, to avoid a branch in the operator inner loop - the runtime stats are only available in non-vector mode.</description>
      <version>2.1.1,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explainanalyze.3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16869" opendate="2017-6-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive returns wrong result when predicates on non-existing columns are pushed down to Parquet reader</summary>
      <description>When hive.optimize.ppd and hive.optimize.index.filter are turned, and a select query has a condition on a column that doesn't exist in Parquet file (such as a partition column), Hive often returns wrong result.Please see below example for details:hive&gt; create table test_parq (a int, b int) partitioned by (p int) stored as parquet;OKTime taken: 0.292 secondshive&gt; insert overwrite table test_parq partition (p=1) values (1, 2);OKTime taken: 5.08 secondshive&gt; select * from test_parq where a=1 and p=1;OK1 2 1Time taken: 0.441 seconds, Fetched: 1 row(s)hive&gt; select * from test_parq where (a=1 and p=1) or (a=999 and p=999);OK1 2 1Time taken: 0.197 seconds, Fetched: 1 row(s)hive&gt; set hive.optimize.index.filter=true;hive&gt; select * from test_parq where (a=1 and p=1) or (a=999 and p=999);OKTime taken: 0.167 secondshive&gt; select * from test_parq where (a=1 or a=999) and (a=999 or p=1);OKTime taken: 0.563 seconds</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetFilterPredicateConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16871" opendate="2017-6-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CachedStore.get_aggr_stats_for has side affect</summary>
      <description>Every get_aggr_stats_for accumulates the stats and propagated to the first partition stats object. It accumulates and gives wrong result in the follow up invocations.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16907" opendate="2017-6-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"INSERT INTO" overwrite old data when destination table encapsulated by backquote</summary>
      <description>A way to reproduce:create database tdb;use tdb;create table t1(id int);create table t2(id int);explain insert into `tdb.t1` select * from t2;+---------------------------------------------------------------------------------------------------------------------------------------------------+| Explain |+---------------------------------------------------------------------------------------------------------------------------------------------------+| STAGE DEPENDENCIES: || Stage-1 is a root stage || Stage-6 depends on stages: Stage-1 , consists of Stage-3, Stage-2, Stage-4 || Stage-3 || Stage-0 depends on stages: Stage-3, Stage-2, Stage-5 || Stage-2 || Stage-4 || Stage-5 depends on stages: Stage-4 || || STAGE PLANS: || Stage: Stage-1 || Map Reduce || Map Operator Tree: || TableScan || alias: t2 || Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE || Select Operator || expressions: id (type: int) || outputColumnNames: _col0 || Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE || File Output Operator || compressed: false || Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE || table: || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat || serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe || name: tdb.t1 || || Stage: Stage-6 || Conditional Operator || || Stage: Stage-3 || Move Operator || files: || hdfs directory: true || destination: hdfs://hacluster/user/hive/warehouse/tdb.db/t1/.hive-staging_hive_2017-06-15_15-52-34_017_849305017872068583-1/-ext-10000 || || Stage: Stage-0 || Move Operator || tables: || replace: true || table: || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat || serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe || name: tdb.t1 || || Stage: Stage-2 || Merge File Operator || Map Operator Tree: || RCFile Merge Operator || merge level: block || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || || Stage: Stage-4 || Merge File Operator || Map Operator Tree: || RCFile Merge Operator || merge level: block || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || || Stage: Stage-5 || Move Operator || files: || hdfs directory: true || destination: hdfs://hacluster/user/hive/warehouse/tdb.db/t1/.hive-staging_hive_2017-06-15_15-52-34_017_849305017872068583-1/-ext-10000 || |+---------------------------------------------------------------------------------------------------------------------------------------------------+Note that 'replace: true' in move operator</description>
      <version>1.1.0,2.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure4.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="1691" opendate="2010-10-5 00:00:00" fixdate="2010-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ANALYZE TABLE command should check columns in partition spec</summary>
      <description>ANALYZE TABEL PARTITION (col1, col2,...) should check whether col1, col2 etc are partition columns.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16978" opendate="2017-6-27 00:00:00" fixdate="2017-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: add current thread ID to the log redirector for the RemoteDriver</summary>
      <description>The log redirector currently doesn't include the thread ID. In case there are multiple sessions running concurrently, it's hard to tell which redirector belongs to which session.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16985" opendate="2017-6-28 00:00:00" fixdate="2017-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: enable SMB join in elevator after the former is fixed</summary>
      <description>We currently skip the IO elevator when we encounter an SMB join (see HIVE-16761). However, it might work with elevator with the code commented out in HIVE-16761. Need to look again after HIVE-16965 is fixed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.llap.smb.q</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16989" opendate="2017-6-29 00:00:00" fixdate="2017-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some issues identified by lgtm.com</summary>
      <description>lgtm.com has identified a number of issues where there may be scope for improvement. The plan is to address some of the alerts found at https://lgtm.com/projects/g/apache/hive/.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.Repeated.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.FlatRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.StaticPermanentFunctionChecker.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="1699" opendate="2010-10-12 00:00:00" fixdate="2010-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect partition pruning ANALYZE TABLE</summary>
      <description>If table T is partitioned, ANALYZE TABLE T PARTITION (...) COMPUTE STATISTICS; will gather stats for all partitions even though partition spec only chooses a subset.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17005" opendate="2017-6-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure REPL DUMP and REPL LOAD are authorized properly</summary>
      <description>Currently, we piggyback REPL DUMP and REPL LOAD on EXPORT and IMPORT auth privileges. However, work is on to not populate all the relevant objects in inputObjs and outputObjs, which then requires that REPL DUMP and REPL LOAD be authorized at a higher level, and simply require ADMIN_PRIV to run,</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17008" opendate="2017-7-1 00:00:00" fixdate="2017-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix boolean flag switchup in DropTableEvent</summary>
      <description>When dropping a non-existent database, the HMS will still fire registered DROP_DATABASE event listeners. This results in an NPE when the listeners attempt to deref the null database parameter.</description>
      <version>None</version>
      <fixedVersion>2.3.2,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="171" opendate="2008-12-12 00:00:00" fixdate="2008-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>distribute by and sort by ignored in the case of select *</summary>
      <description>create table tab1(col1 string, col2 string)select * from tab1 distribute by col1 sort by col2is different from:select col1, col2 from tab1 distribute by col1 sort by col2</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17167" opendate="2017-7-25 00:00:00" fixdate="2017-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore specific configuration tool</summary>
      <description>As part of making the metastore a separately releasable module we need configuration tools that are specific to that module. It cannot use or extend HiveConf as that is in hive common. But it must take a HiveConf object and be able to operate on it.The best way to achieve this is using Hadoop's Configuration object (which HiveConf extends) together with enums and static methods.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17168" opendate="2017-7-25 00:00:00" fixdate="2017-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for stand alone metastore</summary>
      <description>We need to create a separate maven module for the stand alone metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1719" opendate="2010-10-15 00:00:00" fixdate="2010-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move RegexSerDe out of hive-contrib and over to hive-serde</summary>
      <description>RegexSerDe is as much a part of the standard Hive distribution as the other SerDescurrently in hive-serde. I think we should move it over to the hive-serde module so thatusers don't have to go to the added effort of manually registering the contrib jar beforeusing it.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">contrib.src.test.queries.clientpositive.serde.regex.q</file>
      <file type="M">contrib.src.test.queries.clientnegative.serde.regex.q</file>
    </fixedFiles>
  </bug>
  <bug id="1731" opendate="2010-10-19 00:00:00" fixdate="2010-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve miscellaneous error messages</summary>
      <description>This is a place for accumulating error message improvements so that we can update a bunch in batch.</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.function.param2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.dot.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.insert.wrong.number.columns.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.in.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.not.bool.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.function.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.index.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby.key.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.nospec.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.noof.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sample.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17352" opendate="2017-8-17 00:00:00" fixdate="2017-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveSever2 error with "Illegal Operation state transition from CLOSED to FINISHED"</summary>
      <description>HiveSever2 error with "Illegal Operation state transition from CLOSED to FINISHED"Many cases like CANCELED, TIMEDOUT AND CLOSED are handled. Need to handle FINISHED in runQuery() method.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="17364" opendate="2017-8-21 00:00:00" fixdate="2017-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test to "alter view" replication</summary>
      <description>Adding a unit test to HIVE-17354 change.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17381" opendate="2017-8-24 00:00:00" fixdate="2017-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When we enable Parquet Writer Version V2, hive throws an exception: Unsupported encoding: DELTA_BYTE_ARRAY.</summary>
      <description>when we set "hive.vectorized.execution.enabled=true" and "parquet.writer.version=v2" simultaneously, hive throws the following exception:Caused by: java.io.IOException: java.io.IOException: java.lang.UnsupportedOperationException: Unsupported encoding: DELTA_BYTE_ARRAY at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:232) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:142) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:254) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:208) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39) at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:83) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47) at org.apache.spark.scheduler.Task.run(Task.scala:86) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: java.lang.UnsupportedOperationException: Unsupported encoding: DELTA_BYTE_ARRAY at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:365) at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:167) at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:52) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:229) ... 16 more</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17619" opendate="2017-9-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude avatica-core.jar dependency from avatica shaded jar</summary>
      <description>avatica.jar is included in the project but this jar has a dependency on avatica-core.jar and it's pulled into the project as well. If avatica-core.jar is included in the classpath in front of avatica.jar, then hive could run into missing class which is shaded inside avatica.jar.</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17625" opendate="2017-9-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication: update hive.repl.partitions.dump.parallelism to 100</summary>
      <description>Set hive.repl.partitions.dump.parallelism=100</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17661" opendate="2017-9-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBTxnManager.acquireLocks() - MM tables should use shared lock for Insert</summary>
      <description>case INSERT: assert t != null; if(AcidUtils.isFullAcidTable(t)) { compBuilder.setShared(); } else { if (conf.getBoolVar(HiveConf.ConfVars.HIVE_TXN_STRICT_LOCKING_MODE)) {if(AcidUtils.isFullAcidTable(t)) { should probably be if(AcidUtils.isAcidTable(t)) {</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.mm.truncate.cols.q</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.convert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17664" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description></description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17679" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>http-generic-click-jacking for WebHcat server</summary>
      <description>The web UIs do not include the "X-Frame-Options" header to prevent the pages from being framed from another site.Reference:https://www.owasp.org/index.php/Clickjackinghttps://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheethttps://developer.mozilla.org/en-US/docs/Web/HTTP/X-Frame-Options</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="177" opendate="2008-12-15 00:00:00" fixdate="2008-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow specifying custom input and output format in CREATE TABLE</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.createTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17701" opendate="2017-10-5 00:00:00" fixdate="2017-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added restriction to historic queries on web UI</summary>
      <description>The HiveServer2 Web UI (HIVE-12550) shows recently completed queries. However, a user can see the queries run by other users as well, and that is a security/privacy concern.Only admin users should be allowed to see queries from other users (similar to behavior of display for configs, stack trace etc).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17702" opendate="2017-10-5 00:00:00" fixdate="2017-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect isRepeating handling in decimal reader in ORC</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17765" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>expose Hive keywords</summary>
      <description>This could be useful e.g. for BI tools (via ODBC/JDBC drivers) to decide on SQL capabilities of Hive</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.GetInfoType.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TGetInfoType.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="17832" opendate="2017-10-18 00:00:00" fixdate="2017-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow hive.metastore.disallow.incompatible.col.type.changes to be changed in metastore</summary>
      <description>hive.metastore.disallow.incompatible.col.type.changes when set to true, will disallow incompatible column type changes through alter table. But, this parameter is not modifiable in HMS. If HMS in not embedded into HS2, the value cannot be changed.</description>
      <version>2.1.1</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18043" opendate="2017-11-10 00:00:00" fixdate="2017-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Support List type in MapWork</summary>
      <description>Support Complex Types in vectorization is finished in HIVE-16589, but List type is still not support in MapWork. It should be supported to improve the performance when vectorization is enable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.complex.join.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18207" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the test failure for TestCliDriver#vector_complex_join</summary>
      <description>The test result miss the info about bigTableKeyExpressions &amp; bigTableValueExpressions</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.join.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18208" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB Join : Fix the unit tests to run SMB Joins.</summary>
      <description>Most of the SMB Join tests are actually not creating SMB Joins. Need them to test the intended join.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.quotedid.smb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.cache.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18210" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create resource plan allows duplicates</summary>
      <description>Create resource plan allows duplicates. This was seen in a cluster:+----------+-----------+--------------------+| rp_name | status | query_parallelism |+----------+-----------+--------------------+| plan_2 | ACTIVE | 10 || plan_2 | DISABLED | NULL |+----------+-----------+--------------------+</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18212" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure Yetus check always has a full log</summary>
      <description>Some yetus log files are left incomplete, and in these same runs tee subprocesses are left running and dangling on the ptest server.This is because of a bug in the yetus runner velocity template script where we make a redirection of stdout:./dev-support/test-patch.sh ${patchFile} ..... 2&gt;&amp;1 | tee ${logFile}If the yetus output is big enough (&gt;62K) tee will stop writing the log file and is left running even after test-patch.sh finished successfully. This because we don't make anything consume the stdout and most probably some buffers get full on Linux side.We should also make sure that yetus runs(since they are executed parallel to ptest test phase) are not interfering with each other in case they run very long and overlap.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.YetusPhase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18237" opendate="2017-12-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>missing results for insert_only table after DP insert</summary>
      <description>set hive.stats.column.autogather=false;set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.max.dynamic.partitions.pernode=200;set hive.exec.max.dynamic.partitions=200;set hive.support.concurrency=true;set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;create table i0 (p int,v int);insert into i0 values (0,0), (2,2), (3,3);create table p0 (v int) partitioned by (p int) stored as orc tblproperties ("transactional"="true", "transactional_properties"="insert_only");explain insert overwrite table p0 partition (p) select * from i0 where v &lt; 3;insert overwrite table p0 partition (p) select * from i0 where v &lt; 3;select count(*) from p0 where v!=1;The table p0 should contain 2 rows at this point; but the result is 0. seems to be specific to insert_only tables the existing data appears if an insert into is executed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="1824" opendate="2010-12-2 00:00:00" fixdate="2010-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create a new ZooKeeper instance when retrying lock, and more info for debug</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18273" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LLAP-level counters for WM</summary>
      <description>On query fragment level (like IO counters)time queued as guaranteed;time running as guaranteed;time running as speculative.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18274" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add AM level metrics for WM</summary>
      <description>Unused guaranteed tasks (1 metric); guaranteed/speculative tasks x updated/update in progress (4 metrics).It should be possible to view those over time as the query is (was) running, to detect any anomalies. This jira is just to save the correct metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18275" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HS2-level WM metrics</summary>
      <description>E.g. time spent in pool queue. Some existing UIs use perflogger output, so we should also include that.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.QueryExecutionBreakdownSummary.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="18306" opendate="2017-12-19 00:00:00" fixdate="2017-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spark smb tests</summary>
      <description>seems to me that TestSparkCliDriver#testCliDriver[auto_sortmerge_join_10] and TestSparkCliDriver#testCliDriver[bucketsortoptimize_insert_7] is failing since HIVE-18208 is in.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18326" opendate="2017-12-21 00:00:00" fixdate="2017-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Tez scheduler - only preempt tasks if there&amp;#39;s a dependency between them</summary>
      <description>It is currently possible for e.g. two sides of a union (or a join for that matter) to have slightly different priorities. We don't want to preempt running tasks on one side in favor of the other side in such cases.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1833" opendate="2010-12-6 00:00:00" fixdate="2010-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task-cleanup task should be disabled</summary>
      <description>Currently when task fails, a cleanup attempt will be scheduled right after that.This is unnecessary and increase the latency. MapReduce will allow disabling this (see MAPREDUCE-2206).After that patch is committed, we should set the JobConf in HIVE to disable cleanup task.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
    </fixedFiles>
  </bug>
  <bug id="18331" opendate="2017-12-22 00:00:00" fixdate="2017-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renew the Kerberos ticket used by Druid Query runner</summary>
      <description>Druid Http Client has to renew the current user Kerberos ticket when it is close to expire.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.ResponseCookieHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.KerberosHttpClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="18390" opendate="2018-1-6 00:00:00" fixdate="2018-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IndexOutOfBoundsException when query a partitioned view in ColumnPruner</summary>
      <description>IndexOutOfBoundsException is encountered when query a partitioned view.in Column Prunning, each SEL operator collects the accessed column in current SEL operator,When ColumnPrunerSelectProc getting a view's columns accessed, it will first get the index of output column names in the view, then call Table.getCols().get(index).getName() to finally get the name of output column, but Table.getCols() will not return all columns (partitioned column islacked), so if partitioned columns is queried, an IndexOutOfBoundsException will throw.REPRODUCE: create table foo(`a` string) partitioned by (`b` string);create view bar partitioned on (b) asselect a,b from foo;select * from bar; --IndexOutOfBoundsExceptionOPERATORE TREE:TS[0] |SEL[1] |SEL[2] |FS[3]SEL&amp;#91;1&amp;#93; collects accessed column(contains partitioned column b), b's internal column name is '_col1', the corresponding column index is 1, but actually bar's getCols() returned a list of length 1: &amp;#91;&amp;#39;a&amp;#39;&amp;#93;, so tab.getCols().get(1) throw tab.getCols().get(index)HOW TO FIX:instead of call view's getCols() method, we should get all columns including partitioned columns</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18421" opendate="2018-1-10 00:00:00" fixdate="2018-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized execution handles overflows in a different manner than non-vectorized execution</summary>
      <description>In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.Simple test case to reproduce this issueset hive.vectorized.execution.enabled=true;create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;insert into parquettable values (-104, 25), (-112, 24), (54, 9);select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) &lt; 50 order by diff desc;+-------+-----+-------+| t1 | t2 | diff |+-------+-----+-------+| -104 | 25 | 127 || -112 | 24 | 120 || 54 | 9 | 45 |+-------+-----+-------+When vectorization is turned off the same query produces only one row.</description>
      <version>2.1.1,2.2.0,2.3.2,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorTestCode.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestUnaryMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.java</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestClass.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryMinus.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumn.txt</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedArithmeticBench.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18443" opendate="2018-1-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure git gc finished in ptest prep phase before copying repo</summary>
      <description>In ptest's prep phase script first we checkout the latest Hive code from git, and then we make copy of its contents (along .git folder) for that will serve as Yetus' working directory.In some cases we can see errors such as+ cp -R . ../yetuscp: cannot stat ?./.git/gc.pid?: No such file or directorye.g. hereThis is caused by git running its gc feature in the background when our prep script has already started copying. In cases where gc finishes while cp is running, we'll get this error</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug id="18536" opendate="2018-1-25 00:00:00" fixdate="2018-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IOW + DP is broken for insert-only ACID</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18583" opendate="2018-1-30 00:00:00" fixdate="2018-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable DateRangeRules</summary>
      <description>Enable DateRangeRules to translate druid filters to date ranges.Need calcite version to upgrade to 0.16.0 before merging this.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveDruidRules.java</file>
    </fixedFiles>
  </bug>
  <bug id="18586" opendate="2018-1-30 00:00:00" fixdate="2018-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Derby to 10.14.1.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
    </fixedFiles>
  </bug>
  <bug id="18587" opendate="2018-1-30 00:00:00" fixdate="2018-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert DML event may attempt to calculate a checksum on directories</summary>
      <description>Looks like in union case, some code path may pass directories in newFiles. Probably legacy copyData/moveData; both seem to assume that these paths are files, but do not actually enforce it.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="18859" opendate="2018-3-5 00:00:00" fixdate="2018-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect handling of thrift metastore exceptions</summary>
      <description>Currently any run time exception thrown in thrift metastore during the following operations is not getting sent to hive execution engine. grant/revoke role grant/revoke privileges create roleThis is because ThriftHiveMetastore just handles MetaException and throws TException during the processing of these requests. So, the command just fails at thrift metastore end when there is run time exception (Exception can be seen in metastore log) but the hive execution engine will keep on waiting for the response from thrift metatstore.Steps to reproduce this problem :Launch thrift metastoreLaunch hive cli by passing --hiveconf hive.metastore.uris=thrift://127.0.0.1:10000 (pass the thrift metatstore host and port)Execute the following commands: set role admin create role test; (succeeds) create role test; ( hive version 2.1.1 : command is stuck, waiting for the response from thrift metastore; hive version 1.2.1: command fails with exception as null)I have uploaded the patch which has the fix in which I am handling the checked exceptions in MetaException and throwing unchecked exceptions using TException which fixes the problem. Please review and suggest if there is a better way of handling this issue.</description>
      <version>1.2.0,2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19083" opendate="2018-3-30 00:00:00" fixdate="2018-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19451" opendate="2018-5-8 00:00:00" fixdate="2018-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Query Execution fails with ClassNotFoundException org.antlr.v4.runtime.CharStream</summary>
      <description>Stack trace - ERROR : Status: FailedERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1524814504173_1344_45_00, diagnostics=[Task failed, taskId=task_1524814504173_1344_45_00_000029, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1524814504173_1344_45_00_000029_0:java.lang.RuntimeException: java.io.IOException: org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn`, problem: org/antlr/v4/runtime/CharStream at [Source: (String)"{"queryType":"scan","dataSource":{"type":"table","name":"tpcds_real_bin_partitioned_orc_1000.tpcds_denormalized_druid_table_7mcd"},"intervals":{"type":"segments","segments":[{"itvl":"1998-11-30T00:00:00.000Z/1998-12-01T00:00:00.000Z","ver":"2018-05-03T11:35:22.230Z","part":0}]},"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"resultFormat":"compactedList","batchSize":20480,"limit":9223372036854775807,"filter":{"type":"bound","dimension":"i_brand"[truncated 241 chars]; line: 1, column: 376] (through reference chain: org.apache.hive.druid.io.druid.query.scan.ScanQuery["virtualColumns"]-&gt;java.util.ArrayList[0]) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn`, problem: org/antlr/v4/runtime/CharStream at [Source: (String)"{"queryType":"scan","dataSource":{"type":"table","name":"tpcds_real_bin_partitioned_orc_1000.tpcds_denormalized_druid_table_7mcd"},"intervals":{"type":"segments","segments":[{"itvl":"1998-11-30T00:00:00.000Z/1998-12-01T00:00:00.000Z","ver":"2018-05-03T11:35:22.230Z","part":0}]},"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"resultFormat":"compactedList","batchSize":20480,"limit":9223372036854775807,"filter":{"type":"bound","dimension":"i_brand"[truncated 241 chars]; line: 1, column: 376] (through reference chain: org.apache.hive.druid.io.druid.query.scan.ScanQuery["virtualColumns"]-&gt;java.util.ArrayList[0]) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:438) at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157) at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83) at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703) at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662) at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150) at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266) ... 15 moreCaused by: org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn`, problem: org/antlr/v4/runtime/CharStream at [Source: (String)"{"queryType":"scan","dataSource":{"type":"table","name":"tpcds_real_bin_partitioned_orc_1000.tpcds_denormalized_druid_table_7mcd"},"intervals":{"type":"segments","segments":[{"itvl":"1998-11-30T00:00:00.000Z/1998-12-01T00:00:00.000Z","ver":"2018-05-03T11:35:22.230Z","part":0}]},"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"resultFormat":"compactedList","batchSize":20480,"limit":9223372036854775807,"filter":{"type":"bound","dimension":"i_brand"[truncated 241 chars]; line: 1, column: 376] (through reference chain: org.apache.hive.druid.io.druid.query.scan.ScanQuery["virtualColumns"]-&gt;java.util.ArrayList[0]) at org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67) at org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext.instantiationException(DeserializationContext.java:1601) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.wrapAsJsonMappingException(StdValueInstantiator.java:484) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.rewrapCtorProblem(StdValueInstantiator.java:503) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:285) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.ValueInstantiator.createFromObjectWith(ValueInstantiator.java:229) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:195) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:488) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:288) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromArray(BeanDeserializerBase.java:1428) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:185) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:529) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:528) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:417) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68) at org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4001) at org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2992) at org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.initialize(DruidQueryRecordReader.java:104) at org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.initialize(DruidQueryRecordReader.java:123) at org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.getRecordReader(DruidQueryBasedInputFormat.java:297) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:435) ... 24 moreCaused by: java.lang.NoClassDefFoundError: org/antlr/v4/runtime/CharStream at org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn.&lt;init&gt;(ExpressionVirtualColumn.java:60) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hive.druid.com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:124) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:283) ... 57 moreCaused by: java.lang.ClassNotFoundException: org.antlr.v4.runtime.CharStream at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 64 more</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20069" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix reoptimization in case of DPP and Semijoin optimization</summary>
      <description>reported by t3rmin4t0rIn case dynamic partition pruning; the operator statistics became partial; to only reflect the actually scanned partitions; but they are being used as an information about the "full" table...which leads to the exchange of the 2 tables being joined...which is really unfortunate...</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.OperatorStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSources.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20331" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query with union all, lateral view and Join fails with "cannot find parent in the child operator"</summary>
      <description>The following query with Union, Lateral view and Join will fail during execution with the exception below.create table t1(col1 int);SELECT 1 AS `col1`FROM t1UNION ALL SELECT 2 AS `col1` FROM (SELECT col1 FROM t1 ) x1 JOIN (SELECT col1 FROM (SELECT Row_Number() over (PARTITION BY col1 ORDER BY col1) AS `col1` FROM t1 ) x2 lateral VIEW explode(map(10,1))`mapObj` AS `col2`, `col3` ) `expdObj` Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive internal error: cannot find parent in the child operator! at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:362) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.MapOperator.initializeMapOperator(MapOperator.java:509) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:116) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]After debugging, seems we have issues in GenMRFileSink1 class in which we are setting incorrect aliasToWork to the MapWork.</description>
      <version>2.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="20335" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for materialized view rewriting with composite aggregation functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20336" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Masking and filtering policies for materialized views</summary>
      <description>Implement masking and filtering policies for materialized views.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="20443" opendate="2018-8-22 00:00:00" fixdate="2018-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>txn stats cleanup in compaction txn handler is unneeded</summary>
      <description>This is handled via write ID being invalid for the stats.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20527" opendate="2018-9-10 00:00:00" fixdate="2018-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intern table descriptors from spark task</summary>
      <description>Table descriptors from MR tasks and Tez tasks are interned. This fix is to intern table desc from spark tasks as well.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21009" opendate="2018-12-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP - Specify binddn for ldap-search</summary>
      <description>When user accounts cannot do an LDAP search, there is currently no way of specifying a custom binddn to use for the ldap-search.So I'm missing something like that:hive.server2.authentication.ldap.bindn=cn=ldapuser,ou=user,dc=examplehive.server2.authentication.ldap.bindnpw=password</description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21082" opendate="2019-1-3 00:00:00" fixdate="2019-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In HPL/SQL, declare statement does not support variable of type character</summary>
      <description>In the following HPL/SQL programs:DECLARE a character(5); SET a = 'b';when the type of variable 'a' is CHARACTER, it cannot be assigned a value successfully. The support for the character type should be added to DECLARE statement.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="21113" opendate="2019-1-10 00:00:00" fixdate="2019-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For HPL/SQL that contains boolean expression with NOT, incorrect SQL may be generated.</summary>
      <description>In HPL/SQL, ' SELECT * FROM a WHERE NOT (1 = 2) ' will generate to incorrect SQL ' SELECT * FROM a WHERE (1 = 2) ', the 'NOT' in boolean expression is missing.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.out.txt</file>
      <file type="M">hplsql.src.test.queries.offline.select.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
    </fixedFiles>
  </bug>
  <bug id="21739" opendate="2019-5-16 00:00:00" fixdate="2019-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make metastore DB backward compatible with pre-catalog versions of hive.</summary>
      <description>Since the addition of foreign key constraint between Database ('DBS') table and catalogs ('CTLGS') table inHIVE-18755 we are unable to run a simple create database command with an older version of Metastore Server. This is due to older versions having JDO schema as per older schema of 'DBS' which did not have an additional 'CTLG_NAME' column.The error is as follows:org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Exception thrown flushing changes to datastore)....java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails ("metastore_1238"."DBS", CONSTRAINT "CTLG_FK1" FOREIGN KEY ("CTLG_NAME") REFERENCES "CTLGS" ("NAME"))</description>
      <version>1.2.0,2.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.2.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.2.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.2.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="22606" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AvroSerde logs avro.schema.literal under INFO level</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="22645" opendate="2019-12-13 00:00:00" fixdate="2019-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jline can break bash terminal behavior</summary>
      <description>After HIVE-21954, running a qtest can break bash terminal (but not zsh), in a way that, e.g. enter key doesn't break line...As identified by kgyrtkirk, this line could cause this: https://github.com/apache/hive/commit/d645d827d95de36175194407bd1e2f6725362aff#diff-c64cf2b501f04e6710bbc3fcd079156fR84</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
    </fixedFiles>
  </bug>
  <bug id="22647" opendate="2019-12-13 00:00:00" fixdate="2019-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable session pool by default</summary>
      <description>Non pooled session may leak when the client doesn't close the connection.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22661" opendate="2019-12-18 00:00:00" fixdate="2019-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction fails on non bucketed table with data loaded inpath</summary>
      <description>Compaction cannot handle situations where: data was ingested with LOAD DATA INPATH this ingest method is run multiple times, and with different number of files getting created in the delta directories Therefore, for file/dir structures such as:/warehouse/tablespace/managed/hive/comp3/delta_0000001_0000001_0000/warehouse/tablespace/managed/hive/comp3/delta_0000001_0000001_0000/000000_0/warehouse/tablespace/managed/hive/comp3/delta_0000001_0000001_0000/000001_0/warehouse/tablespace/managed/hive/comp3/delta_0000002_0000002_0000/warehouse/tablespace/managed/hive/comp3/delta_0000002_0000002_0000/000000_0/warehouse/tablespace/managed/hive/comp3/delta_0000002_0000002_0000/000001_0/warehouse/tablespace/managed/hive/comp3/delta_0000002_0000002_0000/000002_0 Although the table is not bucketed, bucket is calculated from the (raw) files' names. Compaction in the above case will fail on delta1-1 not having data for 'bucket' 2.Steps to repro using small dataset:set tez.grouping.min-size=8;set tez.grouping.max-size=8;set mapreduce.input.fileinputformat.split.minsize=8;set mapreduce.input.fileinputformat.split.minsize=8;create external table comp0 (a string);insert into comp0 values ("qwertyuiopasdfghjklzxcvbnm");insert into comp0 values ("qwertyuiopasdfghjklzxcvbnm");create external table comp1 stored as orc as select * from comp0;insert into comp0 values ("qwertyuiopasdfghjklzxcvbnm");create external table comp2 stored as orc as select * from comp0;create table comp3 (a string);load data inpath '/warehouse/tablespace/external/hive/comp1' into table comp3;load data inpath '/warehouse/tablespace/external/hive/comp2' into table comp3;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23083" opendate="2020-3-26 00:00:00" fixdate="2020-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable fast serialization in xprod edge</summary>
      <description>select count(*) from store_sales, store, customer, customer_address where ss_store_sk = s_store_sk and s_market_id=10 and ss_customer_sk = c_customer_sk and c_birth_country &lt;&gt; upper(ca_country);This uses "org/apache/hadoop/io/serializer/WritableSerialization" instead of TezBytesWritableSerialization.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23084" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement kill query in multiple HS2 environment</summary>
      <description>KILL &lt;queryId|querytag&gt; command was implemented in: https://issues.apache.org/jira/browse/HIVE-17483 https://issues.apache.org/jira/browse/HIVE-20549But it is not working in an environment where service discovery is enabled and more than one HS2 instance is running (except for manually sending the kill query to all HS2 instance).Solution: If a HS2 instance can't kill a query locally, it should post a kill query request to the Zookeeper Every HS2 should watch the Zookeeper for kill query requests and if its running on that instance kill it Authorization of kill query should work the same</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.ZooKeeperHiveHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.kill.KillQueriesOperation.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestMiniHS2StateWithNoZookeeper.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23109" opendate="2020-3-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query-based compaction omits database</summary>
      <description>E.g. MM major compaction query looks like:insert into tmp_table select * from src_table;it should beinsert into tmp_table select * from src_db.src_table;Therefore compaction fails if db of source table isn't default.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactionQueryBuilder.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestMmCompactorOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorOnTezTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="23118" opendate="2020-4-1 00:00:00" fixdate="2020-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Option for exposing compile time counters as tez counters</summary>
      <description>TezCounters currently are runtime only. Some compile time information from optimizer can be exposed as counters which can then be used by workload management to make runtime decisions.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23119" opendate="2020-4-1 00:00:00" fixdate="2020-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test sort_acid should be run by TestMiniLlapLocalCliDriver only</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23120" opendate="2020-4-1 00:00:00" fixdate="2020-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey related tests should be run by TestMiniLlapLocalCliDriver only</summary>
      <description>TopNKey optimization is only used when the execution framework is Tez.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.windowing.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.grouping.sets.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.grouping.sets.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.topnkey.grouping.sets.order.q</file>
      <file type="M">ql.src.test.queries.clientpositive.topnkey.grouping.sets.functions.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23492" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary FileSystem#exists calls from ql module</summary>
      <description>Wherever there is an exists() call before open() or delete(), remove it and infer from the FileNotFoundException raised in open/delete that the file does not exist. Exists() just checks for a FileNotFoundException so it's a waste of time, especially on higher-latency stores</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FixAcidKeyIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="23494" opendate="2020-5-18 00:00:00" fixdate="2020-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache parent POM to version 23</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23516" opendate="2020-5-20 00:00:00" fixdate="2020-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store hive replication policy execution metrics in the relational DB</summary>
      <description>Details documented in the attached doc</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.AtlasDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.AtlasDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.AtlasLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.AtlasLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplStateLogTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplStateLogWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestAtlasDumpTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestAtlasLoadTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerDumpTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="23519" opendate="2020-5-20 00:00:00" fixdate="2020-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Read Ranger Configs from Classpath</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23520" opendate="2020-5-20 00:00:00" fixdate="2020-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL: repl dump could add support for immutable dataset</summary>
      <description>Currently, "REPL DUMP" ends up copying entire dataset along with partition information, stats etc in its dump folder. However, there are cases (e.g large reference datasets), where we need a way to just retain metadata along with partition information &amp; stats.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23536" opendate="2020-5-22 00:00:00" fixdate="2020-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to skip stats generation for major compaction</summary>
      <description>Currently major MR compaction is regenerates stats every time if the column stats table contains some data. Some configurations do not use stats but because of historical reasons the column stats table can still contain some data.We should provide a possibility to skip stats generation in these cases.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2354" opendate="2011-8-6 00:00:00" fixdate="2011-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support automatic rebuilding of indexes when they go stale</summary>
      <description>Support a mode where indexes will be automatically rebuilt when the table/partition they are based on is modified. So if index foo is built on table bar, and bar has it's contents overwritten, we should support a mode where index foo will automatically rebuild itself.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23614" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Always pass HiveConfig to removeTempOrDuplicateFiles</summary>
      <description>As part of HIVE-23354, we check the provided HiveConf for speculative execution and throw an error if it is enabled. There is one path that did not previously provide a HiveConf value which shows up in test failures inruntime_skewjoin_mapjoin_spark.q,skewjoin.q andskewjoin_onesideskew.q (which we do not run as part of pre-commit tests) or at least not through TestCliDriver.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2695" opendate="2012-1-5 00:00:00" fixdate="2012-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add PRINTF() Udf</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
