<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="16225" opendate="2017-3-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in webhcat service (FileSystem CACHE entries)</summary>
      <description>This is a known beast. here are detailsThe problem seems to be similar to the one discussed in HIVE-13749. If we submit very large number of jobs like 1000 to 2000 then we can see increase in Configuration objects count.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19837" opendate="2018-6-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting to have different default location for external tables</summary>
      <description>Allow external tables to have a different default location than managed tables</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.exim.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19838" opendate="2018-6-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>simplify &amp; fix ColumnizedDeleteEventRegistry load loop</summary>
      <description>Apparently sometimes the delete count in ACID stats doesn't match what merger actually returns.It could be due to some deltas having duplicate deletes from parallel queries (I guess?) that are being squashed by the merger or some other reasons beyond my mortal comprehension.The loop assumes the merger will return the exact number of records, so it fails with array index exception. Also, it could actually be done in a single loop.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="20829" opendate="2018-10-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JdbcStorageHandler range split throws NPE</summary>
      <description>2018-10-29T06:37:14,982 ERROR [HiveServer2-Background-Pool: Thread-44466]: operation.Operation (:()) - Error running hive query:org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:228) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:318) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_161] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_161] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.0.3.0-150.jar:?] at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:338) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:240) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:210) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2707) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2378) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2054) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1752) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1746) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] ... 13 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputSplit.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="20964" opendate="2018-11-22 00:00:00" fixdate="2018-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a test that checks the level of the parallel compilation</summary>
      <description>Create a test which verifies that the hive.driver.parallel.compilation.global.limit is honored and creates a hard limit for the number of parallel compilations</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestCompileLock.java</file>
    </fixedFiles>
  </bug>
  <bug id="21063" opendate="2018-12-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support statistics in cachedStore for transactional table</summary>
      <description>Currently statistics for transactional table is not stored in cached store for consistency issues. Need to add validation for valid write ids and generation of aggregate stats based on valid partitions. </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.UpdateTableColumnStatMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.UpdatePartitionColumnStatMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageBuilder.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONUpdateTableColumnStatMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONUpdatePartitionColumnStatMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.UpdateTableColumnStatEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.UpdatePartitionColumnStatEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestCachedStoreUpdateUsingEvents.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="21065" opendate="2018-12-23 00:00:00" fixdate="2018-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to use ORC 1.5.4</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21113" opendate="2019-1-10 00:00:00" fixdate="2019-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For HPL/SQL that contains boolean expression with NOT, incorrect SQL may be generated.</summary>
      <description>In HPL/SQL, ' SELECT * FROM a WHERE NOT (1 = 2) ' will generate to incorrect SQL ' SELECT * FROM a WHERE (1 = 2) ', the 'NOT' in boolean expression is missing.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.out.txt</file>
      <file type="M">hplsql.src.test.queries.offline.select.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
    </fixedFiles>
  </bug>
  <bug id="21279" opendate="2019-2-15 00:00:00" fixdate="2019-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid moving/rename operation in FileSink op for SELECT queries</summary>
      <description>Currently at the end of a job FileSink operator moves/rename temp directory to another directory from which FetchTask fetches result. This is done to avoid fetching potential partial/invalid files by failed/runway tasks. This operation is expensive for cloud storage. It could be avoided if FetchTask is passed on set of files to read from instead of whole directory.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
    </fixedFiles>
  </bug>
  <bug id="2151" opendate="2011-5-6 00:00:00" fixdate="2011-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too many open files in running negative cli tests</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21510" opendate="2019-3-26 00:00:00" fixdate="2019-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: add support for and/or for (constant,column) cases</summary>
      <description>After HIVE-21001 some selectExpressions will start using VectorUDFAdaptor for "null and x" expressions. Because right now there are 2-3 places from which rewrite will be done to the form of "null and/or x" form; it would be better to support it.[...]selectExpressions: VectorUDFAdaptor((null and dt1 is null))[...]usesVectorUDFAdaptor: true[...]</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarCompareColumn.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21511" opendate="2019-3-26 00:00:00" fixdate="2019-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline -f report no such file if file is not on local fs</summary>
      <description>I test like thisHQL=hdfs://hacluster/tmp/ff.hqlif hadoop fs -test -f ${HQL}then       beeline -f ${HQL}fitest ${HQL} ok, but beeline report ${HQL} no such file or directory</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="21512" opendate="2019-3-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jms-api to 2.0.2</summary>
      <description>I've noticed that for some time that sometimes there are issues with javax.jms:jms:1.1 artifact - because it doesn't seem to be available from maven central for some reason;https://issues.sonatype.org/browse/MVNCENTRAL-4708Alternatively; I think we might try to just upgrade to 2.0.2 version of the jms-api.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2159" opendate="2011-5-11 00:00:00" fixdate="2011-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableSample(percent ) uses one intermediate size to be int, which overflows for large sampled size, making the sampling never triggered.</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="21874" opendate="2019-6-14 00:00:00" fixdate="2019-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement add partitions related methods on temporary table</summary>
      <description>IMetaStoreClient exposes the following add partition related methods:Partition add_partition(Partition partition);int add_partitions(List&lt;Partition&gt; partitions);int add_partitions_pspec(PartitionSpecProxy partitionSpec);List&lt;Partition&gt; add_partitions(List&lt;Partition&gt; partitions, boolean ifNotExists, boolean needResults);These methods should be implemented in order to handle addition of partitions to temporary tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitionsFromPartSpec.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21875" opendate="2019-6-14 00:00:00" fixdate="2019-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement drop partition related methods on temporary tables</summary>
      <description>IMetaStoreClient exposes the following methods related to dropping partitions:boolean dropPartition(String db_name, String tbl_name, List&lt;String&gt; part_vals, boolean deleteData);boolean dropPartition(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, boolean deleteData);boolean dropPartition(String db_name, String tbl_name, List&lt;String&gt; part_vals, PartitionDropOptions options);boolean dropPartition(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, PartitionDropOptions options);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists, boolean needResults);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists, boolean needResults);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, PartitionDropOptions options);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, PartitionDropOptions options);boolean dropPartition(String db_name, String tbl_name, String name, boolean deleteData);boolean dropPartition(String catName, String db_name, String tbl_name, String name, boolean deleteData)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2191" opendate="2011-6-3 00:00:00" fixdate="2011-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow optional [inner] on equi-join.</summary>
      <description>Lot's of databases including mysql support an optional "inner" keyword to explicitely select an equi-join.As shown in the mysql docs: http://dev.mysql.com/doc/refman/5.1/en/join.htmlFor completeness/portability we should allow this.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">docs.xdocs.language.manual.joins.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21911" opendate="2019-6-21 00:00:00" fixdate="2019-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pluggable LlapMetricsListener on Tez side to disable / resize Daemons</summary>
      <description>We need to have a way to plug in different listeners which act upon the LlapDaemon statistics.This listener should be able to disable / resize the LlapDaemons based on health data.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestLlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21912" opendate="2019-6-21 00:00:00" fixdate="2019-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement BlacklistingLlapMetricsListener</summary>
      <description>We should implement a DaemonStatisticsHandler which: If a node average response time is bigger than 150% (configurable) of the other nodes If the other nodes has enough empty executors to handle the requestsThen disables the limping node.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestLlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapMetricsCollector.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21971" opendate="2019-7-9 00:00:00" fixdate="2019-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 leaks classloader due to `ReflectionUtils::CONSTRUCTOR_CACHE` with temporary functions + GenericUDF</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-10329 helped in moving away from hadoop's ReflectionUtils constructor cache issue (https://issues.apache.org/jira/browse/HADOOP-10513).However, there are corner cases where hadoop's ReflectionUtils is in use and this causes gradual build up of memory in HS2.I have observed this in Hive 2.3. But the codepath in master for this has not changed much.Easiest way to repro would be to add a temp function which extends GenericUDF. In FunctionRegistry::cloneGenericUDF, this would end up using org.apache.hadoop.util.ReflectionUtils.newInstance which in turn lands up in COSNTRUCTOR_CACHE of ReflectionUtils. CREATE TEMPORARY FUNCTION dummy AS 'com.hive.test.DummyGenericUDF' USING JAR 'file:///home/test/udf/dummy.jar';select dummy(); at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.cloneGenericUDF(FunctionRegistry.java:1353) at org.apache.hadoop.hive.ql.exec.FunctionInfo.getGenericUDF(FunctionInfo.java:122) at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:983) at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1359) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)Note: Reflection based invocation of hadoop's ReflectionUtils::clear was removed in 2.x.</description>
      <version>2.3.4</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="22236" opendate="2019-9-24 00:00:00" fixdate="2019-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail to create View selecting View containing NOT IN subquery</summary>
      <description>Given a complicated view with a select statement that has subquery containing "NOT IN" Hive fails to create a simple view as SELECT * FROM complicated_view (with CBO disabled).The unparse replacements of the complicated view will be applied to the text of the simple view, resulting in IllegalArgumentException: replace: range invalid exceptions from org.antlr.runtime.TokenRewriteStream.replace.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22238" opendate="2019-9-24 00:00:00" fixdate="2019-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PK/FK selectivity estimation underscales estimations</summary>
      <description>at this point the parent operators rownum is scaled according to pkfkselectivityhowever pkfkselectivity is computed on a whole subtree.Scaling it by that amount will count in estimation already used when parentstats was calculated...so depending on the number of upstream joins - this may lead to severe underestimationswhat happened was: optimization was able to push the filter to the other side of the join as a result the incoming data was already filtered scaling down by the PK selectiviy - was actually already there...but a new "scaling" happened</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.stat.estimate.drill.q</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="2224" opendate="2011-6-15 00:00:00" fixdate="2011-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to add partitions atomically</summary>
      <description>I'd like to see an atomic version of the add_partitions() call.Whether this is to be done by config to affect add_partitions() behaviour (not my preference) or just changing add_partitions() default behaviour (my preference, but likely to affect current behaviour, so will need others' input) or by making a new add_partitions_atomic() call depends on discussion.This looks relatively doable to implement (will need a dependent add_partition_core to not do a ms.commit_partition() early, and to cache list of directories created to remove on rollback, and a list of AddPartitionEvent to trigger in one shot later)Thoughts? This also seems like something to implement for allowing HIVE-1805.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="22241" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement UDF to interpret date/timestamp using its internal representation and Gregorian-Julian hybrid calendar</summary>
      <description>UDF that converts a date/timestamp to new proleptic Gregorian calendar (ISO 8601 standard), which is produced by extending the Gregorian calendar backward to dates preceding its official introduction in 1582, assuming that its internal days/milliseconds since epoch is calculated using legacy Gregorian-Julian hybrid calendar, i.e., calendar that supports both the Julian and Gregorian calendar systems with the support of a single discontinuity, which corresponds by default to the Gregorian date when the Gregorian calendar was instituted.</description>
      <version>None</version>
      <fixedVersion>3.1.3,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="22242" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move TempTable and PartitionTree out of SessionHiveMetastoreClient</summary>
      <description>SessionHiveMetastoreClient is getting too crowded and unreadable. TempTable and PartitionTree should moved into a separate class.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22243" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align Apache Thrift version to 0.9.3-1 in standalone-metastore as well</summary>
      <description>Thrift was bumped to 0.9.3-1 in HIVE-21173, but standalone-metastore was left out of this.Thanks for pointing this out Ashutosh Bapat!</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22244" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added default ACLs for znodes on a non-kerberized cluster</summary>
      <description>Set default ACLs for znodes on a non-kerberized cluster: Create/Read/Delete/Write/Admin to the world</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22245" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make qtest feature parser reuseable</summary>
      <description>right now we have a parser for --! qt:dataset ; to enable further addition of things (I would like to run scheduled query service for some qtests )</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.ql.dataset.TestDatasetParser.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.QTestDatasetHandler.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.DatasetParser.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.DatasetCollection.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="23392" opendate="2020-5-7 00:00:00" fixdate="2020-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore upgrade script TXN_LOCK_TBL rename inconsistency</summary>
      <description>HIVE-23048 introduced a bug in the metastore upgrade scripts, by not renaming correctly the columns in TXN_LOCK_TBL</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="23393" opendate="2020-5-7 00:00:00" fixdate="2020-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLapInputFormat reader policy for Random IO formats</summary>
      <description>Extension of HIVE-23158 for LLAPInputFormat</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="2340" opendate="2011-8-3 00:00:00" fixdate="2011-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize orderby followed by a groupby</summary>
      <description>Before implementing optimizer for JOIN-GBY, try to implement RS-GBY optimizer(cluster-by following group-by).</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.reduce.deduplicate.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.distinct.samekey.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join26.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23404" opendate="2020-5-7 00:00:00" fixdate="2020-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schedules in the past should be accepted</summary>
      <description>Right now if a schedule in the past is set ; an Exception is thrown.However this behaviour can be argued...consider that at some point a schedule is created to run in say every day in 2020's december(but not after it). When the creation happens it's "okay"...but in case we reach 2021 it will be considered invalid..because there is no future execution</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.schq.TestScheduledQueryService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
