<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10129" opendate="2015-3-28 00:00:00" fixdate="2015-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fix ordering of execution modes</summary>
      <description>uber &gt; llap &gt; container execution modes. Fix the ordering in in-place update UI.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="10233" opendate="2015-4-7 00:00:00" fixdate="2015-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: memory manager for grace hash join</summary>
      <description>We need a memory manager in llap/tez to manage the usage of memory across threads.</description>
      <version>llap,2.0.0</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10499" opendate="2015-4-27 00:00:00" fixdate="2015-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure Session/ZooKeeperClient instances are closed</summary>
      <description>Some Session/ZooKeeperClient instances are not closed in some scenario. We need to make sure they are always closed.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="10651" opendate="2015-5-7 00:00:00" fixdate="2015-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC file footer cache should be bounded</summary>
      <description>ORC's file footer cache is currently unbounded and is a soft reference cache. The cache size got from config is used to set initial capacity. We should bound the cache from growing too big and to get a predictable performance.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10652" opendate="2015-5-7 00:00:00" fixdate="2015-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: AM task communication retry is too long</summary>
      <description>Mentioned by Siddharth Seth while discussing HIVE-10648. 45sec (or whatever) is a bit too long.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.configuration.LlapConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="10707" opendate="2015-5-14 00:00:00" fixdate="2015-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: debug logging OOMs</summary>
      <description>hive&gt; source xcross.sql;OKTime taken: 0.837 secondsException in thread "main" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3332) at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137) at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421) at java.lang.StringBuilder.append(StringBuilder.java:136) at org.apache.hadoop.hive.ql.parse.ASTNode.dump(ASTNode.java:111) at org.apache.hadoop.hive.ql.parse.ASTNode.dump(ASTNode.java:119) at org.apache.hadoop.hive.ql.parse.ASTNode.dump(ASTNode.java:119) at org.apache.hadoop.hive.ql.parse.ASTNode.dump(ASTNode.java:119) at org.apache.hadoop.hive.ql.parse.ASTNode.dump(ASTNode.java:119)The query contains 360 join clauses, wrapped in a UNION ALL.Looks like genOpTree does this.ctx.setCboInfo("Plan optimized by CBO."); this.ctx.setCboSucceeded(true); LOG.debug(newAST.dump()); }the debug logging OOMs.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="10709" opendate="2015-5-14 00:00:00" fixdate="2015-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Avro version to 1.7.7</summary>
      <description>We should update the avro version to 1.7.7 to consumer some of the nicer compatibility features.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10969" opendate="2015-6-8 00:00:00" fixdate="2015-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test autogen_colalias failing on trunk</summary>
      <description>Seems like HIVE-10728 didnt have right golden file updates.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="10974" opendate="2015-6-9 00:00:00" fixdate="2015-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Configuration::getRaw() for the Base64 data</summary>
      <description>Inspired by the Twitter HadoopSummit talk if (HiveConf.getBoolVar(conf, ConfVars.HIVE_RPC_QUERY_PLAN)) { LOG.debug("Loading plan from string: "+path.toUri().getPath()); String planString = conf.get(path.toUri().getPath());Use getRaw() in other places where Base64 data is present.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="10975" opendate="2015-6-10 00:00:00" fixdate="2015-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet: Bump the parquet version up to 1.8.1</summary>
      <description>There are lots of changes since parquet's graduation.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10999" opendate="2015-6-13 00:00:00" fixdate="2015-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spark dependency to 1.4 [Spark Branch]</summary>
      <description>Spark 1.4.0 is release. Let's update the dependency version from 1.3.1 to 1.4.0.</description>
      <version>None</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11023" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable directSQL if datanucleus.identifierFactory = datanucleus2</summary>
      <description>We hit an interesting bug in a case where datanucleus.identifierFactory = datanucleus2 .The problem is that directSql handgenerates SQL strings assuming "datanucleus1" naming scheme. If a user has their metastore JDO managed by datanucleus.identifierFactory = datanucleus2 , the SQL strings we generate are incorrect.One simple example of what this results in is the following: whenever DN persists a field which is held as a List&lt;T&gt;, it winds up storing each T as a separate line in the appropriate mapping table, and has a column called INTEGER_IDX, which holds the position in the list. Then, upon reading, it automatically reads all relevant lines with an ORDER BY INTEGER_IDX, which results in the list retaining its order. In DN2 naming scheme, the column is called IDX, instead of INTEGER_IDX. If the user has run appropriate metatool upgrade scripts, it is highly likely that they have both columns, INTEGER_IDX and IDX.Whenever they use JDO, such as with all writes, it will then use the IDX field, and when they do any sort of optimized reads, such as through directSQL, it will ORDER BY INTEGER_IDX.An immediate danger is seen when we consider that the schema of a table is stored as a List&lt;FieldSchema&gt; , and while IDX has 0,1,2,3,... , INTEGER_IDX will contain 0,0,0,0,... and thus, any attempt to describe the table or fetch schema for the table can come up mixed up in the table's native hashing order, rather than sorted by the index.This can then result in schema ordering being different from the actual table. For eg:, if a user has a (a:int,b:string,c:string), a describe on this may return (c:string, a:int, b: string), and thus, queries which are inserting after selecting from another table can have ClassCastExceptions when trying to insert data in the wong order - this is how we discovered this bug. This problem, however, can be far worse, if there are no type problems - it is possible, for eg., that if a,b&amp;c were all strings, that that insert query would succeed but mix up the order, which then results in user table data being mixed up. This has the potential to be very bad.We should write a tool to help convert metastores that use "datanucleus2" to "datanucleus1"(more difficult, needs more one-time testing) or change directSql to support both(easier to code, but increases test-coverage matrix significantly and we should really then be testing against both schemes). But in the short term, we should disable directSql if we see that the identifierfactory is "datanucleus2"</description>
      <version>1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="11026" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make vector_outer_join* test more robust</summary>
      <description>Different file sizes on different OSes result in different Data Size in explain output.</description>
      <version>None</version>
      <fixedVersion>1.2.1,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.outer.join3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.outer.join2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.outer.join1.q</file>
    </fixedFiles>
  </bug>
  <bug id="11031" opendate="2015-6-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC concatenation of old files can fail while merging column statistics</summary>
      <description>Column statistics in ORC are optional protobuf fields. Old ORC files might not have statistics for newly added types like decimal, date, timestamp etc. But column statistics merging assumes column statistics exists for these types and invokes merge. For example, merging of TimestampColumnStatistics directly casts the received ColumnStatistics object without doing instanceof check. If the ORC file contains time stamp column statistics then this will work else it will throw ClassCastException.Also, the file merge operator swallows the exception.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11035" opendate="2015-6-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD: Orc Split elimination fails because filterColumns=[-1]</summary>
      <description>create temporary table xx (x int) stored as orc ;insert into xx values (20),(200);set hive.fetch.task.conversion=none;select * from xx where x is null;This should generate zero tasks after optional split elimination in the app master, instead of generating the 1 task which for sure hits the row-index filters and removes all rows anyway.Right now, this runs 1 task for the stripe containing (min=20, max=200, has_null=false), which is broken.Instead, it returns YES_NO_NULL from the following default casehttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java#L976</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1104" opendate="2010-1-26 00:00:00" fixdate="2010-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Suppress Checkstyle warnings for generated files</summary>
      <description>Suppress Checkstyle warnings for generated files.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">checkstyle.checkstyle.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11040" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Derby dependency version to 10.10.2.0</summary>
      <description>We don't see this on the Apache pre-commit tests because it uses PTest, but running the entire TestCliDriver suite results in failures in some of the partition-related qtests (partition_coltype_literals, partition_date, partition_date2). I've only really seen this on Linux (I was using CentOS).HIVE-8879 changed the Derby dependency version from 10.10.1.1 to 10.11.1.1. Testing with 10.10.1.1 or 10.20.2.0 seems to allow the partition related tests to pass. I'd like to change the dependency version to 10.20.2.0, since that version should also contain the fix for HIVE-8879.</description>
      <version>None</version>
      <fixedVersion>1.2.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11043" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC split strategies should adapt based on number of files</summary>
      <description>ORC split strategies added in HIVE-10114 chose strategies based on average file size. It would be beneficial to choose a different strategy based on number of files as well.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11051" opendate="2015-6-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 1.2.0 MapJoin w/Tez - LazyBinaryArray cannot be cast to [Ljava.lang.Object;</summary>
      <description>I tried to apply: HIVE-10729 which did not solve the issue.The following exception is thrown on a Tez MapJoin with Hive 1.2.0 and Tez 0.5.4/0.5.3Status: Running (Executing on YARN cluster with App id application_1434641270368_1038)-------------------------------------------------------------------------------- VERTICES STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED--------------------------------------------------------------------------------Map 1 .......... SUCCEEDED 3 3 0 0 0 0Map 2 ... FAILED 3 1 0 2 7 0--------------------------------------------------------------------------------VERTICES: 01/02 [=================&gt;&gt;---------] 66% ELAPSED TIME: 7.39 s --------------------------------------------------------------------------------Status: FailedVertex failed, vertexName=Map 2, vertexId=vertex_1434641270368_1038_2_01, diagnostics=[Task failed, taskId=task_1434641270368_1038_2_01_000002, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"cnctevn_id":"002245282386","svcrqst_id":"0000003627217285","svcrqst_crt_dts":"2015-04-23 11:54:39.238357","subject_seq_no":1,"plan_component":"HMOM1 ","cust_segment":"RM ","cnctyp_cd":"001","cnctmd_cd":"D02","cnctevs_cd":"007","svcrtyp_cd":"335","svrstyp_cd":"088","cmpltyp_cd":" ","catsrsn_cd":" ","apealvl_cd":" ","cnstnty_cd":"001","svcrqst_asrqst_ind":"Y","svcrqst_rtnorig_in":"N","svcrqst_vwasof_dt":"null","sum_reason_cd":"98","sum_reason":"Exclude","crsr_master_claim_index":null,"svcrqct_cds":[" "],"svcrqst_lupdt":"2015-04-23 22:14:01.288132","crsr_lupdt":null,"cntevsds_lupdt":"2015-04-23 11:54:40.740061","ignore_me":1,"notes":null} at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"cnctevn_id":"002245282386","svcrqst_id":"0000003627217285","svcrqst_crt_dts":"2015-04-23 11:54:39.238357","subject_seq_no":1,"plan_component":"HMOM1 ","cust_segment":"RM ","cnctyp_cd":"001","cnctmd_cd":"D02","cnctevs_cd":"007","svcrtyp_cd":"335","svrstyp_cd":"088","cmpltyp_cd":" ","catsrsn_cd":" ","apealvl_cd":" ","cnstnty_cd":"001","svcrqst_asrqst_ind":"Y","svcrqst_rtnorig_in":"N","svcrqst_vwasof_dt":"null","sum_reason_cd":"98","sum_reason":"Exclude","crsr_master_claim_index":null,"svcrqct_cds":[" "],"svcrqst_lupdt":"2015-04-23 22:14:01.288132","crsr_lupdt":null,"cntevsds_lupdt":"2015-04-23 11:54:40.740061","ignore_me":1,"notes":null} at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:290) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148) ... 13 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"cnctevn_id":"002245282386","svcrqst_id":"0000003627217285","svcrqst_crt_dts":"2015-04-23 11:54:39.238357","subject_seq_no":1,"plan_component":"HMOM1 ","cust_segment":"RM ","cnctyp_cd":"001","cnctmd_cd":"D02","cnctevs_cd":"007","svcrtyp_cd":"335","svrstyp_cd":"088","cmpltyp_cd":" ","catsrsn_cd":" ","apealvl_cd":" ","cnstnty_cd":"001","svcrqst_asrqst_ind":"Y","svcrqst_rtnorig_in":"N","svcrqst_vwasof_dt":"null","sum_reason_cd":"98","sum_reason":"Exclude","crsr_master_claim_index":null,"svcrqct_cds":[" "],"svcrqst_lupdt":"2015-04-23 22:14:01.288132","crsr_lupdt":null,"cntevsds_lupdt":"2015-04-23 11:54:40.740061","ignore_me":1,"notes":null} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:518) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83) ... 16 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray cannot be cast to [Ljava.lang.Object; at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:426) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837) at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:122) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:97) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:162) at org.apache.hadoop.hive.ql.exec.MapTo Recreate the issue:import table service_request_clean from '/user/user/service_request_clean';import table ct_events_clean from '/user/user/ct_events_clean';drop table ct_events1_test;create table ct_events1_testas select a.*,b.svcrqst_id,b.svcrqct_cds,b.svcrtyp_cd,b.cmpltyp_cd,b.sum_reason_cd as src,b.cnctmd_cd,b.notesfrom ct_events_clean ainner joinservice_request_clean bon a.contact_event_id = b.cnctevn_id;</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="11103" opendate="2015-6-24 00:00:00" fixdate="2015-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add banker&amp;#39;s rounding BROUND UDF</summary>
      <description>Banker's rounding: the value is rounded to the nearest even number. Also known as "Gaussian rounding", and, in German, "mathematische Rundung".Example 2 digits 2 digitsUnrounded "Standard" rounding "Gaussian" rounding 54.1754 54.18 54.18 343.2050 343.21 343.20+106.2038 +106.20 +106.20 ========= ======= ======= 503.5842 503.59 503.58</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.RoundUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="11118" opendate="2015-6-26 00:00:00" fixdate="2015-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data query should validate file formats with destination tables</summary>
      <description>Load data local inpath queries does not do any validation wrt file format. If the destination table is ORC and if we try to load files that are not ORC, the load will succeed but querying such tables will result in runtime exceptions. We can do some simple sanity checks to prevent loading of files that does not match the destination table file format.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="11119" opendate="2015-6-26 00:00:00" fixdate="2015-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spark reduce vectorization doesnt account for scratch columns</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="1112" opendate="2010-1-27 00:00:00" fixdate="2010-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace instances of StringBuffer/Vector with StringBuilder/ArrayList</summary>
      <description>When possible replace instances of StringBuffer and Vector with their non-synchronized counterparts StringBuilder and ArrayList.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.TokenMgrError.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.ParseException.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldList.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.java</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWIAuth.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWIServer.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionManager.java</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RowSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDAFEvaluatorResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.GraphWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.Node.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.InputSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.UDFTestLength.java</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11120" opendate="2015-6-26 00:00:00" fixdate="2015-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generic interface for file format validation</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-11118?focusedCommentId=14602302&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14602302We need generic interfaces for verify if a specified file is of valid format so that load data statement can make some sanity check before copying the file to destination.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.txt.seq.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.rc.seq.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.orc.negative.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.orc.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.orc.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.orc.negative1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.archive.corrupt.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11122" opendate="2015-6-26 00:00:00" fixdate="2015-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC should not record the timezone information when there are no timestamp columns</summary>
      <description>Currently ORC records the time zone information in the stripe footer even when there are no timestamp columns. This will not only add to the size of the footer but also can cause inconsistencies (file size difference) in test cases when run under different time zones.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.static.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.json</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="11139" opendate="2015-6-29 00:00:00" fixdate="2015-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit more lineage information</summary>
      <description>HIVE-1131 emits some column lineage info. But it doesn't support INSERT statements, or CTAS statements. It doesn't emit the predicate information either.We can enhance and use the dependency information created in HIVE-1131, generate more complete lineage info.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.date.trim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.date.trim.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.Generator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.LineageState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.change.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.dictionary.threshold.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11147" opendate="2015-6-30 00:00:00" fixdate="2015-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MetaTool doesn&amp;#39;t update FS root location for partitions with space in name</summary>
      <description>Problem happens when trying to update the FS root location:# HIVE_CONF_DIR=/etc/hive/conf.server/ hive --service metatool -dryRun -updateLocation hdfs://mycluster hdfs://c6401.ambari.apache.org:8020...Looking for LOCATION_URI field in DBS table to update..Dry Run of updateLocation on table DBS..old location: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse new location: hdfs://mycluster/apps/hive/warehouseFound 1 records in DBS table to updateLooking for LOCATION field in SDS table to update..Dry Run of updateLocation on table SDS..old location: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/web_sales/ws_web_site_sk=12 new location: hdfs://mycluster/apps/hive/warehouse/web_sales/ws_web_site_sk=12old location: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/web_sales/ws_web_site_sk=13 new location: hdfs://mycluster/apps/hive/warehouse/web_sales/ws_web_site_sk=13...Found 143 records in SDS table to updateWarning: Found records with bad LOCATION in SDS table..bad location URI: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/customer_demographics/cd_education_status=Advanced Degreebad location URI: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/customer_demographics/cd_education_status=Advanced Degreebad location URI: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/customer_demographics/cd_education_status=4 yr Degreebad location URI: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/customer_demographics/cd_education_status=4 yr Degreebad location URI: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/customer_demographics/cd_education_status=2 yr Degreebad location URI: hdfs://c6401.ambari.apache.org:8020/apps/hive/warehouse/customer_demographics/cd_education_status=2 yr DegreeThe reason why some entries are marked as bad location is that they have space character in the partition name.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="11158" opendate="2015-7-1 00:00:00" fixdate="2015-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for HPL/SQL</summary>
      <description>The new HPL/SQL module does not have any tests.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11169" opendate="2015-7-2 00:00:00" fixdate="2015-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: more out file changes compared to master</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.8.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.llapdecider.q</file>
    </fixedFiles>
  </bug>
  <bug id="11171" opendate="2015-7-2 00:00:00" fixdate="2015-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join reordering algorithm might introduce projects between joins</summary>
      <description>Join reordering algorithm might introduce projects between joins which causes multijoin optimization in SemanticAnalyzer to not kick in.</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="11194" opendate="2015-7-7 00:00:00" fixdate="2015-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exchange partition on external tables should fail with error message when target folder already exists</summary>
      <description>Steps to repro:Create /data/a1/pkey=1 directory with some data in it.Create /data/a2/pkey=1 directory with some data in it.create external table a1 (value string) partitioned by (pkey int) location '/data/a1';create external table a2 (value string) partitioned by (pkey int) location '/data/a2';alter table a2 add partition (pkey=1);alter table a1 exchange partition (pkey=1) with table a2;select * from a1 should now fail.pkey=1 is not a partition of a1 but the folder exists. We should give an error message for that.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11197" opendate="2015-7-7 00:00:00" fixdate="2015-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>While extracting join conditions follow Hive rules for type conversion instead of Calcite</summary>
      <description>Calcite strict type system throws exception in those cases, which are legal in Hive.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
    </fixedFiles>
  </bug>
  <bug id="11198" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix load data query file format check for partitioned tables</summary>
      <description>HIVE-11118 added file format check for ORC format. The check will throw exception when non ORC formats is loaded to ORC managed table. But it does not work for partitioned table. Partitioned tables are allowed to have some partitions with different file format. See this discussion for more detailshttps://issues.apache.org/jira/browse/HIVE-11118?focusedCommentId=14617271&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14617271</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11203" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline force option doesn&amp;#39;t force execution when errors occurred in a script.</summary>
      <description>The force option doesn't function as wiki described. https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="11215" opendate="2015-7-9 00:00:00" fixdate="2015-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized grace hash-join throws FileUtil warnings</summary>
      <description>TPC-DS query13 warnings about a null-file deletion.2015-07-09 03:14:18,880 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows restored from KeyValueContainer: 311842015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Deserializing spilled hash partition...2015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows in hashmap: 311842015-07-09 03:14:18,897 INFO [TezChild] exec.MapJoinOperator: spilled: true abort: false. Clearing spilled partitions.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11223" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): MapJoin and SMBJoin conversion not triggered</summary>
      <description>Information in aux data structures is not complete, thus MapJoin and SMBJoin conversion are not triggered.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join0.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.join0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.auto.join1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.auto.join0.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11225" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running all Hive UTs or itests executes only small subset of tests</summary>
      <description>Trying to run all unit tests runs only a small sub-set of the tests:For example:mvn test -Phadoop-2only ran 272 tests.Others ran into similar issues when running the UTs from a particular package like ql.This is potentially related to HIVE-10941 where an additional option to skip the Spark UTs was added. When I take out &lt;exclude&gt;%regex&amp;#91;${skip.spark.files}&amp;#93;&lt;/exclude&gt;from the exclude list of tests, all tests are executed again. This is not a fix though, instead the spark-test profile should be fixed.I see the problem in both the 1.2 branch as well as master.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11228" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mutation API should use semi-shared locks.</summary>
      <description>An issue was identified by ekoifman in the streaming mutation API (HIVE-10165) where an insufficiently restrictive lock was being used when issuing updates and deletes to ACID tables and partitions. A shared lock was being used where in fact a semi-shared lock is required. Additionally, the current lock scope targets the entire table, whereas in theory if the table is partitioned, then only the affected partitions are required to participate in the semi-shared lock. However, there are a couple of technical challenges that prevent the locks currently being applied on a per-partition basis: It is expected that the affected partitions are not known in advance so individual partition locks would need to be acquired as needed. The API is expected to execute in a clustered environment and so acquiring these locks as on an as needed basis presents a risk that the meta store may become overwhelmed. This is expected to be less of an problem when an HBase based meta store is introduced (HIVE-9452). My understanding is that multiple fine grained lock acquisitions for a single transaction are not possible at present. When they are available theyll introduce the possibility that deadlocks can occur. This should be better handled when HIVE-9675 is complete.Therefore, as advised, at this time the system will obtain a semi-shared lock on participating tables. Although this will prevent other concurrent writes, it will preserve snapshot isolation when reading.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestLock.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.package.html</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
    </fixedFiles>
  </bug>
  <bug id="11229" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mutation API: Coordinator communication with meta store should be optional</summary>
      <description>ekoifman raised a theoretical issue with the streaming mutation API (HIVE-10165) where worker nodes operating in a distributed cluster might overwhelm a meta store while trying to obtain partition locks. Although this does not happen in practice (see HIVE-11228), the API does communicate with the meta store in this manner to obtain partition paths and create new partitions. Therefore the issue described does in fact exist in the current implementation, albeit in a different code path. Id like to make such communication optional like so: When the user chooses not to create partitions on demand, no meta store connection will be created in the MutationCoordinators. Additionally, partition paths will be resolved using org.apache.hadoop.hive.metastore.Warehouse.getPartitionPath(Path, LinkedHashMap&lt;String, String&gt;) which should be suitable so long as standard Hive partition layouts are followed. If the user does choose to create partitions on demand then the system will operate as is does currently; using the meta store to both issue add_partition events and look up partition meta data. The documentation will be updated to describe these behaviours and outline alternative approaches to collecting affected partition names and creating partitions in a less intensive manner.Side note for follow up: The parameter names tblName and dbName seem to be the wrong way around on the method org.apache.hadoop.hive.metastore.IMetaStoreClient.getPartition(String, String, List&lt;String&gt;).</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinatorBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.CreatePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.package.html</file>
    </fixedFiles>
  </bug>
  <bug id="11259" opendate="2015-7-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: clean up ORC dependencies part 1</summary>
      <description>Before there's storage handler module, we can clean some things up</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetadataReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.DebugUtils.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.NoopCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapDataBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionAwareAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.Cache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.orc.OrcCacheKey.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapIoProxy.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LlapMemoryBuffer.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.counters.LowLevelCacheCounters.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.Consumer.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.cache.Allocator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.DiskRangeList.java</file>
    </fixedFiles>
  </bug>
  <bug id="11262" opendate="2015-7-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip MapJoin processing if the join hash table is empty</summary>
      <description>Currently the map join processor processes all rows of the big table, even when the hash table is empty. If it is an inner join, we should be able to skip the join processing, since the result should be empty.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11284" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix cbo_rp_join0 failure on master</summary>
      <description>It first failed in this build : http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4621/</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="11306" opendate="2015-7-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a bloom-1 filter for Hybrid MapJoin spills</summary>
      <description>HIVE-9277 implemented Spillable joins for Tez, which suffers from a corner-case performance issue when joining wide small tables against a narrow big table (like a user info table join events stream).The fact that the wide table is spilled causes extra IO, even though the nDV of the join key might be in the thousands.A cheap bloom-1 filter would add a massive performance gain for such queries, massively cutting down on the spill IO costs for the big-table spills.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1131" opendate="2010-2-4 00:00:00" fixdate="2010-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add column lineage information to the pre execution hooks</summary>
      <description>We need a mechanism to pass the lineage information of the various columns of a table to a pre execution hook so that applications can use that for: auditing dependency checkingand many other applications.The proposal is to expose this through a bunch of classes to the pre execution hook interface to the clients and put in the necessary transformation logic in the optimizer to generate this information.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.insert2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.lazydecompress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insertexternal1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.columnarserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes4.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PreExecute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11310" opendate="2015-7-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid expensive AST tree conversion to String for expressions in WHERE clause</summary>
      <description>We use the AST tree String representation of a condition in the WHERE clause to identify its column in the RowResolver. This can lead to OOM Exceptions when the condition is very large.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="11311" opendate="2015-7-19 00:00:00" fixdate="2015-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid dumping AST tree String in Explain unless necessary</summary>
      <description>Currently, the AST tree String representation is created even if it is not used; we should dump it only if we are going to use it (explain extended).</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="11312" opendate="2015-7-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC format: where clause with CHAR data type not returning any rows</summary>
      <description>Test case:Setup: create table orc_test( col1 string, col2 char(10)) stored as orc tblproperties ("orc.compress"="NONE");insert into orc_test values ('val1', '1');Query:select * from orc_test where col2='1'; Query returns no row.Problem is introduced with HIVE-10286, class RecordReaderImpl.java, method evaluatePredicateRange.Old code: Object baseObj = predicate.getLiteral(PredicateLeaf.FileFormat.ORC); Object minValue = getConvertedStatsObj(min, baseObj); Object maxValue = getConvertedStatsObj(max, baseObj); Object predObj = getBaseObjectForComparison(baseObj, minValue);New code:+ Object baseObj = predicate.getLiteral();+ Object minValue = getBaseObjectForComparison(predicate.getType(), min);+ Object maxValue = getBaseObjectForComparison(predicate.getType(), max);+ Object predObj = getBaseObjectForComparison(predicate.getType(), baseObj);The values for min and max are of type String which contain as many characters as the CHAR column indicated. For example if the type is CHAR(10), and the row has value 1, the value of String min is "1 ";Before Hive 1.2, the method getConvertedStatsObj would call StringUtils.stripEnd(statsObj.toString(), null); which would remove the trailing spaces from min and max. Later in the compareToRange method, it was able to compare "1" with "1".In Hive 1.2 with the use getBaseObjectForComparison method, it simply returns obj.String if the data type is String, which means minValue and maxValue are still "1 ".As a result, the compareToRange method will return a wrong value ("1".compareTo("1 ") -9 instead of 0.</description>
      <version>1.2.0,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.parquet.ppd.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.char.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="11383" opendate="2015-7-27 00:00:00" fixdate="2015-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to Calcite 1.4</summary>
      <description>CLEAR LIBRARY CACHEUpgrade Hive to Calcite 1.4.0-incubating.There is currently a snapshot release, which is close to what will be in 1.4. I have checked that Hive compiles against the new snapshot, fixing one issue. The patch is attached.Next step is to validate that Hive runs against the new Calcite, and post any issues to the Calcite list or log Calcite Jira cases. jcamachorodriguez, can you please do that.pxiong, I gather you are dependent on CALCITE-814, which will be fixed in the new Calcite version.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11397" opendate="2015-7-29 00:00:00" fixdate="2015-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse Hive OR clauses as they are written into the AST</summary>
      <description>When parsing A OR B OR C, hive converts it into (C OR B) OR Ainstead of turning it intoA OR (B OR C) GenericUDFOPOr or = new GenericUDFOPOr(); List&lt;ExprNodeDesc&gt; expressions = new ArrayList&lt;ExprNodeDesc&gt;(2); expressions.add(previous); expressions.add(current);</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11398" opendate="2015-7-29 00:00:00" fixdate="2015-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse wide OR and wide AND trees to flat OR/AND trees</summary>
      <description>Deep trees of AND/OR are hard to traverse particularly when they are merely the same structure in nested form as a version of the operator that takes an arbitrary number of args.One potential way to convert the DFS searches into a simpler BFS search is to introduce a new Operator pair named ALL and ANY.ALL(A, B, C, D, E) represents AND(AND(AND(AND(E, D), C), B), A)ANY(A, B, C, D, E) represents OR(OR(OR(OR(E, D), C),B),A)The SemanticAnalyser would be responsible for generating these operators and this would mean that the depth and complexity of traversals for the simplest case of wide AND/OR trees would be trivial.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.deep.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11407" opendate="2015-7-30 00:00:00" fixdate="2015-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC DatabaseMetaData.getTables with large no of tables call leads to HS2 OOM</summary>
      <description>With around 7000 tables having around 1500 columns each, and 512MB of HS2 memory, I am able to reproduce this OOM .Most of the memory is consumed by the datanucleus objects. Reducing the number of tables for which metadata is fetched at a time via metastore get_multi_table limits the memory footprint.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="11424" opendate="2015-7-31 00:00:00" fixdate="2015-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rule to transform OR clauses into IN clauses in CBO</summary>
      <description>We create a rule that will transform OR clauses into IN clauses (when possible).</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.semijoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="11425" opendate="2015-7-31 00:00:00" fixdate="2015-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>submitting a query via CLI against a running cluster fails with ClassNotFoundException: org.apache.hadoop.hive.common.type.HiveDecimal</summary>
      <description>submitting a query via CLI against a running cluster fails. This is a side effect of the newstorage-api module which is not included hive-exec.jarhive&gt; insert into orders values(1,2);Query ID = ekoifman_20150730182807_a24eee8c-6f59-42dc-9713-ae722916c82eTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1438305627853_0002, Tracking URL = http://localhost:8088/proxy/application_1438305627853_0002/Kill Command = /Users/ekoifman/dev/hwxhadoop/hadoop-dist/target/hadoop-2.7.1-SNAPSHOT/bin/hadoop job -kill job_1438305627853_0002Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12015-07-30 18:28:16,330 Stage-1 map = 0%, reduce = 0%2015-07-30 18:28:33,929 Stage-1 map = 100%, reduce = 100%Ended Job = job_1438305627853_0002 with errorsError during job, obtaining debugging information...Job Tracking URL: http://localhost:8088/proxy/application_1438305627853_0002/Examining task ID: task_1438305627853_0002_m_000000 (and more) from job job_1438305627853_0002Task with the most failures(4): -----Task ID: task_1438305627853_0002_m_000000URL: http://localhost:8088/taskdetails.jsp?jobid=job_1438305627853_0002&amp;tipid=task_1438305627853_0002_m_000000-----Diagnostic Messages for this Task:Error: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 17 moreCaused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:140) ... 22 moreCaused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/common/type/HiveDecimal at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.&lt;clinit&gt;(PrimitiveObjectInspectorUtils.java:234) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:341) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:331) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:392) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:305) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(TypeInfoUtils.java:765) at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.extractColumnInfo(LazySerDeParameters.java:142) at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.&lt;init&gt;(LazySerDeParameters.java:85) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initialize(LazySimpleSerDe.java:125) at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:53) at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) at org.apache.hadoop.hive.ql.plan.PartitionDesc.getDeserializer(PartitionDesc.java:166) at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:302) at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:338) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:109) ... 22 moreCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.type.HiveDecimal at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 37 moreFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTaskMapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 HDFS Read: 0 HDFS Write: 0 FAILTotal MapReduce CPU Time Spent: 0 msec(2,FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask,08S01)</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11427" opendate="2015-7-31 00:00:00" fixdate="2015-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Location of temporary table for CREATE TABLE SELECT broken by HIVE-7079</summary>
      <description>If a user does not have HDFS write permissions to the default database, and attempts to create a table in a private database to which the user does have permissions using CREATE TABLE AS SELECT from a table in the default database, the following happens:use default;create table grisha.blahblah as select * from some_table;FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://nn.example.com/user/hive/warehouse. Error encountered near token 'TOK_TMP_FILEI've edited this issue because my initial explanation was completely bogus. A more likely explanation is in https://github.com/apache/hive/commit/1614314ef7bd0c3b8527ee32a434ababf7711278 - fname = ctx.getExternalTmpPath( + fname = ctx.getExtTmpPathRelTo( // and then something incorrect happens in getExtTmpPathRelTo()In any event - the bug is that the location chosen for the temporary storage is not in the same place as the target table. It should be same as the target table (/user/hive/warehouse/grisha.db in the above example) because this is where presumably the user running the query would have write permissions to.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="11428" opendate="2015-7-31 00:00:00" fixdate="2015-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance: Struct IN() clauses are extremely slow (~10x slower)</summary>
      <description>Hive today does not support tuple IN() clauses today, but provides a way to rewrite (a,b) IN (...) using complex types.select * from table where STRUCT(a,b) IN (STRUCT(1,2), STRUCT(2,3) ...);This would be fine, except it is massively slower due to ObjectConvertors and Struct constructor not being constant folded.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.cast.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11430" opendate="2015-7-31 00:00:00" fixdate="2015-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup HIVE-10166: investigate and fix the two test failures</summary>
      <description>org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_convert_enum_to_stringorg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_rdd_cacheAs show in https://issues.apache.org/jira/browse/HIVE-10166?focusedCommentId=14649066&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14649066.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11434" opendate="2015-8-1 00:00:00" fixdate="2015-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup for HIVE-10166: reuse existing configurations for prewarming Spark executors</summary>
      <description>It appears that the patch other than the latest from HIVE-11363 was committed.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11436" opendate="2015-8-2 00:00:00" fixdate="2015-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : dealing with empty char</summary>
      <description>BaseCharUtils checks whether the length of a char is in between &amp;#91;1,255&amp;#93;. This causes return path to throw error when the the length of a char is 0.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11437" opendate="2015-8-2 00:00:00" fixdate="2015-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : dealing with insert into</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="11442" opendate="2015-8-3 00:00:00" fixdate="2015-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove commons-configuration.jar from Hive distribution</summary>
      <description>Some customer report version conflicting for Hive bundled commons-configuration.jar. Actually commons-configuration.jar is not needed by Hive. It is a transitive dependency of Hadoop/Accumulo. User should be able to pick those jars from Hadoop at runtime.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11445" opendate="2015-8-3 00:00:00" fixdate="2015-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : groupby distinct does not work</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="11448" opendate="2015-8-4 00:00:00" fixdate="2015-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support vectorization of Multi-OR and Multi-AND</summary>
      <description>Support more than 2 children for OR and AND when all children are expressions.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.java</file>
    </fixedFiles>
  </bug>
  <bug id="11452" opendate="2015-8-4 00:00:00" fixdate="2015-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-11321 broke ORC bloom filter index creation</summary>
      <description>HIVE-11321 refactored orc table properties to OrcConf. The conversion of table properties to writer options skipped over orc.bloom.filter.column table property which broke bloom filter index creation.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="11464" opendate="2015-8-5 00:00:00" fixdate="2015-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>lineage info missing if there are multiple outputs</summary>
      <description>If there are multiple outputs, for example,from (select ...) tinsert into table t1 select * from tinsert into table t2 select * from t;The lineage info for table t2 is not emitted.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lineage3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="11466" opendate="2015-8-5 00:00:00" fixdate="2015-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-10166 generates more data on hive.log causing Jenkins to fill all the disk.</summary>
      <description>An issue with HIVE-10166 patch is increasing the size of hive.log and causing jenkins to fail because it does not have more space.Here's a test I run when running TestJdbcWithMiniHS2 before the patch, with the patch, and after other commits.BEFORE HIVE-1016613M Aug 5 11:57 ./hive-unit/target/tmp/log/hive.logWITH HIVE-101662.4G Aug 5 12:07 ./hive-unit/target/tmp/log/hive.logCURRENT HEAD3.2G Aug 5 12:36 ./hive-unit/target/tmp/log/hive.logThis is just a single test, but on Jenkins, hive.log is more than 13G of size.</description>
      <version>None</version>
      <fixedVersion>spark-branch,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="11468" opendate="2015-8-5 00:00:00" fixdate="2015-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorize: Struct IN() clauses</summary>
      <description>Improve performance by vectorizing Struct IN() clauses. Related to HIVE-11428.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java.rej</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.java</file>
    </fixedFiles>
  </bug>
  <bug id="11479" opendate="2015-8-6 00:00:00" fixdate="2015-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: ORC-related refactoring changes broke something</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="11480" opendate="2015-8-6 00:00:00" fixdate="2015-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): char/varchar as input to GenericUDAF</summary>
      <description>Some of the UDAF can not deal with char/varchar correctly when return path is on, for example udaf_number_format.q.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd.java</file>
    </fixedFiles>
  </bug>
  <bug id="11488" opendate="2015-8-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sessionId and queryId info to HS2 log</summary>
      <description>Session is critical for a multi-user system like Hive. Currently Hive doesn't log seessionId to the log file, which sometimes make debugging and analysis difficult when multiple activities are going on at the same time and the log from different sessions are mixed together.Currently, Hive already has the sessionId saved in SessionState and also there is another sessionId in SessionHandle (Seems not used and I'm still looking to understand it). Generally we should have one sessionId from the beginning in the client side and server side. Seems we have some work on that side first.The sessionId then can be added to log4j supported mapped diagnostic context (MDC) and can be configured to output to log file through the log4j property. MDC is per thread, so we need to add sessionId to the HS2 main thread and then it will be inherited by the child threads.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11489" opendate="2015-8-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jenkins PreCommit-HIVE-SPARK-Build fails with TestCliDriver.initializationError</summary>
      <description>The Jenkins job PreCommit-HIVE-SPARK-Build is failing due to many TestCliDriver.initializationError test results.Error MessageUnexpected exception java.io.FileNotFoundException: /data/hive-ptest/working/apache-git-source-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestCliDriverQFileNames.txt (No such file or directory) at java.io.FileInputStream.open(Native Method) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146) at java.io.FileReader.&lt;init&gt;(FileReader.java:72) at org.apache.hadoop.hive.ql.QTestUtil.addTestsToSuiteFromQfileNames(QTestUtil.java:2019) at org.apache.hadoop.hive.cli.TestCliDriver.suite(TestCliDriver.java:120) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.junit.internal.runners.SuiteMethod.testFromSuiteMethod(SuiteMethod.java:35) at org.junit.internal.runners.SuiteMethod.&lt;init&gt;(SuiteMethod.java:24) at org.junit.internal.builders.SuiteMethodBuilder.runnerForClass(SuiteMethodBuilder.java:11) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59) at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:26) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:262) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Stacktracejunit.framework.AssertionFailedError: Unexpected exception java.io.FileNotFoundException: /data/hive-ptest/working/apache-git-source-source/itests/qtest/target/generated-test-sources/java/org/apache/hadoop/hive/cli/TestCliDriverQFileNames.txt (No such file or directory) at java.io.FileInputStream.open(Native Method) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146) at java.io.FileReader.&lt;init&gt;(FileReader.java:72) at org.apache.hadoop.hive.ql.QTestUtil.addTestsToSuiteFromQfileNames(QTestUtil.java:2019) at org.apache.hadoop.hive.cli.TestCliDriver.suite(TestCliDriver.java:120) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.junit.internal.runners.SuiteMethod.testFromSuiteMethod(SuiteMethod.java:35) at org.junit.internal.runners.SuiteMethod.&lt;init&gt;(SuiteMethod.java:24) at org.junit.internal.builders.SuiteMethodBuilder.runnerForClass(SuiteMethodBuilder.java:11) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59) at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:26) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:262) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103) at junit.framework.Assert.fail(Assert.java:57) at org.apache.hadoop.hive.ql.QTestUtil.addTestsToSuiteFromQfileNames(QTestUtil.java:2045) at org.apache.hadoop.hive.cli.TestCliDriver.suite(TestCliDriver.java:120)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug id="11496" opendate="2015-8-6 00:00:00" fixdate="2015-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better tests for evaluating ORC predicate pushdown</summary>
      <description>There were many regressions recently wrt ORC predicate pushdown. We don't have system tests to capture these regressions. Currently there is only junit tests for testing ORC predicate pushdown feature. Since hive counters are not available during qfile test execution there is no easy way to verify if ORC PPD feature worked or not. This jira is add a post execution hook to print hive counters (esp. number of input records) to error stream so that it will appear in qfile test output. This way we can verify ORC SARG evaluation and avoid future regressions.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="11511" opendate="2015-8-10 00:00:00" fixdate="2015-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Output the message of orcfiledump when ORC files are not specified</summary>
      <description>When I execute the orcfiledump command without specifying a ORC file, any message is not output and return value is 0.[root@hive hive]# /usr/local/hive/bin/hive --orcfiledump[root@hive hive]# echo $?0For this behavior, I will be modified to output a error message.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
    </fixedFiles>
  </bug>
  <bug id="11525" opendate="2015-8-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning</summary>
      <description>Logically and functionally bucketing and partitioning are quite similar - both provide mechanism to segregate and separate the table's data based on its content. Thanks to that significant further optimisations like &amp;#91;partition&amp;#93; PRUNING or &amp;#91;bucket&amp;#93; MAP JOIN are possible.The difference seems to be imposed by design where the PARTITIONing is open/explicit while BUCKETing is discrete/implicit.Partitioning seems to be very common if not a standard feature in all current RDBMS while BUCKETING seems to be HIVE specific only.In a way BUCKETING could be also called by "hashing" or simply "IMPLICIT PARTITIONING".Regardless of the fact that these two are recognised as two separate features available in Hive there should be nothing to prevent leveraging same existing query/join optimisations across the two.BUCKET pruningEnable partition PRUNING equivalent optimisation for queries on BUCKETED tablesSimplest example is for queries like:"SELECT  FROM x WHERE colA=123123"to read only the relevant bucket file rather than all file-buckets that belong to a table.</description>
      <version>0.13.0,0.13.1,0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11526" opendate="2015-8-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  <bug id="11541" opendate="2015-8-12 00:00:00" fixdate="2015-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC: Split Strategy should depend on global file count, not per-partition</summary>
      <description>A partitioned table with 4000 partitions and 200 files each doesn't trigger the BISplitStrategy, because each partition qualifies as a Strategy future instead of using the atomic counter upfront.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11542" opendate="2015-8-13 00:00:00" fixdate="2015-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>port fileId support on shims and splits from llap branch</summary>
      <description>This is helpful for any kind of file-based cache.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11546" opendate="2015-8-13 00:00:00" fixdate="2015-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Projected columns read size should be scaled to split size for ORC Splits</summary>
      <description>HIVE-10114 added projected columns data size information to OrcSplit which Tez Split Grouper can take advantage off (HIVE-7428). The annotated data size was for entire file and was not scaled to the split size.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11557" opendate="2015-8-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Convert to flat AND/OR</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11563" opendate="2015-8-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perflogger loglines are repeated</summary>
      <description>After HIVE-11304, the perflogger log lines in qtests are repeated.2015-08-14T12:02:05,765 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(120)) - &lt;PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver&gt;2015-08-14T12:02:05,765 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(120)) - &lt;PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver&gt;2015-08-14T12:02:05,766 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(120)) - &lt;PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver&gt;2015-08-14T12:02:05,766 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(120)) - &lt;PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver&gt;2015-08-14T12:02:05,766 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(120)) - &lt;PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver&gt;2015-08-14T12:02:05,766 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(120)) - &lt;PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver&gt;</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11565" opendate="2015-8-14 00:00:00" fixdate="2015-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Some counters are incorrect</summary>
      <description>1) Tez counters for LLAP are incorrect.2) Some counters, such as cache hit ratio for a fragment, are not propagated.We need to make sure that Tez counters for LLAP are usable.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
    </fixedFiles>
  </bug>
  <bug id="11569" opendate="2015-8-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use PreOrderOnceWalker where feasible</summary>
      <description>Because of its early exit criteria it has better performance than PreOrderWalker</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="11572" opendate="2015-8-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datanucleus loads Log4j1.x Logger from AppClassLoader</summary>
      <description>As part of HIVE-11304, we moved from Log4j1.x to Log4j2. But DataNucleus log messages gets logged to console when launching the hive cli. The reason is DataNucleus is trying to load Log4j1.x Logger by traversing its class loader. Although we use log4j-1.2-api bridge we are loading log4j-1.2.16 jar that was pulled by ZooKeeper. We should make sure that there is no log4j-1.2.16 in datanucleus classloader hierarchy (classpath). DataNucleus logger has this NucleusLogger.class.getClassLoader().loadClass("org.apache.log4j.Logger"); loggerClass = org.datanucleus.util.Log4JLogger.class;</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="11579" opendate="2015-8-17 00:00:00" fixdate="2015-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invoke the set command will close standard error output[beeline-cli]</summary>
      <description>We can easily reproduce the debug by the following steps:hive&gt; set system:xx=yy;hive&gt; lss;hive&gt; The error output disappeared since the err outputstream is closed when closing the Hive statement.This bug occurred also in the upstream when using the embeded mode as the new CLI uses.</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="11591" opendate="2015-8-17 00:00:00" fixdate="2015-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade thrift to 0.9.3 and change generation to use undated annotations</summary>
      <description>Thrift has added class annotations to generated classes; these contain generation date. Because of this, all the Java thrift files change on every re-gen, even if you only make a small change that should not affect bazillion files. We should use undated annotations to avoid this problem.This depends on upgrading to Thrift 0.9.3, which doesn't exist yet.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.constants.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.thrift.hive.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.hive.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.hive.service.constants.rb</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.TCLIService.py</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.TCLIService-remote</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.constants.py</file>
      <file type="M">service.src.gen.thrift.gen-py.hive.service.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-py.hive.service.ThriftHive.py</file>
      <file type="M">service.src.gen.thrift.gen-py.hive.service.ThriftHive-remote</file>
      <file type="M">service.src.gen.thrift.gen-py.hive.service.constants.py</file>
      <file type="M">service.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service.src.gen.thrift.gen-php.ThriftHive.php</file>
      <file type="M">service.src.gen.thrift.gen-php.TCLIService.php</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TUnionTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeQualifierValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeQualifiers.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeId.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeDesc.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTableSchema.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStructTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStringValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStringColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStatusCode.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStatus.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TSessionHandle.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TRowSet.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TRow.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOperationType.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOperationState.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOperationHandle.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TMapTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TI64Value.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TI64Column.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TI32Value.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TI32Column.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TI16Value.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TI16Column.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.THandleIdentifier.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTypeInfoResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTypeInfoReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTableTypesResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTableTypesReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTablesResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTablesReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetSchemasResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetSchemasReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetOperationStatusResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetOperationStatusReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetInfoValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetInfoType.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetInfoResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetInfoReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetFunctionsResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetFunctionsReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetColumnsResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetColumnsReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetCatalogsResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetCatalogsReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TFetchResultsResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TFetchResultsReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TFetchOrientation.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TExecuteStatementResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TExecuteStatementReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TDoubleValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TDoubleColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TColumnValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TColumnDesc.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCloseSessionResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCloseSessionReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCloseOperationResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCloseOperationReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCLIService.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCancelOperationResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCancelOperationReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TByteValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TByteColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TBoolValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TBoolColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TBinaryColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TArrayTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.service.JobTrackerState.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.service.HiveServerException.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.service.HiveClusterStatus.java</file>
      <file type="M">service.src.gen.thrift.gen-cpp.ThriftHive.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.ThriftHive.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.constants.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.constants.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.hive.service.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.hive.service.types.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.hive.service.constants.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.hive.service.constants.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-rb.testthrift.types.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.testthrift.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.serde.types.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.serde.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.megastruct.types.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.megastruct.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.complex.types.rb</file>
      <file type="M">serde.src.gen.thrift.gen-rb.complex.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-py.testthrift.ttypes.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.testthrift.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.org.apache.hadoop.hive.serde.ttypes.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.org.apache.hadoop.hive.serde.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.megastruct.ttypes.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.megastruct.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.complex.ttypes.py</file>
      <file type="M">serde.src.gen.thrift.gen-py.complex.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">serde.src.gen.thrift.gen-php.org.apache.hadoop.hive.serde.Types.php</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.test.ThriftTestObj.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.test.InnerStruct.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.SetIntString.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.PropValueUnion.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.MyEnum.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.IntString.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.Complex.java</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.testthrift.types.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.testthrift.types.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.testthrift.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.testthrift.constants.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.types.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.types.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.megastruct.types.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.megastruct.types.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.megastruct.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.megastruct.constants.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.complex.types.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.complex.types.cpp</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.complex.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.complex.constants.cpp</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.constants.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.constants.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.TaskType.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.Task.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.Stage.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.QueryPlan.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.Query.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.Operator.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.NodeType.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.Graph.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.AdjacencyType.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.Adjacency.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.constants.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.constants.cpp</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Version.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.UnlockRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.UnknownTableException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.UnknownPartitionException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.UnknownDBException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Type.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TxnState.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TxnOpenException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TxnAbortedException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StringColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SerDeInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Role.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ResourceUri.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ResourceType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrivilegeBag.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrincipalType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionWithoutSD.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpecWithSharedSD.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpec.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionListComposingSpec.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionEventType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Order.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEvent.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NoSuchTxnException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NoSuchObjectException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NoSuchLockException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.MetaException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.MetadataPpdResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockState.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockLevel.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockComponent.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InvalidPartitionException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InvalidOperationException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InvalidObjectException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InvalidInputException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HiveObjectType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HiveObjectRef.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GrantRevokeType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AggrStats.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlreadyExistsException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CommitTxnRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Database.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Date.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DateColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Decimal.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EventRequestType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FieldSchema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FileMetadataExprType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequestData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FunctionType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="11593" opendate="2015-8-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add aes_encrypt and aes_decrypt UDFs</summary>
      <description>AES (Advanced Encryption Standard) algorithm.Oracle JRE supports AES-128 out of the boxAES-192 and AES-256 are supported if Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are installed</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFParamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="11594" opendate="2015-8-18 00:00:00" fixdate="2015-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze Table For Columns cannot handle columns with embedded spaces</summary>
      <description>create temporary table events(`user id` bigint, `user name` string);explain analyze table events compute statistics for columns `user id`;FAILED: SemanticException [Error 30009]: Encountered parse error while parsing rewritten query</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11595" opendate="2015-8-18 00:00:00" fixdate="2015-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor ORC footer reading to make it usable from outside</summary>
      <description>If ORC footer is read from cache, we want to parse it without having the reader, opening a file, etc. I thought it would be as simple as protobuf parseFrom bytes, but apparently there's bunch of stuff going on there. It needs to be accessible via something like parseFrom(ByteBuffer), or similar.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
    </fixedFiles>
  </bug>
  <bug id="11597" opendate="2015-8-18 00:00:00" fixdate="2015-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO new return path] Handling of strings of zero-length</summary>
      <description>Exposed by load_dyn_part14.q</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11609" opendate="2015-8-20 00:00:00" fixdate="2015-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Capability to add a filter to hbase scan via composite key doesn&amp;#39;t work</summary>
      <description>It seems like the capability to add filter to an hbase scan which was added as part of HIVE-6411 doesn't work. This is primarily because in the HiveHBaseInputFormat, the filter is added in the getsplits instead of getrecordreader. This works fine for start and stop keys but not for filter because a filter is respected only when an actual scan is performed. This is also related to the initial refactoring that was done as part of HIVE-3420.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key3.q.out</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory3.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseScanRange.java</file>
    </fixedFiles>
  </bug>
  <bug id="11627" opendate="2015-8-24 00:00:00" fixdate="2015-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the number of accesses to hashmaps in PPD</summary>
      <description>We retrieve the ExprInfo from the hashmap each time we want to change any of its properties. Instead, the number of calls to the hashmap could be drastically reduced by retrieving the ExprInfo once, and changing all its properties.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="11629" opendate="2015-8-24 00:00:00" fixdate="2015-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : fix the filter expressions for full outer join and right outer join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11646" opendate="2015-8-25 00:00:00" fixdate="2015-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): fix multiple window spec for PTF operator</summary>
      <description>Current return path only supports a single windowing spec. All the following window spec will overwrite the first one.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11654" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>After HIVE-10289, HBase metastore tests failing</summary>
      <description>After the latest merge from trunk a number of the HBase unit tests are failing.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.java</file>
    </fixedFiles>
  </bug>
  <bug id="11655" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean build on the branch appears to be broken</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
    </fixedFiles>
  </bug>
  <bug id="11658" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data file format validation does not work with directories</summary>
      <description>HIVE-11118 added file format validation to load statement for ORC tables. It does not work when the path is a directory.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.load.orc.part.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.load.orc.part.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11659" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Vectorization use the fast StringExpr everywhere</summary>
      <description>StringExpr::equals() provides a faster path than the simple ::compare() operator.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.java</file>
    </fixedFiles>
  </bug>
  <bug id="11668" opendate="2015-8-27 00:00:00" fixdate="2015-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make sure directsql calls pre-query init when needed</summary>
      <description>See comments in HIVE-11123</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="11684" opendate="2015-8-28 00:00:00" fixdate="2015-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement limit pushdown through outer join in CBO</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11699" opendate="2015-8-31 00:00:00" fixdate="2015-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support special characters in quoted table names</summary>
      <description>Right now table names can only be "&amp;#91;a-zA-z_0-9&amp;#93;+". This patch tries to investigate how much change there should be if we would like to support special characters, e.g., "/" in table names.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11700" opendate="2015-8-31 00:00:00" fixdate="2015-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>exception in logs in Tez test with new logger</summary>
      <description>2015-08-31 11:27:47,400 WARN Error while converting string [${sys:hive.ql.log.PerfLogger.level}] to type [class org.apache.logging.log4j.Level]. Using default value [null]. java.lang.IllegalArgumentException: Unknown level constant [${SYS:HIVE.QL.LOG.PERFLOGGER.LEVEL}]. at org.apache.logging.log4j.Level.valueOf(Level.java:286) at org.apache.logging.log4j.core.config.plugins.convert.TypeConverters$LevelConverter.convert(TypeConverters.java:230) at org.apache.logging.log4j.core.config.plugins.convert.TypeConverters$LevelConverter.convert(TypeConverters.java:226) at org.apache.logging.log4j.core.config.plugins.convert.TypeConverters.convert(TypeConverters.java:336) at org.apache.logging.log4j.core.config.plugins.visitors.AbstractPluginVisitor.convert(AbstractPluginVisitor.java:130) at org.apache.logging.log4j.core.config.plugins.visitors.PluginAttributeVisitor.visit(PluginAttributeVisitor.java:45) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.generateParameters(PluginBuilder.java:247) at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:136) at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:766) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:706) at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:698) at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:358) at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:161) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:361) at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:426) at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:442) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:138) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:147) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41) at org.apache.logging.log4j.LogManager.getContext(LogManager.java:175) at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:102) at org.apache.logging.log4j.jcl.LogAdapter.getContext(LogAdapter.java:39) at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42) at org.apache.logging.log4j.jcl.LogFactoryImpl.getInstance(LogFactoryImpl.java:40) at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:671) at org.apache.hadoop.hive.ql.QTestUtil.&lt;clinit&gt;(QTestUtil.java:122) at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.&lt;clinit&gt;(TestMiniTezCliDriver.java:33)</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11701" opendate="2015-8-31 00:00:00" fixdate="2015-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make tez tests AM logs work with new log4j2 changes</summary>
      <description>MiniTezCliDriver should log AM logs to syslog file. With new log4j2 changes this file is not created anymore.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11706" opendate="2015-9-1 00:00:00" fixdate="2015-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement "show create database"</summary>
      <description>HIVE-967 introduced "show create table". How about "show create database"?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11710" opendate="2015-9-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline embedded mode doesn&amp;#39;t output query progress after setting any session property</summary>
      <description>Connect to beeline embedded mode beeline -u jdbc:hive2://. Then set anything in the session like set aa=true;.After that, any query like select count from src; will only output result but no query progress.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="11711" opendate="2015-9-1 00:00:00" fixdate="2015-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge hbase-metastore branch to trunk</summary>
      <description>Major development of hbase-metastore is done and it's time to merge the branch back into master.Currently hbase-metastore is only invoked when running TestMiniTezCliDriver. The instruction for setting up hbase-metastore is captured in https://cwiki.apache.org/confluence/display/Hive/HBaseMetastoreDevelopmentGuide.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11723" opendate="2015-9-3 00:00:00" fixdate="2015-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect string literal escaping</summary>
      <description>When I execute the following queriesCREATE TABLE t_hive (f1 STRING);INSERT INTO t_hive VALUES ('Cooper\'s');SELECT * FROM t_hive;via the Hive shell or through HiveServer2 directly (via impyla), I would expect that the result to beCooper'sbut instead I actually getCooper\'sActually, I'm not sure how that INSERT query is not even a syntax error.</description>
      <version>1.1.1,1.2.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11726" opendate="2015-9-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pushed IN predicates to the metastore</summary>
      <description>The PointLookupOptimizer can turn off some of the optimizations due to its use of tuple IN() clauses.HIVE-11573 introduced the extraction of sub-clauses that could be pushed down till the TableScan operators, though they wouldn't be pushed down to the metastore.In this issue, we tackle this problem by extending the filter parser of the metastore to support IN clauses, including multiple columns. This allows to push those additional predicates down throw directSQL to the metastore.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.Filter.g</file>
    </fixedFiles>
  </bug>
  <bug id="11727" opendate="2015-9-3 00:00:00" fixdate="2015-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Tez through Oozie: Some queries fail with fnf exception</summary>
      <description>When we read back row containers from disk, a misconfiguration causes us to look for a non-existing file.Caused by: java.io.FileNotFoundException: File file:/grid/0/hadoop/yarn/local/usercache/appcache/application_1440685000561_0028/container_e26_1440685000561_0028_01_000005/container_tokens does not exist at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:608) at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:821) at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:598) at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:414) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.&lt;init&gt;(ChecksumFileSystem.java:140) at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766) at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:169) ... 31 more</description>
      <version>None</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11745" opendate="2015-9-4 00:00:00" fixdate="2015-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table Exchange partition with multiple partition_spec is not working</summary>
      <description>Single partition works, but multiple partitions will not work.Reproduce steps:DROP TABLE IF EXISTS t1;DROP TABLE IF EXISTS t2;DROP TABLE IF EXISTS t3;DROP TABLE IF EXISTS t4;CREATE TABLE t1 (a int) PARTITIONED BY (d1 int);CREATE TABLE t2 (a int) PARTITIONED BY (d1 int);CREATE TABLE t3 (a int) PARTITIONED BY (d1 int, d2 int);CREATE TABLE t4 (a int) PARTITIONED BY (d1 int, d2 int);INSERT OVERWRITE TABLE t1 PARTITION (d1 = 1) SELECT salary FROM jsmall LIMIT 10;INSERT OVERWRITE TABLE t3 PARTITION (d1 = 1, d2 = 1) SELECT salary FROM jsmall LIMIT 10;SELECT * FROM t1;SELECT * FROM t3;ALTER TABLE t2 EXCHANGE PARTITION (d1 = 1) WITH TABLE t1;SELECT * FROM t1;SELECT * FROM t2;ALTER TABLE t4 EXCHANGE PARTITION (d1 = 1, d2 = 1) WITH TABLE t3;SELECT * FROM t3;SELECT * FROM t4;The output:0: jdbc:hive2://10.17.74.148:10000/default&gt; SELECT * FROM t3;+-------+--------+--------+--+| t3.a | t3.d1 | t3.d2 |+-------+--------+--------+--++-------+--------+--------+--+No rows selected (0.227 seconds)0: jdbc:hive2://10.17.74.148:10000/default&gt; SELECT * FROM t4;+-------+--------+--------+--+| t4.a | t4.d1 | t4.d2 |+-------+--------+--------+--++-------+--------+--------+--+No rows selected (0.266 seconds)</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="11746" opendate="2015-9-6 00:00:00" fixdate="2015-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connect command should not to be allowed from user[beeline-cli branch]</summary>
      <description>For new cli, user should not be allowed to connect a server or database.</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="11761" opendate="2015-9-8 00:00:00" fixdate="2015-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DoubleWritable hashcode for GroupBy is not properly generated</summary>
      <description>HIVE-11502 fixed the hashcode for LazyDouble. Additionally we should fix for DoubleWritable as well due to HADOOP-12217 issue. In some cases such as select avg(t) from (select * from over1k cross join src) t group by d; where d is double type, the data is actually in DoubleWritable, not LazyDouble. Thus, before HADOOP-12217 gets fixed, we need to fix hashcode for LazyDouble as well as DoubleWritable.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="11779" opendate="2015-9-10 00:00:00" fixdate="2015-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline-cli: Format the output of describe pretty table command in new CLI[beeline-cli branch]</summary>
      <description>In beeline when we use the describe pretty table, it put the result of every row as the first column,and the remaining columns were assigned "NULL".We want to split every row as there columns.</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.describe.pretty.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1178" opendate="2010-2-17 00:00:00" fixdate="2010-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce bucketing for a table</summary>
      <description>If the table being inserted is a bucketed, currently hive does not try to enforce that.An option should be added for checking that.Moreover, the number of buckets can be higher than the number of maximum reducers, in whichcase a single reducer can write to multiple files.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11786" opendate="2015-9-10 00:00:00" fixdate="2015-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate the use of redundant column in colunm stats related tables</summary>
      <description>The stats tables such as TAB_COL_STATS, PART_COL_STATS have redundant columns such as DB_NAME, TABLE_NAME, PARTITION_NAME since these tables already have foreign key like TBL_ID, or PART_ID referencing to TBLS or PARTITIONS. These redundant columns violate database normalization rules and cause a lot of inconvenience (sometimes difficult) in column stats related feature implementation. For example, when renaming a table, we have to update TABLE_NAME column in these tables as well which is unnecessary.This JIRA is first to deprecate the use of these columns at HMS code level. A followed JIRA is to be opened to focus on DB schema change and upgrade.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="11789" opendate="2015-9-10 00:00:00" fixdate="2015-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better support for functions recognition in CBO</summary>
      <description>Including IN, BETWEEN, and STRUCT (multicolumn). In particular, STRUCT is represented as ROW in Calcite.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="1179" opendate="2010-2-18 00:00:00" fixdate="2010-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDF array_contains</summary>
      <description>Returns true or false, depending on whether an element is in an array.array_contains(T element, array&lt;T&gt; theArray)</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">eclipse-templates..settings.org.eclipse.jdt.ui.prefs</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="118" opendate="2008-12-4 00:00:00" fixdate="2008-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add date related functions to Hive</summary>
      <description>Add FromUnixTime, Year, Month, DayOfMonth, and Date functions as in mysql: http://dev.mysql.com/doc/refman/5.0/en/date-and-time-functions.html</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1180" opendate="2010-2-18 00:00:00" fixdate="2010-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Common Table Expressions (CTEs) in Hive</summary>
      <description>I've seen some presentations from the PostgreSQL recently expounding the utility of CTEs (http://en.wikipedia.org/wiki/Common_table_expressions). Should we try to support these in Hive? I've never used them in practice, so curious to hear if the community would find them useful.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorization.invalid.priv.v1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="11813" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid expensive AST tree conversion to String for expressions in WHERE clause in CBO</summary>
      <description>We use the AST tree String representation of a condition in the WHERE clause to identify its column in the RowResolver. This can lead to OOM Exceptions when the condition is very large.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="11814" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit query time in lineage info</summary>
      <description>Currently, we emit query start time, not the query duration. It is nice to have it too.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="11822" opendate="2015-9-14 00:00:00" fixdate="2015-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>vectorize NVL UDF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="11823" opendate="2015-9-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create a self-contained translation for SARG to be used by metastore</summary>
      <description>See HIVE-11705. This just contains the hbase-metastore-specific methods from that patchNO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11832" opendate="2015-9-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-11802 breaks compilation in JDK 8</summary>
      <description>HIVE-11802 changes breaks JDK 8 compilation. FloatingDecimal constructor accepting float is removed in JDK 8.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.Column.java</file>
    </fixedFiles>
  </bug>
  <bug id="11835" opendate="2015-9-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type decimal(1,1) reads 0.0, 0.00, etc from text file as NULL</summary>
      <description>Steps to reproduce:1. create a text file with values like 0.0, 0.00, etc.2. create table in hive with type decimal(1,1).3. run "load data local inpath ..." to load data into the table.4. run select * on the table.You will see that NULL is displayed for 0.0, 0.00, .0, etc. Instead, these should be read as 0.0.</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="11839" opendate="2015-9-16 00:00:00" fixdate="2015-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization wrong results with filter of (CAST AS CHAR)</summary>
      <description>PROBLEM:For query such asselect count(1) from table where CAST (id as CHAR(4))='1000';gives wrong results 0 than expected results.STEPS TO REPRODUCE:create table s1(id smallint) stored as orc;insert into table s1 values (1000),(1001),(1002),(1003),(1000);set hive.vectorized.execution.enabled=true;select count(1) from s1 where cast(id as char(4))='1000'; this gives 0set hive.vectorized.execution.enabled=false;select count(1) from s1 where cast(id as char(4))='1000'; this gives 2</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="11865" opendate="2015-9-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive PPD optimizer when CBO has optimized the plan</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptfgroupbyjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTSTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11875" opendate="2015-9-17 00:00:00" fixdate="2015-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC Driver does not honor delegation token mechanism when readings params from ZooKeeper</summary>
      <description>Regression introduced in HIVE-11581. When the driver is reading connection params from ZK, in a secure cluster if overrides the delegation token mechanism (specified on client side) with a TGT requiring mechanism (kinit).</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="11882" opendate="2015-9-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetch optimizer should stop source files traversal once it exceeds the hive.fetch.task.conversion.threshold</summary>
      <description>Hive 1.0's fetch optimizer tries to optimize queries of the form "select &lt;C&gt; from &lt;T&gt; where &lt;F&gt; limit &lt;L&gt;" to a fetch task (see the hive.fetch.task.conversion property). This optimization gets the lengths of all the files in the specified partition and does some comparison against a threshold value to determine whether it should use a fetch task or not (see the hive.fetch.task.conversion.threshold property). This process of getting the length of all files. One of the main problems in this optimization is the fetch optimizer doesn't seem to stop once it exceeds the hive.fetch.task.conversion.threshold. It works fine on HDFS, but could cause a significant performance degradation on other supported file systems.</description>
      <version>1.0.0,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SplitSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11889" opendate="2015-9-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test for HIVE-11449</summary>
      <description></description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="11911" opendate="2015-9-21 00:00:00" fixdate="2015-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The stats table limits are too large for innodb</summary>
      <description>The limits were increased to a reasonable value some time ago, apparently these values are too large for innodb due to some index limit nonsense. We need to decrease them a little bit.There's no need to decrease them in an upgrade script; if they were already created successfully it's ok to have them as is.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsSetupConstants.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="11913" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify existence of tests for new changes in HiveQA</summary>
      <description>Would be great if HiveQA could report whether there are test files (Test*, *Test, or qfiles) that are added, or changed.Note not every change would need this, but it should be the best of ability.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
    </fixedFiles>
  </bug>
  <bug id="11922" opendate="2015-9-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better error message when ORC split generation fails</summary>
      <description>When ORC split generation fails, it just prints out "serious problem" message on the console which does not tell anything about the cause of the exception.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11925" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive file format checking breaks load from named pipes</summary>
      <description>Opening the file and mucking with it when hive.fileformat.check is true (the default) breaks the LOAD command from a named pipe. Right now, it's done for all the text files blindly to see if they might be in some other format. Files.getAttribute can be used to figure out if the input is a named pipe (or a socket) and skip the format check.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.InputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11928" opendate="2015-9-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC footer and metadata section can also exceed protobuf message limit</summary>
      <description>Similar to HIVE-11592 but for orc footer.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetadataReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="11932" opendate="2015-9-23 00:00:00" fixdate="2015-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC Driver appends an extra "/" when configuring connection by reading httpPath from ZooKeeper</summary>
      <description></description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="11945" opendate="2015-9-24 00:00:00" fixdate="2015-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC with non-local reads may not be reusing connection to DN</summary>
      <description>When seek + readFully(buffer, offset, length) is used, DFSInputStream ends up going via readWithStrategy(). This sets up BlockReader with length equivalent to that of the block size. So until this position is reached, RemoteBlockReader2.peer would not be added to the PeerCache (Plz refer RemoteBlockReader2.close() in HDFS). So eventually the next call to the same DN would end opening a new socket. In ORC, when it is not a data local read, this has a the possibility of opening/closing lots of connections with DN. In random reads, it would be good to set this length to the amount of data that is to be read (e.g pread call in DFSInputStream which sets up the BlockReaders length correctly &amp; the code path returns the Peer back to peer cache properly). readFully(position, buffer, offset, length) follows this code path and ends up reusing the connections properly. Creating this JIRA to fix this issue.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetadataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="11955" opendate="2015-9-24 00:00:00" fixdate="2015-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add costing for join-groupby transpose rule</summary>
      <description>Currently, its config driven. It needs to be cost driven.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="11966" opendate="2015-9-25 00:00:00" fixdate="2015-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC Driver parsing error when reading principal from ZooKeeper</summary>
      <description></description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="11976" opendate="2015-9-28 00:00:00" fixdate="2015-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend CBO rules to being able to apply rules only once on a given operator</summary>
      <description>Create a way to bail out quickly from HepPlanner if the rule has been already applied on a certain operator.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveConfigContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="11981" opendate="2015-9-28 00:00:00" fixdate="2015-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC Schema Evolution Issues (Vectorized, ACID, and Non-Vectorized)</summary>
      <description>High priority issues with schema evolution for the ORC file format.Schema evolution here is limited to adding new columns and a few cases of column type-widening (e.g. int to bigint).Renaming columns, deleting column, moving columns and other schema evolution were not pursued due to lack of importance and lack of time. Also, it appears a much more sophisticated metadata would be needed to support them.The biggest issues for users have been adding new columns for ACID table (HIVE-11421 Support Schema evolution for ACID tables) and vectorization (HIVE-10598 Vectorization borks when column is added to table).</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorRowObject.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.InputJobInfo.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingAssert.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOConstants.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ConversionTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java.orig</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="11982" opendate="2015-9-28 00:00:00" fixdate="2015-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some test cases for union all fail with recent changes</summary>
      <description>The tests throw java.lang.IndexOutOfBoundsException again. It was supposed to be fixed by HIVE-11271</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="12016" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update log4j2 version to 2.4</summary>
      <description>The latest 2.4 release of log4j2 brought back properties file based configuration. https://logging.apache.org/log4j/2.0/changes-report.html#a2.4bump up the version number to 2.4.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.StringAppender.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1204" opendate="2010-2-27 00:00:00" fixdate="2010-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>typedbytes: writing to stderr kills the mapper</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ScriptDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12045" opendate="2015-10-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassNotFoundException for GenericUDF [Spark Branch]</summary>
      <description>If I execute the following query in beeline, I get ClassNotFoundException for the UDF class.drop function myGenericUdf;create function myGenericUdf as 'org.example.myGenericUdf' using jar 'hdfs:///tmp/myudf.jar';select distinct myGenericUdf(1,2,1) from mytable;In my example, myGenericUdf just looks for the 1st argument's value in the others and returns the index. I don't think this is related to the actual GenericUDF function.Note that:"select myGenericUdf(1,2,1) from mytable;" succeedsIf I use the non-generic implementation of the same UDF, the select distinct call succeeds.StackTrace:15/10/06 05:20:25 ERROR exec.Utilities: Failed to load plan: hdfs://quickstart.cloudera:8020/tmp/hive/hive/f9de3f09-c12d-4528-9ee6-1f12932a14ae/hive_2015-10-06_05-20-07_438_6519207588897968406-20/-mr-10003/27cd7226-3e22-46f4-bddd-fb8fd4aa4b8d/map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: org.example.myGenericUDFSerialization trace:genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)chidren (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)colExprMap (org.apache.hadoop.hive.ql.exec.GroupByOperator)childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: org.example.myGenericUDFSerialization trace:genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)chidren (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)colExprMap (org.apache.hadoop.hive.ql.exec.GroupByOperator)childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:99) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672) at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1069) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:960) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:974) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:416) at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:296) at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:268) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:505) at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:217) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:217) at org.apache.spark.ShuffleDependency.&lt;init&gt;(Dependency.scala:82) at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:80) at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:206) at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:204) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.dependencies(RDD.scala:204) at org.apache.spark.scheduler.DAGScheduler.visit$2(DAGScheduler.scala:338) at org.apache.spark.scheduler.DAGScheduler.getAncestorShuffleDependencies(DAGScheduler.scala:355) at org.apache.spark.scheduler.DAGScheduler.registerShuffleDependencies(DAGScheduler.scala:317) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getShuffleMapStage(DAGScheduler.scala:218) at org.apache.spark.scheduler.DAGScheduler$$anonfun$visit$1$1.apply(DAGScheduler.scala:301) at org.apache.spark.scheduler.DAGScheduler$$anonfun$visit$1$1.apply(DAGScheduler.scala:298) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:298) at org.apache.spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:310) at org.apache.spark.scheduler.DAGScheduler.newStage(DAGScheduler.scala:244) at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:731) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)Caused by: java.lang.ClassNotFoundException: org.example.myGenericUDF at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:270) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 72 more</description>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12046" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-create spark client if connection is dropped</summary>
      <description>Currently, if the connection to the spark cluster is dropped, the spark client will stay in a bad state. A new Hive session is needed to re-establish the connection. It is better to auto reconnect in this case.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.Rpc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="12048" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore file metadata cache should not be used when deltas are present</summary>
      <description>Previous code doesn't check for deltas before getting footers from local cache even though stripe filtering with deltas is not possible; this is because checking local cache is cheap I guess. Make sure we check early for metastore-based cache.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12053" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats performance regression caused by HIVE-11786</summary>
      <description>HIVE-11786 tried to normalize table TAB_COL_STATS/PART_COL_STATS but caused performance regression.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="12055" opendate="2015-10-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create row-by-row shims for the write path</summary>
      <description>As part of removing the row-by-row writer, we'll need to shim out the higher level API (OrcSerde and OrcOutputFormat) so that we maintain backwards compatibility.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.json</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Writer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.JsonFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.filters.BloomFilterIO.java</file>
      <file type="M">orc.src.java.org.apache.orc.TypeDescription.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.BloomFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="12058" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change hive script to record errors when calling hbase fails</summary>
      <description>By default hive will try to find out which jars need to be added to the classpath in order to run MR jobs against an HBase cluster, however if hbase can't be found or if hbase mapredcp fails, the hive script will fail silently and ignore some of the jars to be included into the. That makes very difficult to analyze the real problem.Hive script should record the error not just simply redirect two hbase failures:HBASE_BIN=${HBASE_BIN:-"$(which hbase 2&gt;/dev/null)"}$HBASE_BIN mapredcp 2&gt;/dev/null</description>
      <version>0.14.0,1.1.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="12059" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up reference to deprecated constants in AvroSerdeUtils</summary>
      <description>AvroSerdeUtils contains several deprecated String constants that are used by other Hive modules. Those should be cleaned up.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="12060" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: create separate variable for llap tests</summary>
      <description>No real reason to just reuse tez one; also needed to parallelize the tests</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12062" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable HBase metastore file metadata cache for tez tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12090" opendate="2015-10-12 00:00:00" fixdate="2015-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dead-code: Vectorized map-join murmur hash is run twice</summary>
      <description>Caught by mmccline during code reviews. public JoinUtil.JoinResult setDirect(byte[] bytes, int offset, int length, BytesBytesMultiHashMap.Result hashMapResult) { int keyHash = WriteBuffers.murmurHash(bytes, offset, length); aliasFilter = hashMap.getValueResult(bytes, offset, length, hashMapResult);</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12159" opendate="2015-10-13 00:00:00" fixdate="2015-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create vectorized readers for the complex types</summary>
      <description>We need vectorized readers for the complex types.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.MultiValuedColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestTypeDescription.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.DefaultDataReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestMetadataReaderProperties.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestDataReaderProperties.java</file>
      <file type="M">orc.src.java.org.apache.orc.TypeDescription.java</file>
      <file type="M">orc.src.java.org.apache.orc.RecordReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.Reader.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.MetadataReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerReaderV2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthByteReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.MetadataReaderProperties.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.MetadataReaderImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.MetadataReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.IntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.InStream.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DefaultMetadataReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DataReaderProperties.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.BitFieldReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.DataReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.DataReader.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12168" opendate="2015-10-14 00:00:00" fixdate="2015-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addendum to HIVE-12038</summary>
      <description>In HIVE-12038, missed a case of Error. Originally the assumption that if error is true, then it is always a build error. Apparently there is a TestFailedException.Currently, it incorrectly report failed tests as build errors.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
    </fixedFiles>
  </bug>
  <bug id="12175" opendate="2015-10-14 00:00:00" fixdate="2015-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Kryo version to 3.0.x</summary>
      <description>Current version of kryo (2.22) has some issue (refer exception below and in HIVE-12174) with serializing ArrayLists generated using Arrays.asList(). We need to either replace all occurrences of Arrays.asList() or change the current StdInstantiatorStrategy. This issue is fixed in later versions and kryo community recommends using DefaultInstantiatorStrategy with fallback to StdInstantiatorStrategy. More discussion about this issue is here https://github.com/EsotericSoftware/kryo/issues/216. Alternatively, custom serilization/deserilization class can be provided for Arrays.asList.Also, kryo 3.0 introduced unsafe based serialization which claims to have much better performance for certain types of serialization. Exception:Caused by: java.lang.NullPointerException at java.util.Arrays$ArrayList.size(Arrays.java:2847) at java.util.AbstractList.add(AbstractList.java:108) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112) at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ... 57 more</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.KryoMessageCodec.java</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12187" opendate="2015-10-15 00:00:00" fixdate="2015-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release plan once a query is executed</summary>
      <description>Some clients leave query operations open for a while so that they can retrieve the query results later. That means the allocated memory will be kept around too. We should release those resources not needed for query execution any more once it is executed.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12188" opendate="2015-10-15 00:00:00" fixdate="2015-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DoAs does not work properly in non-kerberos secured HS2</summary>
      <description>The case with following settings is valid but it seems still not work correctly in current HS2==hive.server2.authentication=NONE (or LDAP)hive.server2.enable.doAs= truehive.metastore.sasl.enabled=true (with HMS Kerberos enabled)==Currently HS2 is able to fetch the delegation token to a kerberos secured HMS only when itself is also kerberos secured.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="12204" opendate="2015-10-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez queries stopped running with ApplicationNotRunningException</summary>
      <description>In some error cases, if hive can no longer submit DAGs to tez, there is no use retrying to submit. We need to exit by throwing exception in this case.</description>
      <version>1.0.1,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12207" opendate="2015-10-18 00:00:00" fixdate="2015-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query fails when non-ascii characters are used in string literals</summary>
      <description>While debugging HIVE-11721 I found that using non-ascii characters in string literals causes calcite planner to throw the following exception:2015-10-17T23:07:20,586 ERROR [main]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(292)) - CBO failed, skipping CBO.org.apache.calcite.runtime.CalciteException: Failed to encode '' in character set 'ISO-8859-1'The query is:select concat("", "") from src limit 1;Other queries with non-ascii literals fail as well.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="12208" opendate="2015-10-18 00:00:00" fixdate="2015-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized JOIN NPE on dynamically partitioned hash-join + map-join</summary>
      <description>TPC-DS Q82 with reducer vectorized join optimizations Reducer 5 &lt;- Map 1 (CUSTOM_SIMPLE_EDGE), Map 2 (CUSTOM_SIMPLE_EDGE), Map 3 (BROADCAST_EDGE), Map 4 (CUSTOM_SIMPLE_EDGE)set hive.optimize.dynamic.partition.hashjoin=true;set hive.vectorized.execution.reduce.enabled=true;set hive.mapjoin.hybridgrace.hashtable=false;select i_item_id ,i_item_desc ,i_current_price from item, inventory, date_dim, store_sales where i_current_price between 30 and 30+30 and inv_item_sk = i_item_sk and d_date_sk=inv_date_sk and d_date between '2002-05-30' and '2002-07-30' and i_manufact_id in (437,129,727,663) and inv_quantity_on_hand between 100 and 500 and ss_item_sk = i_item_sk group by i_item_id,i_item_desc,i_current_price order by i_item_id limit 100possibly a trivial plan setup issue, since the NPE is pretty much immediate.Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:368) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852) at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.forwardBigTableBatch(VectorMapJoinGenerateResultOperator.java:603) at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:362) ... 19 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.commonSetup(VectorMapJoinInnerGenerateResultOperator.java:112) at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:96) ... 22 more</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12210" opendate="2015-10-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix a few failing tests: testCliDriver_udf_explode and testCliDriver_udtf_explode</summary>
      <description>The following tests fail after HIVE-11785 because of missing "serialization.escape.crlf true" property in the output.org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explodeorg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12223" opendate="2015-10-21 00:00:00" fixdate="2015-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter on Grouping__ID does not work properly</summary>
      <description>Consider the following query:SELECT key, value, GROUPING__ID, count(*)FROM T1GROUP BY key, valueGROUPING SETS ((), (key))HAVING GROUPING__ID = 1This query will not return results.The reason is that a "constant" placeholder is introduced by SemanticAnalyzer for the GROUPING&amp;#95;_ID column. At execution time, this placeholder is replaced by the actual value of the GROUPING&amp;#95;_ID. As the column is a constant, the Hive optimizer (combination of PPD and constant folding) will evaluate statically whether the condition is met or not, leading to incorrect results.We should be able to recognize the placeholder and avoid PPD pushing the filter predicate to the GroupBy operator.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12224" opendate="2015-10-21 00:00:00" fixdate="2015-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HOLD_DDLTIME</summary>
      <description>This arcane feature was introduced long ago via HIVE-1394 It was broken as soon as it landed, HIVE-1442 and is thus useless. Fact that no one has fixed it since informs that its not really used by anyone. Better is to remove it so no one hits the bug of HIVE-1442</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ddltime.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ddltime.q</file>
      <file type="M">ql.src.test.queries.clientnegative.ddltime.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SelectClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12227" opendate="2015-10-21 00:00:00" fixdate="2015-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: better column vector object pools</summary>
      <description>Vector allocations become a problem in sub-second cases. The pool of 8 per request is too small. Needs to be bigger and potentially pre-populated if we can do it off the main path.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12237" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use slf4j as logging facade</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.counter.SparkCounters.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">shims.scheduler.src.main.java.org.apache.hadoop.hive.schshim.FairSchedulerShim.java</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.DefaultFileAccess.java</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceOperations.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadWithGarbageCleanup.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
      <file type="M">service.src.java.org.apache.hive.service.CompositeService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.AbstractService.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.OpenCSVSerde.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFloat.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyDate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleSerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUnion.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveIntervalYearMonth.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveIntervalDayTime.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinarySerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveIntervalYearMonthWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.DateWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableSerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.InstanceCache.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.java</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.log.TestLog4j2Appenders.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.mapjoin.TestMapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ZooKeeperHiveHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPI.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFE.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorSpark.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.DependencyResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ReloadProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DfsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CryptoProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CompileProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkFileSinkProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MetaDataExportListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.InputSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FileSinkProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AppMasterEventProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.CombineEquivalentWorkResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkRemoveDynamicPruningBySize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SetReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PointLookupOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MemoryDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.OperatorComparatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DummyPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.CuratorFrameworkSingleton.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.StorageFormatFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetFilterPredicateConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MemoryManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NullRowsInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOContextMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CodecPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryViewer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRowNoNulls.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnOrderedMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedCreateHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastValueStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.tools.KeyValuesInputMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.tools.KeyValueInputMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.JobMetricsListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMergeFileRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SmallTableCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RCFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.FlatRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionKeySampler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.Throttle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HiveTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Heartbeater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AutoProgressor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ArchiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LogLevels.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandlerNegative.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestSharedStorageDescriptor.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCache.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyMetaStoreInitListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStoreProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.partition.spec.CompositePartitionSpecProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.PartFilterExprUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreInit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.TephraHBaseConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.StatsCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.PartitionKeyComparator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseImport.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.AggrStatsInvalidatorFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.EventCleanerTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.AggregateStatsCache.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.IndexCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.FadvisedFileRegion.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.FadvisedChunkedFile.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.DirWatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapYarnRegistryImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.HistoryLogger.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDFFileLookup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
      <file type="M">itests.test-serde.src.main.java.org.apache.hadoop.hive.serde2.TestSerDe.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.cbo.rp.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestStorageDescriptorSharing.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseImport.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.HBaseIntegrationTests.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.api.TestHCatClientNotification.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionManager.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWIServer.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWIContextListener.java</file>
      <file type="M">hwi.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LogRetriever.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobStateTracker.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SqoopDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ProxyUserSupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CatchallExceptionMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.CommandTestUtils.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.commands.TestCommands.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseTableSnapshotInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.CompositeHBaseKeyFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.AbstractHBaseKeyPredicateDecomposer.java</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.ColumnMapper.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.ColumnMappingFactory.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.LazyAccumuloRow.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.CompositeAccumuloRowIdFactory.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.Utils.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.serde.DelimitedAccumuloRowIdFactory.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.java</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ClassNameCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SQLCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.util.QFileClient.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">common.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CallableWithNdc.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CompressionUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.tez.TezJsonParser.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmPauseMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.RunnableWithNdc.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ServerUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.SystemVariables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.VariableSubstitution.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">common.src.java.org.apache.hive.common.HiveCompat.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.FixedSizedObjectPool.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveVersionInfo.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestFixedSizedObjectPool.java</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="12246" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc FileDump fails with Missing CLI jar</summary>
      <description>Running hive --orcfiledump fails with "Missing CLI jar"</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.util.execHiveCmd.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12259" opendate="2015-10-24 00:00:00" fixdate="2015-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Command containing semicolon is broken in Beeline</summary>
      <description>The Beeline command (!cmd) containing semicolon is broken. For example:!connect jdbc:hive2://localhost:10001/default;principal=hive/xyz.com@realm.comis broken because the included ";" makes it not to run with execCommandWithPrefix as a whole command. if (line.startsWith(COMMAND_PREFIX) &amp;&amp; !line.contains(";")) { // handle the case "!cmd" for beeline return execCommandWithPrefix(line); } else { return commands.sql(line, getOpts().getEntireLineAsCommand()); }</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="12260" opendate="2015-10-24 00:00:00" fixdate="2015-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestColumnStatistics and TestJsonFileDump test failures in master</summary>
      <description>These failures were introduced by HIVE-11807 commit.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.json</file>
    </fixedFiles>
  </bug>
  <bug id="12261" opendate="2015-10-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool version info exit status should depend on compatibility, not equality</summary>
      <description>Newer versions of metastore schema are compatible with older versions of hive, as only new tables or columns are added with additional information.HIVE-11613 added a check in hive schematool -info command to see if schema version is equal. However, the state where db schema version is ahead of hive software version is often seen when a 'rolling upgrade' or 'rolling downgrade' is happening. This is a state where hive is functional and returning non zero status for it is misleading.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="12265" opendate="2015-10-26 00:00:00" fixdate="2015-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate lineage info only if requested</summary>
      <description>If lineage related hook is not configured, we should not generate lineage info.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12274" opendate="2015-10-27 00:00:00" fixdate="2015-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase width of columns used for general configuration in the metastore.</summary>
      <description>OverviewThis issue is very similar in principle to HIVE-1364. We are hitting a limit when processing JSON data that has a large nested schema. The struct definition is truncated when inserted into the metastore database column COLUMNS_V2.TYPE_NAME as it is greater than 4000 characters in length.Given that the purpose of these columns is to hold very loosely defined configuration values it seems rather limiting to impose such a relatively low length bound. One can imagine that valid use cases will arise where reasonable parameter/property values exceed the current limit. ContextThese limitations were in by the patch attributed to HIVE-1364 which mentions the "max length on Oracle 9i/10g/11g" as the reason. However, nowadays the limit can be increased because: Oracle DB's varchar2 supports 32767 bytes now, by setting the configuration parameter MAX_STRING_SIZE to EXTENDED. (source) Postgres supports a max of 1GB for character datatype. (source) MySQL can support upto 65535 bytes for the entire row. So long as the PARAM_KEY value + PARAM_VALUE is less than 65535, we should be good. (source) SQL Server's varchar max length is 8000 and can go beyond using "varchar(max)" with the same limitation as MySQL being 65535 bytes for the entire row. (source) Derby's varchar can be upto 32672 bytes. (source)ProposalCan these columns not use CLOB-like types as for example as used by TBLS.VIEW_EXPANDED_TEXT? It would seem that suitable type equivalents exist for all targeted database platforms: MySQL: mediumtext Postgres: text Oracle: CLOB Derby: LONG VARCHARI'd suggest that the candidates for type change are: COLUMNS_V2.TYPE_NAME TABLE_PARAMS.PARAM_VALUE SERDE_PARAMS.PARAM_VALUE SD_PARAMS.PARAM_VALUEAfter updating the maximum length the metastore database needs to be configured and restarted with the new settings. Altering MAX_STRING_SIZE will update database objects and possibly invalidate them, as follows: Tables with virtual columns will be updated with new data type metadata for virtual columns of VARCHAR2(4000), 4000-byte NVARCHAR2, or RAW(2000) type. Functional indexes will become unusable if a change to their associated virtual columns causes the index key to exceed index key length limits. Attempts to rebuild such indexes will fail with ORA-01450: maximum key length exceeded. Views will be invalidated if they contain VARCHAR2(4000), 4000-byte NVARCHAR2, or RAW(2000) typed expression columns. Materialized views will be updated with new metadata VARCHAR2(4000), 4000-byte NVARCHAR2, and RAW(2000) typed expression columns So the limitation could be raised to 32672 bytes, with the caveat that MySQL and SQL Server limit the row length to 65535 bytes, so that should also be validated to provide consistency.Finally, will this limitation persist in the work resulting from HIVE-9452?</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.1.0-to-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.1.0-to-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.1.0-to-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.1.0-to-2.2.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-txn-schema-0.14.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.2.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.1.0-to-2.2.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-2.2.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.2.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.022-HIVE-11107.derby.sql</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.scripts.upgrade.oracle.039-HIVE-12274.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug id="12281" opendate="2015-10-28 00:00:00" fixdate="2015-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized MapJoin - use Operator::isLogDebugEnabled</summary>
      <description></description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="12287" opendate="2015-10-28 00:00:00" fixdate="2015-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage for lateral view shows wrong dependencies</summary>
      <description>The lineage dependency graph for select from lateral view is wrong.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lineage2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12288" opendate="2015-10-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend HIVE-11306 changes to apply to Native vectorized map-joins</summary>
      <description>HIVE-11306 applies to the old style VectorMapJoin operators, while the specialized operators go via a different codepath into the HybridHybridHashTable.Apply similar changes to the setDirect() codepath.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12289" opendate="2015-10-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure slf4j-log4j12 jar is not in classpath</summary>
      <description>log4j12 which is version 1.2 gets pulled in by transitive dependency. Need to make sure we only have log4j2 is in classpath, otherwise slf4j may bind to version 1.2</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseSchemaTool.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hwi.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
      <file type="M">common.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12302" opendate="2015-10-30 00:00:00" fixdate="2015-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use KryoPool instead of thread-local caching</summary>
      <description>Kryo 3.x introduces a Pooling mechanism for Kryohttps://github.com/EsotericSoftware/kryo#pooling-kryo-instances// Build pool with SoftReferences enabled (optional)KryoPool pool = new KryoPool.Builder(factory).softReferences().build();</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRowGroupFilter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SplitSparkWorkResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="12304" opendate="2015-10-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"drop database cascade" needs to unregister functions</summary>
      <description>Currently "drop database cascade" command doesn't unregister the functions under the database. If the functions are not unregistered, in some cases like "describe db1.func1" will still show the info for the function; or if the same database is recreated, "drop if exists db1.func1" will throw an exception since the function is considered existing from the registry while it doesn't exist in metastore.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="12315" opendate="2015-11-2 00:00:00" fixdate="2015-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>vectorization_short_regress.q has a wrong result issue for a double calculation</summary>
      <description>I suspect it is related to the fancy optimizations in vectorized double divide that try to quickly process the batch without checking each row for null. public static void setNullAndDivBy0DataEntriesDouble( DoubleColumnVector v, boolean selectedInUse, int[] sel, int n, DoubleColumnVector denoms) { assert v.isRepeating || !denoms.isRepeating; v.noNulls = false; double[] vector = denoms.vector; if (v.isRepeating &amp;&amp; (v.isNull[0] = (v.isNull[0] || vector[0] == 0))) { v.vector[0] = DoubleColumnVector.NULL_VALUE;</description>
      <version>0.14.0,1.0.1,1.1.1,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12317" opendate="2015-11-2 00:00:00" fixdate="2015-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit current database in lineage info</summary>
      <description>It will be easier to emit current database info explicitly instead of finding out such info from normalized column names.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="12330" opendate="2015-11-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix precommit Spark test part2</summary>
      <description>Regression because of HIVE-11489</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug id="12345" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup for HIVE-9013 : Hidden conf vars still visible through beeline</summary>
      <description>HIVE-9013 introduced the ability to hide certain conf variables when output through the "set" command. However, there still exists one further bug in it that causes these variables to still be visible through beeline connecting to HS2, wherein HS2 exposes hidden variables such as the HS2's metastore password when "set" is run.</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12363" opendate="2015-11-7 00:00:00" fixdate="2015-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results with orc ppd across ORC versions</summary>
      <description>Run vector_decimal_cast.q on tez cli driver.The issue is related to the ORC Timestamp column stats, which does not exist in all ORC files. When the timestamp column is missing stats, default to YES_NO_NULL instead of assuming the column is all nulls.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12385" opendate="2015-11-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool failed on MySQL</summary>
      <description>Run schematool and got the following error:CREATE TABLE IF NOT EXISTS `COLUMNS_V2` ( `CD_ID` bigint(20) NOT NULL, `COMMENT` varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL, `COLUMN_NAME` varchar(1000) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL, `TYPE_NAME` varchar(4000) DEFAULT NULL, `INTEGER_IDX` int(11) NOT NULL, PRIMARY KEY (`CD_ID`,`COLUMN_NAME`), KEY `COLUMNS_V2_N49` (`CD_ID`), CONSTRAINT `COLUMNS_V2_FK1` FOREIGN KEY (`CD_ID`) REFERENCES `CDS` (`CD_ID`) ) ENGINE=InnoDB DEFAULT CHARSET=latin1Error: Specified key was too long; max key length is 767 bytes (state=42000,code=1071)</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-1.3.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.022-HIVE-11970.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="12399" opendate="2015-11-12 00:00:00" fixdate="2015-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Native Vector MapJoin can encounter "Null key not expected in MapJoin" and "Unexpected NULL in map join small table" exceptions</summary>
      <description>Instead of throw exception, just filter out NULLs in the Native Vector MapJoin operators.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="124" opendate="2008-12-5 00:00:00" fixdate="2008-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>aggregation on empty table should still return 1 row</summary>
      <description>The query "SELECT COUNT(1) FROM f_status_update fsu WHERE FALSE" should return a single row with value 0.Our code treat that query as "SELECT 1, COUNT(1) FROM f_status_update fsu WHERE FALSE GROUP BY 1", but these 2 queries are not equivalent because the second query will return empty result if the input is empty.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12402" opendate="2015-11-13 00:00:00" fixdate="2015-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split hive.root.logger separately to make it compatible with log4j1.x</summary>
      <description>With new Log4j2.x specifying logger name and log level together will not work.With old logger following will work--hiveconf hive.root.logger=DEBUG,consoleBut with new logger we should specify logger and level separately--hiveconf hive.root.logger=console --hiveconf hive.log.level=DEBUGWe can do this change internally for users still using the old configs.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12434" opendate="2015-11-17 00:00:00" fixdate="2015-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge spark into master 11/17/1015</summary>
      <description>There are still a few patches that are in Spark branch only. We need to merge them to master.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.JobContextImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.JobContext.java</file>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge.incompat2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge.incompat1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.JobMetricsListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12435" opendate="2015-11-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SELECT COUNT(CASE WHEN...) GROUPBY returns 1 for &amp;#39;NULL&amp;#39; in a case of ORC and vectorization is enabled.</summary>
      <description>Run the following query:create table count_case_groupby (key string, bool boolean) STORED AS orc;insert into table count_case_groupby values ('key1', true),('key2', false),('key3', NULL),('key4', false),('key5',NULL);The table contains the following:key1 truekey2 falsekey3 NULLkey4 falsekey5 NULLThe below query returns:SELECT key, COUNT(CASE WHEN bool THEN 1 WHEN NOT bool THEN 0 ELSE NULL END) AS cnt_bool0_ok FROM count_case_groupby GROUP BY key;key1 1key2 1key3 1key4 1key5 1while it expects the following results:key1 1key2 1key3 0key4 1key5 0The query works with hive ver 1.2. Also it works when a table is not orc format.Also even if it's an orc table, when vectorization is disabled, the query works.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12436" opendate="2015-11-17 00:00:00" fixdate="2015-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default hive.metastore.schema.verification to true</summary>
      <description>It enforces metastore schema version consistency</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">beeline.src.test.resources.hive-site.xml</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="12445" opendate="2015-11-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tracking of completed dags is a slow memory leak</summary>
      <description>LLAP daemons track completed DAGs, but never clean up these structures. This is primarily to disallow out of order executions. Evaluate whether that can be avoided - otherwise this structure needs to be cleaned up with a delay.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFileCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="12446" opendate="2015-11-18 00:00:00" fixdate="2015-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tracking jira for changes required for move to Tez 0.8.2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12448" opendate="2015-11-18 00:00:00" fixdate="2015-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change to tracking of dag status via dagIdentifier instead of dag name</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestQueryIdentifier.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.Converters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.QueryFailedHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.KilledTaskHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.HistoryLogger.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="12456" opendate="2015-11-18 00:00:00" fixdate="2015-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryId can&amp;#39;t be stored in the configuration of the SessionState since multiple queries can run in a single session</summary>
      <description>Follow up on HIVE-11488 which stores the queryId in the sessionState conf. If multiple queries run at the same time, then the logging will get wrong queryId from the sessionState.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="12462" opendate="2015-11-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>DPP: DPP optimizers need to run on the TS predicate not FIL</summary>
      <description>HIVE-11398 + HIVE-11791, the partition-condition-remover became more effective.This removes predicates from the FilterExpression which involve partition columns, causing a miss for dynamic-partition pruning if the DPP relies on FilterDesc.The TS desc will have the correct predicate in that condition.$hdt$_0:$hdt$_1:a TableScan (TS_2) alias: a filterExpr: (((account_id = 22) and year(dt) is not null) and (year(dt)) IN (RS[6])) (type: boolean) Filter Operator (FIL_20) predicate: ((account_id = 22) and year(dt) is not null) (type: boolean) Select Operator (SEL_4) expressions: dt (type: date) outputColumnNames: _col1 Reduce Output Operator (RS_8) key expressions: year(_col1) (type: int) sort order: + Map-reduce partition columns: year(_col1) (type: int) Join Operator (JOIN_9) condition map: Inner Join 0 to 1 keys: 0 year(_col1) (type: int) 1 year(_col1) (type: int)</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug id="12466" opendate="2015-11-19 00:00:00" fixdate="2015-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SparkCounter not initialized error</summary>
      <description>During a query, lots of the following error found in executor's log:03:47:28.759 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.03:47:28.762 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.03:47:30.707 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.tmp_tmp] has not initialized before.03:47:33.385 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.03:47:33.388 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.03:47:33.495 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.03:47:35.141 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before............</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12467" opendate="2015-11-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add number of dynamic partitions to error message</summary>
      <description>Currently when using dynamic partition insert we get an error message saying that the client tried to create too many dynamic partitions ("Maximum was set to"). I'll extend the error message to specify the number of dynamic partitions which can be helpful for debugging.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12473" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DPP: UDFs on the partition column side does not evaluate correctly</summary>
      <description>Related to HIVE-12462select count(1) from accounts a, transactions t where year(a.dt) = year(t.dt) and account_id = 22;$hdt$_0:$hdt$_1:a TableScan (TS_2) alias: a filterExpr: (((account_id = 22) and year(dt) is not null) and (year(dt)) IN (RS[6])) (type: boolean)Ends up being evaluated as year(cast(dt as int)) because the pruner only checks for final type, not the column type. ObjectInspector oi = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(TypeInfoFactory .getPrimitiveTypeInfo(si.fieldInspector.getTypeName())); Converter converter = ObjectInspectorConverters.getConverter( PrimitiveObjectInspectorFactory.javaStringObjectInspector, oi);</description>
      <version>1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
    </fixedFiles>
  </bug>
  <bug id="12477" opendate="2015-11-20 00:00:00" fixdate="2015-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Left Semijoins are incompatible with a cross-product</summary>
      <description>with HIVE-12017 in place, a few queries generate left sem-joins without a key.This is an invalid plan and can be produced by doing.explain logical select count(1) from store_sales where ss_sold_date_sk in (select d_date_sk from date_dim where d_date_sk = 1);LOGICAL PLAN: $hdt$_0:$hdt$_0:$hdt$_0:store_sales TableScan (TS_0) alias: store_sales filterExpr: (ss_sold_date_sk = 1) (type: boolean) Filter Operator (FIL_20) predicate: (ss_sold_date_sk = 1) (type: boolean) Select Operator (SEL_2) Reduce Output Operator (RS_9) sort order: Join Operator (JOIN_11) condition map: Left Semi Join 0 to 1 keys: 0 1 Group By Operator (GBY_14) aggregations: count(1) mode: hashwithout CBOsq_1:date_dim TableScan (TS_1) alias: date_dim filterExpr: ((1) IN (RS[6]) and (d_date_sk = 1)) (type: boolean) Filter Operator (FIL_21) predicate: ((1) IN (RS[6]) and (d_date_sk = 1)) (type: boolean) Select Operator (SEL_3) expressions: 1 (type: int) outputColumnNames: _col0 Group By Operator (GBY_5) keys: _col0 (type: int) mode: hash outputColumnNames: _col0 Reduce Output Operator (RS_8) key expressions: _col0 (type: int) sort order: + Map-reduce partition columns: _col0 (type: int) Join Operator (JOIN_9) condition map: Left Semi Join 0 to 1 keys: 0 ss_sold_date_sk (type: int) 1 _col0 (type: int) Group By Operator (GBY_12) aggregations: count(1) mode: hash</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12479" opendate="2015-11-20 00:00:00" fixdate="2015-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Vectorized Date UDFs with up-stream Joins</summary>
      <description>The row-counts expected with and without vectorization differ.The attached small-scale repro case produces 5 rows with vectorized multi-key joins and 53 rows without the vectorized join.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
    </fixedFiles>
  </bug>
  <bug id="12487" opendate="2015-11-20 00:00:00" fixdate="2015-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken MiniLlap tests</summary>
      <description>Currently MiniLlap tests fail with the following error:TestMiniLlapCliDriver - did not produce a TEST-*.xml fileSupposedly, it started happening after HIVE-12319.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12491" opendate="2015-11-22 00:00:00" fixdate="2015-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve ndv heuristic for functions</summary>
      <description>The eased out denominator has to detect duplicate row-stats from different attributes.select account_id from customers c, customer_activation ca where c.customer_id = ca.customer_id and year(ca.dt) = year(c.dt) and month(ca.dt) = month(c.dt) and year(ca.dt) between year('2013-12-26') and year('2013-12-26') private Long getEasedOutDenominator(List&lt;Long&gt; distinctVals) { // Exponential back-off for NDVs. // 1) Descending order sort of NDVs // 2) denominator = NDV1 * (NDV2 ^ (1/2)) * (NDV3 ^ (1/4))) * .... Collections.sort(distinctVals, Collections.reverseOrder()); long denom = distinctVals.get(0); for (int i = 1; i &lt; distinctVals.size(); i++) { denom = (long) (denom * Math.pow(distinctVals.get(i), 1.0 / (1 &lt;&lt; i))); } return denom; }This gets &amp;#91;8007986, 821974390, 821974390&amp;#93;, which is actually 3 columns 2 of which are derived from the same column. Reduce Output Operator (RS_12) key expressions: _col0 (type: bigint), year(_col2) (type: int), month(_col2) (type: int) sort order: +++ Map-reduce partition columns: _col0 (type: bigint), year(_col2) (type: int), month(_col2) (type: int) value expressions: _col1 (type: bigint) Join Operator (JOIN_13) condition map: Inner Join 0 to 1 keys: 0 _col0 (type: bigint), year(_col1) (type: int), month(_col1) (type: int) 1 _col0 (type: bigint), year(_col2) (type: int), month(_col2) (type: int) outputColumnNames: _col3So the eased out denominator is off by a factor of 30,000 or so, causing OOMs in map-joins.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSecond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMinute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFHour.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentUser.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAddMonths.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12492" opendate="2015-11-22 00:00:00" fixdate="2015-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapJoin: 4 million unique integers seems to be a probe plateau</summary>
      <description>After 4 million keys, the map-join implementation seems to suffer from a performance degradation. The hashtable build &amp; probe time makes this very inefficient, even if the data is very compact (i.e 2 ints).Falling back onto the shuffle join or bucket map-join is useful after 2^22 items.(Note: this fixes a statsutil issue - due to the extra clone() in the column stats path)</description>
      <version>1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12499" opendate="2015-11-23 00:00:00" fixdate="2015-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HMS metrics for number of tables and partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="12501" opendate="2015-11-24 00:00:00" fixdate="2015-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: don&amp;#39;t use read(ByteBuffer) in IO</summary>
      <description>read(ByteBuffer) API just copies the data anyway, and there's no readFully. We need to use readFully and copy ourselves.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12512" opendate="2015-11-24 00:00:00" fixdate="2015-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include driver logs in execution-level Operation logs</summary>
      <description>When hive.server2.logging.operation.level is set to EXECUTION (default), operation logs do not include Driver logs, which contain useful info like total number of jobs launched, stage getting executed, etc. that help track high-level progress. It only adds a few more lines to the output.15/11/24 14:09:12 INFO ql.Driver: Semantic Analysis Completed15/11/24 14:09:12 INFO ql.Driver: Starting command(queryId=hive_20151124140909_e8cbb9bd-bac0-40b2-83d0-382de25b80d1): select count(*) from sample_0815/11/24 14:09:12 INFO ql.Driver: Query ID = hive_20151124140909_e8cbb9bd-bac0-40b2-83d0-382de25b80d115/11/24 14:09:12 INFO ql.Driver: Total jobs = 1...15/11/24 14:09:40 INFO ql.Driver: MapReduce Jobs Launched:15/11/24 14:09:40 INFO ql.Driver: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 3.58 sec HDFS Read: 52956 HDFS Write: 4 SUCCESS15/11/24 14:09:40 INFO ql.Driver: Total MapReduce CPU Time Spent: 3 seconds 580 msec15/11/24 14:09:40 INFO ql.Driver: OK</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithMr.java</file>
    </fixedFiles>
  </bug>
  <bug id="12515" opendate="2015-11-25 00:00:00" fixdate="2015-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean the SparkCounters related code after remove counter based stats collection[Spark Branch]</summary>
      <description>As SparkCounters is only used to collection stats, after HIVE-12411, we does not need it anymore.</description>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12517" opendate="2015-11-25 00:00:00" fixdate="2015-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline&amp;#39;s use of failed connection(s) causes failures and leaks.</summary>
      <description>Beeline adds a bad connection(s) to the connection list and makes it the current connection, so any subsequent queries will attempt to use this bad connection and will fail. Even a "!close" would not work.1) all queries fail unless !go is used.2) !closeall cannot close the active connections either.3) !exit will exit while attempting to establish these inactive connections without closing the active connections. So this could hold up server side resources.beeline&gt; !connect jdbc:hive2://localhost:10000 hive1 hive1scan complete in 8msConnecting to jdbc:hive2://localhost:10000Connected to: Apache Hive (version 2.0.0-SNAPSHOT)Driver: Hive JDBC (version 1.1.0-cdh5.7.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://localhost:10000&gt; !connect jdbc:hive2://localhost:10000 hive1 hive1Connecting to jdbc:hive2://localhost:10000Connected to: Apache Hive (version 2.0.0-SNAPSHOT)Driver: Hive JDBC (version 1.1.0-cdh5.7.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ1: jdbc:hive2://localhost:10000&gt; !connect jdbc:hive2://localhost:10000 hive1 hive1Connecting to jdbc:hive2://localhost:10000Connected to: Apache Hive (version 2.0.0-SNAPSHOT)Driver: Hive JDBC (version 1.1.0-cdh5.7.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ2: jdbc:hive2://localhost:10000&gt; !tables+------------+--------------+---------------------+-------------+----------+--+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS |+------------+--------------+---------------------+-------------+----------+--+| | default | char_nested_1 | TABLE | NULL || | default | src | TABLE | NULL || | default | char_nested_struct | TABLE | NULL || | default | src_thrift | TABLE | NULL || | default | x | TABLE | NULL |+------------+--------------+---------------------+-------------+----------+--+2: jdbc:hive2://localhost:10000&gt; !list3 active connections: #0 open jdbc:hive2://localhost:10000 #1 open jdbc:hive2://localhost:10000 #2 open jdbc:hive2://localhost:100002: jdbc:hive2://localhost:10000&gt; !connect jdbc:hive2://localhost:11000 hive1 hive1Connecting to jdbc:hive2://localhost:11000Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)3: jdbc:hive2://localhost:11000 (closed)&gt; !tablesError: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)3: jdbc:hive2://localhost:11000 (closed)&gt; !list4 active connections: #0 open jdbc:hive2://localhost:10000 #1 open jdbc:hive2://localhost:10000 #2 open jdbc:hive2://localhost:10000 #3 closed jdbc:hive2://localhost:110003: jdbc:hive2://localhost:11000 (closed)&gt; !closeError: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)3: jdbc:hive2://localhost:11000 (closed)&gt; !closeallError: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)4: jdbc:hive2://localhost:11000 (closed)&gt; !exitError: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)The workaround is to use !go to set the current connection to a "good" connection.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="12528" opendate="2015-11-25 00:00:00" fixdate="2015-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t start HS2 Tez sessions in a single thread</summary>
      <description>Starting sessions in parallel would improve the HS2 startup time.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12537" opendate="2015-11-28 00:00:00" fixdate="2015-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RLEv2 doesn&amp;#39;t seem to work</summary>
      <description>Perhaps I'm doing something wrong or is actually working as expected.Putting 1 million constant int32 values produces an ORC file of 1MB. Surprisingly, 1 million consecutive ints produces a much smaller file.Code and FileDump attached.ObjectInspector inspector = ObjectInspectorFactory.getReflectionObjectInspector( Integer.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);Writer w = OrcFile.createWriter(new Path("/tmp/my.orc"), OrcFile.writerOptions(new Configuration()) .compress(CompressionKind.NONE) .inspector(inspector) .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION) .version(OrcFile.Version.V_0_12) );for (int i = 0; i &lt; 1000000; ++i) { w.addRow(123);}w.close();</description>
      <version>0.14.0,1.0.1,1.1.1,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java.orig</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.java</file>
    </fixedFiles>
  </bug>
  <bug id="12542" opendate="2015-11-30 00:00:00" fixdate="2015-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HiveRelFactories</summary>
      <description>Calcite 1.5.0 introduced the use of RelFactories to create the operators. In particular, RelFactories contains the factories for all the operators in the system. Although we can still implement old rules by providing each individual factory (the constructor is deprecated, but it won't be removed till Calcite 2.0.0 is out), new rules will only provide constructors based on RelFactories. Thus, we propose to migrate immediately to the new interface.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveWindowingFixRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug id="12543" opendate="2015-11-30 00:00:00" fixdate="2015-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive ConstantPropagate optimizer when CBO has optimized the plan</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.when.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.grp.diff.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12545" opendate="2015-11-30 00:00:00" fixdate="2015-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sessionId and queryId logging support for methods like getCatalogs in HiveSessionImpl class</summary>
      <description>HIVE-11488 added queryId and sessionId logging support for execute() method in HiveSessionImpl while some other functions like getCatalogs(), getSchemas() are still not logging with queryId and sessionId for their logs.We should also support the same for them.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
    </fixedFiles>
  </bug>
  <bug id="12549" opendate="2015-12-1 00:00:00" fixdate="2015-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display execution engine in HS2 webui query view</summary>
      <description>As part of the query info, it would be useful to show the execution engine for the running query.</description>
      <version>2.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="12550" opendate="2015-12-1 00:00:00" fixdate="2015-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache and display last N completed queries in HS2 WebUI</summary>
      <description>Along with the in-flight queries, it would be nice to see the last N (configurable?) completed queries since the last process restart (I don't think this information needs to be persisted anywhere).</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12574" opendate="2015-12-2 00:00:00" fixdate="2015-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>windowing function returns incorrect result when the window size is larger than the partition size</summary>
      <description>In PTF windowing, when the partition is small and the window size is larger than the partition size, we are seeing incorrect result. It happens for max, min, first_value, last_value and sum functions. CREATE TABLE sdy1(ord int,type string);The data is:2 a3 a1 a The result is as follows for the query select ord, min(ord) over (partition by type order by ord rows between 1 preceding and 7 following)1 12 13 1 The expected result is:1 12 13 2</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12576" opendate="2015-12-2 00:00:00" fixdate="2015-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing license headers in preparation for the 2.0 release</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestRecordInspectorImpl.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestPluggableHiveSessionImpl.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroLazyObjectInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.UDFHelloTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MemoryDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprLongColumnLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprDoubleColumnDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.InPlaceUpdates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GlobalWorkMapFactory.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebApp.java</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.HiveStatementTest.java</file>
      <file type="M">itests.test-serde.src.main.java.org.apache.hadoop.hive.udf.example.GenericUDFExampleAdd.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingLayout.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.OperationLoggingAPITestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestCreateUdfEntities.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInEmbed.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestWarehousePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestSequenceValidator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTableSerializer.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.ClientException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.ConnectionException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.HeartbeatFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.HeartbeatTimerTask.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.LockException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.LockFailureListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClientBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.TableType.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.Transaction.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.TransactionException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.doc-files.system-overview.dot</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.HiveConfFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.UgiMetaStoreClientFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolver.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.GroupingValidator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.GroupRevisitedException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MetaStorePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.Mutator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinatorBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.OperationType.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.PartitionCreationException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.PartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspector.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspectorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordSequenceException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.SequenceValidator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.WarehousePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.WorkerException.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestHeartbeatTimerTask.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestLock.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestAcidTableSerializer.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestMutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestTransaction.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.ExampleUseCase.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.ReflectiveMutatorFactory.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingTestUtils.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestBucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestGroupingValidator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMetaStorePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12584" opendate="2015-12-3 00:00:00" fixdate="2015-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized join with partition column of type char does not trim spaces</summary>
      <description>When a table is partitioned on a column of type char and if join is performed on partitioned column then following exception gets thrown from hashtable loaderCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5 at org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(ObjectCache.java:82) at org.apache.hadoop.hive.ql.exec.tez.ObjectCache$1.call(ObjectCache.java:92) ... 4 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5 at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:216) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:293) at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:174) at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:170) at org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(ObjectCache.java:75) ... 5 moreCaused by: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5 at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$LazyBinaryKvWriter.sanityCheckKeyForTag(MapJoinBytesTableContainer.java:276) at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$LazyBinaryKvWriter.getHashFromKey(MapJoinBytesTableContainer.java:247) at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.internalPutRow(HybridHashTableContainer.java:451) at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.putRow(HybridHashTableContainer.java:444) at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:210)</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12591" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cache counters displays -ve value for CacheCapacityUsed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12597" opendate="2015-12-4 00:00:00" fixdate="2015-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP - allow using elevator without cache</summary>
      <description>Elevator is currently tied up with cache due to the way the memory is allocated. We should allow using elevator with the cache disabled.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12598" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable fileId when not supported</summary>
      <description>There is a TODO somewhere in code. We might get a synthetic fileId in absence of the real one in some cases when another FS masquerades as HDFS, we should be able to turn off fileID support explicitly for such cases as they are not bulletproof.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRunLengthIntegerReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRunLengthByteReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInStream.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestBitPack.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SettableUncompressedStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetadataReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">llap-server.src.test.resources.llap-daemon-site.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">data.conf.llap.llap-daemon-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12600" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make index tests more robust</summary>
      <description>There is some interference of index tests because of indices. My theory is some of indices are not dropped in tests and they show up in next one. So, made sure all indices are dropped in tests. Also, renamed index name in a particular test where I saw failure.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.in.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.index.stale.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.stale.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.in.db.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.auto.update.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.auto.mult.tables.compact.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.auto.mult.tables.q</file>
    </fixedFiles>
  </bug>
  <bug id="12609" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove javaXML serialization</summary>
      <description>We use kryo as default serializer and javaXML based serialization is not used in many places and is also not well tested. We should remove javaXML serialization and make kryo as the only serialization option.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.cast.qualified.types.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AggregationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12631" opendate="2015-12-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: support ORC ACID tables</summary>
      <description>LLAP uses a completely separate read path in ORC to allow for caching and parallelization of reads and processing. This path does not support ACID. As far as I remember ACID logic is embedded inside ORC format; we need to refactor it to be on top of some interface, if practical; or just port it to LLAP read path.Another consideration is how the logic will work with cache. The cache is currently low-level (CB-level in ORC), so we could just use it to read bases and deltas (deltas should be cached with higher priority) and merge as usual. We could also cache merged representation in future.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.LlapAwareSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12632" opendate="2015-12-9 00:00:00" fixdate="2015-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: don&amp;#39;t use IO elevator for ACID tables</summary>
      <description>Until HIVE-12631 is fixed, we need to avoid ACID tables in IO elevator. Right now, a FileNotFound error is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12633" opendate="2015-12-9 00:00:00" fixdate="2015-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: package included serde jars</summary>
      <description>Some SerDes like JSONSerde are not packaged with LLAP. One cannot localize jars on the daemon (due to security consideration if nothing else), so we should package them.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12643" opendate="2015-12-10 00:00:00" fixdate="2015-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For self describing InputFormat don&amp;#39;t replicate schema information in partitions</summary>
      <description>Since self describing Input Formats don't use individual partition schemas for schema resolution, there is no need to send that info to tasks.Doing this should cut down plan size.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.tblproperty.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12658" opendate="2015-12-11 00:00:00" fixdate="2015-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task rejection by an llap daemon spams the log with RejectedExecutionExceptions</summary>
      <description>The execution queue throws a RejectedExecutionException - which is logged by the hadoop IPC layer.Instead of relying on an Exception in the protocol - move to sending back an explicit response to indicate a rejected fragment.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="12687" opendate="2015-12-16 00:00:00" fixdate="2015-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Workdirs need to default to YARN local</summary>
      <description>LLAP_DAEMON_WORK_DIRS("hive.llap.daemon.work.dirs", ""is a bad default &amp; fails at startup if not overridden.A better default would be to fall back onto YARN local dirs if this is not configured.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12688" opendate="2015-12-16 00:00:00" fixdate="2015-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-11826 makes hive unusable in properly secured cluster</summary>
      <description>HIVE-11826 makes a change to restrict connections to metastore to users who belong to groups under 'hadoop.proxyuser.hive.groups'.That property was only a meant to be a hadoop property, which controls what users the hive user can impersonate. What this change is doing is to enable use of that to also restrict who can connect to metastore server. This is new functionality, not a bug fix. There is value to this functionality.However, this change makes hive unusable in a properly secured cluster. If 'hadoop.proxyuser.hive.hosts' is set to the proper set of hosts that run Metastore and Hiveserver2 (instead of a very open "*"), then users will be able to connect to metastore only from those hosts.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
    </fixedFiles>
  </bug>
  <bug id="12692" opendate="2015-12-16 00:00:00" fixdate="2015-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make use of the Tez HadoopShim in TaskRunner usage</summary>
      <description>TEZ-2910 adds shims for Hadoop to make use of caller context and other changing hadoop APIs. Hive usage of TezTaskRunner needs to work with this.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12693" opendate="2015-12-16 00:00:00" fixdate="2015-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use Slider Anti-Affinity scheduling mode for daemon distribution</summary>
      <description>Slider has SLIDER-82 which adds anti-affinity placement policies for containers, to avoid colliding on to the same machine when deploying LLAP instances.NO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="12694" opendate="2015-12-16 00:00:00" fixdate="2015-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Slider destroy semantics require force</summary>
      <description>2015-12-16 20:10:55,118 [main] ERROR main.ServiceLauncher - Destroy will permanently delete directories and registries. Reissue this command with the --force option if you want to proceed.NO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="12697" opendate="2015-12-16 00:00:00" fixdate="2015-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated post option from webhcat test files</summary>
      <description>Tests are still having the deprecated post option user.name. Need to remove them and add the same to query stringSubmitting user.name as form parameter in POST method was deprecated in Hive 0.13</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.modifyConfiguration.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.streaming.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission2.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.ddl.conf</file>
    </fixedFiles>
  </bug>
  <bug id="12699" opendate="2015-12-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: hive.llap.daemon.work.dirs setting backward compat name doesn&amp;#39;t work</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="127" opendate="2008-12-5 00:00:00" fixdate="2008-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fetch task does not pass the job to the serde in initialize</summary>
      <description>Fetch task passes a null job to the serde during initialize - it should not assume that the serde does not need the job.This assumption is valid for MetadataTypedColumnSetSerde, but need not be valid always</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1270" opendate="2010-3-23 00:00:00" fixdate="2010-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thread pool size in Thrift metastore server should be configurable</summary>
      <description>Currently, the worker thread pool size in the Thrift metastore server is fixed to have a minimum of 200 threads and no maximum. These limits should be set through a configuration variable.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12708" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark doesn&amp;#39;t work with Kerboresed HBase [Spark Branch]</summary>
      <description>Spark application launcher (spark-submit) acquires HBase delegation token on Hive user's behalf when the application is launched. This mechanism, which doesn't work for long-running sessions, is not in line with what Hive is doing. Hive actually acquires the token automatically whenever a job needs it. The right approach for Spark should be allowing applications to dynamically add whatever tokens they need to the spark context. While this needs work on Spark side, we provide a workaround solution in Hive.</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12724" opendate="2015-12-21 00:00:00" fixdate="2015-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Major compaction fails to include the original bucket files into MR job</summary>
      <description>How the problem happens: Create a non-ACID table Before non-ACID to ACID table conversion, we inserted row one After non-ACID to ACID table conversion, we inserted row two Both rows can be retrieved before MAJOR compaction After MAJOR compaction, row one is losthive&gt; USE acidtest;OKTime taken: 0.77 secondshive&gt; CREATE TABLE t1 (nationkey INT, name STRING, regionkey INT, comment STRING) &gt; CLUSTERED BY (regionkey) INTO 2 BUCKETS &gt; STORED AS ORC;OKTime taken: 0.179 secondshive&gt; DESC FORMATTED t1;OK# col_name data_type commentnationkey intname stringregionkey intcomment string# Detailed Table InformationDatabase: acidtestOwner: wzhengCreateTime: Mon Dec 14 15:50:40 PST 2015LastAccessTime: UNKNOWNRetention: 0Location: file:/Users/wzheng/hivetmp/warehouse/acidtest.db/t1Table Type: MANAGED_TABLETable Parameters: transient_lastDdlTime 1450137040# Storage InformationSerDe Library: org.apache.hadoop.hive.ql.io.orc.OrcSerdeInputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormatCompressed: NoNum Buckets: 2Bucket Columns: [regionkey]Sort Columns: []Storage Desc Params: serialization.format 1Time taken: 0.198 seconds, Fetched: 28 row(s)hive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db;Found 1 itemsdrwxr-xr-x - wzheng staff 68 2015-12-14 15:50 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1hive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;hive&gt; INSERT INTO TABLE t1 VALUES (1, 'USA', 1, 'united states');WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.Query ID = wzheng_20151214155028_630098c6-605f-4e7e-a797-6b49fb48360dTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 2In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Job running in-process (local Hadoop)2015-12-14 15:51:58,070 Stage-1 map = 100%, reduce = 100%Ended Job = job_local73977356_0001Loading data to table acidtest.t1MapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKTime taken: 2.825 secondshive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;Found 2 items-rwxr-xr-x 1 wzheng staff 112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0-rwxr-xr-x 1 wzheng staff 472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0hive&gt; SELECT * FROM t1;OK1 USA 1 united statesTime taken: 0.434 seconds, Fetched: 1 row(s)hive&gt; ALTER TABLE t1 SET TBLPROPERTIES ('transactional' = 'true');OKTime taken: 0.071 secondshive&gt; DESC FORMATTED t1;OK# col_name data_type commentnationkey intname stringregionkey intcomment string# Detailed Table InformationDatabase: acidtestOwner: wzhengCreateTime: Mon Dec 14 15:50:40 PST 2015LastAccessTime: UNKNOWNRetention: 0Location: file:/Users/wzheng/hivetmp/warehouse/acidtest.db/t1Table Type: MANAGED_TABLETable Parameters: COLUMN_STATS_ACCURATE false last_modified_by wzheng last_modified_time 1450137141 numFiles 2 numRows -1 rawDataSize -1 totalSize 584 transactional true transient_lastDdlTime 1450137141# Storage InformationSerDe Library: org.apache.hadoop.hive.ql.io.orc.OrcSerdeInputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormatCompressed: NoNum Buckets: 2Bucket Columns: [regionkey]Sort Columns: []Storage Desc Params: serialization.format 1Time taken: 0.049 seconds, Fetched: 36 row(s)hive&gt; set hive.support.concurrency=true;hive&gt; set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;hive&gt; set hive.compactor.initiator.on=true;hive&gt; set hive.compactor.worker.threads=5;hive&gt; set hive.exec.dynamic.partition.mode=nonstrict;hive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;Found 2 items-rwxr-xr-x 1 wzheng staff 112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0-rwxr-xr-x 1 wzheng staff 472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0hive&gt; INSERT INTO TABLE t1 VALUES (2, 'Canada', 1, 'maple leaf');WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.Query ID = wzheng_20151214155028_630098c6-605f-4e7e-a797-6b49fb48360dTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 2In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Job running in-process (local Hadoop)2015-12-14 15:54:18,943 Stage-1 map = 100%, reduce = 100%Ended Job = job_local1674014367_0002Loading data to table acidtest.t1MapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKTime taken: 1.995 secondshive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1;Found 3 items-rwxr-xr-x 1 wzheng staff 112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0-rwxr-xr-x 1 wzheng staff 472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0drwxr-xr-x - wzheng staff 204 2015-12-14 15:54 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000hive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000;Found 2 items-rw-r--r-- 1 wzheng staff 214 2015-12-14 15:54 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000/bucket_00000-rw-r--r-- 1 wzheng staff 797 2015-12-14 15:54 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/delta_0000007_0000007_0000/bucket_00001hive&gt; SELECT * FROM t1;OK1 USA 1 united states2 Canada 1 maple leafTime taken: 0.1 seconds, Fetched: 2 row(s)hive&gt; ALTER TABLE t1 COMPACT 'MAJOR';Compaction enqueued.OKTime taken: 0.026 secondshive&gt; show compactions;OKDatabase Table Partition Type State Worker Start TimeTime taken: 0.022 seconds, Fetched: 1 row(s)hive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/;Found 3 items-rwxr-xr-x 1 wzheng staff 112 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000000_0-rwxr-xr-x 1 wzheng staff 472 2015-12-14 15:51 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/000001_0drwxr-xr-x - wzheng staff 204 2015-12-14 15:55 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007hive&gt; dfs -ls /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007;Found 2 items-rw-r--r-- 1 wzheng staff 222 2015-12-14 15:55 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007/bucket_00000-rw-r--r-- 1 wzheng staff 802 2015-12-14 15:55 /Users/wzheng/hivetmp/warehouse/acidtest.db/t1/base_0000007/bucket_00001hive&gt; select * from t1;OK2 Canada 1 maple leafTime taken: 0.396 seconds, Fetched: 1 row(s)hive&gt; select count(*) from t1;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.Query ID = wzheng_20151214155028_630098c6-605f-4e7e-a797-6b49fb48360dTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Job running in-process (local Hadoop)2015-12-14 15:56:20,277 Stage-1 map = 100%, reduce = 100%Ended Job = job_local1720993786_0003MapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOK1Time taken: 1.623 seconds, Fetched: 1 row(s)Note, the cleanup doesn't kick in because the compaction fails already. The cleanup itself doesn't have any problem (at least not that we know of for this case).</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12726" opendate="2015-12-22 00:00:00" fixdate="2015-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>exclude hadoop jars from webhcat hcatalog/share/webhcat/svr/lib</summary>
      <description>this is only an issue in 2.0.As far as I can tell the only change between brach-1 and master is the removal of hadoop-2/1 profiles which is where hadoop-auth dependency was specified for webhcat</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12734" opendate="2015-12-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundancy in HiveConfs serialized to UDFContext</summary>
      <description>HCatLoader lands up serializing one HiveConf instance per table-alias, to Pig's UDFContext. This lands up bloating the UDFContext.To reduce the footprint, it makes sense to serialize a default-constructed HiveConf once, and one "diff" per HCatLoader. This should reduce the time taken to kick off jobs from pig -useHCatalog scripts.(Note_to_self: YHIVE-540).</description>
      <version>1.2.1,2.0.0,2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12739" opendate="2015-12-23 00:00:00" fixdate="2015-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log4j2.x needs HADOOP_USER_CLASSPATH_FIRST to be set</summary>
      <description>To load log4j2 jars properly HADOOP_USER_CLASSPATH_FIRST needs to set to true. We should set this in hive script so that users don't have to do it explicitly.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="1274" opendate="2010-3-23 00:00:00" fixdate="2010-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bug in sort merge join if the big table does not have any row</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12743" opendate="2015-12-24 00:00:00" fixdate="2015-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RCFileInputFormat needs to be registered with kryo</summary>
      <description>Ran into an issue with union distinct query that uses RCFile table with the following exceptionCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.RCFileInputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="1276" opendate="2010-3-24 00:00:00" fixdate="2010-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize bucketing</summary>
      <description>If the query results are already clustered by the bucketing column, there is no need for another map-reduce job.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12762" opendate="2015-12-30 00:00:00" fixdate="2015-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Common join on parquet tables returns incorrect result when hive.optimize.index.filter set to true</summary>
      <description>The following query will give incorrect result.CREATE TABLE tbl1(id INT) STORED AS PARQUET;INSERT INTO tbl1 VALUES(1), (2);CREATE TABLE tbl2(id INT, value STRING) STORED AS PARQUET;INSERT INTO tbl2 VALUES(1, 'value1');INSERT INTO tbl2 VALUES(1, 'value2');set hive.optimize.index.filter = true;set hive.auto.convert.join=false;select tbl1.id, t1.value, t2.valueFROM tbl1JOIN (SELECT * FROM tbl2 WHERE value='value1') t1 ON tbl1.id=t1.idJOIN (SELECT * FROM tbl2 WHERE value='value2') t2 ON tbl1.id=t2.id;We are enforcing to use common join and tbl2 will have 2 files after 2 insertions underneath.the map job contains 3 TableScan operators (2 for tbl2 and 1 for tbl1). When hive.optimize.index.filter is set to true, we are incorrectly applying the later filtering condition to each block, which causes no data is returned for the subquery SELECT * FROM tbl2 WHERE value='value1'.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.ExpressionTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="12768" opendate="2015-12-31 00:00:00" fixdate="2015-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thread safety: binary sortable serde decimal deserialization</summary>
      <description>We see thread safety issues due to static decimal buffer in binary sortable serde.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="12769" opendate="2015-12-31 00:00:00" fixdate="2015-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Pick up java_home from System properties for Slider</summary>
      <description>Not setting up JAVA_HOME env var while running --service LLAP can be an easy mistake to make.This can default to the same JDK that is used by bin/hive binary, if there are no user overrides present.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12783" opendate="2016-1-5 00:00:00" fixdate="2016-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix the unit test failures in TestSparkClient and TestSparkSessionManagerImpl</summary>
      <description>This includesorg.apache.hive.spark.client.TestSparkClient.testSyncRpcorg.apache.hive.spark.client.TestSparkClient.testJobSubmissionorg.apache.hive.spark.client.TestSparkClient.testMetricsCollectionorg.apache.hive.spark.client.TestSparkClient.testCountersorg.apache.hive.spark.client.TestSparkClient.testRemoteClientorg.apache.hive.spark.client.TestSparkClient.testAddJarsAndFilesorg.apache.hive.spark.client.TestSparkClient.testSimpleSparkJoborg.apache.hive.spark.client.TestSparkClient.testErrorJoborg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUseorg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUseall of them passed on my laptop. cc'ing szehon, xuefuz, could you please take a look? Shall we ignore them? Thanks.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">orc.pom.xml</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12786" opendate="2016-1-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO may fail for recoverable errors</summary>
      <description>In some cases, CBO may generate an error from which it may be possible to recover.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.InvalidValueBoundary.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12799" opendate="2016-1-7 00:00:00" fixdate="2016-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Always use Schema Evolution for ACID</summary>
      <description>Always use Schema Evolution for ACID &amp;#8211; ignore hive.exec.schema.evolution setting.</description>
      <version>None</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.fetchwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion3.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns3.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.reorder.columns2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.reorder.columns1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.change.serde.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.change.fileformat.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12802" opendate="2016-1-7 00:00:00" fixdate="2016-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver.vector_join_filters.q failure</summary>
      <description>Discovered as part of running :mvn test -Dtest=TestMiniTezCliDriver -Dqfile_regex=vector.* -Dhive.cbo.returnpath.hiveop=true -Dtest.output.overwrite=trueSELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key &gt; 40 AND a.value &gt; 50 AND a.key = a.value AND b.key &gt; 40 AND b.value &gt; 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key &gt; 40 AND c.value &gt; 50 AND c.key = c.value AND b.key &gt; 40 AND b.value &gt; 50 AND b.key = b.value)2016-01-07T11:16:06,198 ERROR [657fd759-7643-467b-9bd0-17cb4958cb69 main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(309)) - CBO failed, skipping CBO.java.lang.IndexOutOfBoundsException: index (10) must be less than size (6) at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:?] at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:?] at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:?] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitInputRef(ExprNodeConverter.java:109) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitInputRef(ExprNodeConverter.java:79) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) ~[calcite-core-1.5.0.jar:1.5.0] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitCall(ExprNodeConverter.java:128) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitCall(ExprNodeConverter.java:79) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.calcite.rex.RexCall.accept(RexCall.java:107) ~[calcite-core-1.5.0.jar:1.5.0] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.convertToExprNode(HiveOpConverter.java:1153) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.translateJoin(HiveOpConverter.java:381) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.visit(HiveOpConverter.java:313) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.dispatch(HiveOpConverter.java:164) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.visit(HiveOpConverter.java:268) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.dispatch(HiveOpConverter.java:162) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.visit(HiveOpConverter.java:397) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.dispatch(HiveOpConverter.java:181) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.convert(HiveOpConverter.java:154) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedHiveOPDag(CalcitePlanner.java:688) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:266) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10094) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:231) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:471) [hive-exec-2.1.0-SNAPSHOT.jar:?]</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12805" opendate="2016-1-7 00:00:00" fixdate="2016-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver skewjoin.q failure</summary>
      <description>Set hive.cbo.returnpath.hiveop=trueFROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))The stack trace:java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:635) at java.util.ArrayList.get(ArrayList.java:411) at org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate$JoinSynthetic.process(SyntheticJoinPredicate.java:183) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:43) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) at org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.transform(SyntheticJoinPredicate.java:100) at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:236) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10170) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:231) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:471)Same error happens in auto_sortmerge_join_6.q.out for select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="12809" opendate="2016-1-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: fast-path for coalesce if input.noNulls = true</summary>
      <description>Coalesce can skip processing other columns, if all the input columns are non-null.Possibly retaining, isRepeating=true.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
    </fixedFiles>
  </bug>
  <bug id="12820" opendate="2016-1-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the check if carriage return and new line are used for separator or escape character</summary>
      <description>The change in HIVE-11785 doesn't allow \r or \n to be used as separator or escape character which may break some existing tables which uses \r as separator or escape character e.g..This case actually can be supported regardless of SERIALIZATION_ESCAPE_CRLF set or not.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="12824" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO doesnt get triggered when aggregate function is used within windowing function</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.gby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query70.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query67.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query51.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
    </fixedFiles>
  </bug>
  <bug id="12826" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: fix VectorUDAF* suspect isNull checks</summary>
      <description>for isRepeating=true, checking isNull[selected&amp;#91;i&amp;#93;] might return incorrect results (without a heavy array fill of isNull).VectorUDAFSum/Min/Max/Avg and SumDecimal impls need to be reviewed for this pattern. private void iterateHasNullsRepeatingSelectionWithAggregationSelection( VectorAggregationBufferRow[] aggregationBufferSets, int aggregateIndex, &lt;ValueType&gt; value, int batchSize, int[] selection, boolean[] isNull) { for (int i=0; i &lt; batchSize; ++i) { if (!isNull[selection[i]]) { Aggregation myagg = getCurrentAggregationBuffer( aggregationBufferSets, aggregateIndex, i); myagg.sumValue(value); } } }</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12827" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: VectorCopyRow/VectorAssignRow/VectorDeserializeRow assign needs explicit isNull[offset] modification</summary>
      <description>Some scenarios do set Double.NaN instead of isNull=true, but all types aren't consistent.Examples of un-set isNull for the valid values are private class FloatReader extends AbstractDoubleReader { FloatReader(int columnIndex) { super(columnIndex); } @Override void apply(VectorizedRowBatch batch, int batchIndex) throws IOException { DoubleColumnVector colVector = (DoubleColumnVector) batch.cols[columnIndex]; if (deserializeRead.readCheckNull()) { VectorizedBatchUtil.setNullColIsNullValue(colVector, batchIndex); } else { float value = deserializeRead.readFloat(); colVector.vector[batchIndex] = (double) value; } } } private class DoubleCopyRow extends CopyRow { DoubleCopyRow(int inColumnIndex, int outColumnIndex) { super(inColumnIndex, outColumnIndex); } @Override void copy(VectorizedRowBatch inBatch, int inBatchIndex, VectorizedRowBatch outBatch, int outBatchIndex) { DoubleColumnVector inColVector = (DoubleColumnVector) inBatch.cols[inColumnIndex]; DoubleColumnVector outColVector = (DoubleColumnVector) outBatch.cols[outColumnIndex]; if (inColVector.isRepeating) { if (inColVector.noNulls || !inColVector.isNull[0]) { outColVector.vector[outBatchIndex] = inColVector.vector[0]; } else { VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex); } } else { if (inColVector.noNulls || !inColVector.isNull[inBatchIndex]) { outColVector.vector[outBatchIndex] = inColVector.vector[inBatchIndex]; } else { VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex); } } } } private static abstract class VectorDoubleColumnAssign extends VectorColumnAssignVectorBase&lt;DoubleColumnVector&gt; { protected void assignDouble(double value, int destIndex) { outCol.vector[destIndex] = value; } }The pattern to imitate would be the earlier code from VectorBatchUtil case DOUBLE: { DoubleColumnVector dcv = (DoubleColumnVector) batch.cols[offset + colIndex]; if (writableCol != null) { dcv.vector[rowIndex] = ((DoubleWritable) writableCol).get(); dcv.isNull[rowIndex] = false; } else { dcv.vector[rowIndex] = Double.NaN; setNullColIsNullValue(dcv, rowIndex); } } break;</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12828" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Spark version to 1.6</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12839" opendate="2016-1-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to Calcite 1.6</summary>
      <description>CLEAR LIBRARY CACHEUpgrade Hive to Calcite 1.6.0-incubating.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdUniqueKeys.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdMemory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdCollation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.FilterSelectivityEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortUnionReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveRelMdCost.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveDefaultCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12864" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverflowError parsing queries with very large predicates</summary>
      <description>We have seen that queries with very large predicates might fail with the following stacktrace:016-01-12 05:47:36,516|beaver.machine|INFO|552|5072|Thread-22|Exception in thread "main" java.lang.StackOverflowError2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:145)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,634|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:37,582|main|INFO|552|4568|MainThread|TEST "test_WideQuery" FAILED in 10.95 secondsThe problem could be solved by reimplementing some of the parsing methods so they are iterative instead of recursive.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="12880" opendate="2016-1-15 00:00:00" fixdate="2016-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark-assembly causes Hive class version problems</summary>
      <description>It looks like spark-assembly contains versions of Hive classes (e.g. HiveConf), and these sometimes (always?) come from older versions of Hive.We've seen problems where depending on classpath perturbations, NoSuchField errors may be thrown for recently added ConfVars because the HiveConf class comes from spark-assembly.Would making sure spark-assembly comes last in the classpath solve the problem?Otherwise, can we depend on something that does not package older Hive classes?Currently, HIVE-12179 provides a workaround (in non-Spark use case, at least; I am assuming this issue can also affect Hive-on-Spark).</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="12890" opendate="2016-1-19 00:00:00" fixdate="2016-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable multi-statment transaction control statements until HIVE-11078</summary>
      <description>HIVE-11077 added support for begin transaction/commit/rollback but the feature is not complete w/o HIVE-11078. Need to disable these statements to prevent user confusion.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12891" opendate="2016-1-20 00:00:00" fixdate="2016-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive fails when java.io.tmpdir is set to a relative location</summary>
      <description>The function SessionState.createSessionDirs fails when trying to create directories where java.io.tmpdir is set to a relative location.\[SubtaskRunner] ERROR o.a.h.hive..ql.Driver - FAILED: IllegalArgumentException java.net.URISyntaxException: Relative path in absolute URI: file:./tmp/&lt;user&gt;/&lt;guid&gt;/hive_2015_12_11_09-12-25_352_4325234652356-1...Minor variations:\[SubtaskRunner] ERROR o.a.h.hive..ql.Driver - FAILED: SemanticException Exception while processing Exception while writing out the local file o.a.h.hive.ql/parse.SemanticException: Exception while processing exception while writing out local file ... caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./tmp/&lt;user&gt;/&lt;guid&gt;/hive_2015_12_11_09-12-25_352_4325234652356-1 at o.a.h.fs.Path.initialize (206) at o.a.h.fs.Path.&lt;init&gt;(197)... at o.a.h.hive.ql.context.getScratchDir(267)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.SystemVariables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12905" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Issue with mapjoin in tez under certain conditions</summary>
      <description>In a specific case where we have an outer join followed by another join on the same key and the non-outer side of the outer join is empty, hive-on-tez produces incorrect results.</description>
      <version>1.0.1,1.2.1,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12907" opendate="2016-1-22 00:00:00" fixdate="2016-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading - II</summary>
      <description>Remove unnecessary calls to metastore.</description>
      <version>0.14.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug id="12908" opendate="2016-1-22 00:00:00" fixdate="2016-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading III</summary>
      <description>Remove unnecessary Namenode calls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12911" opendate="2016-1-22 00:00:00" fixdate="2016-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD might get exercised even when flag is false if CBO is on</summary>
      <description>Introduced in HIVE-11865.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12915" opendate="2016-1-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez session pool has concurrency issues during init</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12927" opendate="2016-1-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase metastore: sequences should be one per row, not all in one row</summary>
      <description>long getNextSequence(byte[] sequence) throws IOException {Is not safe in presence of any concurrency. It should use HBase increment API.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="12931" opendate="2016-1-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shuffle tokens stay around forever in LLAP</summary>
      <description>Shuffle tokens are never cleaned up, resulting in a slow leak.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12933" opendate="2016-1-26 00:00:00" fixdate="2016-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline will hang when authenticating with PAM when libjpam.so is missing</summary>
      <description>When we setup PAM authentication, we need to have libjpam.so under java.library.path. If it happens to misplace the .so file, rather than giving an exception, the client will hang forever.Seems we should catch the exception when the lib is missing.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.PamAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12934" opendate="2016-1-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor llap module structure to allow for a usable client</summary>
      <description>The client isn't really usable at the moment, and all of the code resides in the llap-server module. Restructure this so that the daemon execution code and cache code remains in server, common components move to a different module and relevant client pieces sit in the client module.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.tez.dag.app.rm.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapDaemonProtocolClientProxy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.ContainerFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapUmbilicalPolicyProvider.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapDaemonProtocolClientProxy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapContainerLauncher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.Converters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapServerSecurityInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapDaemonPolicyProvider.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.LlapNodeId.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.LlapManagementProtocolBlockingPB.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.LlapDaemonProtocolBlockingPB.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapManagementProtocolClientImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenProvider.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12935" opendate="2016-1-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Replace Yarn registry with Zookeeper registry</summary>
      <description>Existing YARN registry service for cluster membership has to depend on refresh intervals to get the list of instances/daemons that are running in the cluster. Better approach would be replace it with zookeeper based registry service so that custom listeners can be added to update healthiness of daemons in the cluster.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12945" opendate="2016-1-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning: bucketing for -ve hashcodes have historical issues</summary>
      <description>The different ETL pathways differed in reducer choice slightly for -ve hashcodes.(hashCode &amp; Integer.MAX_VALUE) % numberOfBuckets;!=Math.abs(hashCode) % numberOfBucketsAdd a backwards compat option, which can be used to protect against old data left over from 0.13.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketpruning1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12959" opendate="2016-1-28 00:00:00" fixdate="2016-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add task scheduler timeout when no nodes are alive</summary>
      <description>When there are no llap daemons running task scheduler should have a timeout to fail the query instead of waiting forever.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12964" opendate="2016-1-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestOperationLoggingAPIWithMr,TestOperationLoggingAPIWithTez fail on branch-2.0 (with Java 7, at least)</summary>
      <description></description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug id="12965" opendate="2016-1-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite local directory should perserve the overwritten directory permission</summary>
      <description>In Hive, "insert overwrite local directory" first deletes the overwritten directory if exists, recreate a new one, then copy the files from src directory to the new local directory. This process sometimes changes the permissions of the to-be-overwritten local directory, therefore causing some applications no more to be able to access its content.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="12971" opendate="2016-1-31 00:00:00" fixdate="2016-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Support for Kudu</summary>
      <description>JIRA for tracking work related to Hive/Kudu integration.It would be useful to allow Kudu data to be accessible via Hive. This would involve creating a Kudu SerDe/StorageHandler and implementing support for QUERY and DML commands like SELECT, INSERT, UPDATE, and DELETE. Kudu Input/OutputFormats classes already exist. The work can be staged to support this functionality incrementally.</description>
      <version>2.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.service.AsyncTaskCopyAuxJars.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestMiniClusters.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12983" opendate="2016-2-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a builtin function to get Hive version</summary>
      <description>It would be nice to have a builtin function that would return the Hive version. This would make it easier for a users and tests to programmatically check the Hive version in a SQL script. It's also useful so a client can check the Hive version on a remote cluster.For example:beeline&gt; SELECT version();2.1.0-SNAPSHOT r208ab352311a6cbbcd1f7fcd40964da2dbc6703d</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="12985" opendate="2016-2-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>distribution configs are wrong and out of date</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12987" opendate="2016-2-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics for HS2 active users and SQL operations</summary>
      <description>HIVE-12271 added metrics for all HS2 operations. Sometimes, users are also interested in metrics just for SQL operations.It is useful to track active user count as well.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="12988" opendate="2016-2-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading IV</summary>
      <description>Parallelize copyFiles()</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12993" opendate="2016-2-3 00:00:00" fixdate="2016-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>user and password supplied from URL is overwritten by the empty user and password of the JDBC connection string when it&amp;#39;s calling from beeline</summary>
      <description>When we make the call beeline -u "jdbc:hive2://localhost:10000/;user=aaa;password=bbb", the user and password are overwritten by the blank ones since internally it constructs a "connect &lt;url&gt; '' '' &lt;driver&gt;" call with empty user and password.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="12996" opendate="2016-2-3 00:00:00" fixdate="2016-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp tables shouldn&amp;#39;t be locked</summary>
      <description>Internally, INSERT INTO ... VALUES statements use temp table to accomplish its functionality. But temp tables shouldn't be stored in the metastore tables for ACID, because they are by definition only visible inside the session that created them, and we don't allow multiple threads inside a session. If a temp table is used in a query, it should be ignored by lock manager.mysql&gt; select * from COMPLETED_TXN_COMPONENTS;+-----------+--------------+-----------------------+------------------+| CTC_TXNID | CTC_DATABASE | CTC_TABLE | CTC_PARTITION |+-----------+--------------+-----------------------+------------------+| 1 | acid | t1 | NULL || 1 | acid | values__tmp__table__1 | NULL || 2 | acid | t1 | NULL || 2 | acid | values__tmp__table__2 | NULL || 3 | acid | values__tmp__table__3 | NULL || 3 | acid | t1 | NULL || 4 | acid | values__tmp__table__1 | NULL || 4 | acid | t2p | ds=today || 5 | acid | values__tmp__table__1 | NULL || 5 | acid | t3p | ds=today/hour=12 |+-----------+--------------+-----------------------+------------------+</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1300" opendate="2010-4-12 00:00:00" fixdate="2010-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support: alter table touch partition</summary>
      <description>In some cases, the user wants to touch a partition, since some other operations might be performed on the hdfs directories.Currently, there is no way to do that.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13025" opendate="2016-2-8 00:00:00" fixdate="2016-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>need a better error message for when one needs to run schematool</summary>
      <description>Might as well fix it, since the RC is sunk and this was not obvious to the people testing it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="13032" opendate="2016-2-9 00:00:00" fixdate="2016-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive services need HADOOP_CLIENT_OPTS for proper log4j2 initialization</summary>
      <description>HIVE-12497 removed HADOOP_CLIENT_OPTS as it slowed down cli launch time. But it leads to log4j2 not being initialized when using services other than CLI. Other services like metastore, schematool etc. rely on log4j to initialize the logging based on the presence of log4j2.properties file in the classpath. If we use the standard name for log4j configuration file (log4j2.properties) then automatic initialization will happen. If not, we have to tell log4j to look for specific properties file. This is done via -Dlog4j.configurationFile system property. If we pass this system property via HADOOP_CLIENT_OPTS then all hive services will have logging initialized properly. In HIVE-12497, the problem was we had HADOOP_CLIENT_OPTS at the top of the script. As a result, hadoop and hbase commands tries to initialize logging which took long time slowing down the startup time.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.schemaTool.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13033" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SPDO unnecessarily duplicates columns in key &amp; value of mapper output</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13036" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split hive.root.logger separately to make it compatible with log4j1.x (for remaining services)</summary>
      <description>Similar to HIVE-12402 but for HS2 and metastore this time.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.CommonCliOptions.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13039" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BETWEEN predicate is not functioning correctly with predicate pushdown on Parquet table</summary>
      <description>BETWEEN becomes exclusive in parquet table when predicate pushdown is on (as it is by default in newer Hive versions). To reproduce(in a cluster, not local setup):CREATE TABLE parquet_tbl( key int, ldate string) PARTITIONED BY ( lyear string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';insert overwrite table parquet_tbl partition (lyear='2016') select 1, '2016-02-03' from src limit 1;set hive.optimize.ppd.storage = true;set hive.optimize.ppd = true;select * from parquet_tbl where ldate between '2016-02-03' and '2016-02-03';No row will be returned in a cluster.But if you turn off hive.optimize.ppd, one row will be returned.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.FilterPredicateLeafBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="13040" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle empty bucket creations more efficiently</summary>
      <description></description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.test.results.clientpositive.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13043" opendate="2016-2-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reload function has no impact to function registry</summary>
      <description>With HIVE-2573, users should run "reload function" to refresh cached function registry. However, "reload function" has no impact at all. We need to fix this. Otherwise, HS2 needs to be restarted to see new global functions.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13095" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support view column authorization</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13096" opendate="2016-2-19 00:00:00" fixdate="2016-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cost to choose side table in MapJoin conversion based on cumulative cardinality</summary>
      <description>HIVE-11954 changed the logic to choose the side table in the MapJoin conversion algorithm. Initial heuristic for the cost was based on number of heavyweight operators.This extends that work so the heuristic is based on accumulate cardinality. In the future, we should choose the side based on total latency for the input.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="13097" opendate="2016-2-19 00:00:00" fixdate="2016-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Umbrella] Changes dependent on Tez 0.8.3</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13099" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-SQLOperations lead to Web UI NPE</summary>
      <description>To support display of live operations in the WebUI, we record SQLOperations (in liveSqlOperations). However, to support historic operations, we save all operations in historicSqlOperations, including non-SQLOperations which do not have display entries in liveSqlOperations.This leads to a race condition depending on whether sessions use non-sql operations. Reproduce-able by issuing a 'set' operation.java.lang.NullPointerException at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:131) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)We should save only SQLOperations in historicSqlOperations.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="131" opendate="2008-12-7 00:00:00" fixdate="2008-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert overwrite directory leaves behind uncommitted/tmp files from failed tasks</summary>
      <description>_tmp files are getting left behind on insert overwrite directory:/user/jssarma/ctst1/40422_m_000195_0.deflate &lt;r 3&gt; 13285 2008-12-07 01:47 rw-r-r- jssarma supergroup/user/jssarma/ctst1/40422_m_000196_0.deflate &lt;r 3&gt; 3055 2008-12-07 01:46 rw-r-r- jssarma supergroup/user/jssarma/ctst1/_tmp.40422_m_000033_0 &lt;r 3&gt; 0 2008-12-07 01:53 rw-r-r- jssarma supergroup/user/jssarma/ctst1/_tmp.40422_m_000037_1 &lt;r 3&gt; 0 2008-12-07 01:53 rw-r-r- jssarma supergroupthis happened with speculative execution. the code looks good (in fact in this case many speculative tasks were launched - and only a couple caused problems). Almost seems like these files did not appear in the namespace until after the map-reduce job finished and the movetask did a listing of the output dir ..</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1310" opendate="2010-4-14 00:00:00" fixdate="2010-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioning columns should be of primitive types only</summary>
      <description>If the user specify the partitioning column as complex type (e.g., array, map) we should throw an error in semantic analyzer.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="13105" opendate="2016-2-20 00:00:00" fixdate="2016-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP token hashCode and equals methods are incorrect</summary>
      <description>I had wrong assumptions about object vs functional equality. This would need to go to 2.0.1 (target version field is AWOL)"Luckily" the implications are spurious access denied errors, and not the other way around.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
    </fixedFiles>
  </bug>
  <bug id="13107" opendate="2016-2-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Rotate GC logs periodically to prevent full disks</summary>
      <description>STDOUT cannot be rotated easily, so log GC logs to a different file and rotate periodically with -XX:+UseGCLogFileRotationNO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13108" opendate="2016-2-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operators: SORT BY randomness is not safe with network partitions</summary>
      <description>SORT BY relies on a transient Random object, which is initialized once per deserialize operation.This results in complications during a network partition and when Tez/Spark reuses a cached plan.</description>
      <version>1.2.1,1.3.0,2.0.0,2.0.1</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13125" opendate="2016-2-23 00:00:00" fixdate="2016-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support masking and filtering of rows/columns</summary>
      <description>Traditionally, access control at the row and column level is implemented through views. Using views as an access control method works well only when access rules, restrictions, and conditions are monolithic and simple. It however becomes ineffective when view definitions become too complex because of the complexity and granularity of privacy and security policies. It also becomes costly when a large number of views must be manually updated and maintained. In addition, the ability to update views proves to be challenging. As privacy and security policies evolve, required updates to views may negatively affect the security logic particularly when database applications reference the views directly by name. HIVE row and column access control helps resolve all these problems.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="13141" opendate="2016-2-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark over HBase should accept parameters starting with "zookeeper.znode"</summary>
      <description>HBase related paramters has been added by HIVE-12708.Following the same way,parameters starting with "zookeeper.znode" should be add too,which are also HBase related paramters .Refering to http://blog.cloudera.com/blog/2013/10/what-are-hbase-znodes/I have seen a failure with Hive on Spark over HBase due to customize zookeeper.znode.parent.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13144" opendate="2016-2-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node</summary>
      <description>When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13151" opendate="2016-2-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up UGI objects in FileSystem cache for transactions</summary>
      <description>One issue with FileSystem.CACHE is that it does not clean itself. The key in that cache includes UGI object. When new UGI objects are created and used with the FileSystem api, new entries get added to the cache.We need to manually clean up those UGI objects once they are no longer in use.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13169" opendate="2016-2-26 00:00:00" fixdate="2016-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2: Support delegation token based connection when using http transport</summary>
      <description>HIVE-5155 introduced support for delegation token based connection. However, it was intended for tcp transport mode. We need to have similar mechanisms for http transport.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
    </fixedFiles>
  </bug>
  <bug id="13175" opendate="2016-2-27 00:00:00" fixdate="2016-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow making external tables transactional</summary>
      <description>The fact that compactor rewrites contents of ACID tables is in conflict with what is expected of external tables.Conversely, end user can write to External table which certainly not what is expected of ACID table.So we should explicitly disallow making an external table ACID.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="13186" opendate="2016-2-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER TABLE RENAME should lowercase table name and hdfs location</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
    </fixedFiles>
  </bug>
  <bug id="13188" opendate="2016-3-1 00:00:00" fixdate="2016-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow users of RetryingThriftClient to close transport</summary>
      <description>RetryingThriftCLIClient opens a TTransport and leaves it open. there should be a way to close that.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="13191" opendate="2016-3-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DummyTable map joins mix up columns between tables</summary>
      <description>SELECT a.key, a.a_one, b.b_one, a.a_zero, b.b_zeroFROM( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero) aLEFT JOIN( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero) bON a.key = b.key;11 1 0 0 1This should be 11, 1, 1, 0, 0 instead. Disabling map-joins &amp; using shuffle-joins returns the right result.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mapjoin2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13200" opendate="2016-3-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregation functions returning empty rows on partitioned columns</summary>
      <description>Running aggregation functions like MAX, MIN, DISTINCT against partitioned columns will return empty rows if table has property: 'skip.header.line.count'='1'Reproduce:DROP TABLE IF EXISTS test;CREATE TABLE test (a int) PARTITIONED BY (b int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' TBLPROPERTIES('skip.header.line.count'='1');INSERT OVERWRITE TABLE test PARTITION (b = 1) VALUES (1), (2), (3), (4);INSERT OVERWRITE TABLE test PARTITION (b = 2) VALUES (1), (2), (3), (4);SELECT * FROM test;SELECT DISTINCT b FROM test;SELECT MAX(b) FROM test;SELECT DISTINCT a FROM test;The output:0: jdbc:hive2://localhost:10000/default&gt; SELECT * FROM test;+---------+---------+--+| test.a | test.b |+---------+---------+--+| 2 | 1 || 3 | 1 || 4 | 1 || 2 | 2 || 3 | 2 || 4 | 2 |+---------+---------+--+6 rows selected (0.631 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT DISTINCT b FROM test;+----+--+| b |+----+--++----+--+No rows selected (47.229 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT MAX(b) FROM test;+-------+--+| _c0 |+-------+--+| NULL |+-------+--+1 row selected (49.508 seconds)0: jdbc:hive2://localhost:10000/default&gt; SELECT DISTINCT a FROM test;+----+--+| a |+----+--+| 2 || 3 || 4 |+----+--+3 rows selected (46.859 seconds)</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13206" opendate="2016-3-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a test-sources.jar when -Psources profile is invoked</summary>
      <description>It'd be nice to attach a test-sources jar alongside the others as part of the build, to provide test resources.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13236" opendate="2016-3-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: token renewal interval needs to be set</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13237" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select parquet struct field with upper case throws NPE</summary>
      <description>Query "select msg.fieldone from test" throws NPE if msg's fieldone is actually fieldOne:2016-03-08 17:30:57,772 ERROR [main]: exec.FetchTask (FetchTask.java:initialize(86)) - java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:61) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:980) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:63) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="13248" opendate="2016-3-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change date_add/date_sub/to_date functions to return Date type rather than String</summary>
      <description>Some of the original "date" related functions return string values rather than Date values, because they were created before the Date type existed in Hive. We can try to change these to return Date in the 2.x line.Date values should be implicitly convertible to String.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.offcbo.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
    </fixedFiles>
  </bug>
  <bug id="13249" opendate="2016-3-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hard upper bound on number of open transactions</summary>
      <description>We need to have a safeguard by adding an upper bound for open transactions to avoid huge number of open-transaction requests, usually due to improper configuration of clients such as Storm.Once that limit is reached, clients will start failing.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13281" opendate="2016-3-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update some default configs for LLAP - disable default uber enabled</summary>
      <description>Disable uber mode.Enable llap.io by default</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13283" opendate="2016-3-14 00:00:00" fixdate="2016-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: make sure IO elevator is enabled by default in the daemons</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13285" opendate="2016-3-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc concatenation may drop old files from moving to final path</summary>
      <description>ORC concatenation uses combine hive input format for merging files. Under specific case where all files within a combine split are incompatible for merge (old files without stripe statistics) then these files are added to incompatible file set. But this file set is not processed as closeOp() will not be called (no output file writer will exist which will skip super.closeOp()). As a result, these incompatible files are not moved to final path.</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13286" opendate="2016-3-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query ID is being reused across queries</summary>
      <description>aihuaxu I see this commit made via HIVE-11488. I see that query id is being reused across queries. This defeats the purpose of a query id. I am not sure what the purpose of the change in that jira is but it breaks the assumption about a query id being unique for each query. Please take a look into this at the earliest.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13293" opendate="2016-3-16 00:00:00" fixdate="2016-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache RDD to improve parallel order by performance for HoS</summary>
      <description>I use TPCx-BB to do some performance test on Hive on Spark engine. And found query 10 has performance degradation when enabling parallel order by.It seems that sampling cost much time before running the real query.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
    </fixedFiles>
  </bug>
  <bug id="13299" opendate="2016-3-17 00:00:00" fixdate="2016-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column Names trimmed of leading and trailing spaces</summary>
      <description>PROBLEM:As per the Hive Language DDL: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDLIn Hive 0.12 and earlier, only alphanumeric and underscore characters are allowed in table and column names.In Hive 0.13 and later, column names can contain any Unicode character (see HIVE-6013). Any column name that is specified within backticks (`) is treated literally.However column names` left` resulted in `left`` middle ` resulted in `middle``right ` resulted in `right``middle space` resulted in `middle space`` middle space ` resulted in `middle space`</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13302" opendate="2016-3-17 00:00:00" fixdate="2016-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>direct SQL: cast to date doesn&amp;#39;t work on Oracle</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="13318" opendate="2016-3-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache the result of getTable from metastore</summary>
      <description>getTable by name from metaStore is called many times. We plan to cache it to save calls.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13319" opendate="2016-3-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate external handles in task display</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug id="13320" opendate="2016-3-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
      <description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
      <version>1.2.1,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
    </fixedFiles>
  </bug>
  <bug id="13330" opendate="2016-3-22 00:00:00" fixdate="2016-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC vectorized string dictionary reader does not differentiate null vs empty string dictionary</summary>
      <description>Vectorized string dictionary reader cannot differentiate between the case where all dictionary entries are null vs single entry with empty string. This causes wrong results when reading data out of such files. Vectorization OnSET hive.vectorized.execution.enabled=true;SET hive.fetch.task.conversion=none;select vcol from testnullorc3 limit 1;OKNULLVectorization OffSET hive.vectorized.execution.enabled=false;SET hive.fetch.task.conversion=none;select vcol from testnullorc3 limit 1;OKThe input table testnullorc3 contains a varchar column vcol with few empty strings and few nulls. For this table, non vectorized reader returns empty as first row but vectorized reader returns NULL.</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13335" opendate="2016-3-23 00:00:00" fixdate="2016-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>tune TxnHandler.performTimeOuts() batch size</summary>
      <description>look for usages - it's no longer useful; in fact may be a perf hitmade obsolete by HIVE-12439</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13341" opendate="2016-3-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats state is not captured correctly: differentiate load table and create table</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.db.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.alter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.vecrow.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.array.null.element.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.nonascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.queries.clientpositive.stats20.q</file>
      <file type="M">ql.src.test.results.clientnegative.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13358" opendate="2016-3-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats state is not captured correctly: turn off stats optimizer for sampled table</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13360" opendate="2016-3-24 00:00:00" fixdate="2016-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring Hive Authorization</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyAuthenticator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerShowFilters.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13361" opendate="2016-3-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc concatenation should enforce the compression buffer size</summary>
      <description>With HIVE-11807 buffer size estimation happens by default. This can have undesired effect wrt file concatenation. Consider the following table with filestesttable -- 000000_0 (created before HIVE-11807 which has buffer size 256KB) -- 000001_0 (created before HIVE-11807 which has buffer size 256KB) -- 000002_0 (created after HIVE-11807 with buffer size chosen as 128KB) -- 000003_0 (created after HIVE-11807 with buffer size chosen as 128KB)If we perform ALTER TABLE .. CONCATENATE on the above table with HIVE-11807, then depending on the split arrangement 000000_0 and 000001_0 will be concatenated together to new merged file. But this new merged file will have 128KB buffer size (estimated buffer size and not requested buffer size). Since new ORC writer size does not honor the requested buffer size the new merged files will have smaller buffers than the required 256KB making the file unreadable. Following exception will be thrown when reading the table after concatenation2016-03-24T16:26:33,974 ERROR [a9e27a9a-37cb-411d-9708-6c58a4ce34f2 main]: CliDriver (SessionState.java:printError(1049)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187java.io.IOException: java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187 at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:513) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:420) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:145) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1848) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:256) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13364" opendate="2016-3-26 00:00:00" fixdate="2016-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow llap to work with dynamic ports for rpc, shuffle, ui</summary>
      <description>At the moment - the ports specified in the configuration are the ones which are used to register with the Zookeeper service. Setting the ports to 0 effectively means that the services cannot be discovered.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13365" opendate="2016-3-26 00:00:00" fixdate="2016-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the MiniLLAPCluster to work with a MiniZKCluster</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.llap.LlapItUtils.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13373" opendate="2016-3-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use most specific type for numerical constants</summary>
      <description>tinyint &amp; shortint are currently inferred as ints, if they are without postfix.</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.type.widening.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13381" opendate="2016-3-30 00:00:00" fixdate="2016-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timestamp &amp; date should have precedence in type hierarchy than string group</summary>
      <description>Both sql server &amp; oracle treats date/timestamp higher in hierarchy than varchars</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="13391" opendate="2016-3-30 00:00:00" fixdate="2016-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to LLAP to use keytab to authenticate to read data</summary>
      <description>This can be used for non-doAs case to allow access to clients who don't propagate HDFS tokens.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="13393" opendate="2016-3-30 00:00:00" fixdate="2016-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: Print help message for the --incremental option</summary>
      <description>beeline --help doesn't print the usage tips for the --incremental option.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13395" opendate="2016-3-31 00:00:00" fixdate="2016-5-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lost Update problem in ACID</summary>
      <description>ACID users can run into Lost Update problem.In Hive 1.2, Driver.recordValidTxns() (which records the snapshot to use for the query) is called in Driver.compile().Now suppose to concurrent "update T set x = x + 1" are executed. (for simplicity assume there is exactly 1 row in T)What can happen is that both compile at the same time (more precisely before acquireLocksAndOpenTxn() in runInternal() is called) and thus will lock in the same snapshot, say the value of x = 7 in this snapshot.Now 1 will get the lock on the row, the second will block. Now 1, makes x = 8 and commits.Now 2 proceeds and makes x = 8 again since in it's snapshot x is still 7.This specific issue is solved in Hive 1.3/2.0 (HIVE-11077 which is a large patch that deals with multi-statement txns) by moving recordValidTxns() after locks are acquired which reduces the likelihood of this but doesn't eliminate the problem.Even in 1.3 version of the code, you could have the same issue. Assume the same 2 queries:Both start a txn, say txnid 9 and 10. Say 10 gets the lock first, 9 blocks.10 updates the row (so x = 8) and thus ReaderKey.currentTransactionId=10.10 commits.Now 9 can proceed and it will get a snapshot that includes 10, i.e. it will see x = 8 and it will write x = 9, but it will set ReaderKey.currentTransactionId = 9. Thus when merge logic runs, it will see x = 8 is the later version of this row, i.e. lost update.The problem is that locks alone are insufficient for MVCC architecture. At lower level Row ID has (originalTransactionId, rowid, bucket id, currentTransactionId) and since on update/delete we do a table scan, we could check that we are about to write a row with currentTransactionId &lt; (currentTransactionId of row we've read) and fail the query. Currently, currentTransactionId is not surfaced at higher level where this check can be made.This would not work (efficiently) longer term where we want to support fast update on user defined PK vis streaming ingest.Also, this would not work with multi statement txns since in that case we'd lock in the snapshot at the start of the txn, but then 2nd, 3rd etc queries would use the same snapshot and the locks for these queries would be acquired after the snapshot is locked in so this would be the same situation as pre HIVE-11077.A more robust solution (commonly used with MVCC) is to keep track of start and commit time (logical counter) or each transaction to detect if two txns overlap. The 2nd part is to keep track of write-set, i.e. which data (rows, partitions, whatever appropriate level of granularity is) were modified by any txn and if 2 txns overlap in time and wrote the same element, abort later one. This is called first-committer-wins rule. This requires a MS DB schema changeIt would be most convenient to use the same sequence for txnId, start and commit time (in which case txnid=start time). In this case we'd need to add 1 filed to TXNS table. The complication here is that we'll be using elements of the sequence faster and they are used as part of file name of delta and base dir and currently limited to 7 digits which can be exceeded. So this would require some thought to handling upgrade/migration.Also, write-set tracking requires either additional metastore table or keeping info in HIVE_LOCKS around longer with new state.In the short term, on SQL side of things we could (in auto commit mode only)acquire the locks first and then open the txn AND update these locks with txn id.This implies another Thrift change to pass in lockId to openTxn.The same would not work for Streaming API since it opens several txns at once and then acquires locks for each.(Not sure if that's is an issue or not since Streaming only does Insert).Either way this feels hacky.Here is one simple example why we need Write-Set tracking for multi-statement txnsConsider transactions T 1 and T 2:T 1: r 1[x] -&gt; w 1[y] -&gt; c 1 T 2: w 2[x] -&gt; w 2[y] -&gt; c 2 Suppose the order of operations is r 1[x] w 2[x].... then a conventional R/W lock manager w/o MVCSS will block the write from T 2 With MVCC we don't want readers to interfere with writers and so the following schedule is possible (because Hive's semi-shared (write) don't conflict with shared (read) locks) in Hive's current implementation.r 1[x] w 2[x] w 2[y] c 2 w 1[y] c 1By the time w 1[y] happens, T 2 has committed and released it's locks. But this is a lost update if c 1 is allowed to commit. That's where write-set tracking comes in.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-1.3.0.mysql.sql</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.0.0-to-2.1.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-1.2.0-to-1.3.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-1.3.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.1.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.0.0-to-2.1.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-1.2.0-to-1.3.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-1.3.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.1.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.0.0-to-2.1.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-1.2.0-to-1.3.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.1.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.0.0-to-2.1.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-1.2.0-to-1.3.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.1.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-1.3.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.0.0-to-2.1.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-1.2.0-to-1.3.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-1.3.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.1.0.derby.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13409" opendate="2016-4-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix JDK8 test failures related to COLUMN_STATS_ACCURATE</summary>
      <description>126 failures have crept into JDK8 tests since we resolved HIVE-8607http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/HIVE-TRUNK-JAVA8/Majority relate to the ordering of a "COLUMN_STATS_ACCURATE" partition property.Looks like a simple fix, use ordered map in HiveStringUtils.getPropertiesExplain()</description>
      <version>None</version>
      <fixedVersion>java8,2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13410" opendate="2016-4-2 00:00:00" fixdate="2016-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PerfLog metrics scopes not closed if there are exceptions on HS2</summary>
      <description>If there are errors, the HS2 PerfLog api scopes are not closed. Then there are sometimes messages like 'java.io.IOException: Scope named api_parse is not closed, cannot be opened.'I had simply forgetting to close the dangling scopes if there is an exception. Doing so now.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="13418" opendate="2016-4-4 00:00:00" fixdate="2016-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 HTTP mode should support X-Forwarded-Host header for authorization/audits</summary>
      <description>Apache Knox acts as a proxy for requests coming from the end users. In these cases, the IP address that HiveServer2 passes to the authorization/audit plugins via the HiveAuthzContext object only the IP address of the proxy, and not the end user.For auditing purposes, the IP address of the end user and any proxies in between are useful.HiveServer2 should pass the information from 'X-Forwarded-Host' header to the HiveAuthorizer plugins.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.QueryContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="13421" opendate="2016-4-5 00:00:00" fixdate="2016-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate job progress in operation status</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug id="13428" opendate="2016-4-5 00:00:00" fixdate="2016-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZK SM in LLAP should have unique paths per cluster</summary>
      <description>Noticed this while working on some other patch</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13429" opendate="2016-4-5 00:00:00" fixdate="2016-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to remove dangling scratch dir</summary>
      <description>We have seen in some cases, user will leave the scratch dir behind, and eventually eat out hdfs storage. This could happen when vm restarts and leave no chance for Hive to run shutdown hook. This is applicable for both HiveCli and HiveServer2. Here we provide an external tool to clear dead scratch dir as needed.We need a way to identify which scratch dir is in use. We will rely on HDFS write lock for that. Here is how HDFS write lock works:1. A HDFS client open HDFS file for write and only close at the time of shutdown2. Cleanup process can try to open HDFS file for write. If the client holding this file is still running, we will get exception. Otherwise, we know the client is dead3. If the HDFS client dies without closing the HDFS file, NN will reclaim the lease after 10 min, ie, the HDFS file hold by the dead client is writable again after 10 minSo here is how we remove dangling scratch directory in Hive:1. HiveCli/HiveServer2 opens a well-named lock file in scratch directory and only close it when we about to drop scratch directory2. A command line tool cleardanglingscratchdir will check every scratch directory and try open the lock file for write. If it does not get exception, meaning the owner is dead and we can safely remove the scratch directory3. The 10 min window means it is possible a HiveCli/HiveServer2 is dead but we still cannot reclaim the scratch directory for another 10 min. But this should be tolerable</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13439" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: provide a way to retrieve GUID to query Yarn ATS</summary>
      <description>HIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13442" opendate="2016-4-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: refactor submit API to be amenable to signing</summary>
      <description>This is going to be a wire compat breaking change.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13443" opendate="2016-4-6 00:00:00" fixdate="2016-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: signing for the second state of submit (the event)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.tez.dag.api.TaskSpecBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="13444" opendate="2016-4-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add HMAC signatures to LLAP; verify them on LLAP side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapTokenChecker.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenClientFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13445" opendate="2016-4-7 00:00:00" fixdate="2016-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: token should encode application and cluster ids</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenProvider.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13446" opendate="2016-4-7 00:00:00" fixdate="2016-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: set default management protocol acls to deny all</summary>
      <description>The user needs to set the acls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13447" opendate="2016-4-7 00:00:00" fixdate="2016-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: check ZK acls for registry and fail if they are too permissive</summary>
      <description>Only the current ("hive") user can have write access.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13448" opendate="2016-4-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: check ZK acls for ZKSM and fail if they are too permissive</summary>
      <description>Only the current user should have any access.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13457" opendate="2016-4-7 00:00:00" fixdate="2016-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HS2 REST API endpoints for monitoring information</summary>
      <description>Similar to what is exposed in HS2 webui in HIVE-12338, it would be nice if other UI's like admin tools or Hue can access and display this information as well. Hence, we will create some REST endpoints to expose this information.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13486" opendate="2016-4-11 00:00:00" fixdate="2016-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cast the column type for column masking</summary>
      <description>Quoted from jdere "in the case that getCellValueTransformer() returns a constant value (such as "NULL" to indicate the expression should be null), the resulting data type for that column ends up becoming the constant value NULL, which is different from the original column's type. Might want to add a type check somewhere to make sure the transformed expression and original expression match, or maybe wrap the transformed expression within a CAST to get the types to match."</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MaskAndFilterInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="13520" opendate="2016-4-14 00:00:00" fixdate="2016-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow any test to run for longer than 60minutes in the ptest setup</summary>
      <description>Current timeout for batches is 2hours. This needs to be lowered. 1hour may be too much as well. We can start with this, and reduce timeouts further.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug id="13522" opendate="2016-4-14 00:00:00" fixdate="2016-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>regexp_extract.q hangs on master</summary>
      <description>Disable to unblock Hive QA runs.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13529" opendate="2016-4-15 00:00:00" fixdate="2016-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move around some of the classes created during llap branch work</summary>
      <description>Try to move around some of the classes created, to get the dependencies to work a little better for clients.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.SubmitWorkInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.LlapRowInputFormat.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.LlapInputSplit.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.LlapDump.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.LlapBaseInputFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestLlapInputSplit.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13541" opendate="2016-4-18 00:00:00" fixdate="2016-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass view&amp;#39;s ColumnAccessInfo to HiveAuthorizer</summary>
      <description>RIght now, only table's ColumnAccessInfo is passed to HiveAuthorizer</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestColumnAccess.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13553" opendate="2016-4-20 00:00:00" fixdate="2016-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTE with upperCase alias throws exception</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13561" opendate="2016-4-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used</summary>
      <description>I can repo this on branch-1.2 and branch-2.0.It looks to be the same issues as: HIVE-11408The patch from HIVE-11408 looks to fix the issue as well.I've updated the patch from HIVE-11408 to be aligned with branch-1.2 and master</description>
      <version>1.2.0,1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13562" opendate="2016-4-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vector bridge for all non-vectorized udfs</summary>
      <description>Mechanism already exists for this via VectorUDFAdaptor but we have arbitrarily hand picked few udfs to go through it. I think we should enable this by default for all udfs.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.between.columns.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13565" opendate="2016-4-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>thrift change</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="13566" opendate="2016-4-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto-gather column stats - phase 1</summary>
      <description>This jira adds code and tests for auto-gather column stats. Golden file update will be done in phase 2 - HIVE-11160</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SparkMapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TextRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TextRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="13568" opendate="2016-4-21 00:00:00" fixdate="2016-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDFs to support column-masking</summary>
      <description>HIVE-13125 added support to provide column-masking and row-filtering during select via HiveAuthorizer interface. This JIRA is track addition of UDFs that can be used by HiveAuthorizer implementations to mask column values.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13585" opendate="2016-4-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add counter metric for direct sql failures</summary>
      <description>In case of direct sql failure, metastore query falls back to DataNucleus. It'd be good to record how often this happens as a metrics counter.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="13589" opendate="2016-4-22 00:00:00" fixdate="2016-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline support prompt for password with &amp;#39;-p&amp;#39; option</summary>
      <description>Specifying connection string using commandline options in beeline is convenient, as it gets saved in shell command history, and it is easy to retrieve it from there.However, specifying the password in command prompt is not secure as it gets displayed on screen and saved in the history.It should be possible to specify '-p' without an argument to make beeline prompt for password.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="1359" opendate="2010-5-21 00:00:00" fixdate="2010-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test should be shim-aware</summary>
      <description>Some features in Hive only works for certain Hadoop versions through shim. However the unit test structure is not shim-aware in that there is only one set of queries and expected outputs for all Hadoop versions. This may not be sufficient when we will have different output for different Hadoop versions. One example is CombineHiveInputFormat wich is only available from Hadoop 0.20. The plan using CombineHiveInputFormat and HiveInputFormat may be different. Another example is archival partitions (HAR) which is also only available from 0.20.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13590" opendate="2016-4-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized HS2 with LDAP auth enabled fails in multi-domain LDAP case</summary>
      <description>In a kerberized HS2 with LDAP authentication enabled, LDAP user usually logs in using username in form of username@domain in LDAP multi-domain case. But it fails if the domain was not in the Hadoop auth_to_local mapping rule, the error is as following:Caused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to ctang@mydomain.comat org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:389)at org.apache.hadoop.security.User.&lt;init&gt;(User.java:48)</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcNonKrbSASLWithMiniKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="13609" opendate="2016-4-26 00:00:00" fixdate="2016-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix UDTFs to allow local fetch task to fetch rows forwarded by GenericUDTF.close()</summary>
      <description>From ashutoshc's comments in HIVE-13586, attempt to fix whatever is causing the local fetch task to not get the rows forwarded by UDTF close().</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13618" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trailing spaces in partition column will be treated differently</summary>
      <description>We store the partition spec value in the metastore. In mysql (and derby i think), the trailing space is ignored. That is, if you have a partition column "col" (type varchar or string) with value "a " and then select from the table where col = "a", it will return. However, in postgres and Oracle, the trailing space is not ignored.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13619" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket map join plan is incorrect</summary>
      <description>Same as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1362" opendate="2010-5-21 00:00:00" fixdate="2010-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer statistics on columns in tables and partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-php.queryplan.queryplan.types.php</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="13620" opendate="2016-4-27 00:00:00" fixdate="2016-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge llap branch work to master</summary>
      <description>Would like to try to merge the llap branch work for HIVE-12991 into the master branch.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.tez.dag.api.TaskSpecBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapDataOutputBuffer.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapRowInputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.SubmitWorkInfo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="13625" opendate="2016-4-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Prepared Statement when executed with escape characters in parameter fails</summary>
      <description>When setting parameters to a Hive Prepared Statement, if the parameter has an odd number of escape characters, then the Statement fails.For example, I set one of the parameters to "/somepath/\044{yyyy}/\044{MM}/\044{dd}/". Here, I have escaped the dollar character with \044 because Hive gives an Atlas exception with "$" character. Now, when the parameters are getting set inside Hive, getCharIndexFromSqlByParamLocation throws an Exception.Hive records something called signal count. if (c == '\'' || c == '')// record the count of char "'" and char "\" { signalCount++; } And the parameter is set only if the signalCount %2 is 0.else if (c == cchar &amp;&amp; signalCount % 2 == 0) {// check if the ? is really the parameter num++; if (num == paramLoc) { charIndex = i; break; }Since my parameter has three "\" characters, the signal Count modulo is not 0 and the parameter is not set at all throwing an exception.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13661" opendate="2016-5-1 00:00:00" fixdate="2016-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Refactor] Move common FS operations out of shim layer</summary>
      <description>Avoid overhead of extra function calls.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13676" opendate="2016-5-3 00:00:00" fixdate="2016-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests failing because metastore doesn&amp;#39;t come up</summary>
      <description>In 5-6 test classes, metastore is required to be up for tests to run. The metastore is started in setup Phase asynchronously. But there's no logic to wait till the metastore comes up. Hence, sometimes tests run even when metastore isn't up and fail.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="13704" opendate="2016-5-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t call DistCp.execute() instead of DistCp.run()</summary>
      <description>HIVE-11607 switched DistCp from using run to execute. The run method runs added logic that drives the state of SimpleCopyListing which runs in the driver, and of CopyCommitter which runs in the job runtime.When Hive ends up running DistCp for copy work (Between non matching FS or between encrypted/non-encrypted zones, for sizes above a configured value) this state not being set causes wrong paths to appear on the target (subdirs named after the file, instead of just the file).Hive should call DistCp's Tool run method and not the execute method directly, to not skip the target exists flag that the setTargetPathExists call would set:https://github.com/apache/hadoop/blob/release-2.7.1/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java#L108-L126</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="13710" opendate="2016-5-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP registry ACL check causes error due to namespacing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13716" opendate="2016-5-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading V</summary>
      <description>Parallelize permission settings and other refactoring.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13725" opendate="2016-5-10 00:00:00" fixdate="2016-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Streaming API should synchronize calls when multiple threads use the same endpoint</summary>
      <description>Currently, the streaming endpoint creates a metastore client which gets used for RPC. The client itself is not internally thread safe. Therefore, the API methods should provide the relevant synchronization so that the methods can be called from different threads. A sample use case is as follows:1. Thread 1 creates a streaming endpoint and opens a txn batch.2. Thread 2 heartbeats the txn batch.With the current impl, this can result in an "out of sequence response", since the response of the calls in thread1 might end up going to thread2 and vice-versa.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="13726" opendate="2016-5-10 00:00:00" fixdate="2016-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading VI</summary>
      <description>Parallelize deletes and other refactoring.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13745" opendate="2016-5-12 00:00:00" fixdate="2016-1-12 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>UDF current_datecurrent_timestampunix_timestamp NPE</summary>
      <description>NullPointerException when current_date is used in mapreduce</description>
      <version>2.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13756" opendate="2016-5-13 00:00:00" fixdate="2016-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map failure attempts to delete reducer _temporary directory on multi-query pig query</summary>
      <description>A pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.If one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13760" opendate="2016-5-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a HIVE_QUERY_TIMEOUT configuration to kill a query if a query is running for more than the configured timeout value.</summary>
      <description>Add a HIVE_QUERY_TIMEOUT configuration to kill a query if a query is running for more than the configured timeout value. The default value will be 0 , which means no timeout. This will be useful for user to manage queries with SLA.</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13780" opendate="2016-5-18 00:00:00" fixdate="2016-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow user to update AVRO table schema via command even if table&amp;#39;s definition was defined through schema file</summary>
      <description>If a table is defined as below:CREATE TABLE testSTORED AS AVRO TBLPROPERTIES ('avro.schema.url'='/tmp/schema.json');if user tries to run command:ALTER TABLE test CHANGE COLUMN col1 col1 STRING COMMENT 'test comment';The query will return without any warning, but has no affect to the table.It would be good if we can allow user to ALTER table (add/change column, update comment etc) even though the schema is defined through schema file.</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13782" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile async query asynchronously</summary>
      <description>Currently, when an async query is submitted to HS2, HS2 does the preparation synchronously. One of the preparation step is to compile the query, which may take some time. It will be helpful to provide an option to do the compilation asynchronously.</description>
      <version>None</version>
      <fixedVersion>2.0.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13783" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display a secondary prompt on beeline for multi-line statements</summary>
      <description># beeline -u jdbc:hive2://localhost:10000[...]Beeline version 1.1.0-cdh5.4.5 by Apache Hive0: jdbc:hive2://localhost:10000&gt; "0: jdbc:hive2://localhost:10000&gt; select * from foo;Error: Error while compiling statement: FAILED: ParseException line 2:17 character '&lt;EOF&gt;' not supported here (state=42000,code=40000)0: jdbc:hive2://localhost:10000&gt; After (accidentally) entering a lonely quote character on its own line and pressing Enter, I get back the normal prompt. This easily makes me believe I'm about to type a new command from scratch, e.g. a select query as in the example, which ends up not working due to parsing error.Expected behavior: When a previous command is continued, or a quote is opened or anything like this, a differently looking secondary prompt should be displayed rather than the normal prompt; as this is done in e.g. hive, impala, mysql, bash..., e.g.:# beeline -u jdbc:hive2://localhost:10000[...]Beeline version 1.1.0-cdh5.4.5 by Apache Hive0: jdbc:hive2://localhost:10000&gt; " &gt; ...</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="13794" opendate="2016-5-19 00:00:00" fixdate="2016-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE_RPC_QUERY_PLAN should always be set when generating LLAP splits</summary>
      <description>This option was being added in the test, but really should be set any time we are generating the LLAP input splits.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug id="13799" opendate="2016-5-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize TableScanRule::checkBucketedTable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="138" opendate="2008-12-8 00:00:00" fixdate="2008-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide option to export a HEADER</summary>
      <description>When writing data to directories or files for later analysis, or when exploring data in the hive CLI with raw SELECT statements, it'd be great if we could get a "header" or something so we know which columns our output comes from. Any chance this is easy to add? Just print the column names (or formula used to generate them) in the first row?SELECT foo.* WITH HEADER FROM some_table foo limit 3;col1 col2 col31 9 67 5 07 5 3SELECT f.col1-f.col2, col3 WITH HEADER FROM some_table foo limit 3;f.col1-f.col2 col3-8 62 02 3...etc</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13809" opendate="2016-5-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid Grace Hash Join memory usage estimation didn&amp;#39;t take into account the bloom filter size</summary>
      <description>Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there's enough memory but it turns out not the case, we will run into OOM problem.Currently hybrid grace hash join memory usage estimation didn't take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.The solution is to count in the bloom filter size into memory estimation.Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13813" opendate="2016-5-20 00:00:00" fixdate="2016-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Metrics for the number of Hive operations waiting for compile</summary>
      <description>Currently, without hive.driver.parallel.compilation introduced in HIVE-4239, only one SQL operation can enter the compilation block per HS2 instance, and all the rest will be blocked. We should add metrics info for the number of operations that are blocked.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="13833" opendate="2016-5-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an initial delay when starting the heartbeat</summary>
      <description>Since the scheduling of heartbeat happens immediately after lock acquisition, it's unnecessary to send heartbeat at the time when locks is acquired. Add an initial delay to skip this.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13837" opendate="2016-5-24 00:00:00" fixdate="2016-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>current_timestamp() output format is different in some cases</summary>
      <description>As jdere reports:current_timestamp() udf returns result with different format in some cases.select current_timestamp() returns result with decimal precision:{noformat}hive&gt; select current_timestamp();OK2016-04-14 18:26:58.875Time taken: 0.077 seconds, Fetched: 1 row(s){noformat}But output format is different for select current_timestamp() from all100k union select current_timestamp() from over100k limit 5; {noformat}hive&gt; select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;Query ID = hrt_qa_20160414182956_c4ed48f2-9913-4b3b-8f09-668ebf55b3e3Total jobs = 1Launching Job 1 out of 1Tez session was closed. Reopening...Session re-established.Status: Running (Executing on YARN cluster with App id application_1460611908643_0624)---------------------------------------------------------------------------------------------- VERTICES MODE STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED ----------------------------------------------------------------------------------------------Map 1 .......... llap SUCCEEDED 1 1 0 0 0 0 Map 4 .......... llap SUCCEEDED 1 1 0 0 0 0 Reducer 3 ...... llap SUCCEEDED 1 1 0 0 0 0 ----------------------------------------------------------------------------------------------VERTICES: 03/03 [==========================&gt;&gt;] 100% ELAPSED TIME: 0.92 s ----------------------------------------------------------------------------------------------OK2016-04-14 18:29:56Time taken: 10.558 seconds, Fetched: 1 row(s){noformat}explain plan for select current_timestamp();{noformat}hive&gt; explain extended select current_timestamp();OKABSTRACT SYNTAX TREE: TOK_QUERY TOK_INSERT TOK_DESTINATION TOK_DIR TOK_TMP_FILE TOK_SELECT TOK_SELEXPR TOK_FUNCTION current_timestampSTAGE DEPENDENCIES: Stage-0 is a root stageSTAGE PLANS: Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: TableScan alias: _dummy_table Row Limit Per Split: 1 GatherStats: false Select Operator expressions: 2016-04-14 18:30:57.206 (type: timestamp) outputColumnNames: _col0 ListSinkTime taken: 0.062 seconds, Fetched: 30 row(s){noformat}explain plan for select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;{noformat}hive&gt; explain extended select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;OKABSTRACT SYNTAX TREE: TOK_QUERY TOK_FROM TOK_SUBQUERY TOK_QUERY TOK_FROM TOK_SUBQUERY TOK_UNIONALL TOK_QUERY TOK_FROM TOK_TABREF TOK_TABNAME all100k TOK_INSERT TOK_DESTINATION TOK_DIR TOK_TMP_FILE TOK_SELECT TOK_SELEXPR TOK_FUNCTION current_timestamp TOK_QUERY TOK_FROM TOK_TABREF TOK_TABNAME over100k TOK_INSERT TOK_DESTINATION TOK_DIR TOK_TMP_FILE TOK_SELECT TOK_SELEXPR TOK_FUNCTION current_timestamp _u1 TOK_INSERT TOK_DESTINATION TOK_DIR TOK_TMP_FILE TOK_SELECTDI TOK_SELEXPR TOK_ALLCOLREF _u2 TOK_INSERT TOK_DESTINATION TOK_DIR TOK_TMP_FILE TOK_SELECT TOK_SELEXPR TOK_ALLCOLREF TOK_LIMIT 5STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1STAGE PLANS: Stage: Stage-1 Tez DagId: hrt_qa_20160414183119_ec8e109e-8975-4799-a142-4a2289f85910:7 Edges: Map 1 &lt;- Union 2 (CONTAINS) Map 4 &lt;- Union 2 (CONTAINS) Reducer 3 &lt;- Union 2 (SIMPLE_EDGE) DagName: Vertices: Map 1 Map Operator Tree: TableScan alias: all100k Statistics: Num rows: 100000 Data size: 15801336 Basic stats: COMPLETE Column stats: COMPLETE GatherStats: false Select Operator Statistics: Num rows: 100000 Data size: 4000000 Basic stats: COMPLETE Column stats: COMPLETE Select Operator expressions: 2016-04-14 18:31:19.0 (type: timestamp) outputColumnNames: _col0 Statistics: Num rows: 200000 Data size: 8000000 Basic stats: COMPLETE Column stats: COMPLETE Group By Operator keys: _col0 (type: timestamp) mode: hash outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE Reduce Output Operator key expressions: _col0 (type: timestamp) null sort order: a sort order: + Map-reduce partition columns: _col0 (type: timestamp) Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE tag: -1 TopN: 5 TopN Hash Memory Usage: 0.04 auto parallelism: true Execution mode: llap LLAP IO: no inputs Path -&gt; Alias: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k [all100k] Path -&gt; Partition: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k Partition base file name: all100k input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","s":"true","dc":"true","bo":"true","v":"true","c":"true","ts":"true"}} EXTERNAL TRUE bucket_count -1 columns t,si,i,b,f,d,s,dc,bo,v,c,ts,dt columns.comments columns.types tinyint:smallint:int:bigint:float:double:string:decimal(38,18):boolean:varchar(25):char(25):timestamp:date field.delim | file.inputformat org.apache.hadoop.mapred.TextInputFormat file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k name default.all100k numFiles 1 numRows 100000 rawDataSize 15801336 serialization.ddl struct all100k { byte t, i16 si, i32 i, i64 b, float f, double d, string s, decimal(38,18) dc, bool bo, varchar(25) v, char(25) c, timestamp ts, date dt} serialization.format | serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe totalSize 15901336 transient_lastDdlTime 1460612683 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","s":"true","dc":"true","bo":"true","v":"true","c":"true","ts":"true"}} EXTERNAL TRUE bucket_count -1 columns t,si,i,b,f,d,s,dc,bo,v,c,ts,dt columns.comments columns.types tinyint:smallint:int:bigint:float:double:string:decimal(38,18):boolean:varchar(25):char(25):timestamp:date field.delim | file.inputformat org.apache.hadoop.mapred.TextInputFormat file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k name default.all100k numFiles 1 numRows 100000 rawDataSize 15801336 serialization.ddl struct all100k { byte t, i16 si, i32 i, i64 b, float f, double d, string s, decimal(38,18) dc, bool bo, varchar(25) v, char(25) c, timestamp ts, date dt} serialization.format | serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe totalSize 15901336 transient_lastDdlTime 1460612683 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe name: default.all100k name: default.all100k Truncated Path -&gt; Alias: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k [all100k] Map 4 Map Operator Tree: TableScan alias: over100k Statistics: Num rows: 100000 Data size: 6631229 Basic stats: COMPLETE Column stats: COMPLETE GatherStats: false Select Operator Statistics: Num rows: 100000 Data size: 4000000 Basic stats: COMPLETE Column stats: COMPLETE Select Operator expressions: 2016-04-14 18:31:19.0 (type: timestamp) outputColumnNames: _col0 Statistics: Num rows: 200000 Data size: 8000000 Basic stats: COMPLETE Column stats: COMPLETE Group By Operator keys: _col0 (type: timestamp) mode: hash outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE Reduce Output Operator key expressions: _col0 (type: timestamp) null sort order: a sort order: + Map-reduce partition columns: _col0 (type: timestamp) Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE tag: -1 TopN: 5 TopN Hash Memory Usage: 0.04 auto parallelism: true Execution mode: llap LLAP IO: no inputs Path -&gt; Alias: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k [over100k] Path -&gt; Partition: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k Partition base file name: over100k input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","bo":"true","s":"true","bin":"true"}} EXTERNAL TRUE bucket_count -1 columns t,si,i,b,f,d,bo,s,bin columns.comments columns.types tinyint:smallint:int:bigint:float:double:boolean:string:binary field.delim : file.inputformat org.apache.hadoop.mapred.TextInputFormat file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k name default.over100k numFiles 1 numRows 100000 rawDataSize 6631229 serialization.ddl struct over100k { byte t, i16 si, i32 i, i64 b, float f, double d, bool bo, string s, binary bin} serialization.format : serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe totalSize 6731229 transient_lastDdlTime 1460612798 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"t":"true","si":"true","i":"true","b":"true","f":"true","d":"true","bo":"true","s":"true","bin":"true"}} EXTERNAL TRUE bucket_count -1 columns t,si,i,b,f,d,bo,s,bin columns.comments columns.types tinyint:smallint:int:bigint:float:double:boolean:string:binary field.delim : file.inputformat org.apache.hadoop.mapred.TextInputFormat file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k name default.over100k numFiles 1 numRows 100000 rawDataSize 6631229 serialization.ddl struct over100k { byte t, i16 si, i32 i, i64 b, float f, double d, bool bo, string s, binary bin} serialization.format : serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe totalSize 6731229 transient_lastDdlTime 1460612798 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe name: default.over100k name: default.over100k Truncated Path -&gt; Alias: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k [over100k] Reducer 3 Execution mode: vectorized, llap Needs Tagging: false Reduce Operator Tree: Group By Operator keys: KEY._col0 (type: timestamp) mode: mergepartial outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE Limit Number of rows: 5 Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE File Output Operator compressed: false GlobalTableId: 0 directory: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/tmp/hive/hrt_qa/ec0773d7-0ac2-45c7-b9cb-568bbed2c49c/hive_2016-04-14_18-31-19_532_3480081382837900888-1/-mr-10001/.hive-staging_hive_2016-04-14_18-31-19_532_3480081382837900888-1/-ext-10002 NumFilesPerFileSink: 1 Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE Stats Publishing Key Prefix: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/tmp/hive/hrt_qa/ec0773d7-0ac2-45c7-b9cb-568bbed2c49c/hive_2016-04-14_18-31-19_532_3480081382837900888-1/-mr-10001/.hive-staging_hive_2016-04-14_18-31-19_532_3480081382837900888-1/-ext-10002/ table: input format: org.apache.hadoop.mapred.SequenceFileInputFormat output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat properties: columns _col0 columns.types timestamp escape.delim \ hive.serialization.extend.additional.nesting.levels true serialization.escape.crlf true serialization.format 1 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe TotalFiles: 1 GatherStats: false MultiFileSpray: false Union 2 Vertex: Union 2 Stage: Stage-0 Fetch Operator limit: 5 Processor Tree: ListSinkTime taken: 0.301 seconds, Fetched: 284 row(s){noformat}Both the queries used return timestamp with YYYY-MM-DD HH:MM:SS.fff format in past releases.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="13838" opendate="2016-5-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set basic stats as inaccurate for all ACID tables</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.globallimit.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.globallimit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.globallimit.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.orig.table.use.metadata.q</file>
    </fixedFiles>
  </bug>
  <bug id="13858" opendate="2016-5-26 00:00:00" fixdate="2016-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: A preempted task can end up waiting on completeInitialization if some part of the executing code suppressed the interrupt</summary>
      <description>An interrupt along with a HiveProcessor.abort call is made when attempting to preempt a task.In this specific case, the task was in the middle of HDFS IO - which 'handled' the interrupt by retrying. As a result the interrupt status on the thread was reset - so instead of skipping the future.get in completeInitialization - the task ended up blocking there.End result - a single executor slot permanently blocked in LLAP. Depending on what else is running - this can cause a cluster level deadlock.</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13906" opendate="2016-6-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove guava dependence from storage-api module</summary>
      <description>Guava is a very problematic library to depend on because of the version incompatibilities and the use of it in the storage-api module causes it to leak into everything that depends on it.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.DiskRangeInfo.java</file>
      <file type="M">storage-api.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13910" opendate="2016-6-1 00:00:00" fixdate="2016-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Ranger-Hive] select from a table is not working if used as &lt;dbname.tablename&gt;</summary>
      <description>set hive.mapred.mode=nonstrict;set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;create database newdb;use newdb;create table masking_test as select cast(key as int) as key, value from default.src;use default;explain select * from newdb.masking_test;select * from newdb.masking_test;explain select * from newdb.masking_test where key &gt; 0;select * from newdb.masking_test where key &gt; 0;</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13932" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive SMB Map Join with small set of LIMIT failed with NPE</summary>
      <description>1) prepare sample data:a=1while [[ $a -lt 100 ]]; do echo $a ; let a=$a+1; done &gt; data2) prepare source hive table:CREATE TABLE `s`(`c` string);load data local inpath 'data' into table s;3) prepare the bucketed table:set hive.enforce.bucketing=true;set hive.enforce.sorting=true;CREATE TABLE `t`(`c` string) CLUSTERED BY (c) SORTED BY (c) INTO 5 BUCKETS;insert into t select * from s;4) reproduce this issue:SET hive.auto.convert.sortmerge.join = true;SET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;SET hive.auto.convert.sortmerge.join.noconditionaltask = true;SET hive.optimize.bucketmapjoin = true;SET hive.optimize.bucketmapjoin.sortedmerge = true;select * from t join t t1 on t.c=t1.c limit 1;</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13984" opendate="2016-6-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use multi-threaded approach to listing files for msck</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14014" opendate="2016-6-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>zero length file is being created for empty bucket in tez mode (II)</summary>
      <description>The same problem happens when source table is not empty, e.g,, when "limit 0" is not there.</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14028" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats is not updated</summary>
      <description>DROP TABLE users;CREATE TABLE users(key string, state string, country string, country_id int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = "info:state,info:country,info:country_id");INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0 FROM src;desc formatted users;the result is#### A masked pattern was here ####Table Type: MANAGED_TABLETable Parameters: COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\"} numFiles 0 numRows 0 rawDataSize 0 storage_handler org.apache.hadoop.hive.hbase.HBaseStorageHandler totalSize 0</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="14037" opendate="2016-6-16 00:00:00" fixdate="2016-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ClassNotFoundException for the jar in hive.reloadable.aux.jars.path in mapreduce</summary>
      <description>The jars in hive.reloadable.aux.jars.path seem to be available in HS2 process while they are not available in the Mapper or Reducer nodes which will throw the following exception.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isDeterministic(ExprNodeGenericFuncEvaluator.java:152) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.iterate(ExprNodeEvaluatorFactory.java:97) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.toCachedEvals(ExprNodeEvaluatorFactory.java:71) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:59) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425) at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:193) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:431) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126) ... 22 moreCaused by: java.lang.ClassNotFoundException: test.UDF at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:270) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:132) ... 36 more</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14070" opendate="2016-6-21 00:00:00" fixdate="2016-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.tez.exec.print.summary=true returns wrong performance numbers on HS2</summary>
      <description>On master, we have Query Execution Summary----------------------------------------------------------------------------------------------OPERATION DURATION----------------------------------------------------------------------------------------------Compile Query -1466208820.74sPrepare Plan 0.00sSubmit Plan 1466208825.50sStart DAG 0.26sRun DAG 4.39s----------------------------------------------------------------------------------------------Task Execution Summary---------------------------------------------------------------------------------------------- VERTICES DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS---------------------------------------------------------------------------------------------- Map 1 1014.00 1,534 11 1,500 1 Reducer 2 96.00 541 0 1 0----------------------------------------------------------------------------------------------sounds like a real issue.</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="14071" opendate="2016-6-21 00:00:00" fixdate="2016-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-14014 breaks non-file outputs</summary>
      <description>Cannot avoid creating outputs when outputs are e.g. streaming</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14072" opendate="2016-6-21 00:00:00" fixdate="2016-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryIds reused across different queries</summary>
      <description>While testing HIVE-14023, and running TestMiniLlapCluster - query ids were re-uesd for the entire init scripts. 30+ different queries - same queryId, new Tez dag submission, for different queries.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperationDisplay.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="1413" opendate="2010-6-17 00:00:00" fixdate="2010-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bring a table/partition offline</summary>
      <description>There should be a way to bring a table/partition offline.At that time, no read/write operations should be supported on that table.It would be very useful for housekeeping operations</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14132" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t fail config validation for removed configs</summary>
      <description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.set.metaconf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14153" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: beeline history doesn&amp;#39;t work on Hive2</summary>
      <description>The up arrow on console is supposed to display history, which is broken currently. Changes in HIVE-6758 broke it.</description>
      <version>1.2.1,2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  <bug id="14158" opendate="2016-7-4 00:00:00" fixdate="2016-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deal with derived column names</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MaskAndFilterInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.masking.8.q</file>
    </fixedFiles>
  </bug>
  <bug id="14204" opendate="2016-7-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize loading dynamic partitions</summary>
      <description>Lots of time is spent in sequential fashion to load dynamic partitioned dataset in driver side. E.g simple dynamic partitioned load as follows takes 300+ secondsINSERT INTO web_sales_test partition(ws_sold_date_sk) select * from tpcds_bin_partitioned_orc_200.web_sales;Time taken to load dynamic partitions: 309.22 seconds</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14205" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive doesn&amp;#39;t support union type with AVRO file format</summary>
      <description>Reproduce steps:hive&gt; CREATE TABLE avro_union_test &gt; PARTITIONED BY (p int) &gt; ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' &gt; STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' &gt; OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' &gt; TBLPROPERTIES ('avro.schema.literal'='{ &gt; "type":"record", &gt; "name":"nullUnionTest", &gt; "fields":[ &gt; { &gt; "name":"value", &gt; "type":[ &gt; "null", &gt; "int", &gt; "long" &gt; ], &gt; "default":null &gt; } &gt; ] &gt; }');OKTime taken: 0.105 secondshive&gt; alter table avro_union_test add partition (p=1);OKTime taken: 0.093 secondshive&gt; select * from avro_union_test;FAILED: RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException: Failed with exception Hive internal error inside isAssignableFromSettablePrimitiveOI void not supported yet.java.lang.RuntimeException: Hive internal error inside isAssignableFromSettablePrimitiveOI void not supported yet. at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettablePrimitiveOI(ObjectInspectorUtils.java:1140) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1149) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1187) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1220) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1200) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219) at org.apache.hadoop.hive.ql.exec.FetchOperator.setupOutputObjectInspector(FetchOperator.java:581) at org.apache.hadoop.hive.ql.exec.FetchOperator.initialize(FetchOperator.java:172) at org.apache.hadoop.hive.ql.exec.FetchOperator.&lt;init&gt;(FetchOperator.java:140) at org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:79) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:482) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:311) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1194) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1289) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1120) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1108) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:218) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:170) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:381) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:773) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:691) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Another test case to show this problem is:hive&gt; create table avro_union_test2 (value uniontype&lt;int,bigint&gt;) stored as avro;OKTime taken: 0.053 secondshive&gt; show create table avro_union_test2;OKCREATE TABLE `avro_union_test2`( `value` uniontype&lt;void,int,bigint&gt; COMMENT '')ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'LOCATION 'hdfs://localhost/user/hive/warehouse/avro_union_test2'TBLPROPERTIES ( 'transient_lastDdlTime'='1468173589')Time taken: 0.051 seconds, Fetched: 12 row(s)Although column value is defined as uniontype&lt;int,bigint&gt; in create table command, its type becomes uniontype&lt;void,int,bigint&gt; after table is defined. Hive accidentally make the nullable definition in avro schema (["null", "int", "long"]) into union definition.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14207" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Strip HiveConf hidden params in webui conf</summary>
      <description>HIVE-12338 introduced a new web ui, which has a page that displays the current HiveConf being used by HS2. However, before it displays that config, it does not strip entries from it which are considered "hidden" conf parameters, thus exposing those values from a web-ui for HS2. We need to add stripping to this.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14221" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>set SQLStdHiveAuthorizerFactoryForTest as default HIVE_AUTHORIZATION_MANAGER</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionHooks.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionCleanup.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestPlainSaslHelper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestViewEntity.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestColumnAccess.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestHooks.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.commands.TestCommands.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14229" opendate="2016-7-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>the jars in hive.aux.jar.paths are not added to session classpath</summary>
      <description>The jars in hive.reloadable.aux.jar.paths are being added to HiveServer2 classpath while hive.aux.jar.paths is not. Then the local task like 'select udf from src' will fail to find needed udf class.</description>
      <version>2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ReloadProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="14236" opendate="2016-7-14 00:00:00" fixdate="2016-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS with UNION ALL puts the wrong stats in Tez</summary>
      <description>to repo. in Tez, create table t as select * from src union all select * from src;</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14291" opendate="2016-7-19 00:00:00" fixdate="2016-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>count(*) on a table written by hcatstorer returns incorrect result</summary>
      <description>count(*) on a table written by hcatstorer returns wrong result. steps to repro the issue:1) create hive tablecreate table ${DEST_TABLE}(name string, age int, gpa float) row format delimited fields terminated by '\t' stored as textfile;2) load data into table using hcatstorerA = LOAD '$DATA_1' USING PigStorage() AS (name:chararray, age:int, gpa:float);B = LOAD '$DATA_2' USING PigStorage() AS (name:chararray, age:int, gpa:float);C = UNION A, B;STORE C INTO '$HIVE_TABLE' USING org.apache.hive.hcatalog.pig.HCatStorer();</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14322" opendate="2016-7-24 00:00:00" fixdate="2016-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgres db issues after Datanucleus 4.x upgrade</summary>
      <description>With the upgrade to datanucleus 4.x versions in HIVE-6113, hive does not work properly with postgres.The nullable fields in the database have string "NULL::character varying" instead of real NULL values. This causes various issues.One example is -hive&gt; create table t(i int);OKTime taken: 1.9 secondshive&gt; create view v as select * from t;OKTime taken: 0.542 secondshive&gt; select * from v;FAILED: SemanticException Unable to fetch table v. java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying</description>
      <version>2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14324" opendate="2016-7-25 00:00:00" fixdate="2016-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC PPD for floats is broken</summary>
      <description>ORC stores min/max stats, bloom filters by passing floats as doubles using java's widening conversion. So if we write a float value of 0.22 to ORC file, the min/max stats and bloom filter will use 0.2199999988079071 double value.But when we do PPD, SARG creates literals by converting float to string and then to double which compares 0.22 to 0.2199999988079071 and fails PPD evaluation. hive&gt; create table orc_float (f float) stored as orc;hive&gt; insert into table orc_float values(0.22);hive&gt; set hive.optimize.index.filter=true;hive&gt; select * from orc_float where f=0.22;OKhive&gt; set hive.optimize.index.filter=false;hive&gt; select * from orc_float where f=0.22;OK0.22This is not a problem for doubles and decimals.This issue was introduced in HIVE-8460 but back then there was no strict type check when SARGs are created and also PPD evaluation does not convert to column type. But now predicate leaf creation in SARG enforces strict type check for boxed literals and predicate type and PPD evaluation converts stats and constants to column type (predicate).</description>
      <version>1.3.0,2.0.0,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.basic.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
    </fixedFiles>
  </bug>
  <bug id="14367" opendate="2016-7-28 00:00:00" fixdate="2016-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Estimated size for constant nulls is 0</summary>
      <description>since type is incorrectly assumed as void.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.deep.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantfolding.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="14558" opendate="2016-8-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for listing views similar to "show tables"</summary>
      <description>Users have been asking for such feature where they can get a lists of views separately.So perhaps a syntax similar to "show tables" command?show views &amp;#91;in/from &lt;dbName&gt;&amp;#93; &amp;#91;&lt;pattern&gt;&amp;#93;Does it make sense to add such command? or is it not worth the effort?</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowTablesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="14590" opendate="2016-8-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path) Incorrect result set when limit is present in one of the union branches</summary>
      <description>Limit gets propagated outside union.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.null.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.null.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14672" opendate="2016-8-30 00:00:00" fixdate="2016-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timestamps to startup message in hive scripts.</summary>
      <description>if hive services are started as background processes (most certainly the case) with output from stdout and stderr redirected to a file, certain fatal errors, like OutOfMemoryErrors, are printed to stderr. Such errors often do not have a timestamp associated with it. So overtime, after multiple restarts of the hive processes, the stdout/err redirect log looks like thisStarting Hive Metastore ServerStarting Hive Metastore ServerException in thread "main" java.lang.OutOfMemoryError: Requested array size exceeds VM limitStarting Hive Metastore ServerIt would be useful to have a timestamp associated with the "Starting Hive Metastore Server" message to help debug when such errors may have occurred so we could look in the corresponding logs to help narrow down the issue.</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.metastore.sh</file>
      <file type="M">bin.ext.hiveserver2.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15054" opendate="2016-10-25 00:00:00" fixdate="2016-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive insertion query execution fails on Hive on Spark</summary>
      <description>The query of insert overwrite table tbl1 sometimes will fail with the following errors. Seems we are constructing taskAttemptId with partitionId which is not unique if there are multiple attempts.ava.lang.IllegalStateException: Hit error while closing operators - failing tree: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_task_tmp.-ext-10002/_tmp.002148_0 to: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_tmp.-ext-10002/002148_0at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:202)at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.closeRecordProcessor(HiveMapFunctionResultList.java:58)at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:106)at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)at scala.collection.Iterator$class.foreach(Iterator.scala:727)at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)</description>
      <version>2.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="15056" opendate="2016-10-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support index shifting for struct fields</summary>
      <description>In HIVE-13873, the following case doesn't work:select s.c from tblwhere tbl is of schema:a ints struct&lt;b:int,c:string&gt;This is because currently we generate a "pruned" schema (in terms of GroupType) for Parquet reader to scan the data. However, on the Hive side the object inspector still uses the original schema. In particular, in this case for s.c the data returned by Parquet reader is in index 0, but the object inspector tries to read it in index 1. Therefore, in correct result will be returned.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="15057" opendate="2016-10-25 00:00:00" fixdate="2016-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nested column pruning: support all operators</summary>
      <description>Currently only SELECT operators are supported for nested column pruning. We should add support for other types of operators so the optimization can work for complex queries.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.TestColumnPrunerProcCtx.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestDataWritableReadSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FieldNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15060" opendate="2016-10-25 00:00:00" fixdate="2016-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the autoCommit warning from beeline</summary>
      <description>WARN jdbc.HiveConnection: Request to set autoCommit to false; Hive does not support autoCommit=false.Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://ctr-e89-1466633100028-0275-01By default, this beeline setting is false, while hive only support autoCommit=true for now. So this warning does mot make sense and should be removed.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="15096" opendate="2016-10-29 00:00:00" fixdate="2016-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hplsql registerUDF conflicts with pom.xml</summary>
      <description>in hplsql code, registerUDF code is sql.add("ADD JAR " + dir + "hplsql.jar"); sql.add("ADD JAR " + dir + "antlr-runtime-4.5.jar"); sql.add("ADD FILE " + dir + Conf.SITE_XML);but pom configufation is &lt;parent&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive&lt;/artifactId&gt; &lt;version&gt;2.2.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;hive-hplsql&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Hive HPL/SQL&lt;/name&gt; &lt;dependency&gt; &lt;groupId&gt;org.antlr&lt;/groupId&gt; &lt;artifactId&gt;antlr4-runtime&lt;/artifactId&gt; &lt;version&gt;4.5&lt;/version&gt; &lt;/dependency&gt;when run hplsql , errors occur as below Error while processing statement: /opt/apps/apache-hive-2.0.0-bin/lib/hplsql.jar does not exist</description>
      <version>2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
    </fixedFiles>
  </bug>
  <bug id="15112" opendate="2016-11-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Parquet vectorization reader for Struct type</summary>
      <description>Like HIVE-14815, we need support Parquet vectorized reader for struct type.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedDictionaryEncodingColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedColumnReaderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedStructColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedColumnReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15160" opendate="2016-11-8 00:00:00" fixdate="2016-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t order by an unselected column</summary>
      <description>If a grouping key hasn't been selected, Hive complains. For comparison, Postgres does not.Example. Notice i_item_id is not selected:select i_item_desc ,i_category ,i_class ,i_current_price ,sum(cs_ext_sales_price) as itemrevenue ,sum(cs_ext_sales_price)*100/sum(sum(cs_ext_sales_price)) over (partition by i_class) as revenueratio from catalog_sales ,item ,date_dim where cs_item_sk = i_item_sk and i_category in ('Jewelry', 'Sports', 'Books') and cs_sold_date_sk = d_date_sk and d_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30 days) group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratiolimit 100;</description>
      <version>2.0.0,2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15161" opendate="2016-11-8 00:00:00" fixdate="2016-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate ColumnStats to use jackson</summary>
      <description>json.org has license issues jackson can provide a fully compatible alternative to it there are a few flakiness issues caused by the order of the map entries of the columns...this cat be addressed, org.json api was unfriendly in this manner</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestStatsSetupConst.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug id="15164" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default RPC port for llap to be a dynamic port</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15234" opendate="2016-11-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin cardinality estimation can be improved</summary>
      <description>Currently calcite optimization rules rely on (Hive)SemiJoin to represent semi join node, whereas Stats estimate use leftSemiJoin field of Join to estimate stats. As a result semi-join specific stats calculation logic is never hit since at plan generation time HiveSemiJoin is created and leftSemiJoin field of Join is never set.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
    </fixedFiles>
  </bug>
  <bug id="15236" opendate="2016-11-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>timestamp and date comparison should happen in timestamp</summary>
      <description>Currently it happens in string, which results in incorrect result.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cast.on.constant.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cast.on.constant.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="15247" opendate="2016-11-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass the purge option for drop table to storage handlers</summary>
      <description>This gives storage handler more control on how to handle drop table.</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15298" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test failures in TestCliDriver sample[2,4,6,7,9]</summary>
      <description>Failing for the past 5 builds:https://builds.apache.org/job/PreCommit-HIVE-Build/2301/testReport/</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample2.q</file>
    </fixedFiles>
  </bug>
  <bug id="15299" opendate="2016-11-29 00:00:00" fixdate="2016-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn-cluster and yarn-client deprecated in Spark 2.0</summary>
      <description>Need to use master "yarn" with specified deploy mode instead.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15311" opendate="2016-11-29 00:00:00" fixdate="2016-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze column stats should skip non-primitive column types</summary>
      <description>after this patch, when you compute column stats, it will skip the non-primitive column types and give you warning on the console.</description>
      <version>1.2.0,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.columnstats.tbllvl.complex.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15312" opendate="2016-11-30 00:00:00" fixdate="2016-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce logging in certain places</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15517" opendate="2016-12-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NOT (x &lt;=&gt; y) returns NULL if x or y is NULL</summary>
      <description>I created a table as following:create table test(x string, y string);insert into test values ('q', 'q'), ('q', 'w'), (NULL, 'q'), ('q', NULL), (NULL, NULL);Then I try to compare values taking NULLs into account:select *, x&lt;=&gt;y, not (x&lt;=&gt; y), (x &lt;=&gt; y) = false from test;OKq q true false falseq w false true trueq NULL false NULL trueNULL q false NULL trueNULL NULL true NULL falseI expected that 4th column will be the same as 5th one but actually got NULL as result of "not false" and "not true" expressions.Hive 1.2.1000.2.5.0.0-1245Subversion git://c66-slave-20176e25-3/grid/0/jenkins/workspace/HDP-parallel-centos6/SOURCES/hive -r da6c690d384d1666f5a5f450be5cbc54e2fe4bd6Compiled by jenkins on Fri Aug 26 01:39:52 UTC 2016From source with checksum c30648316a632f7a753f4359e5c8f4d6</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
    </fixedFiles>
  </bug>
  <bug id="15518" opendate="2016-12-27 00:00:00" fixdate="2016-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring rows and range related classes to put the window type on Window</summary>
      <description>/* * - A Window Frame that has only the /start/boundary, then it is interpreted as: BETWEEN &lt;start boundary&gt; AND CURRENT ROW * - A Window Specification with an Order Specification and no Window * Frame is interpreted as: ROW BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW * - A Window Specification with no Order and no Window Frame is interpreted as: ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING */The comments in WindowSpec above doesn't really match what it's claimed to do. Correct the comment to reduce the confusion.Also currently the window type is specified on each BoundarySpec but makes sense to put the type (rows or range) for each window.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15522" opendate="2016-12-28 00:00:00" fixdate="2016-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental ALTER_TABLE/ALTER_PTN including renames</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15766" opendate="2017-1-31 00:00:00" fixdate="2017-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBNotificationlistener leaks JDOPersistenceManager</summary>
      <description></description>
      <version>2.0.0,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15769" opendate="2017-1-31 00:00:00" fixdate="2017-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support view creation in CBO</summary>
      <description>Right now, set operator needs to run in CBO. If a view contains a set op, it will throw exception. We need to support view creation in CBO.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.selectDistinctStarNeg.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="16230" opendate="2017-3-16 00:00:00" fixdate="2017-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable CBO in presence of hints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder.q</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.on.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.comments.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.bucketmapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pcs.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.distinct.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.queries.clientnegative.bucket.mapjoin.mismatch1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.bucket.mapjoin.wrong.table.metadata.1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.bucket.mapjoin.wrong.table.metadata.2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.mapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join28.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join29.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join32.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join35.q</file>
      <file type="M">ql.src.test.queries.clientnegative.smb.bucketmapjoin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.smb.mapjoin.14.q</file>
      <file type="M">ql.src.test.queries.clientnegative.sortmerge.mapjoin.mismatch.1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.union22.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.comments.q</file>
      <file type="M">ql.src.test.queries.clientpositive.infer.bucket.sort.map.operators.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join25.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join26.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join27.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join30.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join36.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join37.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join39.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.map.ppr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.on.varchar.q</file>
    </fixedFiles>
  </bug>
  <bug id="16249" opendate="2017-3-17 00:00:00" fixdate="2017-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>With column stats, mergejoin.q throws NPE</summary>
      <description>stack trace:2017-03-17T16:00:26,356 ERROR [3d512d4d-72b5-48fc-92cb-0c72f7c876e5 main] parse.CalcitePlanner: CBO failed, skipping CBO.java.lang.NullPointerException at org.apache.calcite.rel.metadata.RelMdUtil.estimateFilteredRows(RelMdUtil.java:719) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:123) ~[calcite-core-1.10.0.jar:1.10.0] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:201) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:132) ~[calcite-core-1.10.0.jar:1.10.0] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:201) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.swapInputs(LoptOptimizeJoinRule.java:1866) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.createJoinSubtree(LoptOptimizeJoinRule.java:1739) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.addToTop(LoptOptimizeJoinRule.java:1216) ~[calcite-core-1.10.0.jar:1.10.0]</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.FilterSelectivityEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16372" opendate="2017-4-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable DDL statement for non-native tables (add/remove table properties)</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.alter.non.native.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="16386" opendate="2017-4-5 00:00:00" fixdate="2017-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add debug logging to describe why runtime filtering semijoins are removed</summary>
      <description>Add a few logging statements to detail the reason why semijoin optimizations are being removed, which can help during debugging.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="16511" opendate="2017-4-22 00:00:00" fixdate="2017-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO looses inner casts on constants of complex type</summary>
      <description>type for map &lt;10, cast(null as int)&gt; becomes map &lt;int,string&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16552" opendate="2017-4-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit the number of tasks a Spark job may contain</summary>
      <description>It's commonly desirable to block bad and big queries that takes a lot of YARN resources. One approach, similar to mapreduce.job.max.map in MapReduce, is to stop a query that invokes a Spark job that contains too many tasks. The proposal here is to introduce hive.spark.job.max.tasks with a default value of -1 (no limit), which an admin can set to block queries that trigger too many spark tasks.Please note that this control knob applies to a spark job, though it's possible that one query can trigger multiple Spark jobs (such as in case of map-join). Nevertheless, the proposed approach is still helpful.</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16553" opendate="2017-4-27 00:00:00" fixdate="2017-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value for hive.tez.bigtable.minsize.semijoin.reduction</summary>
      <description>Current value is 1M rows, would like to bump this up to make sure we are not creating semjoin optimizations on dimension tables, since having too many semijoin optimizations can cause serialized execution of tasks if lots of tasks are waiting for semijoin optimizations to be computed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16566" opendate="2017-5-1 00:00:00" fixdate="2017-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set column stats default as true when creating new tables/partitions</summary>
      <description></description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.int96.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.array.null.element.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compustat.avro.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.nonascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16628" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix query25 when it uses a mix of MergeJoin and MapJoin</summary>
      <description></description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
    </fixedFiles>
  </bug>
  <bug id="16633" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>username for ATS data shall always be the uid who submit the job</summary>
      <description>When submitting query via HS2, username for ATS data becomes HS2 process uid in case of hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator. This should always be the real user id to make ATS data more secure and useful.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16634" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Use a pool of connections to a single AM from a daemon</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="16653" opendate="2017-5-11 00:00:00" fixdate="2017-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mergejoin should give itself a correct tag</summary>
      <description>In a non-self-join mergejoin, e.g., we join tab_a and tab_b, it will give different branches different tags. However in a self-join mergejoin, both of the branches comes from the same table scan, we need to give it a correct tag.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MergeJoinProc.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16702" opendate="2017-5-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use LazyBinarySerDe for LLAP InputFormat</summary>
      <description>Currently using LazySimpleSerDe</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapRowInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16703" opendate="2017-5-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive may add the same file to the session and vertex in Tez</summary>
      <description>In newer versions of Tez, the check was added that treats this as an error.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16799" opendate="2017-6-1 00:00:00" fixdate="2017-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Control the max number of task for a stage in a spark job</summary>
      <description>HIVE-16552 gives admin an option to control the maximum number of tasks a Spark job may have. However, this may not be sufficient as this tends to penalize jobs that have many stages while favoring jobs that has fewer stages. Ideally, we should also limit the number of tasks in a stage, which is closer to the maximum number of mappers or reducers in a MR job.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="168" opendate="2008-12-11 00:00:00" fixdate="2008-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>join on a subquery with a group by does not work</summary>
      <description>SELECT a.key, a.value, b.key, b.value FROM ( SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key ) a FULL OUTER JOIN ( SELECT src2.key as key, count(distinct(src2.value)) AS value FROM src1 src2 group by src2.key ) b ON (a.key = b.key);does not generate the plan correctly</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RuleRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DefaultRuleDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16815" opendate="2017-6-2 00:00:00" fixdate="2017-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from error for the rest of modules</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
      <file type="M">testutils.src.java.org.apache.hive.testutils.jdbc.HiveBurnInClient.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingTransaction.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionIterable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsRebuildLockHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceOperations.java</file>
      <file type="M">service.src.java.org.apache.hive.service.Service.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.java</file>
      <file type="M">common.src.java.org.apache.hive.http.ProfileServlet.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.GenericMR.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseStructValue.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseValueFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.ConsumerFeedback.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataInputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataOutputStream.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.CustomQueryFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.SearchResultHandler.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.UserFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PasswdAuthenticationProvider.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
    </fixedFiles>
  </bug>
  <bug id="16819" opendate="2017-6-2 00:00:00" fixdate="2017-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add MM test for temporary table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
    </fixedFiles>
  </bug>
  <bug id="17019" opendate="2017-7-4 00:00:00" fixdate="2017-1-4 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add support to download debugging information as an archive.</summary>
      <description>Given a queryId or dagId, get all information related to it: like, tez am, task logs, hive ats data, tez ats data, slider am status, etc. Package it into and archive.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortUnionReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRemoveSqCountCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveDefaultCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="18092" opendate="2017-11-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception on tables handled by HBaseHandler if columnsstats are auto-gathered</summary>
      <description>currently; if column.stats.autogather is enabled; the hbase tests are run into an exception which arises from the fact that basic stats are not collected on that table...</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.snapshot.q</file>
    </fixedFiles>
  </bug>
  <bug id="18093" opendate="2017-11-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging when HoS application is killed</summary>
      <description>When a HoS jobs is explicitly killed via a user (via a yarn command), the logs just say "RPC channel closed"</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.BaseProtocol.java</file>
    </fixedFiles>
  </bug>
  <bug id="18095" opendate="2017-11-17 00:00:00" fixdate="2017-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a unmanaged flag to triggers (applies to container based sessions)</summary>
      <description>cc prasanth_jIt should be impossible to attach global triggers for pools. Setting global flag should probably automatically remove attachments to pools.Global triggers would only support actions that Tez supports (for simplicity; also, for now, move doesn't make a lot of sense because the trigger would apply again after the move).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMTrigger.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMTrigger.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.SessionTriggerProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrDropTriggerToPoolMappingDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ResourcePlanParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.045-HIVE-17566.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.046-HIVE-17566.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.031-HIVE-17566.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.046-HIVE-17566.derby.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersNoTezSessionPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="181" opendate="2008-12-16 00:00:00" fixdate="2008-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore UDFTestLength unit test for UDFs</summary>
      <description>HADOOP-4070 added a unit test in ql/src/test/queries/clientpositive/udf2.q (@697035), but it was (accidentally?) lost in the commit for HADOOP-4230 (@706704).</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18251" opendate="2017-12-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loosen restriction for some checks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18737" opendate="2018-2-16 00:00:00" fixdate="2018-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to disable LLAP IO ACID for non-original files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18738" opendate="2018-2-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO ACID - includes handling is broken</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.GenericColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18739" opendate="2018-2-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Import/Export from Acid table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnAddPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExportWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20325" opendate="2018-8-6 00:00:00" fixdate="2018-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlakyTest: TestMiniDruidCliDriver</summary>
      <description>TestMiniDruidCliDriver is failing intermittently a significant percentage of the time.druid_timestamptzdruidmini_joinsdruidmini_maskingdruidmini_test1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniDruidKafkaCliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20326" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create constraints with RELY as default instead of NO RELY</summary>
      <description>Currently constraints such as NOT NULL, CHECK are created with ENABLE and NO RELY as default, instead it should be created with ENABLE and RELY as default so that optimizer could take advantage of these constraints.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20723" opendate="2018-10-10 00:00:00" fixdate="2018-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow per table specification of compaction yarn queue</summary>
      <description>Currently compactions of full CRUD transactional tables are Map-Reduce jobs submitted to a yarn queue defined byhive.compactor.job.queue property.If would be useful to be able to override this on per table basis by putting it into table properties so that compactions for different tables can use different queues.There is already ability to override other compaction related configs via table props, though this will need additional handling to set the queue name CompactorMr.createBaseJobConfhttps://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-TablePropertiesSee CopactorMR.COMPACTOR_PREFIX and Initiator.COMPACTORTHRESHOLD_PREFIX</description>
      <version>2.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="20727" opendate="2018-10-11 00:00:00" fixdate="2018-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable flaky test: stat_estimate_related_col.q</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="21034" opendate="2018-12-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to schematool to drop Hive databases</summary>
      <description>An option to remove all Hive managed data could be a useful addition to schematool.I propose to introduce a new flag -dropAllDatabases that would drop all databases with CASCADE to remove all data of managed tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="22416" opendate="2019-10-28 00:00:00" fixdate="2019-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR-related operation logs missing when parallel execution is enabled</summary>
      <description>Repro steps: 1. Happy path, parallel execution disabled0: jdbc:hive2://localhost:10000&gt; set hive.exec.parallel=false;No rows affected (0.023 seconds)0: jdbc:hive2://localhost:10000&gt; select count (*) from t1;INFO : Compiling command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d): select count (*) from t1INFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)INFO : Completed compiling command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d); Time taken: 0.309 secondsINFO : Executing command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d): select count (*) from t1WARN : INFO : Query ID = karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7dINFO : Total jobs = 1INFO : Launching Job 1 out of 1INFO : Starting task [Stage-1:MAPRED] in serial modeINFO : Number of reduce tasks determined at compile time: 1INFO : In order to change the average load for a reducer (in bytes):INFO : set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;INFO : In order to limit the maximum number of reducers:INFO : set hive.exec.reducers.max=&lt;number&gt;INFO : In order to set a constant number of reducers:INFO : set mapreduce.job.reduces=&lt;number&gt;DEBUG : Configuring job job_local495362389_0008 with file:/tmp/hadoop/mapred/staging/karencoppage495362389/.staging/job_local495362389_0008 as the submit dirDEBUG : adding the following namenodes' delegation tokens:[file:///]DEBUG : Creating splits at file:/tmp/hadoop/mapred/staging/karencoppage495362389/.staging/job_local495362389_0008INFO : number of splits:0INFO : Submitting tokens for job: job_local495362389_0008INFO : Executing with tokens: []INFO : The url to track the job: http://localhost:8080/INFO : Job running in-process (local Hadoop)INFO : 2019-10-28 15:26:22,537 Stage-1 map = 0%, reduce = 100%INFO : Ended Job = job_local495362389_0008INFO : MapReduce Jobs Launched: INFO : Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSINFO : Total MapReduce CPU Time Spent: 0 msecINFO : Completed executing command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d); Time taken: 6.497 secondsINFO : OKDEBUG : Shutting down query select count (*) from t1+-----+| c0 |+-----+| 0 |+-----+1 row selected (11.874 seconds)2. Faulty path, parallel execution enabled0: jdbc:hive2://localhost:10000&gt; set hive.server2.logging.operation.level=EXECUTION;No rows affected (0.236 seconds)0: jdbc:hive2://localhost:10000&gt; set hive.exec.parallel=true;No rows affected (0.01 seconds)0: jdbc:hive2://localhost:10000&gt; select count (*) from t1;INFO : Compiling command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77): select count (*) from t1INFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)INFO : Completed compiling command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77); Time taken: 4.707 secondsINFO : Executing command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77): select count (*) from t1WARN : INFO : Query ID = karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77INFO : Total jobs = 1INFO : Launching Job 1 out of 1INFO : Starting task [Stage-1:MAPRED] in parallelINFO : MapReduce Jobs Launched: INFO : Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSINFO : Total MapReduce CPU Time Spent: 0 msecINFO : Completed executing command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77); Time taken: 44.577 secondsINFO : OKDEBUG : Shutting down query select count (*) from t1+-----+| c0 |+-----+| 0  |+-----+1 row selected (54.665 seconds)The issue is that Log4J stores the session ID and query ID in some atomic thread metadata (org.apache.logging.log4j.ThreadContext.getImmutableContext()). If the queryId is missing from this metadata, then the RoutingAppender (which is defined programmatically in LogDivertAppender) will route the log to a NullAppender, which logs nothing. If the queryId is present, then the RoutingAppender routes the event to the "query-appender", which will log the line in the operation log/Beeline. This is not happening in a multi-threaded context since new threads created for parallel query execution do not have the queryId/sessionId metadata.The solution is to addthe queryId/sessionId metadata to any new threads created for MR work.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23026" opendate="2020-3-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for custom YARN application name for TEZ queries</summary>
      <description>Currently tez on hiveServer2 cannot specify yarn application name, which is not very convenient for locating the problem SQL, so i added a configuration item to support setting tez job name</description>
      <version>2.0.0</version>
      <fixedVersion>2.4.0,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23029" opendate="2020-3-16 00:00:00" fixdate="2020-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Shuffle Handler should support Index Cache configuration</summary>
      <description>Queries like Q78 at large scale misses index cache with unordered edges. (24 * 1009 =24216. With the default 10 MB cache size, it can accommodate only 400+ entries).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.IndexCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="23030" opendate="2020-3-16 00:00:00" fixdate="2020-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable sketch union-s to be rolled up</summary>
      <description>Enabling rolling up sketch aggregates could enable the matching of materialized views created for higher dimensions to be applied for lower dimension cases.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DataSketchesFunctions.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23031" opendate="2020-3-16 00:00:00" fixdate="2020-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to enable transparent rewrite of count(distinct) into sketch functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.sketches.materialized.view.rollup.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DataSketchesFunctions.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23032" opendate="2020-3-16 00:00:00" fixdate="2020-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add batching in Lock generation</summary>
      <description>Replace multi-row insert in Oracle with batching. Performance tests showed significant performance improvement after turning batching on.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.HikariCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.DbCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.BoneCPDataSourceProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="2674" opendate="2011-12-22 00:00:00" fixdate="2011-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_partitions_ps throws TApplicationException if table doesn&amp;#39;t exist</summary>
      <description>If the table passed to get_partition_ps doesn't exist, a NPE is thrown by getPartitionPsQueryResults. There should be a check here, which throws a NoSuchObjectException if the table doesn't exist.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="4243" opendate="2013-3-28 00:00:00" fixdate="2013-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix column names in FileSinkOperator</summary>
      <description>All of the ObjectInspectors given to SerDe's by FileSinkOperator have virtual column names. Since the files are part of tables, Hive knows the column names. For self-describing file formats like ORC, having the real column names will improve the understandability.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.json</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Writer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4246" opendate="2013-3-28 00:00:00" fixdate="2013-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement predicate pushdown for ORC</summary>
      <description>By using the push down predicates from the table scan operator, ORC can skip over 10,000 rows at a time that won't satisfy the predicate. This will help a lot, especially if the file is sorted by the column that is used in the predicate.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.BitFieldReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgument.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestBitPack.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInStream.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRunLengthByteReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRunLengthIntegerReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4924" opendate="2013-7-24 00:00:00" fixdate="2013-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Support query timeout for jdbc</summary>
      <description>support Statement.setQueryTimeout(int timeout)</description>
      <version>2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationState.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ICLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.EmbeddedCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TOperationState.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TExecuteStatementReq.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6535" opendate="2014-3-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: provide an async API to execute query and fetch results</summary>
      <description>The hive jdbc client waits query completion during execute() call. It would be better to block in the jdbc for completion when the results are being fetched.This way the application using hive jdbc driver can do other tasks while asynchronous query execution is happening, until it needs to fetch the result set.</description>
      <version>0.14.0,1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="9499" opendate="2015-1-28 00:00:00" fixdate="2015-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.limit.query.max.table.partition makes queries fail on non-partitioned tables</summary>
      <description>If you use hive.limit.query.max.table.partition to limit the amount of partitions that can be queried it makes queries on non-partitioned tables fail.Example:CREATE TABLE tmp(test INT);SELECT COUNT(*) FROM TMP; -- works fineSET hive.limit.query.max.table.partition=20;SELECT COUNT(*) FROM TMP; -- generates NPE (FAILED: NullPointerException null)SET hive.limit.query.max.table.partition=-1;SELECT COUNT(*) FROM TMP; -- works fine again</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="95" opendate="2008-12-2 00:00:00" fixdate="2008-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve cli error messages by lowering backtracking to 1</summary>
      <description>Stop antlr from backtracking so much should (and does) improve error messages since antlr will report the error closer to where it happened.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9696" opendate="2015-2-15 00:00:00" fixdate="2015-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address RB comments for HIVE-9425 [Spark Branch]</summary>
      <description>A followup task of HIVE-9425.The pending RB comment can be found here.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="9699" opendate="2015-2-16 00:00:00" fixdate="2015-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend PTFs to provide referenced columns for CP</summary>
      <description>As described in HIVE-9341, If PTFs can provide referenced column names, column pruner can use that.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.matchpath.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.MatchPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9756" opendate="2015-2-23 00:00:00" fixdate="2015-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: use log4j 2 for llap (log to separate files, etc.)</summary>
      <description>For the INFO logging, we'll need to use the log4j-jcl 2.x upgrade-path to get throughput friendly logging.http://logging.apache.org/log4j/2.0/manual/async.html#Performance</description>
      <version>2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils.java</file>
      <file type="M">llap-server.src.main.resources.templates.py</file>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.bin.llap-daemon-env.sh</file>
    </fixedFiles>
  </bug>
</bugrepository>
