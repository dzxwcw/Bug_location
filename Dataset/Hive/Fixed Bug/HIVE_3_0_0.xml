<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10364" opendate="2015-4-16 00:00:00" fixdate="2015-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The HMS upgrade script test does not publish results when prepare.sh fails.</summary>
      <description>The HMS upgrade script must publish succeed or failure results to JIRA. This bug is not publishing any results on JIRA is the prepare.sh script fails.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="11297" opendate="2015-7-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Combine op trees for partition info generating tasks</summary>
      <description>Currently, for dynamic partition pruning in Spark, if a small table generates partition info for more than one partition columns, multiple operator trees are created, which all start from the same table scan op, but have different spark partition pruning sinks.As an optimization, we can combine these op trees and so don't have to do table scan multiple times.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
    </fixedFiles>
  </bug>
  <bug id="12734" opendate="2015-12-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundancy in HiveConfs serialized to UDFContext</summary>
      <description>HCatLoader lands up serializing one HiveConf instance per table-alias, to Pig's UDFContext. This lands up bloating the UDFContext.To reduce the footprint, it makes sense to serialize a default-constructed HiveConf once, and one "diff" per HCatLoader. This should reduce the time taken to kick off jobs from pig -useHCatalog scripts.(Note_to_self: YHIVE-540).</description>
      <version>1.2.1,2.0.0,2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15016" opendate="2016-10-19 00:00:00" fixdate="2016-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run tests with Hadoop 3.0.0-beta1</summary>
      <description>Hadoop 3.0.0-alpha1 was released back on Sep/16 to allow other components run tests against this new version before GA.We should start running tests with Hive to validate compatibility against Hadoop 3.0.NOTE: The patch used to test must not be committed to Hive until Hadoop 3.0 GA is released.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.HdfsUtils.java</file>
      <file type="M">common.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseRowSerializer.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ResultWritable.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestPutResultWritable.java</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.bulk.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.bulk.q</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.handler.bulk.q.out</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.mock.MockUriInfo.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.ManyMiniCluster.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.java</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.external1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.external2.q.out</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1555" opendate="2010-8-18 00:00:00" fixdate="2010-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC Storage Handler</summary>
      <description>With the Cassandra and HBase Storage Handlers I thought it would make sense to include a generic JDBC RDBMS Storage Handler so that you could import a standard DB table into Hive. Many people must want to perform HiveQL joins, etc against tables in other systems etc.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15995" opendate="2017-2-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Syncing metastore table with serde schema</summary>
      <description>Hive enables table schema evolution via properties. For avro e.g. we can alter the 'avro.schema.url' property to update table schema to the next version. Updating properties however doesn't affect column list stored in metastore DB so the table is not in the newest version when returned from metastore API. This is problem for tools working with metastore (e.g. Presto).To solve this issue I suggest to introduce new DDL statement syncing metastore columns with those from serde:ALTER TABLE user_test1 UPDATE COLUMNSNote that this is format independent solution. To reproduce, follow the instructions below: Create table based on avro schema version 1 (cxv1.avsc)CREATE EXTERNAL TABLE user_test1 PARTITIONED BY (dt string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' LOCATION '/tmp/schema-evolution/user_test1' TBLPROPERTIES ('avro.schema.url'='/tmp/schema-evolution/cx1.avsc'); Update schema to version 2 (cx2.avsc)ALTER TABLE user_test1 SET TBLPROPERTIES ('avro.schema.url' = '/tmp/schema-evolution/cx2.avsc'); Print serde columns (top info) and metastore columns (Detailed Table Information):DESCRIBE EXTENDED user_test1</description>
      <version>1.2.1,2.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16225" opendate="2017-3-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in webhcat service (FileSystem CACHE entries)</summary>
      <description>This is a known beast. here are detailsThe problem seems to be similar to the one discussed in HIVE-13749. If we submit very large number of jobs like 1000 to 2000 then we can see increase in Configuration objects count.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16305" opendate="2017-3-27 00:00:00" fixdate="2017-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM</summary>
      <description>This is a followup for HIVE-16160. We see additional ClassLoaderResolverImpl leaks even with the patch.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16316" opendate="2017-3-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prepare master branch for 3.0.0 development.</summary>
      <description>master branch is now being used for 3.0.0 development. The build files will need to reflect this change.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.2.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.2.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.2.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.2.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.2.0-to-3.0.0.derby.sql</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-util.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">serde.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16318" opendate="2017-3-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cache: address some issues in 2.2/2.3</summary>
      <description>We've run into HIVE-16233 and HIVE-15665 and given that 2.2 and 2.3 releases are approaching we are going to add workarounds for them, and then commit the above patches and revert the workarounds as soon as we can.Unfortunately this will result in cache wasting some memory on some datasets, but the alternatives, when they are encountered (usually only on large datasets), are worse.</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16319" opendate="2017-3-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Better handling of an empty wait queue, should try scheduling checks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.EvictingPriorityBlockingQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="1633" opendate="2010-9-13 00:00:00" fixdate="2010-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineHiveInputFormat fails with "cannot find dir for emptyFile"</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16334" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query lock contains the query string, which can cause OOM on ZooKeeper</summary>
      <description>When there are big number of partitions in a query this will result in a huge number of locks on ZooKeeper. Since the query object contains the whole query string this might cause serious memory pressure on the ZooKeeper services.It would be good to have the possibility to truncate the query strings that are written into the locks</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16343" opendate="2017-3-31 00:00:00" fixdate="2017-5-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Publish YARN&amp;#39;s ProcFs based memory usage to metrics for monitoring</summary>
      <description>Publish MemInfo from ProcfsBasedProcessTree to llap metrics. This will useful for monitoring and also setting up triggers via JMC.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapDaemonInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="16383" opendate="2017-4-5 00:00:00" fixdate="2017-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to HikariCP as default connection pooling</summary>
      <description>Since 3.0 is planned to move to JDK8, we can now switch to HikariCP as default connection pooling for DN because of its improved performance over others.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16388" opendate="2017-4-5 00:00:00" fixdate="2017-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Log rotation for daemon, history and gc files</summary>
      <description>GC logs need to be rotated by date.LLAP daemon history logs as wellIdeally, the daemon.out file needs the sameNeed to be able to download relevant logfiles for a time window.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16390" opendate="2017-4-5 00:00:00" fixdate="2017-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO should take job config into account; also LLAP config should load defaults</summary>
      <description>Ensure the config is used consistently with task-based execution by default; the exceptions should be specific (settings we don't want overridden, like zero-copy).</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="16394" opendate="2017-4-6 00:00:00" fixdate="2017-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS does not support queue name change in middle of session</summary>
      <description>The mapreduce.job.queuename only effects when HoS executes its query first time. After that, changing mapreduce.job.queuename won't change the query yarn scheduler queue name.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16395" opendate="2017-4-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConcurrentModificationException on config object in HoS</summary>
      <description>Looks like this is happening inside spark executors, looks to be some race condition when modifying Configuration objects.Stack-Trace:java.io.IOException: java.lang.reflect.InvocationTargetException at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:267) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&lt;init&gt;(HadoopShimsSecure.java:213) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:334) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:682) at org.apache.spark.rdd.HadoopRDD$$anon$1.&lt;init&gt;(HadoopRDD.scala:240) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:253) ... 21 moreCaused by: java.util.ConcurrentModificationException at java.util.Hashtable$Enumerator.next(Hashtable.java:1167) at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2455) at org.apache.hadoop.fs.s3a.S3AUtils.propagateBucketOptions(S3AUtils.java:716) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:181) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2815) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:98) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2852) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2834) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:387) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296) at org.apache.hadoop.mapred.LineRecordReader.&lt;init&gt;(LineRecordReader.java:108) at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&lt;init&gt;(CombineHiveRecordReader.java:68) ... 26 more</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16402" opendate="2017-4-6 00:00:00" fixdate="2017-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 2.8.0</summary>
      <description>Hadoop 2.8.0 has been out since March, we should upgrade to it. Release notes for Hadoop 2.8.x are here: http://hadoop.apache.org/docs/r2.8.0/index.htmlIt has a number of useful features, improvements for S3 support, ADLS support, etc. along with a bunch of other fixes. This should also help us on our way to upgrading to Hadoop 3.x (HIVE-15016).</description>
      <version>None</version>
      <fixedVersion>2.2.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16403" opendate="2017-4-6 00:00:00" fixdate="2017-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP UI shows the wrong number of executors</summary>
      <description>Queued tasks are added twice.</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.metrics.js</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonMXBean.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16425" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: unload old hashtables before reloadHashTable</summary>
      <description>@Override protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException { // The super method will reload a hash table partition of one of the small tables. // Currently, for native vector map join it will only be one small table. super.reloadHashTable(pos, partitionId); MapJoinTableContainer smallTable = spilledMapJoinTables[pos]; vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable); needHashTableSetup = true; LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName()); if (isLogDebugEnabled) { LOG.debug(CLASS_NAME + " reloadHashTable!"); } }The super call causes an OOM because of existing memory usage by vectorMapJoinHashTable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16429" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</summary>
      <description>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16436" opendate="2017-4-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Response times in "Task Execution Summary" at the end of the job is not correct</summary>
      <description>"Task execution summary" is printed at the of running a hive query. E.gTask Execution Summary---------------------------------------------------------------------------------------------- VERTICES DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS---------------------------------------------------------------------------------------------- Map 1 277869.00 0 0 1,500,000,000 1,500,000,000 Map 2 277868.00 0 0 5,999,989,709 31,162,299 Reducer 3 59875.00 0 0 1,531,162,299 2,018 Reducer 4 2436.00 0 0 2,018 2 Reducer 5 375.00 0 0 2 0----------------------------------------------------------------------------------------------Response times reported here for Map-1/Map-2 is not correct. Not sure if this is broken due to any other patch. Creating this jira for tracking purpose.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.RenderStrategy.java</file>
    </fixedFiles>
  </bug>
  <bug id="1645" opendate="2010-9-16 00:00:00" fixdate="2010-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ability to specify parent directory for zookeeper lock manager</summary>
      <description>For concurrency support, it would be desirable if all the locks were created under a common parent, so that zookeeper can be usedfor different purposes.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestParse.vm</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16460" opendate="2017-4-17 00:00:00" fixdate="2017-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In the console output, show vertex list in topological order instead of an alphabetical sort</summary>
      <description>cc prasanth_j</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.monitoring.TestTezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16462" opendate="2017-4-17 00:00:00" fixdate="2017-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Enabling hybrid grace disables specialization of all reduce side joins</summary>
      <description>Observed by gopalv.Having grace hash join enabled prevents the specialized vector hash joins during the vectorizer stage of query planning. However hive.llap.enable.grace.join.in.llap will later disable grace hash join (LlapDecider runs after Vectorizer). If we can disable the grace hash join before vectorization kicks in then we can still benefit from the specialized vector hash joins.This can be special cased for the llap.execution.mode=only case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16503" opendate="2017-4-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Oversubscribe memory for noconditional task size</summary>
      <description>When running map joins in llap, it can potentially use more memory for hash table loading (assuming other executors in the daemons have some memory to spare). This map join conversion decision has to be made during compilation that can provide some more room for LLAP.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.vector.dynpart.hashjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.user.level.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16536" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various improvements in TestPerfCliDriver</summary>
      <description>Goal is to reduce the size of stats file used to import stats in metastore. This will help to run this tests on partitioned tables.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.files.tpcds-perf.metastore.export.csv.TAB.COL.STATS.txt</file>
      <file type="M">data.files.tpcds-perf.metastore.export.csv.TABLE.PARAMS.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16540" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>dynamic_semijoin_user_level is failing on MiniLlap</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16545" opendate="2017-4-27 00:00:00" fixdate="2017-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: bug in arena size determination logic</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16546" opendate="2017-4-27 00:00:00" fixdate="2017-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fail map join tasks if hash table memory exceeds threshold</summary>
      <description>When map join task is running in llap, it can potentially use lot more memory than its limit which could be memory per executor or no conditional task size. If it uses more memory, it can adversely affect other query performance or it can even bring down the daemon. In such cases, it is better to fail the query than to bring down the daemon.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashSet.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.mapjoin.TestMapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastValueStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxIntervalDayTime.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarDecimal.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilterMerge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountMerge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdPopTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdSampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarPopTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarSampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashMultiSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="16547" opendate="2017-4-27 00:00:00" fixdate="2017-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: may not unlock buffers in some cases</summary>
      <description>Actually this is a pretty major bug, no idea how it slipped before.If last RG is not selected, dictionary buffers will not be unlocked because of bad assumptions about what isLastRg means (last in processing vs last in the stripe).</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="1655" opendate="2010-9-17 00:00:00" fixdate="2010-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding consistency check at jobClose() when committing dynamic partitions</summary>
      <description>In case of dynamic partition insert, FileSinkOperator generated a directory for a new partition and the files in the directory is named with '_tmp*'. When a task succeed, the file is renamed to remove the "_tmp", which essentially implement the "commit" semantics. A lot of exceptions could happen (process got killed, machine dies etc.) could left the _tmp files exist in the DP directory. These _tmp files should be deleted ("rolled back") at successful jobClose(). After the deletion, we should also delete any empty directories.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16559" opendate="2017-4-28 00:00:00" fixdate="2017-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet schema evolution for partitioned tables may break if table and partition serdes differ</summary>
      <description>Parquet schema evolution should make it possible to have partitions/tables backed by files with different schemas. Hive should match the table columns with file columns based on the column name if possible.However if the serde for a table is missing columns from the serde of a partition Hive fails to match the columns together.Steps to reproduce:CREATE TABLE myparquettable_parted( name string, favnumber int, favcolor string, age int, favpet string)PARTITIONED BY (day string)STORED AS PARQUET;INSERT OVERWRITE TABLE myparquettable_partedPARTITION(day='2017-04-04')SELECT 'mary' as name, 5 AS favnumber, 'blue' AS favcolor, 35 AS age, 'dog' AS favpet;alter table myparquettable_partedREPLACE COLUMNS(favnumber int,age int); &lt;!--- No cascade option, so the partition will not be altered. SELECT * FROM myparquettable_parted where day='2017-04-04';will fail with:java.lang.UnsupportedOperationException: Cannot inspect org.apache.hadoop.io.IntWritableHive should either match the columns together or prevent the user from dropping columns from the table.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16571" opendate="2017-5-2 00:00:00" fixdate="2017-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2: Prefer LIFO over round-robin for Tez session reuse</summary>
      <description>Currently Tez session reuse is entirely round-robin, which means a single user might have to run upto 32 queries before reusing a warm session on a HiveServer2.This is not the case when session reuse is disabled, with a user warming up their session on the 1st query.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="16573" opendate="2017-5-3 00:00:00" fixdate="2017-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In-place update for HoS can&amp;#39;t be disabled</summary>
      <description>hive.spark.exec.inplace.progress has no effect</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
    </fixedFiles>
  </bug>
  <bug id="16581" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>a bug in HIVE-16523</summary>
      <description>A bug</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
    </fixedFiles>
  </bug>
  <bug id="16584" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Warning messages should use LogHelper.printInfo instead of printing to the infoStream directly</summary>
      <description>There are several cases when warning messages are printed to the console outputstream directly. These warnings do not show up in the BeeLine output.We should use the the printInfo method instead.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SparkCrossProductCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16591" opendate="2017-5-5 00:00:00" fixdate="2017-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DR for function Binaries on HDFS</summary>
      <description>We have to make sure that during incremental dump we dont allow functions to be copied if they have local filesystem "file://" resources. &amp;#8211; depends how much system side work we want to do, We are going to explicitly provide a caveat for replicating functions where in, only functions created "using" clause will be replicated and the "using" clause prohibits creating functions with the local "file://" resources and hence doing additional checks when doing repl dump might not be required. We have to make sure that during the bootstrap / incremental dump we append the namenode host + port if functions are created without the fully qualified location of uri on hdfs, not sure how this would play for S3 or WASB filesystem. We have to copy the binaries of a function resource list on CREATE / DROP FUNCTION . The change management file system has to keep a copy of the binary when a DROP function is called, to provide capability of updating binary definition for existing functions along with DR. An example of list of steps is given in doc (ReplicateFunctions.pdf ) attached in parent Issue.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.MetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FunctionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16592" opendate="2017-5-5 00:00:00" fixdate="2017-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Long hashCodes should bit-mix into lower bits</summary>
      <description>public static int calculateLongHashCode(long key) { key = (~key) + (key &lt;&lt; 21); // key = (key &lt;&lt; 21) - key - 1; key = key ^ (key &gt;&gt;&gt; 24); key = (key + (key &lt;&lt; 3)) + (key &lt;&lt; 8); // key * 265 key = key ^ (key &gt;&gt;&gt; 14); key = (key + (key &lt;&lt; 2)) + (key &lt;&lt; 4); // key * 21 key = key ^ (key &gt;&gt;&gt; 28); key = key + (key &lt;&lt; 31); return (int) key; }Does not mix enough bits into the lower 32 bits, which are used for the bucket probes.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimalImpl.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HashCodeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="16599" opendate="2017-5-5 00:00:00" fixdate="2017-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in runtime filtering cost when handling SMB Joins</summary>
      <description>A test with SMB joins failed with NPE in runtime filtering costing logic.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="166" opendate="2008-12-11 00:00:00" fixdate="2008-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create eclipse project template</summary>
      <description>In order to make it easier for developers using Eclipse to get started we could provide project templates much like Hadoop does. These can then be copied into place by an ant target. We should also add the copies into svn:ignore as per HIVE-101 to avoid having them pop up when committing code.See HADOOP-1228 for details.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.get.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16605" opendate="2017-5-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce NOT NULL constraints</summary>
      <description>Since NOT NULL is so common it would be great to have tables start to enforce that.ekoifman described a possible approach in HIVE-16575:One way to enforce not null constraint is to have the optimizer add enforce_not_null UDF which throws if it sees a NULL, otherwise it's pass through.So if 'b' has not null constraint,Insert into T select a,b,c... would becomeInsert into T select a, enforce_not_null(b), c.....This would work for any table type.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.with.constraints.enforced.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.with.constraints.enable.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.create.with.constraints.enforced.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.with.constraints.enable.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16635" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Progressbar: Use different timeouts for running queries</summary>
      <description>Fewer getProgress() calls can speed up Tez by ~20% - see TEZ-3719 for other half of this improvement.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16637" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve end-of-data checking for LLAP input format</summary>
      <description>The existing end of stream checking in the record reader is too forgiving of errors and does not recognize situations where the server connection has closed abruptly like HIVE-14093.Try to add a way to indicate that we have truly hit the end of the stream.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16639" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Derive shuffle thread counts and keep-alive connections from instance count</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16640" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The ASF Headers have some errors in some class</summary>
      <description>I found some class license hive placed in an incorrect location, some classes missing license</description>
      <version>3.0.0</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestOpenCSVSerde.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapperTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperatorNames.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestSparkCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.src.main.java.hive.it.custom.udfs.vector.VectorStringRot13.java</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.src.main.java.hive.it.custom.udfs.GenericUDFRot13.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestLogUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CopyOnFirstWriteProperties.java</file>
    </fixedFiles>
  </bug>
  <bug id="16642" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Events created as part of replv2 potentially break replv1</summary>
      <description>We have a couple of new events introduced, such as {CREATE,DROP}{INDEX,FUNCTION} since the introduction of replv1, but those which do not have a replv1 ReplicationTask associated with them.Thus, for users like Falcon, we potentially wind up throwing a IllegalStateException if replv1 based HiveDR is running on a cluster with these updated events.Thus, we should be more graceful when encountering them, returning a NoopReplicationTask equivalent that they can make use of, or ignore, for such newer events.In addition, we should add additional test cases so that we track whether or not the creation of these events leads to any backward incompatibility we introduce. To this end, if any of the events should change so that we introduce a backward incompatibility, we should have these tests fail, and alert us to that possibility.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory.java</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16646" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alias in transform ... as clause shouldn&amp;#39;t be case sensitive</summary>
      <description>Create a table like below:CREATE TABLE hive_bug(col1 string);Run below query in Hive:from hive_bug select transform(col1) using '/bin/cat' as (AAAA string);The result would be:0: jdbc:hive2://localhost:10000&gt; from hive_bug select transform(col1) using '/bin/cat' as (AAAA string);......INFO : OK+-------+--+| AAAA |+-------+--++-------+--+The output column name is AAAA instead of the lowercase aaaa.</description>
      <version>None</version>
      <fixedVersion>2.3.2,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16659" opendate="2017-5-12 00:00:00" fixdate="2017-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query plan should reflect hive.spark.use.groupby.shuffle</summary>
      <description>It's useful to show the shuffle type used in the query plan. Currently it shows "GROUP" no matter what we set for hive.spark.use.groupby.shuffle.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RepartitionShuffler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16660" opendate="2017-5-12 00:00:00" fixdate="2017-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not able to add partition for views in hive when sentry is enabled</summary>
      <description>Repro:create table tesnit (a int) partitioned by (p int);insert into table tesnit partition (p = 1) values (1);insert into table tesnit partition (p = 2) values (1);create view test_view partitioned on (p) as select * from tesnit where p =1;alter view test_view add partition (p = 2);Error: Error while compiling statement: FAILED: SemanticException &amp;#91;Error 10056&amp;#93;: The query does not reference any valid partition. To run this query, set hive.mapred.mode=nonstrict (state=42000,code=10056)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
    </fixedFiles>
  </bug>
  <bug id="16667" opendate="2017-5-15 00:00:00" fixdate="2017-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL metastore handling of CLOB types for COLUMNS_V2.TYPE_NAME and other field is incorrect</summary>
      <description>The CLOB JDO type introduced with HIVE-12274 does not work correctly with PostgreSQL. The value is written out-of-band and the LOB handle is written,as an INT, into the table. SELECTs return the INT value, which should had been read via the lo_get PG built-in, and then cast into string.Furthermore, the behavior is different between fields upgraded from earlier metastore versions (they retain their string storage) vs. values inserted after the upgrade (inserted as LOB roots).Teh code in MetasoreDirectSql.getPartitionsFromPartitionIds/extractSqlClob expects the underlying JDO/Datanucleus to map the column to a Clob but that does not happen, the value is a Java String containing the int which is the LOB root saved by PG.This manifests at runtime with errors like:hive&gt; select * from srcpart;Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Error: type expected at the position 0 of '24030:24031' but '24030' is found.the 24030:24031 should be 'string:string'.repro:CREATE TABLE srcpart (key STRING COMMENT 'default', value STRING COMMENT 'default') PARTITIONED BY (ds STRING, hr STRING) STORED AS TEXTFILE;LOAD DATA LOCAL INPATH "${hiveconf:test.data.dir}/kv1.txt" OVERWRITE INTO TABLE srcpart PARTITION (ds="2008-04-09", hr="11");select * from srcpart;I did not see the issue being hit by non-partitioned/textfile tables, but that is just the luck of the path taken by the code. Inspection of my PG metastore shows all the CLOB fields suffering from this issue.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
    </fixedFiles>
  </bug>
  <bug id="16671" opendate="2017-5-16 00:00:00" fixdate="2017-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: BufferUnderflowException may happen in very rare(?) cases due to ORC end-of-CB estimation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16673" opendate="2017-5-16 00:00:00" fixdate="2017-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test for HIVE-16413</summary>
      <description>unit test for HIVE-16413</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16678" opendate="2017-5-16 00:00:00" fixdate="2017-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate on temporary table fails with "table not found" error.</summary>
      <description>Queries:create temporary table s10k as select * from studenttab10k;truncate table s10k;Truncate table query on temporary table failing with error:Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Exception while processing (state=08S01,code=1)From hsi logs:NoSuchObjectException(message:default.s10k table not found) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:2118) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.truncate_table(HiveMetaStore.java:2008) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) at com.sun.proxy.$Proxy31.truncate_table(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.truncateTable(HiveMetaStoreClient.java:1115) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173) at com.sun.proxy.$Proxy32.truncateTable(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2306) at com.sun.proxy.$Proxy32.truncateTable(Unknown Source) at org.apache.hadoop.hive.ql.metadata.Hive.truncateTable(Hive.java:1217) at org.apache.hadoop.hive.ql.exec.DDLTask.truncateTable(DDLTask.java:4462) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:556) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1905) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1607) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1354) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:348) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16697" opendate="2017-5-17 00:00:00" fixdate="2017-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema table validator should return a sorted list of missing tables</summary>
      <description>SchemaTool's validate feature has a schema table validator that checks to see if the HMS schema is missing tables. This validator reports a list of tables that are deemed to be missing. This list is currently unsorted (depends on the order of create table statements in the schema file, which is different for different DB schema files). This makes it hard to write a unit test that parses the results.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="167" opendate="2008-10-31 00:00:00" fixdate="2008-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: add a RegularExpressionDeserializer</summary>
      <description>We need a RegularExpressionDeserializer to read data based on a regex. This will be very useful for reading files like apache log.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1670" opendate="2010-9-25 00:00:00" fixdate="2010-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapJoin throws EOFExeption when the mapjoined table has 0 column selected</summary>
      <description>select /*+mapjoin(b) */ sum(a.key) from src a join src b on (a.key=b.key); throws EOFException</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16700" opendate="2017-5-17 00:00:00" fixdate="2017-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log ZK discovery info (hostname &amp; port) for HTTP mode when connection is established</summary>
      <description>Currently it seems we only do this for binary mode. See here. For debugging purpose, it would be good to do this for http mode too.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="16715" opendate="2017-5-19 00:00:00" fixdate="2017-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from errors in modules llap-client, metastore, spark-client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcDispatcher.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.PartFilterExprUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreThread.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreFilterHook.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreSchemaInfo.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.MetadataStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.DatabaseProduct.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstance.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16716" opendate="2017-5-19 00:00:00" fixdate="2017-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from errors in module ql</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.Expression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.UpgradeTool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFParseUrl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.SettableUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.MatchPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToIntervalYearMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToIntervalDayTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CryptoProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorGroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UDTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowCreateDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ListBucketingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExportWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AcidExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkMapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.CountDistinctRewriteProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAggregateIncrementalRewritingRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCProjectPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAggregationPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExceptRewriteRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.PartitionIterable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PidFilePatternConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleExactMatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.Node.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.FlatFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperGeneral.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSetStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSetStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMapStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashKeyRef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDAF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.TaskTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.FunctionEvent.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnCompareDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnCompareDecimal64Scalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ScalarCompareDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimal64ColumnCompareDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimal64ColumnCompareDecimal64Scalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimal64ScalarCompareDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareDecimalColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareDecimalScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalScalarCompareDecimalColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDTIColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDTIScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringGroupScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupScalarCompareStringGroupColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringGroupScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupScalarCompareStringGroupColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapArrowRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.ScriptErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="16729" opendate="2017-5-22 00:00:00" fixdate="2017-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve location validator to check for blank paths.</summary>
      <description>Currently, the schema tool location validator succeeds even when the location for hive table/partitions have paths likehdfs://myhost.com:8020/hdfs://myhost.com:8020where there is actually no "real" path. Having the validator report such path would be beneficial in preventing runtime errors.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16737" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Shuffle handler TCP listen queue overflows</summary>
      <description>$ netstat -s | grep "listen queue of a socket"localhost: 297070 times the listen queue of a socket overflowed$ ss -tlLISTEN 0 50 *:15551 *:*</description>
      <version>3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16744" opendate="2017-5-24 00:00:00" fixdate="2017-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP index update may be broken after ORC switch</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16748" opendate="2017-5-24 00:00:00" fixdate="2017-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integreate YETUS to Pre-Commit</summary>
      <description>After HIVE-15051, we should automate the yetus run for the Pre-Commit tests, so the results are added in comments like https://issues.apache.org/jira/browse/YARN-6363?focusedCommentId=15937570&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15937570</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">dev-support.hive-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16754" opendate="2017-5-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Print hive version info on llap daemon startup</summary>
      <description>For debugging purpose, print out hive version info on llap daemon startup.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="16755" opendate="2017-5-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: incorrect assert may trigger in tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16761" opendate="2017-5-25 00:00:00" fixdate="2017-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: SMB joins fail elevator</summary>
      <description>Caused by: java.io.IOException: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:153) at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:78) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:360) ... 26 moreCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextString(BatchToRowReader.java:334) at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextValue(BatchToRowReader.java:602) at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:149) ... 28 moreset hive.enforce.sortmergebucketmapjoin=false;set hive.optimize.bucketmapjoin=true;set hive.optimize.bucketmapjoin.sortedmerge=true;set hive.auto.convert.sortmerge.join=true;set hive.auto.convert.join=true;set hive.auto.convert.join.noconditionaltask.size=500;select year,quarter,count(*) from transactions_raw_orc_200 a join customer_accounts_orc_200 b on a.account_id=b.account_id group by year,quarter;</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16765" opendate="2017-5-26 00:00:00" fixdate="2017-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ParquetFileReader should be closed to avoid resource leak</summary>
      <description>ParquetFileReader should be closed to avoid resource leak</description>
      <version>2.3.0,3.0.0</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16777" opendate="2017-5-26 00:00:00" fixdate="2017-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use separate tokens and UGI instances when an external client is used</summary>
      <description>Otherwise leads to errors since the token is shared, and there's different nodes running Umbilical.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="16778" opendate="2017-5-27 00:00:00" fixdate="2017-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: better refcount management</summary>
      <description>Looks like task cancellation can close the UGI, causing the background thread to die with an exception, leaving a bunch of unreleased cache buffers.Overall, it's probably better to modify how refcounts are handled - if there's some bug in the code we don't want to leak them.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="1678" opendate="2010-10-1 00:00:00" fixdate="2010-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in MapJoin</summary>
      <description>The query with two map joins and a group by fails with following NPE:Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177) at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84) at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16780" opendate="2017-5-27 00:00:00" fixdate="2017-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Case "multiple sources, single key" in spark_dynamic_pruning.q fails</summary>
      <description>script.qset hive.optimize.ppd=true;set hive.ppd.remove.duplicatefilters=true;set hive.spark.dynamic.partition.pruning=true;set hive.optimize.metadataonly=false;set hive.optimize.index.filter=true;set hive.strict.checks.cartesian.product=false;set hive.spark.dynamic.partition.pruning=true;-- multiple sources, single keyselect count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)if disabling "hive.optimize.index.filter", case passes otherwise it always hang out in the first job. Exception17/05/27 23:39:45 DEBUG Executor task launch worker-0 PerfLogger: &lt;/PERFLOG method=SparkInitializeOperators start=1495899585574 end=1495899585933 duration=359 from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler&gt;17/05/27 23:39:45 INFO Executor task launch worker-0 Utilities: PLAN PATH = hdfs://bdpe41:8020/tmp/hive/root/029a2d8a-c6e5-4ea9-adea-ef8fbea3cde2/hive_2017-05-27_23-39-06_464_5915518562441677640-1/-mr-10007/617d9dd6-9f9a-4786-8131-a7b98e8abc3e/map.xml17/05/27 23:39:45 DEBUG Executor task launch worker-0 Utilities: Found plan in cache for name: map.xml17/05/27 23:39:45 DEBUG Executor task launch worker-0 DFSClient: Connecting to datanode 10.239.47.162:5001017/05/27 23:39:45 DEBUG Executor task launch worker-0 MapOperator: Processing alias(es) srcpart_hour for file hdfs://bdpe41:8020/user/hive/warehouse/srcpart_hour/000008_017/05/27 23:39:45 DEBUG Executor task launch worker-0 ObjectCache: Creating root_20170527233906_ac2934e1-2e58-4116-9f0d-35dee302d689_DynamicValueRegistry17/05/27 23:39:45 ERROR Executor task launch worker-0 SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"hr":"11","hour":"11"}org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"hr":"11","hour":"11"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562) at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974) at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) at org.apache.spark.scheduler.Task.run(Task.scala:85) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_7_srcpart__col3_min at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126) at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101) at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead._evaluate(ExprNodeEvaluatorHead.java:44) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68) at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:112) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:62) at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:51) at org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.retrieve(ObjectCacheWrapper.java:40) at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:119) ... 41 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:60) ... 44 more17/05/27 23:39:45 ERROR Executor task launch worker-0 Executor: Exception in task 1.0 in stage 0.0 (TID 1)java.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"hr":"11","hour":"11"} at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:149) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48) at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27) at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85) at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127) at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974) at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) at org.apache.spark.scheduler.Task.run(Task.scala:85) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"hr":"11","hour":"11"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562) at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136) ... 16 moreCaused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_7_srcpart__col3_min at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126) at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101) at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug id="16815" opendate="2017-6-2 00:00:00" fixdate="2017-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from error for the rest of modules</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
      <file type="M">testutils.src.java.org.apache.hive.testutils.jdbc.HiveBurnInClient.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingTransaction.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionIterable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsRebuildLockHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceOperations.java</file>
      <file type="M">service.src.java.org.apache.hive.service.Service.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.java</file>
      <file type="M">common.src.java.org.apache.hive.http.ProfileServlet.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.GenericMR.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseStructValue.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseValueFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.ConsumerFeedback.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataInputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataOutputStream.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.CustomQueryFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.SearchResultHandler.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.UserFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PasswdAuthenticationProvider.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
    </fixedFiles>
  </bug>
  <bug id="16819" opendate="2017-6-2 00:00:00" fixdate="2017-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add MM test for temporary table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
    </fixedFiles>
  </bug>
  <bug id="16821" opendate="2017-6-3 00:00:00" fixdate="2017-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: support Explain Analyze in vectorized mode</summary>
      <description>Currently, to avoid a branch in the operator inner loop - the runtime stats are only available in non-vector mode.</description>
      <version>2.1.1,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explainanalyze.3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16847" opendate="2017-6-7 00:00:00" fixdate="2017-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP queue order issue</summary>
      <description>There's an ordering issue with the LLAP queue that we've seen on some run.</description>
      <version>None</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestEvictingPriorityBlockingQueue.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.EvictingPriorityBlockingQueue.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.FirstInFirstOutComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16850" opendate="2017-6-7 00:00:00" fixdate="2017-4-7 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Converting table to insert-only acid may open a txn in an inappropriate place</summary>
      <description>This would work for unit-testing, but would need to be fixed for production.HiveTxnManager txnManager = SessionState.get().getTxnMgr(); if (txnManager.isTxnOpen()) { mmWriteId = txnManager.getCurrentTxnId(); } else { mmWriteId = txnManager.openTxn(new Context(conf), conf.getUser()); txnManager.commitTxn(); }</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.conversions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.conversions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16864" opendate="2017-6-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add validation to stream position search in LLAP IO</summary>
      <description>There's a TODO there to add the checks. We've seen some issues before where incorrect ranges lead to obscure errors after this method returns a bad result due to absence of validity checks; we also see one now.Adding the checks.</description>
      <version>None</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16867" opendate="2017-6-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend shared scan optimizer to reuse computation from other operators</summary>
      <description>Follow-up of the work in HIVE-16602.HIVE-16602 introduced an optimization that identifies scans on input tables that can be merged so the data is read only once.This extension to that rule allows to reuse the computation that is done in the work containing those scans. In particular, we traverse both parts of the plan upstream and reuse the operators if possible.Currently, the optimizer will not go beyond the output edge(s) of that work. Follow-up extensions might remove this limitation.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query38.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.OperatorComparatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedScanOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPruningEventDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.except.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.shared.scan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query33.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16869" opendate="2017-6-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive returns wrong result when predicates on non-existing columns are pushed down to Parquet reader</summary>
      <description>When hive.optimize.ppd and hive.optimize.index.filter are turned, and a select query has a condition on a column that doesn't exist in Parquet file (such as a partition column), Hive often returns wrong result.Please see below example for details:hive&gt; create table test_parq (a int, b int) partitioned by (p int) stored as parquet;OKTime taken: 0.292 secondshive&gt; insert overwrite table test_parq partition (p=1) values (1, 2);OKTime taken: 5.08 secondshive&gt; select * from test_parq where a=1 and p=1;OK1 2 1Time taken: 0.441 seconds, Fetched: 1 row(s)hive&gt; select * from test_parq where (a=1 and p=1) or (a=999 and p=999);OK1 2 1Time taken: 0.197 seconds, Fetched: 1 row(s)hive&gt; set hive.optimize.index.filter=true;hive&gt; select * from test_parq where (a=1 and p=1) or (a=999 and p=999);OKTime taken: 0.167 secondshive&gt; select * from test_parq where (a=1 or a=999) and (a=999 or p=1);OKTime taken: 0.563 seconds</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetFilterPredicateConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16871" opendate="2017-6-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CachedStore.get_aggr_stats_for has side affect</summary>
      <description>Every get_aggr_stats_for accumulates the stats and propagated to the first partition stats object. It accumulates and gives wrong result in the follow up invocations.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16888" opendate="2017-6-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite to 1.13 and Avatica to 1.10</summary>
      <description>I'm creating this early to be able to ptest the current Calcite 1.13.0-SNAPSHOT</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveDruidProjectFilterTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewFilterScanRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.MaterializedViewSubstitutionVisitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.SubstitutionVisitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16893" opendate="2017-6-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move replication dump related work in semantic analysis phase to execution phase using a task</summary>
      <description>Since we run in to the possibility of creating a large number tasks during replication bootstrap dump we may not be able to hold all of them in memory for really large databases, which might not hold true once we complete HIVE-16892 Also a compile time lock is taken such that only one query is run in this phase which in replication bootstrap scenario is going to be a very long running task and hence moving it to execution phase will limit the lock period in compile phase.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16895" opendate="2017-6-14 00:00:00" fixdate="2017-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-threaded execution of bootstrap dump of partitions</summary>
      <description>to allow faster execution of bootstrap dump phase we dump multiple partitions from same table simultaneously. even though dumping functions is not going to be a blocker, moving to similar execution modes for all metastore objects will make code more coherent. Bootstrap dump at db level does : boostrap of all tables boostrap of all partitions in a table. (scope of current jira) boostrap of all functions</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16896" opendate="2017-6-14 00:00:00" fixdate="2017-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move replication load related work in semantic analysis phase to execution phase using a task</summary>
      <description>we want to not create too many tasks in memory in the analysis phase while loading data. Currently we load all the files in the bootstrap dump location as FileStatus[] and then iterate over it to load objects, we should rather move to org.apache.hadoop.fs.RemoteIterator&lt;LocatedFileStatus&gt; listFiles(Path f, boolean recursive)which would internally batch and return values. additionally since we cant hand off partial tasks from analysis pahse =&gt; execution phase, we are going to move the whole repl load functionality to execution phase so we can better control creation/execution of tasks (not related to hive Task, we may get rid of ReplCopyTask)Additional consideration to take into account at the end of this jira is to see if we want to specifically do a multi threaded load of bootstrap dump.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.repl.load.requires.admin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16898" opendate="2017-6-14 00:00:00" fixdate="2017-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validation of source file after distcp in repl load</summary>
      <description>time between deciding the source and destination path for distcp to invoking of distcp can have a change of the source file, hence distcp might copy the wrong file to destination, hence we should an additional check on the checksum of the source file path after distcp finishes to make sure the path didnot change during the copy process. if it has take additional steps to delete the previous file on destination and copy the new source and repeat the same process as above till we copy the correct file.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16911" opendate="2017-6-15 00:00:00" fixdate="2017-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade groovy version to 2.4.11</summary>
      <description>Hive currently uses groovy 2.4.4 which has security issue (https://access.redhat.com/security/cve/cve-2016-6814). Need to upgrade to 2.4.8 or later.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16912" opendate="2017-6-15 00:00:00" fixdate="2017-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve table validator&amp;#39;s performance against Oracle</summary>
      <description>Currently, this validator uses DatabaseMetaData.getTables() that takes in the order of minutes to return because of the number of SYSTEM tables present in Oracle.Providing a schema name via a system property would limit the number of tables being returned and thus improve performance.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16915" opendate="2017-6-16 00:00:00" fixdate="2017-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition column count is not determined correctly in LLAP IO non-vectorized wrapper</summary>
      <description>May be related to HIVE-16761</description>
      <version>None</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16917" opendate="2017-6-19 00:00:00" fixdate="2017-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 guard rails - Limit concurrent connections from user</summary>
      <description>Rogue applications can make HS2 unusable for others by making too many connections at a time.HS2 should start rejecting the number of connections from a user, after it has reached a configurable threshold.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestPluggableHiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16927" opendate="2017-6-21 00:00:00" fixdate="2017-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Slider takes down all daemons when some daemons fail repeatedly</summary>
      <description>When some containers fail repeatedly, slider thinks application is in unstable state which brings down all llap daemons.</description>
      <version>3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16947" opendate="2017-6-23 00:00:00" fixdate="2017-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin Reduction : Task cycle created due to multiple semijoins in conjunction with hashjoin</summary>
      <description>Typically a semijoin branch and a mapjoin may create a cycle when on same operator tree. This is already handled, however, a semijoin branch can serve more than one filters and the cycle detection logic currently only handles the 1st one causing cycles preventing the queries from running.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="16949" opendate="2017-6-23 00:00:00" fixdate="2017-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leak of threads from Get-Input-Paths and Get-Input-Summary thread pool</summary>
      <description>The commit 20210de which was part of HIVE-15546 introduced a thread pool which is not shutdown upon completion of its threads. This leads to a leak of threads for each query which uses more than 1 partition. They are not removed automatically. When queries spanning multiple partitions are made the number of threads increases and is never reduced. On my machine hiveserver2 starts to get slower and slower once 10k threads are reached.Thread pools only shutdown automatically in special circumstances (see documentation section Finalization). This is not currently the case for the Get-Input-Paths thread pool. I would add a pool.shutdown() in a finally block just before returning the result to make sure the threads are really shutdown.My current workaround is to set hive.exec.input.listing.max.threads = 1. This prevents the the thread pool from being spawned [1] [2].The same issue probably also applies to the Get-Input-Summary thread pool.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="1697" opendate="2010-10-6 00:00:00" fixdate="2010-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migration scripts should increase size of PARAM_VALUE in PARTITION_PARAMS</summary>
      <description>The migration scripts should increase the size of column PARAM_VALUE in the table PARTITION_PARAMS to 4000 chars to follow the description in package.jdo.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.6.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.6.0.derby.sql</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16974" opendate="2017-6-27 00:00:00" fixdate="2017-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the sort key for the schema tool validator to be &lt;ID&gt;</summary>
      <description>In HIVE-16729, we introduced ordering of results/failures returned by schematool's validators. This allows fault injection testing to expect results that can be verified. However, they were sorted on NAME values which in the HMS schema can be NULL. So if the introduced fault has a NULL/BLANK name column value, the result could be different depending on the backend database(if they sort NULLs first or last).So I think it is better to sort on a non-null column value.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16975" opendate="2017-6-27 00:00:00" fixdate="2017-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fully vectorize CAST date as TIMESTAMP so VectorUDFAdaptor is now used</summary>
      <description>Fix VectorUDFAdaptor(CAST(d_date as TIMESTAMP)) to be native.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.casts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.java</file>
    </fixedFiles>
  </bug>
  <bug id="16978" opendate="2017-6-27 00:00:00" fixdate="2017-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: add current thread ID to the log redirector for the RemoteDriver</summary>
      <description>The log redirector currently doesn't include the thread ID. In case there are multiple sessions running concurrently, it's hard to tell which redirector belongs to which session.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="170" opendate="2008-12-11 00:00:00" fixdate="2008-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>map-side aggregations does not work properly</summary>
      <description>map-side aggregation depends on runtime.freememory() which is not guaranteed to return the freeable memory - it depends on when the garbage collector is invoked last.It might be a good idea to estimate the number of rows that can fit in the hash table and then flush the hash table based on that</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17013" opendate="2017-7-3 00:00:00" fixdate="2017-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete request with a subquery based on select over a view</summary>
      <description>Hi, I based my DDL on this exemple https://fr.hortonworks.com/tutorial/using-hive-acid-transactions-to-insert-update-and-delete-data/.In a delete request, the use of a view in a subquery throw an exception : FAILED: IllegalStateException Expected 'insert into table default.mydim select ROW_ID from default.mydim sort by ROW_ID' to be in sub-query or set operation.{code:sql}drop table if exists mydim;create table mydim (key int, name string, zip string, is_current boolean)clustered by(key) into 3 bucketsstored as orc tblproperties ('transactional'='true');insert into mydim values (1, 'bob', '95136', true), (2, 'joe', '70068', true), (3, 'steve', '22150', true);drop table if exists updates_staging_table;create table updates_staging_table (key int, newzip string);insert into updates_staging_table values (1, 87102), (3, 45220);drop view if exists updates_staging_view;create view updates_staging_view (key, newzip) as select key, newzip from updates_staging_table;delete from mydimwhere mydim.key in (select key from updates_staging_view);FAILED: IllegalStateException Expected 'insert into table default.mydim select ROW__ID from default.mydim sort by ROW__ID' to be in sub-query or set operation.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="17034" opendate="2017-7-5 00:00:00" fixdate="2017-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The spark tar for itests is downloaded every time if md5sum is not installed</summary>
      <description>I think we should either skip verifying md5, or fail the build to let developer know md5sum is required.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17036" opendate="2017-7-5 00:00:00" fixdate="2017-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage: Minor CPU/Mem optimization for lineage transform</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.Generator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleRegExp.java</file>
    </fixedFiles>
  </bug>
  <bug id="17052" opendate="2017-7-6 00:00:00" fixdate="2017-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove logging of predicate filters</summary>
      <description>HIVE-16869 added the filter predicate to the debug log of HS2, but since these filters may contain sensitive information they should not be logged out.The log statement should be changed back to the original form.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17054" opendate="2017-7-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose SQL database constraints to Calcite</summary>
      <description>Hive already has support to declare multiple SQL constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE, and NOT NULL). Although these constraints cannot be currently enforced on the data, they can be made available to the optimizer by using the 'RELY' keyword.Currently, even when they are declared with the RELY keyword, they are not exposed to Calcite.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.join.pushdown.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.join.pushdown.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="17055" opendate="2017-7-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Flaky test: TestMiniLlapCliDriver.testCliDriver[llap_smb]</summary>
      <description>Following tests are also failing with same symptoms: TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;tez_smb_1&amp;#93; TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;tez_smb_main&amp;#93;Client Execution succeeded but contained differences (error code = 1) after executing llap_smb.q 324,325c324,325&lt; 2000 9 52&lt; 2001 0 139630&amp;#8212;&gt; 2001 4 139630&gt; 2001 6 52</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17067" opendate="2017-7-10 00:00:00" fixdate="2017-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add http endpoint to provide system level configurations</summary>
      <description>Add an endpoint to get kernel and network configs via sysctl. Also memory related configs like transparent huge pages config can be added. "ulimit -a" can be added to llap startup script as it needs a shell.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.bin.llapDaemon.sh</file>
      <file type="M">common.src.java.org.apache.hive.http.StackServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17069" opendate="2017-7-10 00:00:00" fixdate="2017-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor OrcRawRecrodMerger.ReaderPair</summary>
      <description>this should be done post HIVE-16177 so as not to obscure the functional changes completelyMake ReaderPair an interfaceReaderPairImpl - will do what ReaderPair currently does, i.e. handle "normal" code pathOriginalReaderPair - same as now but w/o incomprehensible override/variable shadowing logic.Perhaps split it into 2 - 1 for compaction 1 for "normal" read with common base class.Push discoverKeyBounds() into appropriate implementation</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="1707" opendate="2010-10-13 00:00:00" fixdate="2010-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bug when different partitions are present in different dfs</summary>
      <description>The following does not work:create table T -&gt; default location dfs1insert overwrite T partition (ds='1') select * from src;alter table T location 'dfs2';insert overwrite T partition (ds='1') select * from src;It tries to insert back in dfs1 - due to which the move task fails.It would be cleaner to keep the same semantics as fileformat - whenever a partition is being inserted into, itinherits the properties from the table. So, after the insert, the partition should belong to dfs1.It does not matter whether the partition exists before or not,.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17073" opendate="2017-7-11 00:00:00" fixdate="2017-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect result with vectorization and SharedWorkOptimizer</summary>
      <description>We get incorrect result with vectorization and multi-output Select operator created by SharedWorkOptimizer. It can be reproduced in the following way.Correctselect count(*) as h8_30_to_9 from src join src1 on src.key = src1.key where src1.value = "val_278";OK2Correctselect count(*) as h9_to_9_30 from src join src1 on src.key = src1.key where src1.value = "val_255";OK2Incorrectselect * from ( select count(*) as h8_30_to_9 from src join src1 on src.key = src1.key where src1.value = "val_278") s1join ( select count(*) as h9_to_9_30 from src join src1 on src.key = src1.key where src1.value = "val_255") s2;OK2 0Problem seems to be that some ds in the batch row need to be re-initialized after they have been forwarded to each output.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17079" opendate="2017-7-12 00:00:00" fixdate="2017-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use FQDN by default for work submission</summary>
      <description>HIVE-14624 added FDQN for work submission. We should enable it by default to avoid DNS issues.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17086" opendate="2017-7-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: JMX Metric for max file descriptors used so far</summary>
      <description>LlapDaemonMaxFileDescriptorCount shows max file descriptors that system will allow. For debugging purpose we could also store the max value that was seen so far to know if we have hit the limit.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="17088" opendate="2017-7-13 00:00:00" fixdate="2017-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 WebUI throws a NullPointerException when opened</summary>
      <description>After bumping the Jetty version to 3.9 and excluding several other dependencies on HIVE-16049, the HS2 webui stopped working and throwing a NPE error.HTTP ERROR 500Problem accessing /hiveserver2.jsp. Reason: Server ErrorCaused by:java.lang.NullPointerException at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:181) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70) at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) at org.eclipse.jetty.server.Server.handle(Server.java:534) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:240) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) at java.lang.Thread.run(Thread.java:748)Powered by Jetty:// 9.3.19.v20170502</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1709" opendate="2010-10-13 00:00:00" fixdate="2010-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Postgres metastore schema migration scripts (0.5 -&gt; 0.6)</summary>
      <description>Yuanjun Li provided a schema upgrade script for postgres. This should be included in the 0.6 release.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17090" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark.only.query.files are not being run by ptest</summary>
      <description>Checked a recent run of Hive QA and it doesn't look like qtests specified in spark.only.query.files are being run.I think some modifications to ptest config files are required to get this working - e.g. the deployed master-m2.properties file for ptest should contain mainProperties.${spark.only.query.files} in the qFileTest.miniSparkOnYarn.groups.normal key.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17093" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP ssl configs need to be localized to talk to a wire encrypted hdfs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="17095" opendate="2017-7-14 00:00:00" fixdate="2017-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Long chain repl loads do not complete in a timely fashion</summary>
      <description>Per performance testing done by sapinamin (thus, I'm setting him as reporter), we were able to discover an important bug affecting replication. It has the potential to affect other large DAGs of Tasks that hive generates as well, if those DAGs have multiple paths to child Task nodes.Basically, we find that incremental REPL LOAD does not finish in a timely fashion. The test, in this case was to add 400 partitions, and replicate them. Associated with each partition, there was an ADD PTN and a ALTER PTN. For each of the ADD PTN tasks, we'd generate a DDLTask, a CopyTask and a MoveTask. For each Alter ptn, there'd be a single DDLTask. And order of execution is important, so it would chain in dependency collection tasks between phases.Trying to root cause this shows us that it seems to stall forever at the Driver instantiation time, and it almost looks like the thread doesn't proceed past that point.Looking at logs, it seems that the way this is written, it looks for all tasks generated that are subtrees of all nodes, without looking for duplicates, and this is done simply to get the number of execution tasks!And thus, the task visitor will visit every subtree of every node, which is fine if you have graphs that look like open trees, but is horrible for us, since we have dependency collection tasks between each phase. Effectively, this is what's happening:We have a DAG, say, like this:4 tasks in parallel -&gt; DEP col -&gt; 4 tasks in parallel -&gt; DEP col -&gt; ...This means that for each of the 4 root tasks, we will do a full traversal of every graph (not just every node) past the DEP col, and this happens recursively, and this leads to an exponential growth of number of tasks visited as the length and breadth of the graph increase. In our case, we had about 800 tasks in the graph, with roughly a width of about 2-3, with 200 stages, a dep collection before and after, and this meant that leaf nodes of this DAG would have something like 2^200 - 3^200 ways in which they can be visited, and thus, we'd visit them in all those ways. And all this simply to count the number of tasks to schedule - we would revisit this function multiple more times, once per each hook, once for the MapReduceCompiler and once for the TaskCompiler.We have not been sending such large DAGs to the Driver, thus it has not yet been a problem, and there are upcoming changes to reduce the number of tasks replication generates(as part of a memory addressing issue), but we still should fix the way we do Task traversal so that a large DAG cannot cripple us.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="17109" opendate="2017-7-17 00:00:00" fixdate="2017-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove calls to RelMetadataQuery.instance() after Calcite 1.13 upgrade</summary>
      <description>After CALCITE-1812 the code should retrieve the RelMetadataQuery from the planner, if needed. Calling RelMetadatQuery.instance() invalidates the Calcite RelNode properties memoization cache.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1711" opendate="2010-10-13 00:00:00" fixdate="2010-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE LIKE should not set stats in the new table</summary>
      <description>CREATE TABLE T LIKE S; will copy every parameters from T to S. It should not copy table level stats.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.stats13.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hbase-handler.src.test.results.hbase.stats.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17111" opendate="2017-7-17 00:00:00" fixdate="2017-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TestLocalSparkCliDriver</summary>
      <description>The TestSparkCliDriver sets the spark.master to local-cluster&amp;#91;2,2,1024&amp;#93; but the HoS still uses decides to use the RemoteHiveSparkClient rather than the LocalHiveSparkClient.The issue is with the following check in HiveSparkClientFactory: if (master.equals("local") || master.startsWith("local[")) { // With local spark context, all user sessions share the same spark context. return LocalHiveSparkClient.getInstance(generateSparkConf(sparkConf)); } else { return new RemoteHiveSparkClient(hiveconf, sparkConf); }When master.startsWith("local[") it checks the value of spark.master and sees that it doesn't start with local[ and then decides to use the RemoteHiveSparkClient.We should fix this so that the LocalHiveSparkClient is used. It should speed up some of the tests, and also makes qtests easier to debug since everything will now be run in the same process.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="17114" opendate="2017-7-18 00:00:00" fixdate="2017-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: Possible skew in shuffling when data is not really skewed</summary>
      <description>Observed in HoS and may apply to other engines as well.When we join 2 tables on a single int key, we use the key itself as hash code in ObjectInspectorUtils.hashCode: case INT: return ((IntObjectInspector) poi).get(o);Suppose the keys are different but are all some multiples of 10. And if we choose 10 as #reducers, the shuffle will be skewed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ppd.join.filter.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constprog.semijoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
    </fixedFiles>
  </bug>
  <bug id="17125" opendate="2017-7-19 00:00:00" fixdate="2017-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage: Generate lineage information on need basis when atlas hook is enabled</summary>
      <description>When atlas hook is enabled, it would be good to generate lineage information only for insert/create/view statements and skip for select statements. Currently, in some of the complex select queries, generating lineage information turns out to in 100s of seconds, which can be avoided with this change.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.Generator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17144" opendate="2017-7-21 00:00:00" fixdate="2017-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>export of temporary tables not working and it seems to be using distcp rather than filesystem copy</summary>
      <description>create temporary table t1 (i int);insert into t1 values (3);export table t1 to 'hdfs://somelocation';above fails. additionally it should use filesystem copy and not distcp to do the job.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.CopyUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExport.java</file>
    </fixedFiles>
  </bug>
  <bug id="17147" opendate="2017-7-21 00:00:00" fixdate="2017-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Add code for testing MapJoin operator in isolation and measuring its performance with JMH</summary>
      <description>Current limitations:Only a one long key test currently. Need more tests.The hive-jmh test doesn't handle multiple iterations. And, the number of rows and keys being driven through is way too small to be meaningful.The focus of this change was to get things started.NOTE: This change does change main line code by adding test hooks.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedCreateHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17163" opendate="2017-7-25 00:00:00" fixdate="2017-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Microbenchmark for vector op processing</summary>
      <description>Add JMH based operator level benchmarks for vector operators.</description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.batchgen.VectorBatchGenerateUtil.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedLikeBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneStringKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneLongKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinMultiKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.AbstractMapJoin.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.AbstractExpression.java</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17164" opendate="2017-7-25 00:00:00" fixdate="2017-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Support PTF (Part 2: Unbounded Support-- Turn ON by default)</summary>
      <description>Add disk storage backing. Turn hive.vectorized.execution.ptf.enabled on by default.Add hive.vectorized.ptf.max.memory.buffering.batch.count to specify the maximum number of vectorized row batch to buffer in memory before spilling to disk.Add hive.vectorized.testing.reducer.batch.size parameter to have the Tez Reducer make small batches for making a lot of key group batches that cause memory buffering and disk storage backing.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.StructColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.MultiValuedColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.MapColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ListColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.windowing.expressions.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.ptf.part.simple.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinRowBytesContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17167" opendate="2017-7-25 00:00:00" fixdate="2017-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore specific configuration tool</summary>
      <description>As part of making the metastore a separately releasable module we need configuration tools that are specific to that module. It cannot use or extend HiveConf as that is in hive common. But it must take a HiveConf object and be able to operate on it.The best way to achieve this is using Hadoop's Configuration object (which HiveConf extends) together with enums and static methods.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17168" opendate="2017-7-25 00:00:00" fixdate="2017-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for stand alone metastore</summary>
      <description>We need to create a separate maven module for the stand alone metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1717" opendate="2010-10-15 00:00:00" fixdate="2010-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ant clean should delete stats database</summary>
      <description>If a test failed, the derby database used for storing intermediate stats may be in an inconsistent state. This database is shared with future tests and prevent them from connecting to the database.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17181" opendate="2017-7-27 00:00:00" fixdate="2017-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatOutputFormat should expose complete output-schema (including partition-keys) for dynamic-partitioning MR jobs</summary>
      <description>Map/Reduce jobs that use HCatalog APIs to write to Hive tables using Dynamic partitioning are expected to call the following API methods: HCatOutputFormat.setOutput() to indicate which table/partitions to write to. This call populates the OutputJobInfo with details fetched from the Metastore. HCatOutputFormat.setSchema() to indicate the output-schema for the data being written.It is a common mistake to invoke HCatOUtputFormat.setSchema() as follows:HCatOutputFormat.setSchema(conf, HCatOutputFormat.getTableSchema(conf));Unfortunately, getTableSchema() returns only the record-schema, not the entire table's schema. We'll need a better API for use in M/R jobs to get the complete table-schema.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17190" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema changes for bitvectors for unpartitioned tables</summary>
      <description>Missed in HIVE-16997</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.043-HIVE-16997.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.044-HIVE-16997.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.044-HIVE-16997.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.029-HIVE-16997.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.044-HIVE-16997.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.022-HIVE-11107.derby.sql</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.perf-reg.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17191" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for StorageHandler APIs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.InputEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="17192" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for Stats Collection APIs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ClientStatsPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="17193" opendate="2017-7-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: don&amp;#39;t combine map works that are targets of different DPPs</summary>
      <description>Suppose srcpart is partitioned by ds. The following query can trigger the issue:explainselect * from (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) ajoin (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.value) bon a.key=b.key;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkPartitionPruningSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.CombineEquivalentWorkResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17194" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Implement Gzip compression for HTTP mode</summary>
      <description>POST /cliservice HTTP/1.1Content-Type: application/x-thriftAccept: application/x-thriftUser-Agent: Java/THttpClient/HCAuthorization: Basic YW5vbnltb3VzOmFub255bW91cw==Content-Length: 71Host: localhost:10007Connection: Keep-AliveAccept-Encoding: gzip,deflateX-XSRF-HEADER: trueThe Beeline client clearly sends out HTTP compression headers which are ignored by the HTTP service layer in HS2.After patch, result looks likeHTTP/1.1 200 OKDate: Tue, 01 Aug 2017 01:47:23 GMTContent-Type: application/x-thriftVary: Accept-Encoding, User-AgentContent-Encoding: gzipTransfer-Encoding: chunkedServer: Jetty(9.3.8.v20160314)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17202" opendate="2017-7-28 00:00:00" fixdate="2017-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for HMS Listener APIs</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStorePreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreReadTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreReadDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreCreateTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAuthorizationCallEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAlterTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAlterIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAddIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.ListenerEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropFunctionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateFunctionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.ConfigChangeEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AddPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AddIndexEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="17203" opendate="2017-7-28 00:00:00" fixdate="2017-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for HCat APIs</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.Command.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.MetadataSerializer.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatPartition.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatNotificationEvent.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatDatabase.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatCreateDBDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageDeserializer.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.InsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.HCatEventMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatLoader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.OutputJobInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecord.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatException.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="17205" opendate="2017-7-29 00:00:00" fixdate="2017-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add functional support for unbucketed tables</summary>
      <description>make sure unbucketed tables can be marked transactional=truemake insert/update/delete/compaction work</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.update.non.acid.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.delete.non.acid.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.not.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.create.not.acid.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTezWithSplitUpdate.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.package.html</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17220" opendate="2017-7-31 00:00:00" fixdate="2017-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bloomfilter probing in semijoin reduction is thrashing L1 dcache</summary>
      <description>gopalv observed perf profiles showing bloomfilter probes as bottleneck for some of the TPC-DS queries and resulted L1 data cache thrashing. This is because of the huge bitset in bloom filter that doesn't fit in any levels of cache, also the hash bits corresponding to a single key map to different segments of bitset which are spread out. This can result in K-1 memory access (K being number of hash functions) in worst case for every key that gets probed because of locality miss in L1 cache. Ran a JMH microbenchmark to verify the same. Following is the JMH perf profile for bloom filter probingPerf stats:-------------------------------------------------- 5101.935637 task-clock (msec) # 0.461 CPUs utilized 346 context-switches # 0.068 K/sec 336 cpu-migrations # 0.066 K/sec 6,207 page-faults # 0.001 M/sec 10,016,486,301 cycles # 1.963 GHz (26.90%) 5,751,692,176 stalled-cycles-frontend # 57.42% frontend cycles idle (27.05%) &lt;not supported&gt; stalled-cycles-backend 14,359,914,397 instructions # 1.43 insns per cycle # 0.40 stalled cycles per insn (33.78%) 2,200,632,861 branches # 431.333 M/sec (33.84%) 1,162,860 branch-misses # 0.05% of all branches (33.97%) 1,025,992,254 L1-dcache-loads # 201.099 M/sec (26.56%) 432,663,098 L1-dcache-load-misses # 42.17% of all L1-dcache hits (14.49%) 331,383,297 LLC-loads # 64.952 M/sec (14.47%) 203,524 LLC-load-misses # 0.06% of all LL-cache hits (21.67%) &lt;not supported&gt; L1-icache-loads 1,633,821 L1-icache-load-misses # 0.320 M/sec (28.85%) 950,368,796 dTLB-loads # 186.276 M/sec (28.61%) 246,813,393 dTLB-load-misses # 25.97% of all dTLB cache hits (14.53%) 25,451 iTLB-loads # 0.005 M/sec (14.48%) 35,415 iTLB-load-misses # 139.15% of all iTLB cache hits (21.73%) &lt;not supported&gt; L1-dcache-prefetches 175,958 L1-dcache-prefetch-misses # 0.034 M/sec (28.94%) 11.064783140 seconds time elapsedThis shows 42.17% of L1 data cache misses. This jira is to use cache efficient bloom filter for semijoin probing.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilterMerge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.AggrStatsInvalidatorFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17228" opendate="2017-8-2 00:00:00" fixdate="2017-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump tez version to 0.9.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17229" opendate="2017-8-2 00:00:00" fixdate="2017-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastore HMSHandler locks during initialization, even though its static variable threadPool is not null</summary>
      <description>A thread pool is used to accelerate adding partitions operation, since HIVE-13901. However, HMSHandler needs a lock during initialization every time, even though its static variable threadPool is not null</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17231" opendate="2017-8-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ColumnizedDeleteEventRegistry.DeleteReaderValue optimization</summary>
      <description>For unbucketed tables DeleteReaderValue will currently return all delete events. Once we trust that the N in bucketN for "base" spit is reliable, all delete events not matching N can be skipped.This is useful to protect against extreme cases where someone runs an update/delete on a partition that matches 10 billion rows thus generates very many delete events.Since HIVE-19890 all acid data files must have bucketid/writerid in the file name match bucketid/writerid in ROW__ID in the data.OrcRawRecrodMerger.getDeltaFiles() should only return files representing the right bucket</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17233" opendate="2017-8-2 00:00:00" fixdate="2017-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set "mapred.input.dir.recursive" for HCatInputFormat-based jobs.</summary>
      <description>This has to do with HIVE-15575. TezCompiler seems to set mapred.input.dir.recursive to true. This is acceptable for Hive jobs, since this allows Hive to consume its peculiar UNION ALL output, where the output of each relation is stored in a separate sub-directory of the output-dir.For such output to be readable through HCatalog (via Pig/HCatLoader), mapred.input.dir.recursive should be set from HCatInputFormat as well. Otherwise, one gets zero records for that input.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17234" opendate="2017-8-2 00:00:00" fixdate="2017-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HBase metastore from master</summary>
      <description>No new development has been done on the HBase metastore in at least a year, and to my knowledge no one is using it (nor is it even in a state to be fully usable). Given the lack of interest in continuing to develop it, we should remove it rather than leave dead code hanging around and extra tests taking up time in test runs.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.BooleanColumnStatsMerger.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestSharedStorageDescriptor.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseFilterPlanUtil.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCacheWithBitVector.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCache.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.MockUtils.java</file>
      <file type="M">metastore.src.protobuf.org.apache.hadoop.hive.metastore.hbase.hbase.metastore.proto.proto</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.TephraHBaseConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.StringColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.LongColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.DateColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.ColumnStatsMergerFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.ColumnStatsMerger.java</file>
      <file type="M">bin.ext.hbaseimport.sh</file>
      <file type="M">bin.ext.hbaseschematool.sh</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.HBaseIntegrationTests.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseImport.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestStorageDescriptorSharing.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCliConfig.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.metastore.hbase.HBaseStoreTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">metastore.src.gen.protobuf.gen-java.org.apache.hadoop.hive.metastore.hbase.HbaseMetastoreProto.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.AggrStatsInvalidatorFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.Counter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseFilterPlanUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseImport.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseSchemaTool.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.ObjectCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.PartitionCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.PartitionKeyComparator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.StatsCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BinaryColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BooleanColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.IExtrapolatePartStatus.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.BinaryColumnStatsMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="17235" opendate="2017-8-3 00:00:00" fixdate="2017-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ORC Decimal64 Serialization/Deserialization (Part 1)</summary>
      <description>The storage-api changes for ORC-209.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.HiveDecimalTestBase.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimalImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="17263" opendate="2017-8-7 00:00:00" fixdate="2017-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce debug logging for S3 tables</summary>
      <description>When log level is set to debug operations accessing tables on amazon s3 will output a significant amount of logs, a lot of which is about the http communication (http headers and requests) which may not be that useful even for debugging purposes.Since some ZooKeeper, Hadoop, DataNucleus etc. loggers are by default set to INFO+ levels I suggest we do the same for Apache Http and AWS.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17265" opendate="2017-8-8 00:00:00" fixdate="2017-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache merged column stats from retrieved partitions</summary>
      <description>Currently when we retrieve stats from the metastore for a column in a partitioned table, we will execute the logic to merge the column stats coming from each partition multiple times.Even though we avoid multiple calls to metastore if the cache for the stats in enabled, merging the stats for a given column can take a large amount of time if there is a large number of partitions.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.empty.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.CompilationOpContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsList.java</file>
    </fixedFiles>
  </bug>
  <bug id="17267" opendate="2017-8-8 00:00:00" fixdate="2017-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make HMS Notification Listeners typesafe</summary>
      <description>Currently in the HMS we support two types of notification listeners, transactional and non-transactional ones. Transactional listeners will only be invoked if the jdbc transaction finished successfully while non-transactional ones are supposed to be resilient and will be invoked in any case, even for failures.Having the same type for these two is a source of confusion and opens the door for misconfigurations. We should try to fix this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="17268" opendate="2017-8-8 00:00:00" fixdate="2017-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI / QueryPlan: query plan is sometimes null when explain output conf is on</summary>
      <description>The Hive WebUI's Query Plan tab displays "SET hive.log.explain.output TO true TO VIEW PLAN" even when hive.log.explain.output is set to true, when the query cannot be compiled, because the plan is null in this case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.servlet.QueryProfileServlet.java</file>
      <file type="M">service.src.jamon.org.apache.hive.tmpl.QueryProfileTmpl.jamon</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug id="17276" opendate="2017-8-9 00:00:00" fixdate="2017-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check max shuffle size when converting to dynamically partitioned hash join</summary>
      <description>Currently we only check that the max number of entries in the hashmap for a MapJoin surpasses a certain threshold to decide whether to execute a dynamically partitioned hash join.We would like to factor the size of the large input that we will shuffle for the dynamically partitioned hash join into the cost model too.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.join.max.hashtable.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17277" opendate="2017-8-9 00:00:00" fixdate="2017-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastoreClient Log name is wrong</summary>
      <description>The name of Log for HiveMetastoreClient is "hive.metastore". It's confused for users to trace hive log</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="17283" opendate="2017-8-9 00:00:00" fixdate="2017-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable parallel edges of semijoin along with mapjoins</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-16260 removes parallel edges of semijoin with mapjoin. However, in some cases it maybe beneficial to have it.We need a config which can enable it.The default should be false which maintains the existing behavior.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17285" opendate="2017-8-10 00:00:00" fixdate="2017-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes for bit vector retrievals and merging</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17286" opendate="2017-8-10 00:00:00" fixdate="2017-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid expensive String serialization/deserialization for bitvectors</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.double.q.out</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StringColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DateColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.test.results.clientpositive.tunable.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.table.update.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.varchar.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exec.parallel.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.long.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.empty.table.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.FMSketch.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.fm.FMSketchUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.hll.HyperLogLog.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.NumDistinctValueEstimator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.NumDistinctValueEstimatorFactory.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.ndv.fm.TestFMSketchSerialization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreStatsMerge.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.ColumnStatsMergerFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.StringColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.quoting.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.decimal.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17289" opendate="2017-8-10 00:00:00" fixdate="2017-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>EXPORT and IMPORT shouldn&amp;#39;t perform distcp with doAs privileged user.</summary>
      <description>Currently, EXPORT uses distcp to dump data files to dump directory and IMPORT uses distcp to copy the larger files/large number of files from dump directory to table staging directory. But, this copy fails as distcp is always done with doAs user specified in hive.distcp.privileged.doAs, which is "hdfs' by default.Need to remove usage of doAs user when try to distcp from EXPORT/IMPORT flow.Privileged user based distcp should be done only for REPL DUMP/LOAD commands.Also, need to set the default config for hive.distcp.privileged.doAs to "hive" as "hdfs" super-user is never allowed.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.CopyUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1729" opendate="2010-10-18 00:00:00" fixdate="2010-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Satisfy ASF release management requirements</summary>
      <description>We need to make sure we satisfy the ASF release requirements: http://www.apache.org/dev/release.html http://incubator.apache.org/guides/releasemanagement.html</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.build.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">lib.json-LICENSE.txt</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17290" opendate="2017-8-10 00:00:00" fixdate="2017-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should use equals() rather than == to compare strings</summary>
      <description>There are number of places, where strings have been compared with == or !=. Seems like it works now, thanks to string interning, but it would be better not to be relied upon.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17297" opendate="2017-8-11 00:00:00" fixdate="2017-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow AM to use LLAP guaranteed tasks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17307" opendate="2017-8-12 00:00:00" fixdate="2017-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the metastore to not use the metrics code in hive/common</summary>
      <description>As we move code into the standalone metastore module, it cannot use the metrics in hive-common. We could copy the current Metrics interface or we could change the metastore code to directly use codahale metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HMSMetricsListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="17308" opendate="2017-8-12 00:00:00" fixdate="2017-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement in join cardinality estimation</summary>
      <description>Currently during logical planning join cardinality is estimated assuming no correlation among join keys (This estimation is done using exponential backoff). Physical planning on the other hand consider correlation for multi keys and uses different estimation. We should consider correlation during logical planning as well.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17317" opendate="2017-8-14 00:00:00" fixdate="2017-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Dbcp configurable using hive properties in hive-site.xml</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.datasource.TestDataSourceProviderFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.datasource.HikariCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.datasource.DataSourceProviderFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.datasource.BoneCPDataSourceProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="17318" opendate="2017-8-14 00:00:00" fixdate="2017-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hikari CP configurable using hive properties in hive-site.xml</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.datasource.TestDataSourceProviderFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.datasource.DataSourceProviderFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17319" opendate="2017-8-14 00:00:00" fixdate="2017-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make BoneCp configurable using hive properties in hive-site.xml</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17322" opendate="2017-8-15 00:00:00" fixdate="2017-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serialise BeeLine qtest execution to prevent flakyness</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.Parallelized.java</file>
    </fixedFiles>
  </bug>
  <bug id="17347" opendate="2017-8-17 00:00:00" fixdate="2017-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniSparkOnYarnCliDriver[spark_dynamic_partition_pruning_mapjoin_only] is failing every time</summary>
      <description>As lirui identified there was a missing file from this patch: HIVE-17247 - HoS DPP: UDFs on the partition column side does not evaluate correctly</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.mapjoin.only.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17351" opendate="2017-8-17 00:00:00" fixdate="2017-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use new slider package installation command in run.sh</summary>
      <description>The old syntax does not include some perf improvements in newer versions of Slider.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="17354" opendate="2017-8-18 00:00:00" fixdate="2017-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix "alter view" for incremental replication</summary>
      <description>There is a bug that "alter view" operation is resulting in a view creation operation instead of a overwriting/replacement operation.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17359" opendate="2017-8-18 00:00:00" fixdate="2017-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deal with TypeInfo dependencies in the metastore</summary>
      <description>The metastore uses TypeInfo, which resides in the serdes package. In order to move the metastore to be separately releasable we need to deal with this.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.MockPartitionExpressionForMetastore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.PartitionExpressionProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="1736" opendate="2010-10-20 00:00:00" fixdate="2010-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove "-dev" suffix from release package name and generate MD5 checksum using Ant</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17360" opendate="2017-8-19 00:00:00" fixdate="2017-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez session reopen appears to use a wrong conf object</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17362" opendate="2017-8-21 00:00:00" fixdate="2017-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The MAX_PREWARM_TIME should be configurable on HoS</summary>
      <description>When using HIVE_PREWARM_ENABLED, we are waiting MAX_PREWARM_TIME for the containers to warm up. This is currently set to 5s. This is often not enough for a spark session to initialize the executors. We should be able to configure this, so we can set a value which has an effect.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17366" opendate="2017-8-21 00:00:00" fixdate="2017-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constraint replication in bootstrap</summary>
      <description>Incremental constraint replication is tracked in HIVE-15705. This is to track the bootstrap replication.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.BootstrapEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17367" opendate="2017-8-21 00:00:00" fixdate="2017-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IMPORT table doesn&amp;#39;t load from data dump if a metadata-only dump was already imported.</summary>
      <description>Repl v1 creates a set of EXPORT/IMPORT commands to replicate modified data (as per events) across clusters.For instance, let's say, insert generates 2 events such asALTER_TABLE (ID: 10)INSERT (ID: 11)Each event generates a set of EXPORT and IMPORT commands.ALTER_TABLE event generates metadata only export/importINSERT generates metadata+data export/import.As Hive always dump the latest copy of table during export, it sets the latest notification event ID as current state of it. So, in this example, import of metadata by ALTER_TABLE event sets the current state of the table as 11.Now, when we try to import the data dumped by INSERT event, it is noop as the table's current state(11) is equal to the dump state (11) which in-turn leads to the data never gets replicated to target cluster.So, it is necessary to allow overwrite of table/partition if their current state equals the dump state.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.PartitionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.BootStrapReplicationSpecFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExport.java</file>
    </fixedFiles>
  </bug>
  <bug id="17372" opendate="2017-8-22 00:00:00" fixdate="2017-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update druid dependency to druid 0.10.1</summary>
      <description>Update to most recent druid version to be released August 23.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17373" opendate="2017-8-22 00:00:00" fixdate="2017-1-22 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Upgrade some dependency versions</summary>
      <description>Upgrade some libraries including log4j to 2.8.2, accumulo to 1.8.1 and commons-httpclient to 3.1.</description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppender.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneStringKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneLongKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinMultiKeyBench.java</file>
    </fixedFiles>
  </bug>
  <bug id="17376" opendate="2017-8-23 00:00:00" fixdate="2017-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade snappy version to 1.1.4</summary>
      <description>Upgrade the snappy java version to 1.1.4. The older version has some issues like memory leak (https://github.com/xerial/snappy-java/issues/91).</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17385" opendate="2017-8-24 00:00:00" fixdate="2017-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incremental repl error for non-native tables</summary>
      <description>See below error with incremental replication for non-native (storage handler based) tables. The bug is that we are not checking a table should be dumped/exported or not during incremental dump.2017-08-02T12:31:48,195 ERROR &amp;#91;HiveServer2-Background-Pool: Thread-8078&amp;#93;: exec.DDLTask (DDLTask.java:failed(632)) - org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:LOCATION may not be specified for HBase.)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17386" opendate="2017-8-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support LLAP workload management in HS2 (low level only)</summary>
      <description>This makes use of HIVE-17297 and creates building blocks for workload management policies, etc.For now, there are no policies - a single yarn queue is designated for all LLAP query AMs, and the capacity is distributed equally.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.SampleTezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.java</file>
      <file type="M">llap-common.src.protobuf.LlapPluginProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.ProtobufProxy.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapPluginProtocolClientImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.AsyncPbRpcProxy.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos.java</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.llap.TestAsyncPbRpcProxy.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.TezAmInstance.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17387" opendate="2017-8-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement Tez AM registry in Hive</summary>
      <description>Necessary for HS2 HA, to transfer AMs between HS2s, etc.Helpful for workload management.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.SampleTezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.java</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17389" opendate="2017-8-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yetus is always failing on rat checks</summary>
      <description>Rat checks are failing on metastore_db/dblock and files under patchprocess created by Yetus itself.Both directories should be excluded from rat checks.CC: pvary kgyrtkirk</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17409" opendate="2017-8-29 00:00:00" fixdate="2017-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor LLAP ZK registry to make the ZK-registry part reusable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceStateChangeListener.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstance.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.InactiveServiceInstance.java</file>
    </fixedFiles>
  </bug>
  <bug id="17410" opendate="2017-8-29 00:00:00" fixdate="2017-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>repl load task during subsequent DAG generation does not start from the last partition processed</summary>
      <description>DAG generation for repl load task was to be generated dynamically such that if the load break happens at a partition load time then for subsequent runs we should start post the last partition processed.We currently identify the point from where we have to process the event but reinitialize the iterator to start from beginning of all partition's to process.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="17411" opendate="2017-8-30 00:00:00" fixdate="2017-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO may incorrectly release a refcount in some rare cases</summary>
      <description>java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x69a390d5(-1) In a large stream whose buffers are not reused, and that is separated into many CB (e.g. due to a small ORC compression buffer size), it may happen that some, but not all, buffers that are read together as a unit are evicted from cache.If CacheBuffer follows BufferChunk in the buffer list when a stream like this is read, the latter will be converted to ProcCacheChunk; it is possible for early refcount release logic from the former to release the refcount (for a dictionary stream, the initial refCount is always released early), and then backtrack to the latter to see if we can unlock more buffers. It would then to decref an uninitialized MemoryBuffer in ProcCacheChunk because ProcCacheChunk looks like a CacheChunk. PCC initial refcounts are released separately after the data is uncompressed.I'm assuming this would almost never happen with non-stripe-level streams because one would need a large RG to span 2+ CBs, no overlap with next/previous RGs in 2+ buffers for the early release to kick in, and an unfortunate eviction order. However it's possible with large-ish dictionaries.</description>
      <version>None</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17417" opendate="2017-8-31 00:00:00" fixdate="2017-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LazySimple Timestamp is very expensive</summary>
      <description>In a specific case where a schema contains array&lt;struct&gt; with timestamp and date fields (array size &gt;10000). Any access to this column very very expensive in terms of CPU as most of the time is serialization of timestamp and date. Refer attached profiles. &gt;70% time spent in serialization + tostring conversions.</description>
      <version>2.4.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
    </fixedFiles>
  </bug>
  <bug id="17420" opendate="2017-8-31 00:00:00" fixdate="2017-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bootstrap - get replid before object dump</summary>
      <description>when we do the bootstrap dump, the idea is that we take note of the replication id of a object (table in case of this bug) and then request the table metadata from the rdbms which is then serialized to _metadata file for the table containing lastReplicationId + table metadata.This is to make sure that we only apply events after the event id in _metadata on the table, which implies that its best to order the call to get currentReplicationId and tablemetadata retrieval in the event of multiple concurrent operations happening on the same table along with bootstrap.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17421" opendate="2017-8-31 00:00:00" fixdate="2017-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear incorrect stats after replication</summary>
      <description>After replication, some stats summary are incorrect. If hive.compute.query.using.stats set to true, we will get wrong result on the destination side.This will not happen with bootstrap replication. This is because stats summary are in table properties and will be replicated to the destination. However, in incremental replication, this won't work. When creating table, the stats summary are empty (eg, numRows=0). Later when we insert data, stats summary are updated with update_table_column_statistics/update_partition_column_statistics, however, both events are not captured in incremental replication. Thus on the destination side, we will get count&amp;#40;*&amp;#41;=0. The simple solution is to remove COLUMN_STATS_ACCURATE property after incremental replication.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17422" opendate="2017-8-31 00:00:00" fixdate="2017-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip non-native/temporary tables for all major table/partition related scenarios</summary>
      <description>Currently during incremental dump, the non-native/temporary table info is partially dumped in metadata file and will be ignored later by the repl load. We can optimize it by moving the check (whether the table should be exported or not) earlier so that we don't save any info to dump file for such types of tables. CreateTableHandler already has this optimization, so we just need to apply similar logic to other scenarios.The change is to apply the EximUtil.shouldExportTable check to all scenarios (e.g. alter table) that calls into the common dump method.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17457" opendate="2017-9-5 00:00:00" fixdate="2017-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IOW Acid Insert Overwrite when the transaction fails</summary>
      <description>HIVE-14988 adds support for Insert Overwrite for Acid tables.once we have direct write to target dir (i.e. no move op) - how do we handle the case where the txn running IOW aborts? See if getAcidState() does the right thing</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17464" opendate="2017-9-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix to be able to disable max shuffle size DHJ config</summary>
      <description>Setting hive.auto.convert.join.shuffle.max.size to -1 does not work as expected.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="17468" opendate="2017-9-6 00:00:00" fixdate="2017-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade and package appropriate jackson version for druid storage handler</summary>
      <description>Currently we are excluding all the jackson core dependencies coming from druid. This is wrong in my opinion since this will lead to the packaging of unwanted jackson library from other projects.As you can see the file hive-druid-deps.txt currently jacskon core is coming from calcite and the version is 2.6.3 which is very different from 2.4.6 used by druid. This patch exclude the unwanted jars and make sure to bring in druid jackson dependency from druid it self.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17472" opendate="2017-9-6 00:00:00" fixdate="2017-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop-partition for multi-level partition fails, if data does not exist.</summary>
      <description>Raising this on behalf of cdrome and selinazh. Here's how to reproduce the problem:CREATE TABLE foobar ( foo STRING, bar STRING ) PARTITIONED BY ( dt STRING, region STRING ) STORED AS RCFILE LOCATION '/tmp/foobar';ALTER TABLE foobar ADD PARTITION ( dt='1', region='A' ) ;dfs -rm -R -skipTrash /tmp/foobar/dt=1;ALTER TABLE foobar DROP PARTITION ( dt='1' );This causes a client-side error as follows:15/02/26 23:08:32 ERROR exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: Unknown error. Please check logs.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.3.2,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17473" opendate="2017-9-7 00:00:00" fixdate="2017-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement workload management pools</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17475" opendate="2017-9-7 00:00:00" fixdate="2017-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable mapjoin using hint</summary>
      <description>Using hint disable mapjoin for a given query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17479" opendate="2017-9-7 00:00:00" fixdate="2017-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Staging directories do not get cleaned up for update/delete queries</summary>
      <description>When these queries are internally rewritten, a new context is created with a new execution id. This id is used to create the scratch directories. However, only the original context is cleared, and thus the directories created with the original execution id.The solution is to pass the execution id to the new context when the queries are internally rewritten.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="1748" opendate="2010-10-25 00:00:00" fixdate="2010-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Statistics broken for tables with size in excess of Integer.MAX_VALUE</summary>
      <description>ANALYZE TABLE x COMPUTE STATISTICS would fail to update the table size if it exceeded Integer.MAX_VALUE because it used parseInt instead of parseLong.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17480" opendate="2017-9-7 00:00:00" fixdate="2017-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>repl dump sub dir should use UUID instead of timestamp</summary>
      <description>This is to fix the concurrency issue that multiple dump operations could end up using the same timestamp for the dump dir name.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="17482" opendate="2017-9-7 00:00:00" fixdate="2017-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External LLAP client: acquire locks for tables queried directly by LLAP</summary>
      <description>When using the LLAP external client with simple queries (filter/project of single table), the appropriate locks should be taken on the table being read like they are for normal Hive queries. This is important in the case of transactional tables being queried, since the compactor relies on the presence of table locks to determine whether it can safely delete old versions of compacted files without affecting currently running queries.This does not have to happen in the complex query case, since a query is used (with the appropriate locking mechanisms) to create/populate the temp table holding the results to the complex query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
    </fixedFiles>
  </bug>
  <bug id="17489" opendate="2017-9-8 00:00:00" fixdate="2017-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate client-facing and server-side Kerberos principals, to support HA</summary>
      <description>On deployments of the Hive metastore where a farm of servers is fronted by a VIP, the hostname of the VIP (e.g. mycluster-hcat.blue.myth.net) will differ from the actual boxen in the farm (.e.g mycluster-hcat-[0..3].blue.myth.net).Such a deployment messes up Kerberos auth, with principals like hcat/mycluster-hcat.blue.myth.net@GRID.MYTH.NET. Host-based checks will disallow servers behind the VIP from using the VIP's hostname in its principal when accessing, say, HDFS.The solution would be to decouple the server-side principal (used to access other services like HDFS as a client) from the client-facing principal (used from Hive-client, BeeLine, etc.).</description>
      <version>None</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.metastore.security.TestHadoopAuthBridge23.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17493" opendate="2017-9-9 00:00:00" fixdate="2017-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve PKFK cardinality estimation in Physical planning</summary>
      <description>Cardinality estimation of a join, after PK-FK relation has been ascertained, could be improved if parent of the join operator is LEFT outer or RIGHT outer join.Currently estimation is done by estimating reduction of rows occurred on PK side, then multiplying the reduction to FK side row count. This estimation of reduction currently doesn't distinguish b/w INNER or OUTER joins. This could be improved to handle outer joins better.TPC-DS query45 is impacted by this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17496" opendate="2017-9-9 00:00:00" fixdate="2017-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap repl is not cleaning up staging dirs</summary>
      <description>This will put more pressure on the HDFS file limit.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.PathInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17504" opendate="2017-9-11 00:00:00" fixdate="2017-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip ACID table for replication</summary>
      <description>Currently we are not supporting replicate ACID tables (which will be future work).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17508" opendate="2017-9-11 00:00:00" fixdate="2017-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement global execution triggers based on counters</summary>
      <description>Workload management can defined Triggers that are bound to a resource plan. Each trigger can have a trigger expression and an action associated with it. Trigger expressions are evaluated at runtime after configurable check interval, based on which actions like killing a query, moving a query to different pool etc. will get invoked. Simple execution trigger could be something likeCREATE TRIGGER slow_query IN globalWHEN execution_time_ms &gt; 10000MOVE TO slow_queue</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.AsyncPbRpcProxy.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17512" opendate="2017-9-12 00:00:00" fixdate="2017-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not use doAs if distcp privileged user same as user running hive</summary>
      <description>Click to add description</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17514" opendate="2017-9-12 00:00:00" fixdate="2017-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use SHA-256 for cookie signer to improve security</summary>
      <description>See HIVE-17226 for detailed description.</description>
      <version>None</version>
      <fixedVersion>2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17519" opendate="2017-9-12 00:00:00" fixdate="2017-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transpose column stats display</summary>
      <description>currently describe formatted table1 insert_num shows the column informations in a table like format...which is very hard to read - because there are to many columns# col_name data_type min max num_nulls distinct_count avg_col_len max_col_len num_trues num_falses comment bitVector insert_num int from deserializer I think it would be better to show the same information like this:col_name insert_num data_type int min max num_nulls distinct_count avg_col_len max_col_len num_trues num_falses comment from deserializer bitVector</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.table.stats.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tunable.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.3.exim.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.2.exim.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.table.update.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.timestamp2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.date2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.array.null.element.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.named.column.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fsstat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.2.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lb.fs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.skewtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.desc.tbl.part.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.nonascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.defaultformats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compustat.avro.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.infinity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.add.column3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.blobstore.to.blobstore.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.blobstore.to.local.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.blobstore.to.warehouse.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.local.to.blobstore.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.blobstore.to.blobstore.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.blobstore.to.local.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.blobstore.to.warehouse.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.local.to.blobstore.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.2columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidcolname.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidtype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.partial.spec.dyndisabled.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.partialscan.autogether.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.change.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.onto.nocurrent.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17521" opendate="2017-9-12 00:00:00" fixdate="2017-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve defaults for few runtime configs</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.hint.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17522" opendate="2017-9-12 00:00:00" fixdate="2017-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cleanup old &amp;#39;repl dump&amp;#39; dirs</summary>
      <description>We want to clean up the old dump dirs to save space and reduce scan time when needed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1755" opendate="2010-10-27 00:00:00" fixdate="2010-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBM diff in test caused by Hive-1641</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17552" opendate="2017-9-18 00:00:00" fixdate="2017-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable bucket map join by default for Tez</summary>
      <description>Currently bucket map join is disabled by default, however, it is potentially most optimal join we have. Need to enable it by default.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.5.q</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17553" opendate="2017-9-19 00:00:00" fixdate="2017-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO wrongly type cast decimal literal to int</summary>
      <description>explain select 100.000BD from fSTAGE PLANS: Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: TableScan alias: f Select Operator expressions: 100 (type: int) outputColumnNames: _col0 ListSinkNotice that the expression 100.000BD is of type int instead of decimal.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.wrong.column.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17558" opendate="2017-9-19 00:00:00" fixdate="2017-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip non-native/temporary tables for constraint related scenarios</summary>
      <description>The change would be similar to HIVE-17422.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17560" opendate="2017-9-19 00:00:00" fixdate="2017-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastore doesn&amp;#39;t start in secure cluster if repl change manager is enabled</summary>
      <description>When hive.repl.cm.enabled=true, ReplChangeManager tries to access HDFS before metastore does kerberos login using keytab.Metastore startup code doesn't do an explicit login using keytab, but instead relies on kinit by saslserver for use by thrift to do it.It would be cleaner to do an explicit UGI.loginFromKeytab instead to avoid such issues in future as well.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17569" opendate="2017-9-20 00:00:00" fixdate="2017-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compare filtered output files in BeeLine tests</summary>
      <description>When running the BeeLine tests agains different configurations, the output of certain commands, like explain can be different. Also the result of the describe extended/formatted commands when running them on BeeLine have different result then in the golden out files. To be able to reuse these out files, we should have an option to filter out these commands.The idea is to introduce a new property "test.beeline.compare.portable" with the default value false and if this property is set to true, these commands will be filtered out from the out files before the diff.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="1757" opendate="2010-10-28 00:00:00" fixdate="2010-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test cleanup for Hive-1641</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JDBMSinkOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17570" opendate="2017-9-20 00:00:00" fixdate="2017-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix view deletion related test failures (create_view.q etc)</summary>
      <description>Fixing the bug introduced by HIVE-17459. Sorry that did not capture that in a timely fashion.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="17571" opendate="2017-9-20 00:00:00" fixdate="2017-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update sql standard authorization config whitelist to include distcp options for replication</summary>
      <description>Additional distcp config options (added in HIVE-16686) need to be added to whitelist of configs that can be updated at runtime, for sql standard authorization.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17574" opendate="2017-9-21 00:00:00" fixdate="2017-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid multiple copies of HDFS-based jars when localizing job-jars</summary>
      <description>Raising this on behalf of Selina Zhang. (For my own reference: YHIVE-1035.)This has to do with the classpaths of Hive actions run from Oozie, and affects scripts that adds jars/resources from HDFS locations.As part of Oozie's "sharelib" deploys, foundation jars (such as Hive jars) tend to be stored in HDFS paths, as are any custom user-libraries used in workflows. An ADD JAR|FILE|ARCHIVE statement in a Hive script causes the following steps to occur: Files are downloaded from HDFS to local temp dir. UDFs are resolved/validated. All jars/files, including those just downloaded from HDFS, are shipped right back to HDFS-based scratch-directories, for job submission.For HDFS-based files, this is wasteful and time-consuming. #3 above should skip shipping HDFS-based resources, and add those directly to the Tez session.We have a patch that's being used internally at Yahoo.</description>
      <version>2.2.0,2.4.0,3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestAddResource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1758" opendate="2010-10-28 00:00:00" fixdate="2010-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize group by hash map memory</summary>
      <description>Group By map side's hash map consumes a lot of memory, thereby decreasing its effectiveness.We can use some of the optimizations from map-join to reduce the memory footprint: class KeyWrapper { int hashcode; ArrayList&lt;Object&gt; keys; // decide whether this is already in hashmap (keys in hashmap are deepcopied // version, and we need to use 'currentKeyObjectInspector'). boolean copy = false;1. Changes keys to Array2. Optimize the scenario when keys is of a small size (1,2) etcLet us start profiling it and take it from there</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17584" opendate="2017-9-22 00:00:00" fixdate="2017-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix mapred.job.queue.name in sql standard authorization config whitelist</summary>
      <description>The SQL std authorization config white list has mapred.job.queuename, it should be mapred.job.queue.name (see https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/DeprecatedProperties.html)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17585" opendate="2017-9-22 00:00:00" fixdate="2017-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve thread safety when loading dynamic partitions in parallel</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.metastore.SynchronizedMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="17595" opendate="2017-9-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct DAG for updating the last.repl.id for a database during bootstrap load</summary>
      <description>We update the last.repl.id as a database property. This is done after all the bootstrap tasks to load the relevant data are done and is the last task to be run. however we are currently not setting up the DAG correctly for this task. This is getting added as the root task for now where as it should be the last task to be run in a DAG. This becomes more important after the inclusion of HIVE-17426 since this will lead to parallel execution and incorrect DAG's will lead to incorrect results/state of the system.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="176" opendate="2008-12-15 00:00:00" fixdate="2008-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>structured log for obtaining query stats/info</summary>
      <description>Josh &lt;josh@besquared.net&gt; wrote:When launching off hive queries using hive -e is there a way to get the job id so that I can just queue them up and go check their statuses later? What's the general pattern for queueing and monitoring without using the libraries directly?I'm gonna throw my vote in for a structured log format. Users could tail it and use whatever queuing or monitoring they wish. It's also probably just a 30 minute project for someone already familiar with the code. I suggest ^A seperated key=value pairs per log line.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17601" opendate="2017-9-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve error handling in LlapServiceDriver</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17609" opendate="2017-9-26 00:00:00" fixdate="2017-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to manipulate delegation tokens</summary>
      <description>This was precipitated by OOZIE-2797. We had a case in production where the number of active metastore delegation tokens outstripped the ZooKeeper jute.maxBuffer size. Delegation tokens could neither be fetched, nor be cancelled. The root-cause turned out to be a miscommunication, causing delegation tokens fetched by Oozie not to be cancelled automatically from HCat. This was sorted out as part of OOZIE-2797.The issue exposed how poor the log-messages were, in the code pertaining to token fetch/cancellation. We also found need for a tool to query/list/purge delegation tokens that might have expired already. This patch introduces such a tool, and improves the log-messages.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.Security.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1761" opendate="2010-11-1 00:00:00" fixdate="2010-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support show locks for a particular table</summary>
      <description>Currently, only show locks is supported - it would be very useful to show locks for a particular table</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lock2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lock1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lock2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.lock1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17610" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: an exception in exception handling can hide the original exception</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17613" opendate="2017-9-27 00:00:00" fixdate="2017-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove object pools for short, same-thread allocations</summary>
      <description>Objects pools probably don't have much effect in this case.They are only useful when allocations are shared between threads.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17617" opendate="2017-9-27 00:00:00" fixdate="2017-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rollup of an empty resultset should contain the grouping of the empty grouping set</summary>
      <description>runningdrop table if exists tx1;create table tx1 (a integer,b integer,c integer);select sum(c), grouping(b)from tx1group by rollup (b);returns 0 rows; however according to the standard:The &lt;empty grouping set&gt; is regarded as the shortest such initial sublist. For example, ROLLUP ( (A, B), (C, D) )is equivalent to GROUPING SETS ( (A, B, C, D), (A, B), () ).so I think the totals row (the grouping for () should be present) - psql returns it.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17621" opendate="2017-9-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-site settings are ignored during HCatInputFormat split-calculation</summary>
      <description>Another one that Selina Zhang and Chris Drome worked on.The production hive-site.xml could well contain settings that differ from the defaults in HiveConf.java. In our case, we introduced a custom ORC split-strategy, which we introduced as the site-wide default.We noticed that during HCatInputFormat::getSplits(), if the user-script did not contain the setting, the site-wide default was ignored in favour of the HiveConf default. HCat would not convey hive-site settings to the input-format (or anywhere downstream).The forthcoming patch fixes this problem.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17623" opendate="2017-9-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Select query Fix Double column serde and some refactoring</summary>
      <description>This PR has 2 fixes.First, fixes the limit of results returned by Select query that used to be limited to 16K rowsSecond fixes the type inference for the double type newly added to druid.Use Jackson polymorphism to infer types and parse results from druid nodes.Removes duplicate codes form RecordReaders.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDeUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17627" opendate="2017-9-27 00:00:00" fixdate="2017-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use druid scan query instead of the select query.</summary>
      <description>The biggest difference between select query and scan query is that, scan query doesn't retain all rows in memory before rows can be returned to client.It will cause memory pressure if too many rows required by select query.Scan query doesn't have this issue.Scan query can return all rows without issuing another pagination query, which is extremely useful when query against historical or realtime node directly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17631" opendate="2017-9-28 00:00:00" fixdate="2017-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade orc to 1.4.1</summary>
      <description>It seems like orc 1.4.0 has a latest and stable version:https://orc.apache.org/docs/releases.html</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17633" opendate="2017-9-28 00:00:00" fixdate="2017-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to override the query results directory in TestBeeLineDriver</summary>
      <description>It would be good to have the possibility to override where the TestBeeLineDriver looks for the golden files</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">data.scripts.q.test.init.sql</file>
    </fixedFiles>
  </bug>
  <bug id="17639" opendate="2017-9-28 00:00:00" fixdate="2017-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t reuse planner context when re-parsing the query</summary>
      <description>The error is "java.lang.AssertionError: Unexpected type UNEXPECTED", e.g. on CTAS</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17665" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update netty-all to latest 4.0.x.Final</summary>
      <description>Update netty version to latest 4.0.x.Final to address http://www.cvedetails.com/cve/CVE-2016-4970/</description>
      <version>2.4.0,3.0.0</version>
      <fixedVersion>2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17669" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache to optimize SearchArgument deserialization</summary>
      <description>And another, from selinazh and cdrome. (YHIVE-927)When a mapper needs to process multiple ORC files, it might land up having use essentially the same SearchArgument over several files. It would be good not to have to deserialize from string, over and over again. Caching the object against the string-form should speed things up.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1767" opendate="2010-11-4 00:00:00" fixdate="2010-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge files does not work with dynamic partition</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17672" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite version to 1.14</summary>
      <description>Calcite 1.14.0 has been recently released.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDeUtils.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlCountAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveConfPlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveGroupingID.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExceptRewriteRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveIntersectRewriteRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelColumnsAlignment.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFDateFormatGranularity.java</file>
      <file type="M">ql.src.test.queries.clientpositive.druid.timeseries.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druid.topn.q</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17681" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to log bootstrap dump progress state property to HS2 logs.</summary>
      <description>Currently, rename is disabled if bootstrap dump in progress. This is achieved by setting a "ACTIVE" flag in database properties which is referred by rename operation. If HiveServer2 crashes when dump in progress, then this flag is not unset (to IDLE state) which make the rename operation disabled for ever. User need to manually enable rename in this scenario. So, need to log the property name associated with bootstrap dump which was in progress when HS2 crashes. Using this, user will reset this property of the database to enable rename again.Also, need document update for the same.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17682" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: IF stmt produces wrong results</summary>
      <description>A query using with a vectorized IF(condition, thenExpr, elseExpr) function can produce wrong results.</description>
      <version>1.2.2,2.3.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17684" opendate="2017-10-3 00:00:00" fixdate="2017-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS memory issues with MapJoinMemoryExhaustionHandler</summary>
      <description>We have seen a number of memory issues due the HashSinkOperator use of the MapJoinMemoryExhaustionHandler. This handler is meant to detect scenarios where the small table is taking too much space in memory, in which case a MapJoinMemoryExhaustionError is thrown.The configs to control this logic are:hive.mapjoin.localtask.max.memory.usage (default 0.90)hive.mapjoin.followby.gby.localtask.max.memory.usage (default 0.55)The handler works by using the MemoryMXBean and uses the following logic to estimate how much memory the HashMap is consuming: MemoryMXBean#getHeapMemoryUsage().getUsed() / MemoryMXBean#getHeapMemoryUsage().getMax()The issue is that MemoryMXBean#getHeapMemoryUsage().getUsed() can be inaccurate. The value returned by this method returns all reachable and unreachable memory on the heap, so there may be a bunch of garbage data, and the JVM just hasn't taken the time to reclaim it all. This can lead to intermittent failures of this check even though a simple GC would have reclaimed enough space for the process to continue working.We should re-think the usage of MapJoinMemoryExhaustionHandler for HoS. In Hive-on-MR this probably made sense to use because every Hive task was run in a dedicated container, so a Hive Task could assume it created most of the data on the heap. However, in Hive-on-Spark there can be multiple Hive Tasks running in a single executor, each doing different things.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17690" opendate="2017-10-4 00:00:00" fixdate="2017-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add distcp.options.p* in sql standard authorization config whitelist</summary>
      <description>distcp arguments for "-p" are specified right after the "-p" without space. eg "-px"Whitelist needs to be modified to allow this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17706" opendate="2017-10-5 00:00:00" fixdate="2017-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a possibility to run the BeeLine tests on the default database</summary>
      <description>Currently it is possible to run the BeeLine tests sequentially but it still relies on cleaning up after the tests by cleaning up the database. Some of the tests could be run only against the default database. We need a cleanup mechanism between the tests</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFileBeeLineClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="17708" opendate="2017-10-5 00:00:00" fixdate="2017-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade surefire to 2.20.1</summary>
      <description>with the current 2.18.1 jdk9 test execution result in:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project hive-common: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: java.lang.NoClassDefFoundError: java/sql/Timestamp: java.sql.Timestamp -&gt; [Help 1]make sure that it works reliably (2.19.1 had a bug which made idea debuging problematic)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17717" opendate="2017-10-5 00:00:00" fixdate="2017-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable rule to push post-aggregations into Druid</summary>
      <description>Enable rule created by CALCITE-1803.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17720" opendate="2017-10-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bitvectors are not shown in describe statement on beeline</summary>
      <description>Describe statement takes different codepath for HS2 where bit vectors weren't displayed.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17721" opendate="2017-10-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>with Postgres rdbms for metastore and dbnotification enabled, hive DDL SQL query fails</summary>
      <description>with postgres rdbms for hive-metastore any DDL fails when dbnotification is enabled, the reason being a lock on the notification sequence is required, which for Postgres requires the column-names and table names to enclosed in "(double quotes) as we are using direct SQL and not going through datanucleus and postgres is case sensitive.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17731" opendate="2017-10-6 00:00:00" fixdate="2017-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a backward compat option for external users to HIVE-11985</summary>
      <description>See HIVE-11985.Some external callers (e.g. Presto) do not appear to process types from deserializer correctly, relying on DB types. Ideally, it should be resolved via HIVE-17714, hiding the custom SerDe logic from users.For now we can add a backward compatibility config for such cases.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1774" opendate="2010-11-6 00:00:00" fixdate="2010-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>merge_dynamic_part&amp;#39;s result is not deterministic</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.merge.dynamic.partition.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17749" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple class have missed the ASF header</summary>
      <description>Multiple class have missed the ASF header</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.rules.TestHiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.repl.DumpDirCleanerTask.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17756" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable subquery related Qtests for Hive on Spark</summary>
      <description>HIVE-15456 and HIVE-15192 using Calsite to decorrelate and plan subqueries. This JIRA is to indroduce subquery test and verify the subqueries plan for Hive on Spark</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17757" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD need to use customised configurations to execute distcp/remote copy.</summary>
      <description>As REPL LOAD command needs to read repl dump directory and data files from source cluster, it needs to use some of the configurations to read data securely through distcp.Some of the HDFS configurations cannot be added to whitelist as they pose security threat. So, it is necessary for REPL LOAD command to take such configs as input and use it when trigger distcp.Proposed syntax:REPL LOAD [&lt;dbname&gt;&amp;#91;.&lt;tablename&gt;&amp;#93;] FROM &lt;dirname&gt; &amp;#91;WITH (&amp;#39;key1&amp;#39;=&amp;#39;value1&amp;#39;, &amp;#39;key2&amp;#39;=&amp;#39;value2&amp;#39;)&amp;#93;;</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="1776" opendate="2010-11-8 00:00:00" fixdate="2010-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parallel execution and auto-local mode combine to place plan file in wrong file system</summary>
      <description>A query (that i can't reproduce verbatim) submits a job to a MR cluster with a plan file that is resident on the local file system. This job obviously fails.This seems to result from an interaction between the parallel execution (which is trying to run one local and one remote job at the same time). Turning off either the parallel execution mode or the auto-local mode seems to fix the problem.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17761" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate hive.druid.select.distribute property for Druid</summary>
      <description>Execution of SELECT queries is distributed among the different historical/realtime nodes containing the data when the property is true. This is the default mode and one that has been extensively tested.Previously SELECT queries were split and sent in parallel to the broker nodes, but this mode is not recommended anymore and deprecated. Thus, that code can be removed.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17762" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude older jackson-annotation.jar from druid-handler shaded jar</summary>
      <description>hive-druid-handler.jar is shading jackson core dependencies in hive-17468 but older versions are brought in from the transitive dependencies.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17771" opendate="2017-10-11 00:00:00" fixdate="2017-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement commands to manage resource plan</summary>
      <description>Please see parent jira about llap workload management.This jira is to implement create and show resource plan commands in hive to configure resource plans for llap workload. The following are the proposed commands implemented as part of the jira:CREATE RESOURCE PLAN plan_name WITH QUERY_PARALLELISM parallelism;SHOW RESOURCE PLAN plan_name;SHOW RESOURCE PLANS;ALTER RESOURCE PLAN plan_name SET QUERY_PARALLELISM = parallelism;ALTER RESOURCE PLAN plan_name RENAME TO new_name;ALTER RESOURCE PLAN plan_name ACTIVATE;ALTER RESOURCE PLAN plan_name DISABLE;ALTER RESOURCE PLAN plan_name ENABLE;DROP RESOURCE PLAN;It will be followed up with more jiras to manage pools, triggers and copy resource plans.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug id="17778" opendate="2017-10-11 00:00:00" fixdate="2017-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for custom counters in trigger expression</summary>
      <description>HIVE-17508 only supports limited counters. This ticket is to extend it to support custom counters (counters that are not supported by execution engine will be dropped).</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.wm.TestTrigger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.ExpressionFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17781" opendate="2017-10-11 00:00:00" fixdate="2017-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map MR settings to Tez settings via DeprecatedKeys</summary>
      <description>Here's one that cdrome and thiruvel worked on:We found that certain Hadoop Map/Reduce settings that are set in site config files do not take effect in Hive jobs, because the Tez site configs do not contain the same settings.In Yahoo's case, the problem was that, at the time, there was no mapping between MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART and TEZ_SHUFFLE_VERTEX_MANAGER_MAX_SRC_FRACTION. There were situations where significant capacity on production clusters were being used up doing nothing, while waiting for slow tasks to complete. This would have been avoided, were the mappings in place.Tez provides a DeprecatedKeys utility class, to help map MR settings to Tez settings. Hive should use this to ensure that the mappings are in sync.(Note to self: YHIVE-883)</description>
      <version>3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="17782" opendate="2017-10-12 00:00:00" fixdate="2017-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent cast behavior from string to numeric types with regards to leading/trailing spaces</summary>
      <description>select cast(' 1 ' as tinyint), cast(' 1 ' as smallint), cast(' 1 ' as int), cast(' 1 ' as bigint), cast(' 1 ' as float), cast(' 1 ' as double), cast(' 1 ' as decimal(10,2))NULL NULL NULL NULL 1.0 1.0 1Looks like integer types (short, int, etc) fail the conversion due to the leading/trailing spaces and return NULL, while float/double/decimal do not. In fact, Decimal used to also return NULL in previous versions up until HIVE-10799.Let's try to make this behavior consistent across all of these types, should be simple enough to strip spaces before passing to number formatter.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyShort.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyByte.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17787" opendate="2017-10-12 00:00:00" fixdate="2017-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply more filters on the BeeLine test output files (follow-up on HIVE-17569)</summary>
      <description>When running the q tests with BeeLine, some known differences came up which should be filtered out if the "test.beeline.compare.portable" parameter is set to true.The result of the following commands can be different when running them via BeeLine then in the golden out file: DESCRIBE SHOW TABLES SHOW FORMATTED TABLES SHOW DATABASES TABLESAlso the join warnings and the mapreduce jobtracker address can be different so it would make sense to filter them out.For example:Warning: Map Join MAPJOIN[13][bigTable=?] in task 'Stage-3:MAPRED' is a cross productWarning: MASKED is a cross productmapreduce.jobtracker.address=localmapreduce.jobtracker.address=MASKED</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="1779" opendate="2010-11-10 00:00:00" fixdate="2010-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement GenericUDF str_to_map</summary>
      <description>People need way to load their data to map.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17790" opendate="2017-10-12 00:00:00" fixdate="2017-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export/Import: Bug while getting auth entities due to which we write partition info during compilation phase</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
    </fixedFiles>
  </bug>
  <bug id="17793" opendate="2017-10-12 00:00:00" fixdate="2017-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parameterize Logging Messages</summary>
      <description>Use SLF4J parameterized logging Remove use of archaic Util's "stringifyException" and simply allow logging framework to handle formatting of output. Also saves having to create the error message and then throwing it away when the logging level is set higher than the logging message Add some LOG.isDebugEnabled around complex debug messages</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="17799" opendate="2017-10-13 00:00:00" fixdate="2017-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Ellipsis For Truncated Query In Hive Lock</summary>
      <description>HIVE-16334 introduced truncation for storing queries in ZK lock nodes. This Jira is to add ellipsis into the query to let the operator know that truncation has occurred and therefore they will not find the specific query in their logs, only a prefix match will work.-- Truncation of query may be confusing to operator-- Without truncationSELECT * FROM TABLE WHERE COL=1-- With truncation (operator will not find this query in workload)SELECT * FROM TABLE-- With truncation (operator will know this is only a prefix match)SELECT * FROM TABLE...</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
    </fixedFiles>
  </bug>
  <bug id="17806" opendate="2017-10-13 00:00:00" fixdate="2017-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create directory for metrics file if it doesn&amp;#39;t exist</summary>
      <description>HIVE-17563 changed metrics code to use local file system operations instead of Hadoop local file system operations. There is an unintended side effect - hadoop file systems create the directory if it doesn't exist and java nio interfaces don't. The purpose of this fix is to revert the behavior to the original one to avoid surprises.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.metrics.JsonReporter.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17807" opendate="2017-10-13 00:00:00" fixdate="2017-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute maven commands in batch mode for ptests</summary>
      <description>No need to run in interactive mode in CI environment.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17809" opendate="2017-10-13 00:00:00" fixdate="2017-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement per pool trigger validation and move sessions across pools</summary>
      <description>HIVE-17508 trigger validation is applied for all pools at once. This is follow up to implement trigger validation at per pool level. This should also implement resolution for multiple applicable actions, as per the RB discussion</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.wm.TestTrigger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.VertexCounterLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggersFetcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggerExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.Trigger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TimeCounterLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.SessionTriggerProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.MetastoreResourcePlanTriggersFetcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.MetastoreGlobalTriggersFetcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.FileSystemCounterLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.ExpressionFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.Expression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.ExecutionTrigger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.CustomCounterLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerViolationActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KillTriggerActionHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersNoTezSessionPool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.AbstractJdbcTriggersTest.java</file>
      <file type="M">data.conf.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17813" opendate="2017-10-15 00:00:00" fixdate="2017-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.exec.move.files.from.source.dir does not work with partitioned tables</summary>
      <description>Setting hive.exec.move.files.from.source.dir=true causes data to not be moved properly during inserts to partitioned tables.Looks like the file path checking in Utilties.moveSpecifiedFiles() needs to recursively check into directories.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="17815" opendate="2017-10-16 00:00:00" fixdate="2017-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>prevent OOM with Atlas Hive hook</summary>
      <description>as part of HIVE-17814 we are going to handle the issue w.r.t to hive as well as the post execution hook api's . However for atlas which is a commonly used hive post execution hook, we want to prevent additional memory usage. Also Atlas currently does not handle / work on replication queries hence overloading the hookContext with TaskRunner objects is just using a lot of memory. The same should be true for other execution hooks as well since replication is a new functionality,This task is to reduce that for replication related queries.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="17822" opendate="2017-10-16 00:00:00" fixdate="2017-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to skip shading of jars</summary>
      <description>Maven shade plugin does not have option to skip. Adding it under a profile can help with skip shade reducing build times.Maven build profile shows druid and jdbc shade plugin to be slowest (also hive-exec). For devs not working on druid or jdbc, it will be good to have an option to skip shading via a profile. With this it will be possible to get a subminute dev build.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17824" opendate="2017-10-17 00:00:00" fixdate="2017-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>msck repair table should drop the missing partitions from metastore</summary>
      <description>msck repair table &lt;tablename&gt; is often used in environments where the new partitions are loaded as directories on HDFS or S3 and users want to create the missing partitions in bulk. However, currently it only supports addition of missing partitions. If there are any partitions which are present in metastore but not on the FileSystem, it should also delete them so that it truly repairs the table metadata.We should be careful not to break backwards compatibility so we should either introduce a new config or keyword to add support to delete unnecessary partitions from the metastore. This way users who want the old behavior can easily turn it off.</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MsckDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QOutProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17826" opendate="2017-10-18 00:00:00" fixdate="2017-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error writing to RandomAccessFile after operation log is closed</summary>
      <description>We are seeing the error from HS2 process stdout.2017-09-07 10:17:23,933 AsyncLogger-1 ERROR Attempted to append to non-started appender query-file-appender2017-09-07 10:17:23,934 AsyncLogger-1 ERROR Attempted to append to non-started appender query-file-appender2017-09-07 10:17:23,935 AsyncLogger-1 ERROR Unable to write to stream /var/log/hive/operation_logs/dd38df5b-3c09-48c9-ad64-a2eee093bea6/hive_20170907101723_1a6ad4b9-f662-4e7a-a495-06e3341308f9 for appender query-file-appender2017-09-07 10:17:23,935 AsyncLogger-1 ERROR An exception occurred processing Appender query-file-appender org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to RandomAccessFile /var/log/hive/operation_logs/dd38df5b-3c09-48c9-ad64-a2eee093bea6/hive_20170907101723_1a6ad4b9-f662-4e7a-a495-06e3341308f9 at org.apache.logging.log4j.core.appender.RandomAccessFileManager.flush(RandomAccessFileManager.java:114) at org.apache.logging.log4j.core.appender.RandomAccessFileManager.write(RandomAccessFileManager.java:103) at org.apache.logging.log4j.core.appender.OutputStreamManager.write(OutputStreamManager.java:136) at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:105) at org.apache.logging.log4j.core.appender.RandomAccessFileAppender.append(RandomAccessFileAppender.java:89) at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:152) at org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:125) at org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:116) at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:84) at org.apache.logging.log4j.core.appender.routing.RoutingAppender.append(RoutingAppender.java:112) at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:152) at org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:125) at org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:116) at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:84) at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:390) at org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:378) at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:362) at org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:79) at org.apache.logging.log4j.core.async.AsyncLogger.actualAsyncLog(AsyncLogger.java:385) at org.apache.logging.log4j.core.async.RingBufferLogEvent.execute(RingBufferLogEvent.java:103) at org.apache.logging.log4j.core.async.RingBufferLogEventHandler.onEvent(RingBufferLogEventHandler.java:43) at org.apache.logging.log4j.core.async.RingBufferLogEventHandler.onEvent(RingBufferLogEventHandler.java:28) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: Stream Closed at java.io.RandomAccessFile.writeBytes(Native Method) at java.io.RandomAccessFile.write(RandomAccessFile.java:525) at org.apache.logging.log4j.core.appender.RandomAccessFileManager.flush(RandomAccessFileManager.java:111) ... 25 more</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppender.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingLayout.java</file>
    </fixedFiles>
  </bug>
  <bug id="17831" opendate="2017-10-18 00:00:00" fixdate="2017-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveSemanticAnalyzerHookContext does not update the HiveOperation after sem.analyze() is called</summary>
      <description>The SemanticAnalyzer.analyze() called on the Driver.compile() method updates the HiveOperation based on the analysis this does. However, the patch done on HIVE-17048 does not update such operation and is send an invalid operation to the postAnalyze() call.</description>
      <version>2.2.1,2.3.1,2.4.0,3.0.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.2,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.hooks.TestHs2Hooks.java</file>
    </fixedFiles>
  </bug>
  <bug id="17833" opendate="2017-10-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish split generation counters</summary>
      <description>With TEZ-3856, tez counters are exposed via input initializers which can be used to publish split generation counters.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.non.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.mm.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.wm.TestTrigger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.VertexCounterLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersNoTezSessionPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="17834" opendate="2017-10-19 00:00:00" fixdate="2017-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix flaky triggers test</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-12631?focusedCommentId=16209803&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16209803</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17836" opendate="2017-10-19 00:00:00" fixdate="2017-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Persisting nulls in bit vector field fails for postgres backed metastore</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.SQLGenerator.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17838" opendate="2017-10-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make org.apache.hive.spark.client.rpc logging HoS specific and other logging cleanup</summary>
      <description>A lot of the logging in org.apache.hive.spark.client.rpc emits information that a generic RPC library would, but this package is only used by HoS. We should make the logging more HoS specific so users don't get confused as to what this RPC library is doing. Ideally, they aren't even aware that a RPC library is in place, it should be more transparent to the user.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientFactory.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcServer.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcDispatcher.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.Rpc.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.KryoMessageCodec.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.metrics.ShuffleWriteMetrics.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.metrics.ShuffleReadMetrics.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.metrics.Metrics.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.metrics.InputMetrics.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.BaseProtocol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="17839" opendate="2017-10-19 00:00:00" fixdate="2017-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot generate thrift definitions in standalone-metastore</summary>
      <description>mvn clean install -Pthriftif -Dthrift.home=... does not regenerate the thrift sources. This is after the https://issues.apache.org/jira/browse/HIVE-17506 fix.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17856" opendate="2017-10-19 00:00:00" fixdate="2017-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables - IOW is not ACID compliant</summary>
      <description>The following tests were removed from mm_all during "integration"... I should have never allowed such manner of intergration.MM logic should have been kept intact until ACID logic could catch up. Alas, here we are.drop table iow0_mm;create table iow0_mm(key int) tblproperties("transactional"="true", "transactional_properties"="insert_only");insert overwrite table iow0_mm select key from intermediate;insert into table iow0_mm select key + 1 from intermediate;select * from iow0_mm order by key;insert overwrite table iow0_mm select key + 2 from intermediate;select * from iow0_mm order by key;drop table iow0_mm;drop table iow1_mm; create table iow1_mm(key int) partitioned by (key2 int) tblproperties("transactional"="true", "transactional_properties"="insert_only");insert overwrite table iow1_mm partition (key2)select key as k1, key from intermediate union all select key as k1, key from intermediate;insert into table iow1_mm partition (key2)select key + 1 as k1, key from intermediate union all select key as k1, key from intermediate;select * from iow1_mm order by key, key2;insert overwrite table iow1_mm partition (key2)select key + 3 as k1, key from intermediate union all select key + 4 as k1, key from intermediate;select * from iow1_mm order by key, key2;insert overwrite table iow1_mm partition (key2)select key + 3 as k1, key + 3 from intermediate union all select key + 2 as k1, key + 2 from intermediate;select * from iow1_mm order by key, key2;drop table iow1_mm;drop table simple_mm;create table simple_mm(key int) stored as orc tblproperties ("transactional"="true", "transactional_properties"="insert_only");insert into table simple_mm select key from intermediate;-insert overwrite table simple_mm select key from intermediate;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17858" opendate="2017-10-19 00:00:00" fixdate="2017-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM - some union cases are broken</summary>
      <description>mm_all test no longer runs on LLAP; if it's executed in LLAP, one can see that some union cases no longer work.Queries on partunion_mm, skew_dp_union_mm produce no results.I'm not sure what part of "integration" broke it.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17864" opendate="2017-10-20 00:00:00" fixdate="2017-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTestClient cannot start during Precommit tests</summary>
      <description>HIVE-17807 has bumped version number in testutils/ptest2/pom.xml from 1.0 to 3.0 resulting failure to start PTestClient during Precommit runs:java -cp '/home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target/hive-ptest-1.0-classes.jar:/home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target/lib/*' org.apache.hive.ptest.api.client.PTestClient --command testStart --outputDir /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target --password '[*******]' --testHandle PreCommit-HIVE-Build-7389 --endpoint http://104.198.109.242:8080/hive-ptest-1.0 --logsEndpoint http://104.198.109.242/logs/ --profile master-mr2 --patch https://issues.apache.org/jira/secure/attachment/12893016/HIVE-17842.0.patch --jira HIVE-17842Error: Could not find or load main class org.apache.hive.ptest.api.client.PTestClient</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17871" opendate="2017-10-21 00:00:00" fixdate="2017-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add non nullability flag to druid time column</summary>
      <description>Druid time column is non null all the time.Adding the non nullability flag will enable extra calcite goodness like transforming select count(`__time`) from table to select count(*) from table</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druid.timeseries.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">data.scripts.q.test.init.sql</file>
    </fixedFiles>
  </bug>
  <bug id="17873" opendate="2017-10-22 00:00:00" fixdate="2017-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External LLAP client: allow same handleID to be used more than once</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17879" opendate="2017-10-24 00:00:00" fixdate="2017-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Datanucleus Maven Plugin</summary>
      <description>when build hive with jdk9got following error[ERROR] Failed to execute goal org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance (default) on project hive-standalone-metastore: Error executing DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer: InvocationTargetException: java/sql/Date: java.sql.Date -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance (default) on project hive-standalone-metastore: Error executing DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288) at org.apache.maven.cli.MavenCli.main(MavenCli.java:199) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.MojoExecutionException: Error executing DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer at org.datanucleus.maven.AbstractDataNucleusMojo.executeInJvm(AbstractDataNucleusMojo.java:350) at org.datanucleus.maven.AbstractEnhancerMojo.enhance(AbstractEnhancerMojo.java:266) at org.datanucleus.maven.AbstractEnhancerMojo.executeDataNucleusTool(AbstractEnhancerMojo.java:72) at org.datanucleus.maven.AbstractDataNucleusMojo.execute(AbstractDataNucleusMojo.java:126) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207) ... 20 moreCaused by: java.lang.reflect.InvocationTargetException at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.datanucleus.maven.AbstractDataNucleusMojo.executeInJvm(AbstractDataNucleusMojo.java:333) ... 25 moreCaused by: java.lang.NoClassDefFoundError: java/sql/Date at org.datanucleus.ClassConstants.&lt;clinit&gt;(ClassConstants.java:66) at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensions(NonManagedPluginRegistry.java:206) at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensionPoints(NonManagedPluginRegistry.java:155) at org.datanucleus.plugin.PluginManager.&lt;init&gt;(PluginManager.java:63) at org.datanucleus.plugin.PluginManager.createPluginManager(PluginManager.java:430) at org.datanucleus.AbstractNucleusContext.&lt;init&gt;(AbstractNucleusContext.java:85) at org.datanucleus.enhancer.EnhancementNucleusContextImpl.&lt;init&gt;(EnhancementNucleusContextImpl.java:48) at org.datanucleus.enhancer.EnhancementNucleusContextImpl.&lt;init&gt;(EnhancementNucleusContextImpl.java:37) at org.datanucleus.enhancer.DataNucleusEnhancer.&lt;init&gt;(DataNucleusEnhancer.java:161) at org.datanucleus.enhancer.CommandLineHelper.createDataNucleusEnhancer(CommandLineHelper.java:153) at org.datanucleus.enhancer.DataNucleusEnhancer.main(DataNucleusEnhancer.java:1108) ... 30 moreCaused by: java.lang.ClassNotFoundException: java.sql.Date at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:466) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:563) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496) ... 41 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17880" opendate="2017-10-24 00:00:00" fixdate="2017-1-24 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>hive tez get error result whith using "join" join two subquery</summary>
      <description>table broadcast_time partitioned by par_date(yyyyMMdd)createtime:yyyy-MM-dd HH:mm:ss1,when use alias field par_date(same name with partitioned field)SELECT par_date, count(1)FROM ( SELECT qid, substr(createtime, 1, 11) par_date FROM broadcast_time WHERE par_date = 20171023 AND qid = 1111111 ) t1 JOIN ( SELECT qid FROM room_info WHERE par_date = 20171023 ) r ON t1.qid = r.qidGROUP BY par_date;get reuslt:20171023 392,when use alias field new_par_date(different with partitioned field)SELECT new_par_date, count(1)FROM ( SELECT qid, substr(createtime, 1, 11) new_par_date FROM broadcast_time WHERE par_date = 20171023 AND qid = 3015850 ) t1 JOIN ( SELECT qid FROM room_info WHERE par_date = 20171023 ) r ON t1.qid = r.qidGROUP BY new_par_date;get result:2015-10-19 12015-10-20 52015-10-21 32015-10-25 12015-10-31 12015-11-21 22015-11-24 22016-02-29 12016-03-01 22016-03-06 12016-03-11 12016-03-15 22016-03-16 12016-03-17 12016-03-21 12016-04-16 72016-05-07 22016-09-24 12017-05-12 22017-06-19 12017-06-20 1</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.druid.timeseries.q</file>
      <file type="M">pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17888" opendate="2017-10-24 00:00:00" fixdate="2017-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display the reason for query cancellation</summary>
      <description>For user convenience and easy debugging, if a trigger kills a query return the reason for the killing the query. Currently the query kill will only display the following which is not very usefulError: Query was cancelled (state=01000,code=0)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.Trigger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.NullKillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.KillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerViolationActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KillTriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17897" opendate="2017-10-25 00:00:00" fixdate="2017-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"repl load" in bootstrap phase fails when partitions have whitespace</summary>
      <description>The issue is that Path.toURI().toString() is being used to serialize the location, while new Path(String) is used to deserialize it. URI escapes chars such as space, so the deserialized location doesn't point to the correct file location.Following exception is seen - 2017-10-24T11:58:34,451 ERROR [d5606640-8174-4584-8b54-936b0f5628fa main] exec.Task: Failed with exception nulljava.lang.NullPointerException at org.apache.hadoop.hive.ql.parse.repl.CopyUtils.regularCopy(CopyUtils.java:211) at org.apache.hadoop.hive.ql.parse.repl.CopyUtils.copyAndVerify(CopyUtils.java:71) at org.apache.hadoop.hive.ql.exec.ReplCopyTask.execute(ReplCopyTask.java:137) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:206) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2276) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1906) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1623) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1362) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1352) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:409) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:827) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:765) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:692) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:239) at org.apache.hadoop.util.RunJar.main(RunJar.java:153)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="179" opendate="2008-12-16 00:00:00" fixdate="2008-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR function should work like other databases</summary>
      <description>Positions start at 1, not 0. Negative positions start at the end of the string and count backwards.Oracle returns null for lengths less than 1 or non-existent substrings (any empty strings are null). MySQL and PostgreSQL return empty strings, never null. PostgreSQL errors for negative lengths. I suggest we follow the MySQL behavior.Oracle treats position 0 the same as 1. Perhaps we should too? SUBSTR('ABCDEFG',3,4): CDEFSUBSTR('ABCDEFG',-5,4): CDEF SUBSTR('ABCDEFG',3): CDEFG SUBSTR('ABCDEFG',-5): CDEFG SUBSTR('ABC',1,1): AMySQL: SUBSTR('ABC',0,1): &lt;empty&gt; SUBSTR('ABC',0,2): &lt;empty&gt; SUBSTR('ABC',1,0): &lt;empty&gt; SUBSTR('ABC',1,-1): &lt;empty&gt;Oracle: SUBSTR('ABC',0,1): A SUBSTR('ABC',0,2): AB SUBSTR('ABC',1,0): &lt;null&gt; SUBSTR('ABC',1,-1): &lt;null&gt;</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.queries.positive.groupby6.q</file>
      <file type="M">ql.src.test.queries.positive.groupby5.q</file>
      <file type="M">ql.src.test.queries.positive.groupby4.q</file>
      <file type="M">ql.src.test.queries.positive.groupby3.q</file>
      <file type="M">ql.src.test.queries.positive.groupby2.q</file>
      <file type="M">ql.src.test.queries.positive.groupby1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby3.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby1.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby1.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17904" opendate="2017-10-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle internal Tez AM restart in registry and WM</summary>
      <description>After the plan update patch is committed. The current code doesn't account very well for it; registry may have races, and an event needs to be added to WM when some AM resets, at least to make sure we discard the update errors that pertain to the old AM.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestGuaranteedTaskAllocator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.AmPluginNode.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.ServiceInstanceStateChangeListener.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="17905" opendate="2017-10-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>propagate background LLAP cluster changes to WM</summary>
      <description>HIVE-17841 already adds a method, it just needs to be called when there are relevant cluster changes that HS2 can detect</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.QueryAllocationManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17906" opendate="2017-10-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use kill query mechanics to kill queries in WM</summary>
      <description>Right now it just closes the session (see HIVE-17841). The sessions would need to be reused after the kill, or closed after the kill if the total QP has decreased</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
    </fixedFiles>
  </bug>
  <bug id="17907" opendate="2017-10-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable and apply resource plan commands in HS2</summary>
      <description>Enabling and applying the RP should only be runnable in HS2 with active WM. Both should validate the full resource plan (or at least enable should; users cannot modify the RP via normal means once enabled, but it might be worth double checking since we have to fetch it anyway to apply).Then, apply should propagate the resource plan to the WM instance.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanRequest.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMPool.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMTrigger.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMPool.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMMapping.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
      <file type="M">metastore.scripts.upgrade.derby.046-HIVE-17566.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.031-HIVE-17566.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.046-HIVE-17566.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.045-HIVE-17566.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterTriggerRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterTriggerResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateTriggerRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateTriggerResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropTriggerRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropTriggerResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetResourcePlanResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="17926" opendate="2017-10-27 00:00:00" fixdate="2017-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support triggers for non-pool sessions</summary>
      <description>Current trigger implementation works only with tez session pools. In case when tez sessions pools are not used, a new session gets created for every query in which case trigger validation does not happen. It will be good to support such one-off session case as well.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17945" opendate="2017-10-31 00:00:00" fixdate="2017-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support column projection for index access when using Parquet Vectorization</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
    </fixedFiles>
  </bug>
  <bug id="17952" opendate="2017-11-1 00:00:00" fixdate="2017-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix license headers to avoid dangling javadoc warnings</summary>
      <description>All license headers starts with "/**" which are assumed to be javadocs and IDE warns about dangling javadoc pointing to license headers.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorTestCode.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">testutils.src.java.org.apache.hive.testutils.jdbc.HiveBurnInClient.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomKFilter.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.VersionTestBase.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritableVersion.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritable.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.util.TestJavaDataModel.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestUnionColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestTimestampColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestStructColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestMapColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestListColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestBytesColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestStringExpr.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimalVersion.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.HiveDecimalTestBase.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.TestValidReadTxnList.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.io.TestDiskRangeList.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.IntervalDayTimeUtils.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritableVersionV2.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritableVersionV1.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritableV1.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.DateWritable.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.TimestampUtils.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgument.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.LiteralDelegate.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.ExpressionTree.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.StructColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.MultiValuedColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.MapColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ListColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.Decimal64ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidTxnList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.RandomTypeUtil.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimalVersionV2.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimalVersionV1.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimalV1.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimalImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.DataTypePhysicalVariation.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.Pool.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.MemoryBufferOrBuffers.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.MemoryBuffer.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRange.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DataCache.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.Allocator.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.DiskRangeInfo.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreSchemaFactory.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.datasource.TestDataSourceProviderFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TableType.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.SessionPropertiesListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenTool.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenSelector.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.DBTokenStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.PartitionExpressionProxy.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.parser.package-info.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMTrigger.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMResourcePlan.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMPool.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMMapping.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MVersionTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MType.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTablePrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MStringList.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MSerDeInfo.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MRoleMap.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MRole.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MResourceUri.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPrincipalDesc.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionEvent.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartition.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MOrder.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MNotificationNextId.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MNotificationLog.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MMetastoreDBProperties.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MMasterKey.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MIndex.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MFunction.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MFieldSchema.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MDelegationToken.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MDBPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MDatabase.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MConstraint.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MColumnDescriptor.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterDatabaseMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.AlterDatabaseMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsInvalidationCache.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationInvalidationInfo.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHookLoader.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.FileFormatProxy.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.events.AlterDatabaseEvent.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.DeadlineException.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.columnstats.cache.StringColumnStatsDataInspector.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.columnstats.cache.LongColumnStatsDataInspector.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.columnstats.cache.DoubleColumnStatsDataInspector.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.columnstats.cache.DecimalColumnStatsDataInspector.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.columnstats.cache.DateColumnStatsDataInspector.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.annotation.NoReconnect.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.counter.SparkCounters.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.counter.SparkCounterGroup.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.counter.SparkCounter.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">shims.scheduler.src.main.java.org.apache.hadoop.hive.schshim.FairSchedulerShim.java</file>
      <file type="M">shims.common.src.main.test.org.apache.hadoop.hive.io.TestHdfsUtils.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TUGIContainingTransport.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TFilterTransport.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.Utils.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.SchedulerShim.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.JettyShims.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HiveHarFileSystem.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.CombineHiveKey.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HiveIOExceptionNextHandleResult.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HiveIOExceptionHandler.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.DefaultFileAccess.java</file>
      <file type="M">shims.0.23.src.main.test.org.apache.hadoop.hive.shims.TestHadoop23Shims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.MiniSparkOnYARNCluster.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">service.src.test.org.apache.hive.service.TestCookieSigner.java</file>
      <file type="M">service.src.test.org.apache.hive.service.server.TestServerOptionsProcessor.java</file>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestHiveSQLException.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestCLIServiceRestore.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionHooks.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionCleanup.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.operation.TestSQLOperationMetrics.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.operation.TestQueryLifeTimeHooksWithSQLOperation.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestPlainSaslHelper.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.User.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestUserSearchFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestUserFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestSearchResultHandler.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestQueryFactory.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestQuery.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestLdapUtils.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestLdapSearch.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestGroupFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestCustomQueryFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestChainFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.LdapTestUtils.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.LdapAuthenticationTestCase.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.Credentials.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.QueryProfileServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceStateChangeListener.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceOperations.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceException.java</file>
      <file type="M">service.src.java.org.apache.hive.service.Service.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadWithGarbageCleanup.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.FilterService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
      <file type="M">service.src.java.org.apache.hive.service.CompositeService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeQualifiers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeDescriptor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThreadPoolExecutorWithOomHook.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TezProgressMonitorStatusMapper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TableSchema.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionProxy.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionHookContextImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionHookContext.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionHook.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.SessionHandle.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.RowSetFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.RowSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.RowBasedSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ProgressMonitorStatusMapper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.PatternOrIdentifier.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMappingFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.QueryInfoCache.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetPrimaryKeysOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetCrossReferenceOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationType.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationState.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationHandle.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.JobProgressUpdate.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ICLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.HiveSQLException.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.HandleIdentifier.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Handle.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.GetInfoValue.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.GetInfoType.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.FetchType.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.FetchOrientation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.EmbeddedCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnDescriptor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnBasedSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.BreakableService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSubjectAssumingTransport.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.SaslQOP.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PasswdAuthenticationProvider.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PamAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.UserSearchFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.UserFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.SearchResultHandler.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.QueryFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.Query.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapSearchFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapSearch.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.FilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.Filter.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.DirSearchFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.DirSearch.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.CustomQueryFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.ChainFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.KerberosSaslHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthenticationException.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthConstants.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.CustomAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.AuthenticationProviderFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.AnonymousAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.AbstractService.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.VerifyFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.typeinfo.TestTypeInfoUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.thrift.test.CreateSequenceFile.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.thrift.TestColumnBuffer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestStatsSerde.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestOpenCSVSerde.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestColumnProjectionUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.SerdeRandomRowSource.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestUnionStructObjectInspector.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestThriftObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestReflectionObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestProtocolBuffersObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestFullMapEqualComparer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.MyStruct.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe2.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinaryFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.MyTestClassSmaller.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.MyTestClassBigger.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestTimestampWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestTimestampTZWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestHiveVarcharWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.columnar.TestBytesRefArrayWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestPrimitiveClass.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.package-info.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.TypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.WriteTextProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.WriteNullsProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.Type.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TReflectionUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ThriftJDBCBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ThriftFormatter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.SkippableTProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ConfigurableTProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ColumnBuffer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.StructObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.Serializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeStatsStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeStats.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeSpec.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeException.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.OpenCSVSerde.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.UnionObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ThriftUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ThriftStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ThriftObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StructField.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardConstantStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardConstantMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardConstantListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SimpleMapEqualComparer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SettableUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SettableListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ProtocolBuffersStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampLocalTZObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveIntervalYearMonthObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveIntervalDayTimeObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampLocalTZObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveIntervalYearMonthObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveIntervalDayTimeObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.VoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampLocalTZObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableTimestampLocalTZObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveIntervalYearMonthObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveIntervalDayTimeObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampLocalTZObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveIntervalYearMonthObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveIntervalDayTimeObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaConstantBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveIntervalYearMonthObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveIntervalDayTimeObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.MapEqualComparer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.InspectableObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.FullMapEqualComparer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.CrossMapEqualComparer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.NullStructSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.NoOpFetchFormatter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.VerifyLazy.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampLocalTZObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyObjectInspectorParametersImpl.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyObjectInspectorParameters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveIntervalYearMonthObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveIntervalDayTimeObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyVoid.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUnion.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestampLocalTZ.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyString.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyShort.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyObjectBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyNonPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveIntervalYearMonth.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveIntervalDayTime.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFloat.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyDate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyByte.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBoolean.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyArray.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.StringToDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleSerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.ByteArrayRef.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioShort.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioHiveDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioFloat.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioByte.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazydio.LazyDioBinary.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryVoid.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUnion.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryTimestampLocalTZ.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryShort.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe2.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryNonPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryInteger.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveVarchar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveIntervalYearMonth.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveIntervalDayTime.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveChar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFloat.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryByte.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBoolean.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBinary.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinarySerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.ShortWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.ParquetHiveRecord.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveIntervalYearMonthWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveCharWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.DoubleWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.ByteWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.FetchFormatter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.fast.SerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.fast.DeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeString.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeSet.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeList.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypei64.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypei32.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypei16.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypedef.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeByte.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeBool.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeStructBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeSimpleNode.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFunction.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldType.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldRequiredness.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldList.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeField.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.Deserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.DefaultFetchFormatter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnSet.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.LazyDecompressionCallback.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarSerDeBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ByteStreamTypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ByteStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableSerializeWrite.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDeWithEndPrefix.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.BaseStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractSerializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractDeserializer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.util.TestDateTimeMath.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.xml.TestUDFXPathUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.xml.TestReusableStringReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFVersion.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUUID.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUnhex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUnbase64.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFSign.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFSha1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFMd5.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFMath.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFJson.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFHex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFDateFormatGranularity.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFCrc32.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFBase64.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestToInteger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestBlockedUdf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFWidthBucket.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrunc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSubstringIndex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFStringToMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSoundex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSortArrayByField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSortArray.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSha2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFReplace.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRegexp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFQuarter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFNullif.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFNextDay.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMonthsBetween.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMapValues.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMacro.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLikeAny.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLikeAll.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLevenshtein.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLeast.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLastDay.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFInternalInterval.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFInitCap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFGreatest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFromUtcTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFactorial.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFExtractUnionValueConverter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFExtractUnionObjectInspectorConverter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFExtractUnion.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEncode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDecode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFChr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCbrt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFBRound.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFBridge.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAesEncrypt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAesDecrypt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAddMonths.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFCorrelation.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFBinarySetFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.AbstractTestGenericUDFOPNumeric.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingMin.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingMax.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.tool.TestLineageInfo.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.OperatorTestUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.DataBuilder.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestErrorMsg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestAddResource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveOperationType.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerCLI.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestSetProcessor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestResetProcessor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestCompileProcessor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestViewEntity.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestTezWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestExecutionEngineWorkConcurrency.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestDropMacroDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestCreateMacroDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUnpermittedCharsInColumnNameCreateTableNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBSubQuery.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestParseDriverIntervals.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestParseDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestEximUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestColumnAccess.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.TestCopyUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.TestEventDumpDirComparator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.dump.TestHiveWrapper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.dump.events.TestEventHandlerFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.positive.TestTransactionStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.plugin.sqlstd.TestOperation2Privilege.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.ListSizeMatcher.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.TestNestedColumnFieldPruningUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.TestGenMapRedUtilsUsePartitionColumnsPositive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.TestGenMapRedUtilsUsePartitionColumnsNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.TestColumnPrunerProcCtx.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBOMaxNumToCNF.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.rules.TestHiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestTableIterable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveCopyFiles.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.StringAppender.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.formatting.TestJsonRPFormatter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.log.TestSlidingFilenameRolloverStrategy.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.log.TestLog4j2Appenders.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lib.TestRuleRegExp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRecordIdentifier.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestPerformTestRCFileAndSeqFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestIOContextMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveFileFormatUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestCombineHiveInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.StorageFormats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.VectorizedColumnReaderTestBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedMapColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedListColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedDictionaryEncodingColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRowGroupFilter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestMapStructures.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestDataWritableWriter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestArrayCompatibility.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestDataWritableReadSupport.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.HiveParquetSchemaTestUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.AbstractTestParquetDirect.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcStruct.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.encoded.TestEncodedReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.avro.TestAvroGenericRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.TestSplitFilter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.TestIndexType.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.TestHiveInputSplitComparator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.SplitFilterTestCase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.MockInputFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.MockIndexResult.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.index.MockHiveInputSplits.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestQueryHooks.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestMetricsQueryLifeTimeHook.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestHooks.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestATSHook.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorVerifyFast.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromRepeats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromLongIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperatorDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureVectorToRowOutputOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.batchgen.VectorColumnGroupGenerator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.batchgen.VectorBatchGenerator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.batchgen.VectorBatchGenerateUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.batchgen.VectorBatchGenerateStream.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.batchgen.TestVectorBatchGenerate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.AllVectorTypesRecord.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.legacy.LongUDF.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.legacy.ConcatTextLongDoubleUDF.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.generic.GenericUDFIsNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorRowObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorRowBytesContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestTimestampWritableAndColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.TestMapJoinOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.TestDebugDisplay.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.MapJoinTestDescription.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.MapJoinTestData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.MapJoinTestConfig.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VerifyFastRow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastRowHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMultiSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.RandomLongStream.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.RandomByteArrayStream.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CommonFastHashTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CheckFastRowHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CheckFastHashTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestUnaryMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestConstantVectorExpression.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.rowobjects.RowTestObjectsMultiSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.rowobjects.RowTestObjectsMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.rowobjects.RowTestObjects.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.DescriptionTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.collectoroperator.RowVectorCollectorTestOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.collectoroperator.RowCollectorTestOperatorBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.collectoroperator.RowCollectorTestOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.collectoroperator.CountVectorCollectorTestOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.collectoroperator.CountCollectorTestOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.collectoroperator.CollectorTestOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestInputSplitComparator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestGuaranteedTaskAllocator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestCustomPartitionVertex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.SampleTezSessionState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.monitoring.TestTezProgressMonitor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPartitionKeySampler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperatorNames.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckCreatePartitionsInBatches.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestKeyWrapperFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestHiveCredentialProviders.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestHiveKVResultCache.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TestTaskTracker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.Utilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestRowContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinRowContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinKey.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinEqualityTableContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestHashPartition.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.mr.TestMapRedTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.mr.TestMapredLocalTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.mapjoin.TestMapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.InputEstimatorTestClass.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandlerNoConnectionPool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.CounterLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ZooKeeperHiveHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.DependencyResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.DateTimeMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFVersion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUUID.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUnhex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUnbase64.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFTan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSqrt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSpace.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSha1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSecond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFReverse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFReplace.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRepeat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRegExpReplace.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRadians.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPI.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFParseUrl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPLongDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitXor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitShiftRightUnsigned.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitShiftRight.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitShiftLeft.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitNot.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMinute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMd5.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog10.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFJson.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFHour.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFHex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFindInSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFE.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDegrees.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDayOfWeek.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorWeek.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorSecond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorQuarter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorMinute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorHour.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloorDay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCrc32.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCos.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFChr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseBitOP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBase64.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAtan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAsin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAscii.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAcos.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.SettableUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.ValueBoundaryScanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopWithMapStreaming.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopStreaming.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.Noop.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.MatchPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.RoundUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NDV.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.LeadLagBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFReplicateRows.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFPosExplode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFWidthBucket.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrunc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUtcTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToTimestampLocalTZ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToIntervalYearMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToIntervalDayTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSubstringIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSQCountCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSoundex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArrayByField.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSha2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSentences.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFQuarter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFPower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFParamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPositive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNumericPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNumericMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotFalse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqualNS.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPFalse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDTIPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDTIMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOctetLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNullif.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNextDay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMonthsBetween.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMaskShowLastN.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMaskShowFirstN.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMaskLastN.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMaskHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMaskFirstN.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLoggedInUser.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLikeAny.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLikeAll.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLevenshtein.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeast.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLastDay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInternalInterval.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInitCap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFGrouping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFGreatest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloorCeilBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFactorial.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFExtractUnion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapEmpty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapAnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEpochMilli.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEncode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFElt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDecode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentUser.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCharacterLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCeil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCbrt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCardinalityViolation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseUnary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNwayCompare.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseDTI.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseArithmetic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAssertTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAesEncrypt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAesDecrypt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAesBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAddMonths.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVarianceSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumEmptyIsZero.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStdSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFParameterInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovarianceSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBinarySetFunctions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.Collector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.BaseMaskUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFReflect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.tools.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.Partish.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.IStatsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ColumnStatisticsObjTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ColStatsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ClientStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.NullKillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.LineageState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.KillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.ClearDanglingScratchDir.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HiveMetastoreAuthenticationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeScope.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivilegeType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RevokePrivAuthUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.SettableConfigUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObjectUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AbstractHiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveMultiPartitionAuthorizationProviderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryLifeTimeHookRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ResetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ReloadProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ListResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DfsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CryptoProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CompileProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SimplePredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorTableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSparkPartitionPruningSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSparkHashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSMBJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSelectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionConversion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorLimitDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorizationCondition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorGroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorFilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorFileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorAppMasterEventDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ValidationUtility.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UnlockTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UnlockDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UnionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UnionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.UDTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.StatsWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkHashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkBucketMapJoinContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SMBJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowTxnsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowTablesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowCreateDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowConfDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SelectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ScriptDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SchemaDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RoleDDLDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RevokeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReloadFunctionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RCFileMergeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PartitionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.OrderExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.OrderDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PrivilegeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PrincipalDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OrcFileMergeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OperatorExplainVectorization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MuxDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MsckDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LockTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ListSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ListBucketingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LimitDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LateralViewForwardDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.KillQueryDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinCondDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.IStatsGatherDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableDummyDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GrantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FunctionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ForwardDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileMergeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeSubQueryDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDynamicListDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplosionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Explain.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPruningEventDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DummyStoreDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropWMTriggerDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropWMPoolDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropWMMappingDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropMacroDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropFunctionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescFunctionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DependencyCollectionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DemuxDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateWMTriggerDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrDropTriggerToPoolMappingDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrAlterWMPoolDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrAlterWMMappingDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateMacroDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CommonMergeJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CollectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CacheMetadataDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BasicStatsWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BasicStatsNoJobWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ArchiveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ArchiveDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AppMasterEventDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterWMTriggerDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterMaterializedViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AggregationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractVectorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbortTxnsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingComponentizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.StorageFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SplitSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkSMBMapJoinInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkFileSinkProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.OptimizeSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWorkWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemiJoinHint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.ReplState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.ReplLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.MetadataJson.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.MetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.IncrementalLoadEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.IncrementalLoadEnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.IncrementalLoadBegin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.BootstrapLoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.BootstrapLoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.BootstrapLoadEnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.state.BootstrapLoadBegin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.IncrementalLoadLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.BootstrapLoadLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.EventDumpDirComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.IncrementalDumpEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.IncrementalDumpEnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.IncrementalDumpBegin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.BootstrapDumpTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.BootstrapDumpFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.BootstrapDumpEnd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.BootstrapDumpBegin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.IncrementalDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.BootstrapDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.ConstraintsSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PrintOpTreeProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseError.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.OpParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.NamedJoinInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MetaDataExportListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MaskAndFilterInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LeadLagInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.JoinType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.JoinCond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.InputSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenMapRedWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FileSinkProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainConfiguration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsAutoGatherContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTErrorNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AppMasterEventProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AnalyzeCommandUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Transform.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TablePropertyEnrichmentOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SplitSparkWorkResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSortMergeJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSortMergeJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSMBJoinHintOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSkewJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSkewJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkPartitionPruningSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkJoinHintOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.CombineEquivalentWorkResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkRemoveDynamicPruning.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkMapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapjoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SetReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.RedundantDynamicPruningConditionsRemoval.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.OpWalkerCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.ExprPrunerInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PointLookupOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.VectorizerReason.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SparkMapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SparkDynamicPartitionPruningResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SparkCrossProductCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalPlanResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapPreVectorizationPass.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.IndexWhereResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CrossProductHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AnnotateRunTimeStatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.NestedColumnFieldPruningUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateOpTraitsProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MergeJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpWalkerCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpPartitionWalkerCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.Generator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.JoinReorder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMROperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FieldNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.CountDistinctRewriteProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkJoinDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplicationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForReturnPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinTypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinCondTypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.TraitsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.SubqueryConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdUniqueKeys.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdMemory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdCollation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.FilterSelectivityEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveWindowingFixRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortUnionReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRemoveSqCountCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelColumnsAlignment.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectOverIntersectRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectFilterPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePartitionPruneRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinCommuteRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveIntersectRewriteRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveIntersectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTSTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterAggregateTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExceptRewriteRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveDruidRules.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateReduceRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregatePullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableFunctionScan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortExchange.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveRelNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveIntersect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveGroupingID.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFloorDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExtractDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExcept.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveBetween.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveSubQRemoveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexExecutorImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelCollation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveConfPlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlMinMaxAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlCountAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.CanAggregateDistinct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveRelMdCost.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveDefaultCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveCost.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteViewSemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSubquerySemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapjoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.UniqueConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.TableIterable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Sample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.RandomDimension.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.PartitionIterable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.NotNullConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.InvalidTableException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.InputEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveFatalException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DummyPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Dimension.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.CheckResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.CheckJDOException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.AuthorizationException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.MapRedStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PidFilePatternConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.NullAppender.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.HiveEventCounter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.CuratorFrameworkSingleton.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.LockException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockMode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLock.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.TypeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.TaskGraphWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleExactMatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.Rule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.PreOrderWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.NodeProcessorCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.NodeProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.Node.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.LevelOrderWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.GraphWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.ForwardWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.ExpressionWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.Dispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.CompositeProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ZeroRowsInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.TextFileStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SyntheticFileId.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymbolicInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.StreamingOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.StorageFormatFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.StorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.StatsProvidingRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.StatsProvidingRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SequenceFileStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SelfDescribingInputFormatInterface.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SchemaAwareCompressionOutputStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SchemaAwareCompressionInputStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ReworkMapredInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RecordIdentifier.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedStructColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedMapColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedListColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetFooterInputFromCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.BaseVectorizedColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetTableUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetFilterPredicateConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.FilterPredicateLeafBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.Repeated.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ConverterParent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ParquetFileStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Writer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcUnion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOiBatchToRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileFormatProxy.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetastoreExternalCachesByConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.LocalCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ExternalCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.IncompleteCb.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Consumer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.CompressionKind.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ORCFileStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.OneNullRowInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NullScanFileSystem.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NullRowsInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.LlapWrappableInputFormatInterface.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.LlapCacheOnlyInputFormatInterface.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.LlapAwareSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOPrepareCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOContextMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOConstants.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.InputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HivePartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.FlatFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.DefaultHivePartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ContentSummaryInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.ColumnarSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CodecPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BatchToRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BatchToRowInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AvroStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AbstractStorageFormatDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.SplitFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexSearchCondition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapOuterQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapInnerQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.AggregateIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.AbstractIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.IDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Redactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookWithParseHooks.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookContextImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecOrcRowGroupCountPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.MetricsQueryLifeTimeHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryUpdateHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.MaterializedViewRegistryPropertiesHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HooksLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Hook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Entity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.DriverTestHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryProxyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.HashTableLoaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorUtilBatchObjectPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSparkPartitionPruningSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedUDAFs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedInputFormatInterface.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressionsSupportDecimal64.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContextRegion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnSourceMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnOutputMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnOrderedMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssign.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorBatchDebug.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.rowbytescontainer.VectorRowBytesContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkEmptyKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedMultiKeyHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedMultiKeyHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedMultiKeyHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedCreateHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashMultiSetResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashMapResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastValueStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSingleImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSerializedImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSerialized.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesMultiSerialized.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesLongSerialized.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesBytesSerialized.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeries.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexStringCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexLongCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexDoubleCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfWeekTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfWeekString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfWeekDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VarCharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncStringOutput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TimestampToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StructColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringInitCap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringHex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupConcatColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.OctetLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColNotEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ITimestampInExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IStructInExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IStringInExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ILongInExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IDoubleInExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IDecimalInExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncTimestampToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncHex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncBRoundWithNumDigitsDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncBin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStructColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterConstantBooleanVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DynamicValueVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.Decimal64Util.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateScalarSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConvertDecimal64ToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalYearMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalDayTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringGroupToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringGroupToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringGroupToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastMillisecondsLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastFloatToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastFloatToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastFloatToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToVarCharViaLongToVarChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToStringViaLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToCharViaLongToChar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.BRoundWithNumDigitsDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal64ToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal64.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountMerge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilterMerge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.AggregateDefinition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFClassLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFArgumentException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDAFEvaluatorResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDAF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerMxBean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.tools.TezMergedLogicalInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.tools.KeyValuesInputMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.tools.KeyValueInputMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RestrictedConfigChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.QueryAllocationManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.RenderStrategy.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.QueryExecutionBreakdownSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.PrintSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.LlapWmSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.LLAPioSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.FSCountersSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.Constants.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileTezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapTezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KeyValuesFromKeyValues.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KeyValuesFromKeyValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KeyValuesAdapter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.InterruptibleProcessing.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DataInputByteBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomVertexConfiguration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.AmPluginNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TezDummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TextRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TextRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TerminalOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TemporaryHashSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskHandle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskExecutionException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Stat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkStageProgress.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobRef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SparkMetricsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobRef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.JobMetricsListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatisticsBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatisticGroup.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatistic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReporter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMergeFileRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SmallTableCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ReduceTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveVoidFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GroupByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.CacheTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RowSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplStateLogWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplStateLogTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RCFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFTopNHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFRollingPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.ReusableGetAdaptorDirectAccess.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerDirectAccess.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinPersistableTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectSerDeContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.FlatRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.AbstractRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionTableFunctionDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionKeySampler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericUDAFEvaluatorResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericUDAF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NoMatchingMethodException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NodeUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.Throttle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MemoryMonitorInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ImportCommitWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ImportCommitTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HiveTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FooterBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorRef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.ScriptErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.MapAggrMemErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.ErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.DataCorruptErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DynamicValueRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Description.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultUDAFEvaluatorResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ByteWritable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.BucketMatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.BinaryRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AutoProgressor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ArchiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AmbiguousMethodException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.debug.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.CompilationOpContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.CommandNeedRetryException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.metastore.SynchronizedMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.DebugUtils.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxIntervalDayTime.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal64.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimalMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimal64ToDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestClass.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupScalarCompareStringGroupColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringGroupScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarDivideColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprScalarScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprScalarColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupScalarCompareStringGroupColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringGroupScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDTIScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDTIColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalScalarCompareDecimalColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareDecimalScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareDecimalColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimal64ScalarCompareDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimal64ColumnCompareDecimal64Scalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimal64ColumnCompareDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIScalarArithmeticDTIColumnNoConvert.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIColumnArithmeticDTIScalarNoConvert.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIColumnArithmeticDTIColumnNoConvert.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DecimalColumnUnaryFunc.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ScalarArithmeticDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnArithmeticDecimal64Scalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnArithmeticDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryMinus.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryFunc.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideScalarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumn.txt</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.filemeta.OrcFileMetadataHandler.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerInfo.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginPolicyProvider.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestQueryIdentifier.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapTokenChecker.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestEvictingPriorityBlockingQueue.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocatorForceEvict.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.IndexCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapDaemonPolicyProvider.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonIOInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCustomMetricsInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileEstimateErrors.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.MetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.ConsumerStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.ConsumerFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.CompressionBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.TezCounterSource.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.GenericColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.SystemConfigurationServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapIoMemoryServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonMXBean.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.EvictingPriorityBlockingQueue.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.WmFragmentCounters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.FragmentCountersMap.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.ConsumerFeedback.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.status.LlapStatusHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SimpleBufferManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SimpleAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapOomDebugDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapDataBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.FileCacheCleanupThread.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.FileCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionListener.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionDispatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionAwareAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BufferUsageManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocatorMXBean.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapRowInputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.testhelpers.ControlledClock.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.io.TestChunkedInputStream.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SigningSecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClient.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapSignerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapSigner.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.Schema.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.Row.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.NotTezEventHelper.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.metrics.MetricsUtils.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.metrics.LlapMetricsSystem.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.log.LogHelpers.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.io.ChunkedOutputStream.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.io.ChunkedInputStream.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.FieldDesc.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.counters.LlapWmCounters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.counters.LlapIOCounters.java</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.llap.registry.impl.TestSlotZnode.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.ServiceInstanceStateChangeListener.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClientImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.SlotZnode.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapProxy.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapIo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.coordinator.LlapCoordinator.java</file>
      <file type="M">kryo-registrator.src.main.java.org.apache.hive.spark.HiveKryoRegistrator.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientException.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.XsrfHttpRequestInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.logs.InPlaceUpdateStream.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcUriParseException.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcTable.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumnAttributes.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpTokenAuthInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpRequestInterceptorBase.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpKerberosRequestInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpBasicAuthInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveMetaDataResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveCallableStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ClosedOrCancelledStatementException.java</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.TestDanglingQOuts.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFileBeeLineClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.Parallelized.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.package-info.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.ConvertedOutputFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.util.ElapsedTimeLoggingWrapper.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.scripts.extracturl.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDFTestLength2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDFTestLength.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDFTestErrorOnFalse.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDFRunWorker.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDFFileLookup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.UDAFTestMax.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.stats.KeyVerifyingStatsAggregator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.stats.DummyStatsPublisher.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.stats.DummyStatsAggregator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.MetastoreAuthzAPIDisallowAuthorizer.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyAuthenticator.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerForTest.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestProcessExecResult.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.io.udf.Rot13OutputFormat.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.io.udf.Rot13InputFormat.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyTableDirectoryIsEmptyHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifySessionStateLocalErrorsHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsSubdirectoryOfTableHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsNotSubdirectoryOfTableHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyOverriddenConfigsHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyOutputTableLocationSchemeIsFileHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyIsLocalModeHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyCachingPrintStreamHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.ShowMapredStatsHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.MapJoinCounterHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckTableAccessHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckQueryPropertiesHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.llap.LlapItUtils.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreDummy.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBlobstoreNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBlobstoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreAccumuloCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliAdapter.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCliConfig.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.test-serde.src.main.java.org.apache.hadoop.hive.serde2.TestSerDe.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hive.TestDummy.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestTezPerfCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestNegativeCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniTezCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMinimrCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestHBaseCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestContribCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestCompareCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestBeeLineDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.DummyCliDriver.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.ContribNegativeCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestSparkCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.java</file>
      <file type="M">itests.qtest-spark.src.test.java.org.apache.hadoop.hive.cli.TestLocalSparkCliDriver.java</file>
      <file type="M">itests.qtest-accumulo.src.test.java.org.apache.hive.TestDummy.java</file>
      <file type="M">itests.qtest-accumulo.src.test.java.org.apache.hadoop.hive.cli.TestAccumuloCliDriver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.TestDFSErrorHandling.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.server.TestHS2ClearDanglingScratchDir.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIServiceFeatures.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftCLIServiceWithHttp.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftCLIServiceWithBinary.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftCliServiceMessageSize.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestMiniHS2StateWithNoZookeeper.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithMr.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TestCustomAuthentication.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestXSRFFilter.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestServiceDiscoveryWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestServiceDiscovery.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestNoSaslAuth.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHA.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHiveServer2SessionTimeout.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.cbo.rp.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthUDFBlacklist.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestCLIAuthzSessionContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeelinePasswordOption.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.BeelineWithHS2ConnectionFileTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.UtilsForTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.serde2.TestSerdeWithFieldComments.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.serde2.dynamic.type.TestDynamicSerDe.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestMTQueries.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestMetaStoreLimitPartitionRequest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAutoPurgeTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.session.TestClearDanglingScratchDir.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestPasswordWithConfig.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.StorageBasedMetastoreTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerShowFilters.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.metadata.TestSemanticAnalyzerHookLoading.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.BaseTestQueries.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestServerSpecificConfig.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.security.TestZooKeeperTokenStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.security.TestDBTokenStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.io.TestHadoopFileStatus.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.hooks.TestHs2Hooks.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hive.jdbc.TestSchedulerQueue.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestPasswordWithCredentialProvider.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.metastore.security.TestHadoopAuthBridge23.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestMiniHiveKdc.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthHttp.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthBinary.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdc.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithDBTokenStore.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcNonKrbSASLWithMiniKdc.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestHiveAuthFactory.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.MiniHiveKdc.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedLogicBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedLikeBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedComparisonBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedArithmeticBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneStringKeyBenchBase.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneStringKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneLongKeyBenchBase.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinOneLongKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinMultiKeyBenchBase.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.MapJoinMultiKeyBench.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.mapjoin.AbstractMapJoin.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.AbstractExpression.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.storage.ColumnarStorageBench.java</file>
      <file type="M">itests.hive-blobstore.src.test.java.org.apache.hadoop.hive.cli.TestBlobstoreNegativeCliDriver.java</file>
      <file type="M">itests.hive-blobstore.src.test.java.org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.ManyMiniCluster.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.api.TestHCatClientNotification.java</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.src.main.java.hive.it.custom.udfs.vector.VectorStringRot13.java</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.src.main.java.hive.it.custom.udfs.GenericUDFRot13.java</file>
      <file type="M">itests.custom-udfs.udf-classloader-util.src.main.java.hive.it.custom.udfs.Util.java</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.src.main.java.hive.it.custom.udfs.UDF2.java</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.src.main.java.hive.it.custom.udfs.UDF1.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomTextStorageFormatDescriptor.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomTextSerDe.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomSerDe5.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomSerDe4.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomSerDe3.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomSerDe2.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomSerDe1.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.java</file>
      <file type="M">itests.custom-serde.src.main.java.org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlUdf.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Udf.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Timer.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.StreamGobbler.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Signal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Scope.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Row.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Query.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Package.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Interval.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Hplsql.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Handler.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionString.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionOra.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionMisc.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Ftp.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.File.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Copy.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Converter.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conf.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Column.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Cmp.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Arguments.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestServer.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestConcurrentJobRequestsThreadsAndTimeout.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestConcurrentJobRequestsThreads.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestConcurrentJobRequests.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.mock.MockUriInfo.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.mock.MockServer.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.mock.MockExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.MockAnswerTestHelper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.ConcurrentJobRequestsTestBase.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.WadlConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.VersionDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.UgiFactory.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.TooManyRequestsException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.SingleInputFormat.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.NullSplit.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.NullRecordReader.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.NotFoundException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LogRetriever.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobStateTracker.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.DelegationTokenCache.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.TempletonDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.TablePropertyDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.TableLikeDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.TableDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SimpleWebException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SimpleExceptionMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.QueueStatusBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.QueueException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ProxyUserSupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PartitionDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.NotAuthorizedException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.MaxByteArrayOutputStream.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JsonBuilder.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JobRequestExecutor.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JobItemBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JobCallable.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HcatException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.EnqueueBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DatabaseDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CompleteBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ColumnDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CatchallExceptionMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CallbackFailedException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.BusyException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.BadParam.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.TestReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.ReplicationV1CompatRule.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.exim.TestEximReplicationTasks.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.CommandTestUtils.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.commands.TestNoopCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.commands.TestCommands.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.StagingDirectoryProvider.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.NoopReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.HCatReplicationTaskIterator.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.InsertReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.DropTableReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.DropPartitionReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.DropDatabaseReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.CreateTableReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.CreateDatabaseReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.AlterTableReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.AlterPartitionReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.exim.AddPartitionReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ErroredReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.commands.NoopCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.commands.ImportCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.commands.ExportCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.commands.DropTableCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.commands.DropPartitionCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.commands.DropDatabaseCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.Command.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.ObjectNotFoundException.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.MetadataSerializer.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.MetadataJSONSerializer.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatPartitionSpec.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatPartition.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatNotificationEvent.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatDatabase.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatCreateDBDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.ConnectionFailureException.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestDelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.TestMutations.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingAssert.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.MutableRecord.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionError.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatchUnAvailable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingIOFailure.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.SerializationError.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.QueryFailedException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.PartitionCreationFailed.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidTrasactionState.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidPartition.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidColumn.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.ImpersonationFailed.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HeartBeatFailure.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.ConnectionError.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.WriteTextPartitioned.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.WriteText.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.WriteRC.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.WriteJson.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.Util.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.TypeDataCheck.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.SumNumbers.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.StoreNumbers.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.StoreDemo.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.StoreComplex.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.SimpleRead.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.ReadWrite.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.ReadText.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.ReadRC.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.ReadJson.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.HCatTypeCheckHive.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.HCatTypeCheck.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.HCatTestDriver.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.GroupByAge.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.DataWriterSlave.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.DataWriterMaster.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.DataReaderSlave.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.udfs.java.org.apache.hive.hcatalog.utils.DataReaderMaster.java</file>
      <file type="M">hcatalog.src.test.e2e.hcatalog.tools.generate.java.org.apache.hadoop.hive.tools.generate.RCFileGenerator.java</file>
      <file type="M">hcatalog.src.java.org.apache.hive.hcatalog.package-info.java</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestMsgBusConnection.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageDeserializer.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONInsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.jms.MessagingUtils.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.InsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.HCatEventMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestTextFileHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestSequenceFileHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestRCFileHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestPigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestParquetHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestParquetHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestOrcHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestOrcHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestAvroHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestAvroHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.MyPigStorage.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.MockLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.HCatStorerWrapper.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.AbstractHCatLoaderTest.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.PigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatBaseStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatBaseLoader.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.NoExitSecurityManager.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.MiniCluster.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestInputJobInfo.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMutableNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMutableDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatInputFormatMethods.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatExternalNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.HcatTestUtils.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.ExitException.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestReaderWriter.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestLazyHCatRecord.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestHCatRecordSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestDefaultHCatRecord.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.schema.TestHCatSchema.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.HCatDataCheckUtil.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.common.TestHiveClientCache.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.common.TestHCatUtil.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestUseDatabase.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.TestHCatAuthUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.oozie.JavaAction.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.StorerInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.StaticPartitionFileRecordWriterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.Security.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.RecordWriterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.ProgressReporter.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.PartInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.OutputJobInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.OutputFormatContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.InternalUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.InputJobInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.InitializeInput.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatTableInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatRecordReader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatFileUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.DefaultRecordWriterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.WriterContext.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.WriteEntity.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.state.StateProvider.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.state.DefaultStateProvider.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.ReaderContext.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.ReadEntity.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.impl.WriterContextImpl.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.HCatReader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.EntityBase.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.DataTransferFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.ReaderWriter.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.Pair.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.LazyHCatRecord.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.JsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordObjectInspector.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordable.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecord.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.DefaultHCatRecord.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.DataType.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HiveClientCache.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatException.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatContext.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.ErrorType.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatAuthUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatDriver.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestPutResultWritable.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestLazyHBaseCellMap.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory3.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory2.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestStructSerializer.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestCompositeKey.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestAvroSchemaRetriever.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.StructHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseStructValue.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.DefaultHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ResultWritable.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.PutWritable.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.package-info.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseCellMap.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseTableSnapshotInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSplit.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseScanRange.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseRowSerializer.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseMetaHook.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseKeyFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.DefaultHBaseKeyFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.DataOutputOutputStream.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.DataInputInputStream.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.CompositeHBaseKeyFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ColumnMappings.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.AbstractHBaseKeyPredicateDecomposer.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.AbstractHBaseKeyFactory.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidStorageHandler2.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe2.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.PeriodGranularitySerializer.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.HiveDruidSerializationModule.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidWritable.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDeUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.HiveDruidSplit.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.serde2.TestRegexSerDe.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.mr.TestGenericMR.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.Type.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udf.UDFRowSequence.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udf.example.UDFExampleStructPrint.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udf.example.UDFExampleMapConcat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udf.example.UDFExampleFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udf.example.UDFExampleArraySum.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMinN.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxN.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.s3.S3LogStruct.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.Reducer.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.Output.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.Mapper.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.GenericMR.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.example.WordCountReduce.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.example.IdentityMapper.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.metastore.hooks.SampleURLHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFAdd10.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestTimestampParser.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestShutdownHookManager.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestRetryUtilities.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestHiveStringUtils.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestFixedSizedObjectPool.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestDateParser.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestACLConfigurationParser.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.MockFileSystem.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestVariableSubstitution.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestSystemVariables.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConfUtil.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConfRestrictList.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveAsyncLogging.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestUnsignedInt128.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestTimestampTZ.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestSqlMathUtil.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestSignedInt128.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveVarchar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveIntervalYearMonth.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveIntervalDayTime.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimalOrcSerializationUtils.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveChar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.HiveDecimalTestBase.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestTezJsonParser.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestLogUtils.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestBlobStorageUtils.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.TestLegacyMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestMetricVariableRatioGauge.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleReportersConf.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.jsonexplain.tez.TestTezJsonParser.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.jsonexplain.TestVertex.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.jsonexplain.TestStage.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.jsonexplain.TestOp.java</file>
      <file type="M">common.src.java.org.apache.hive.http.StackServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.http.Log4j2ConfiguratorServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.http.JMXJsonServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hive.http.ConfServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.http.AdminAuthorizedServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.TimestampParser.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.StreamPrinter.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.RetryUtilities.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ReflectionUtil.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.Ref.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveVersionInfo.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HashCodeUtil.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.FixedSizedObjectPool.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.Decimal128FastBuffer.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.DateUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.DateParser.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.AnnotationUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ACLConfigurationParser.java</file>
      <file type="M">common.src.java.org.apache.hive.common.HiveVersionAnnotation.java</file>
      <file type="M">common.src.java.org.apache.hive.common.HiveCompat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.VariableSubstitution.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Validator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.valcoersion.VariableCoercion.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.valcoersion.JavaIOTmpdirVariableCoercion.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.SystemVariables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveVariableSource.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.UnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.TimestampTZUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.TimestampTZ.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.SqlMathUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.SignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveIntervalYearMonth.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StringableMap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ServerUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ObjectPair.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.MetricsMBean.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.MetricVariableRatioGauge.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.MetricsReporting.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.Metrics2Reporter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.JmxMetricsReporter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.ConsoleMetricsReporter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleReporter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.LegacyMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsVariable.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsScope.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.Metrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.MemoryEstimate.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.ProgressMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.LogRedirector.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmPauseMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Vertex.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.tez.TezJsonParser.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Stage.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.spark.SparkJsonParser.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Printer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.JsonParserFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.JsonParser.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParserUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParser.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Connection.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.SortPrintStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.SortAndDigestPrintStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.FetchConverter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.DigestPrintStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.io.CachingPrintStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveStatsUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveInterruptUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveInterruptCallback.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CopyOnFirstWriteProperties.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CompressionUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.ShellCmdExecutor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.IHiveFileProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.HiveFileProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.BlobStorageUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.auth.HiveAuthUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ant.GenHiveTemplate.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestRCFileCat.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestOptionsProcessor.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliSessionState.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.RCFileCat.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestShutdownHook.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestIncrementalRows.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestCommands.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestClientCommandHookFactory.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBufferedRows.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineOpts.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineHistory.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineExceptionHandling.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.ProxyAuthTest.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.hs2connection.TestUserHS2ConnectionFileParser.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.XMLElementOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.XMLAttributeOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.VerticalOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableNameCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SunSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SQLCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Reflector.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ReflectiveCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.OutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.OutputFile.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.logs.BeelineInPlaceUpdateStream.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.IncrementalRowsWithNormalization.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.IncrementalRows.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloDefaultIndexScanner.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloIndexLexicoder.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloIndexScanner.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloIndexScannerException.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.AccumuloIndexDefinition.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.AccumuloIndexedOutputFormat.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.IndexOutputConfigurator.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.package-info.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.package-info.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.package-info.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.package-info.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloIndexParameters.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.CompositeAccumuloRowIdFactory.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.package-info.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.Utils.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloDefaultIndexScanner.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloIndexLexicoder.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCommandCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BooleanCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BufferedRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ClientCommandHookFactory.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ClientHook.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.cli.CliOptionsProcessor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.cli.HiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ColorBuffer.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.CommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnections.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DeprecatedSeparatedValuesOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DriverInfo.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.BeelineHS2ConnectionFileParseException.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.HiveSiteHS2ConnectionFileParser.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.HS2ConnectionFileParser.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.HS2ConnectionFileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17953" opendate="2017-11-1 00:00:00" fixdate="2017-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metrics should move to destination atomically</summary>
      <description>HIVE-17563 reimplemented metrics using native nio interfaces. It used the assumption that{{Files.move()}} is atomic operation. It turns out that by default it isn't, unless ATOMIC_MOVE option is specified. Otherwise the destination file is unlinked and then the source file is copied.This may cause test failure since the file may be temporarily unavailable.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.metrics.JsonReporter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="17954" opendate="2017-11-1 00:00:00" fixdate="2017-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement pool, user, group and trigger to pool management API&amp;#39;s.</summary>
      <description>Implement the following commands:&amp;#8211; Pool management.CREATE POOL `resource_plan`.`pool_path` WITH ALLOC_FRACTION=`fraction`, QUERY_PARALLELISM=`parallelism`, SCHEDULING_POLICY=`policy`;ALTER POOL `resource_plan`.`pool_path` SET PATH = `new_path`, ALLOC_FRACTION = `fraction`, QUERY_PARALLELISM = `parallelism`, SCHEDULING_POLICY = `policy`;DROP POOL `resource_plan`.`pool_path`;&amp;#8211; Adding triggers to pools.ALTER POOL `resource_plan`.`pool_path` ADD TRIGGER `trigger_name`;ALTER POOL `resource_plan`.`pool_path` DROP TRIGGER `trigger_name`;&amp;#8211; User/Group to pool mappings.CREATE USER|GROUP MAPPING `resource_plan`.`group_or_user_name` TO `pool_path` WITH ORDERING `order_no`;DROP USER|GROUP MAPPING `resource_plan`.`group_or_user_name`;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropWMPoolDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropWMMappingDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrDropTriggerToPoolMappingDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrAlterWMPoolDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateOrAlterWMMappingDesc.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MWMPool.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
      <file type="M">metastore.scripts.upgrade.derby.046-HIVE-17566.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.031-HIVE-17566.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.046-HIVE-17566.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.045-HIVE-17566.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterWMTriggerDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateWMTriggerDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMMapping.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
    </fixedFiles>
  </bug>
  <bug id="17965" opendate="2017-11-2 00:00:00" fixdate="2017-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HIVELIMITTABLESCANPARTITION support</summary>
      <description>HIVE-13884 marked it as deprecated</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.limit.partition.stats.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.limit.partition.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.partition.metadataonly.q</file>
      <file type="M">ql.src.test.queries.clientnegative.limit.partition.stats.q</file>
      <file type="M">ql.src.test.queries.clientnegative.limit.partition.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17969" opendate="2017-11-2 00:00:00" fixdate="2017-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore to alter table in batches of partitions when renaming table</summary>
      <description>I'm currently trying to speed up the alter table rename to feature of HMS. The recently submitted change (HIVE-9447) already helps a lot especially on Oracle HMS DBs.This time I intend to gain throughput independently of DB types by enabling HMS to execute this alter table command on batches of partitions (rather than 1by1)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17972" opendate="2017-11-3 00:00:00" fixdate="2017-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Parquet vectorization reader for Map type</summary>
      <description>Parquet vectorized reader can't support map type, it should be supported to improve the performance when the query with map type.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.VectorizedColumnReaderTestBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17982" opendate="2017-11-3 00:00:00" fixdate="2017-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move metastore specific itests</summary>
      <description>There are a number of tests in itests/hive-unit/.../metastore that are metastore specific. I suspect they were initially placed in itests only because the metastore pulling in a few plugins from ql.Given that we need to be able to release the metastore separately, we need to be able to test it completely as a standalone entity. So I propose to move a number of the itests over into standalone-metastore. I will only move tests that are isolated to the metastore. Anything that tests wider functionality I plan to leave in itests.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.client.builder.TableBuilder.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.client.builder.PartitionBuilder.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.client.builder.IndexBuilder.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteUGIHiveMetaStoreIpAddress.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStoreInitRetry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMarkPartition.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestAdminUser.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.FakeDerby.java</file>
    </fixedFiles>
  </bug>
  <bug id="17983" opendate="2017-11-4 00:00:00" fixdate="2017-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the standalone metastore generate tarballs etc.</summary>
      <description>In order to be separately installable the standalone metastore needs its own tarballs, startup scripts, etc. All of the SQL installation and upgrade scripts also need to move from metastore to standalone-metastore.I also plan to create Dockerfiles for different database types so that developers can test the SQL installation and upgrade scripts.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.LogUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreSchemaInfo.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">binary-package-licenses.README</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="17988" opendate="2017-11-6 00:00:00" fixdate="2017-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace patch utility usage with git apply in ptest</summary>
      <description>It would be great to replace the standard diff util because git can do a 3-way merge - which in most cases successfull.This could reduce the ptest results which are erroring out because of build failure.error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:7003Falling back to three-way merge...Applied patch to 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java' cleanly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17994" opendate="2017-11-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Serialization bottlenecked on irrelevant hashmap lookup</summary>
      <description>On machines with slower NUMA, the hashmap lookup for TypeInfo::getPrimitiveCategory is the slowest part of the vectorized serialization loops. The static object references run hot with the NUMA access speeds penalizing half the threads.This lookup is done for every column, for every row - though vectorization enforces that this type cannot change at all.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
    </fixedFiles>
  </bug>
  <bug id="17996" opendate="2017-11-7 00:00:00" fixdate="2017-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ASF headers</summary>
      <description>Yetus check reports some ASF header related issues in Hive code. Let's fix them up.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.resources.log4j2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumEmptyIsZeroAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="180" opendate="2008-12-16 00:00:00" fixdate="2008-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data Generator for thrift-serialized sequence files</summary>
      <description>Add a data generator to create thrift-serialized sequence files.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18002" opendate="2017-11-7 00:00:00" fixdate="2017-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add group support for pool mappings</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="18003" opendate="2017-11-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add explicit jdbc connection string args for mappings</summary>
      <description>1) Force using unmanaged/containers execution.2) Optional - specify pool name (config setting to gate this, disabled by default?).In phase 2 (or 4?) we might allow #2 to be used by a user to choose between multiple mappings if they have multiple pools they could be mapped to (i.e. to change the ordering essentially).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18005" opendate="2017-11-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve size estimation for array() to be not 0</summary>
      <description>happens only in case the array is not from a column; and the array contains no column referencesEXPLAINSELECT sort_array(array("b", "d", "c", "a")),array("1","2") FROM t... Statistics: Num rows: 1 Data size: 0 Basic stats: COMPLETE Column stats: COMPLETE ListSink</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18007" opendate="2017-11-7 00:00:00" fixdate="2017-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address maven warnings</summary>
      <description>[WARNING] Some problems were encountered while building the effective model for org.apache.hive:hive-metastore:jar:3.0.0-SNAPSHOT[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-jar-plugin @ line 299, column 15[WARNING] Some problems were encountered while building the effective model for org.apache.hive:hive-standalone-metastore:jar:3.0.0-SNAPSHOT[WARNING] 'build.plugins.plugin.version' for org.antlr:antlr3-maven-plugin is missing. @ line 538, column 15[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18010" opendate="2017-11-8 00:00:00" fixdate="2017-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hbase version</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="18026" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive webhcat principal configuration optimization</summary>
      <description>Hive webhcat principal configuration optimizationwhen you configure:&lt;property&gt; &lt;name&gt;templeton.kerberos.principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@&lt;REALM&gt;&lt;/value&gt;&lt;/property&gt;The '_HOST' should be replaced by specific host name.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
    </fixedFiles>
  </bug>
  <bug id="18028" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix WM based on cluster smoke test; add logging</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18029" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline - support proper usernames based on the URL arg</summary>
      <description>Update: looks like the argument connection URL is not handled consistently with the connect command one, and the latter passes on user name that's entered in the prompt correctly.So,!connect (url) =&gt; prompt; the username on HS2 side is whatever is entered in the promptbeeline -u (url) =&gt; anonymous (no prompt)!connect (url);user=foo =&gt; foobeeline -u (url);user=foo =&gt; anonymousbeeline -n foo -u (url with or without the user) =&gt; fooI'm going to add support for extracting the user from the -u argument, similar to connect argument</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="1803" opendate="2010-11-20 00:00:00" fixdate="2010-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement bitmap indexing in Hive</summary>
      <description>Implement bitmap index handler to complement compact indexing.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.IndexMetadataChangeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.IndexMetadataChangeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">lib.README</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18031" opendate="2017-11-9 00:00:00" fixdate="2017-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replication for Alter Database operation.</summary>
      <description>Currently alter database operations to alter the database properties or owner info are not generating any events due to which it is not getting replicated.Need to add an event for this.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.EventMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="18034" opendate="2017-11-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improving logging with HoS executors spend lots of time in GC</summary>
      <description>There are times when Spark will spend lots of time doing GC. The Spark History UI shows a bunch of red flags when too much time is spent in GC. It would be nice if those warnings are propagated to Hive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestMetricsCollection.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.metrics.Metrics.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.MetricsCollection.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SparkMetricsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.JobMetricsListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatisticsBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatisticGroup.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="18046" opendate="2017-11-10 00:00:00" fixdate="2017-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore: default IS_REWRITE_ENABLED=false instead of NULL</summary>
      <description>The materialized view impl breaks old metastore sql write access, by complaining that the new table creation does not set this column up. `IS_REWRITE_ENABLED` bit(1) NOT NULL,NOT NULL DEFAULT 0 would allow old metastore direct sql compatibility (not thrift).2017-11-09T07:11:58,331 ERROR [HiveServer2-Background-Pool: Thread-2354] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@249dbf1" using statement "INSERT INTO `TBLS` (`TBL_ID`,`CREATE_TIME`,`DB_ID`,`LAST_ACCESS_TIME`,`OWNER`,`RETENTION`,`SD_ID`,`TBL_NAME`,`TBL_TYPE`,`VIEW_EXPANDED_TEXT`,`VIEW_ORIGINAL_TEXT`) VALUES (?,?,?,?,?,?,?,?,?,?,?)" failed : Field 'IS_REWRITE_ENABLED' doesn't have a default valueat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:720)at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:740)at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:1038)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.3.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.036-HIVE-14496.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.3.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.037-HIVE-14496.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.3.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.037-HIVE-14496.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.3.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.2.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.022-HIVE-14496.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.3.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.2.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.037-HIVE-14496.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18048" opendate="2017-11-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Support Struct type with vectorization</summary>
      <description>Struct type is not supported in MapWork with vectorization, it should be supported to improve the performance. New UDF will be added to access the field of Struct.Note: Nested complex type won't be tested in this ticket.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18053" opendate="2017-11-13 00:00:00" fixdate="2017-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support different table types for MVs</summary>
      <description>MVs backed by MM tables, managed tables, external tables. This might work already, but we need to add tests.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1806" opendate="2010-11-23 00:00:00" fixdate="2010-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The merge criteria on dynamic partitons should be per partiton</summary>
      <description>Currently the criteria of whether a merge job should be fired on dynamic generated partitions are is the average file size of files across all dynamic partitions. It is very common that some dynamic partitions contains mostly large files and some contains mostly small files. Even though the average size of the total files are larger than the hive.merge.smallfiles.avgsize, we should merge those partitions containing small files only.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18067" opendate="2017-11-15 00:00:00" fixdate="2017-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove extraneous golden files</summary>
      <description>TestDanglingQouts makes sure that there are no unneeded files in repo. This is currently failing.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.windowspec4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.input.format.excludes.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.exim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.6b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.column.mixcase.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18068" opendate="2017-11-15 00:00:00" fixdate="2017-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Calcite 1.15</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregatePullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18071" opendate="2017-11-15 00:00:00" fixdate="2017-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HS2 jmx information about pools and current resource plan</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18072" opendate="2017-11-15 00:00:00" fixdate="2017-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix various WM bugs based on cluster testing - part 2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18073" opendate="2017-11-15 00:00:00" fixdate="2017-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AM may assert when its guaranteed task count is reduced</summary>
      <description>Sometimes it asserts that it doesn't have so many ducks to give away. This should never happen, need to debug.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18076" opendate="2017-11-15 00:00:00" fixdate="2017-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>killquery doesn&amp;#39;t actually work for non-trigger WM kills</summary>
      <description>Not sure what's wrong with it, need to take a look.It dumps a lot of info about everything being cancelled, instead of a nice message like triggers do.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18078" opendate="2017-11-16 00:00:00" fixdate="2017-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM getSession needs some retry logic</summary>
      <description>When we get a bad session (e.g. no registry info because AM has gone catatonic), the failure by the timeout future fails the getSession call.The retry model in TezTask is that it would get a session (which in original model can be completely unusable, but we still get the object), and then retry (reopen) if it's a lemon. If the reopen fails, we fail.getSession is not covered by this retry scheme, and should thus do its own retries (or the retry logic needs to be changed)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18084" opendate="2017-11-16 00:00:00" fixdate="2017-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle version to support lambdas</summary>
      <description>Current version does not support lambdas in source files so it skips them. We need to upgrade checkstyle version to fix this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18088" opendate="2017-11-16 00:00:00" fixdate="2017-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add WM event traces at query level for debugging</summary>
      <description>For debugging and testing purpose, expose workload manager events via /jmx endpoint and print summary at the scope of query.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.TriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.Trigger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.PrintSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KillTriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KillMoveTriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.AmPluginNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.AbstractJdbcTriggersTest.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18089" opendate="2017-11-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for few tests</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorize.grant.public.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.public.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1809" opendate="2010-11-23 00:00:00" fixdate="2010-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive comparison operators are broken for NaN values</summary>
      <description>Comparisons between NaN values and doubles do not work as expected:hive&gt; select 'NaN' = 4.3 from data_one limit 1;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorExecution log at: /tmp/pbutler/pbutler_20101123145656_d23f9b77-8907-4ed3-aef9-8b99a1cc3138.logJob running in-process (local Hadoop)2010-11-23 14:56:40,488 null map = 100%, reduce = 0%Ended Job = job_local_0001OKtrueTime taken: 9.47 secondshive&gt; select 4 &lt;&gt; 'NaN' from data_one limit 1;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorExecution log at: /tmp/pbutler/pbutler_20101123145858_0d243ac2-f745-4e25-9a38-509bef3bb370.logJob running in-process (local Hadoop)2010-11-23 14:58:45,689 null map = 100%, reduce = 0%Ended Job = job_local_0001OKfalseTime taken: 3.938 seconds</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18109" opendate="2017-11-21 00:00:00" fixdate="2017-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix identifier usage in parser</summary>
      <description>HIVE-17902 broke exposed this</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="18123" opendate="2017-11-22 00:00:00" fixdate="2017-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain formatted improve column expression map display</summary>
      <description>HIVE-17898 introduced columnExprMap in explain formatted. Formatting of that map was a little off. This jira is to improve the formatting.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="18131" opendate="2017-11-22 00:00:00" fixdate="2017-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate table for Acid tables</summary>
      <description>How should this work? Should it work like Insert Overwrite T select * from T where 1=2?This should create a new empty base_x/ and thus operate w/o violating Snapshot Isolation semantics.This makes sense for specific partition or unpartitioned table. What about "Truncate T" where T is partitioned? Is the expectation to wipe out all partition info or to make each partition empty?</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
    </fixedFiles>
  </bug>
  <bug id="18134" opendate="2017-11-22 00:00:00" fixdate="2017-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some alter resource plan fixes</summary>
      <description>Part of HIVE-18075</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
    </fixedFiles>
  </bug>
  <bug id="18138" opendate="2017-11-23 00:00:00" fixdate="2017-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix columnstats problem in case schema evolution</summary>
      <description>column stats are kept in case the main table schema is altered; and this causes all kind of problems.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="18140" opendate="2017-11-23 00:00:00" fixdate="2017-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioned tables statistics can go wrong in basic stats mixed case</summary>
      <description>suppose the following scenario: part1 has basic stats RC=10,DS=1K all other partition has no basic stats (and a bunch of rows)then this condition would be false; which in turn produces estimations for the whole partitioned table: RC=10,DS=1K</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.join.reordering.no.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.Partish.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.stats8.q</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18147" opendate="2017-11-25 00:00:00" fixdate="2017-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests can fail with java.net.BindException: Address already in use</summary>
      <description>Between the time a new port is picked up to start HMS or HS2, and the time when HMS or HS2 is started, the port can get used by some other service resulting in test failure.</description>
      <version>None</version>
      <fixedVersion>2.3.9,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.StorageBasedMetastoreTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
    </fixedFiles>
  </bug>
  <bug id="18157" opendate="2017-11-28 00:00:00" fixdate="2017-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization : Insert in bucketed table is broken with vectorization</summary>
      <description>create temporary table foo (x int) clustered by into 4 buckets;insert overwrite table foo values(1),(2),(3),(4),(9);select , regexp_extract(INPUT_FILE_NAME, './(.*)', 1) from foo;OK9 000000_04 000000_03 000000_02 000000_01 000000_0set hive.vectorized.execution.enabled=false;insert overwrite table foo values(1),(2),(3),(4),(9);select , regexp_extract(INPUT_FILE_NAME, './(.*)', 1) from foo;OK4 000000_09 000001_01 000001_02 000002_03 000003_0</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18159" opendate="2017-11-28 00:00:00" fixdate="2017-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Support Map type in MapWork</summary>
      <description>Support Complex Types in vectorization is finished in HIVE-16589, but Map type is still not support in MapWork. This ticket is target to support it for MapWork when vectorization is enable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18161" opendate="2017-11-28 00:00:00" fixdate="2017-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hive.stats.atomic</summary>
      <description>It might have its purpose back then... when it was introduced; but currently enabling this property may at best prevent the stats collector from working properly And moreover: hive.stats.reliable is a very similar prop; and it would be better to use only one property which enables correctness checks.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18188" opendate="2017-11-30 00:00:00" fixdate="2017-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestSSL failures in master</summary>
      <description>HIVE-18170 broke TestSSL tests.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="1819" opendate="2010-12-1 00:00:00" fixdate="2010-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>maintain lastAccessTime in the metastore</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.sample10.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18190" opendate="2017-12-1 00:00:00" fixdate="2017-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consider looking at ORC file schema rather than using _metadata_acid file</summary>
      <description>See if it's possible to just look at the schema of the file in base_ or delta_ to see if it has Acid metadata columns. If not, it's an 'original' file and needs ROW_IDs generated.see more discussion at https://reviews.apache.org/r/64131/</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18193" opendate="2017-12-1 00:00:00" fixdate="2017-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate existing ACID tables to use write id per table rather than global transaction id</summary>
      <description>dependent upon HIVE-18192For existing ACID Tables we need to update the table level write id metatables/sequences so any new operations on these tables works seamlessly without any conflicting data in existing base/delta files.1. Need to create metadata tables such as NEXT_WRITE_ID and TXN_TO_WRITE_ID.2. Add entries for each ACID/MM tables intoNEXT_WRITE_ID where NWI_NEXT is set to current value of NEXT_TXN_ID.NTXN_NEXT.3. All current open/abort transactions to have an entry in TXN_TO_WRITE_IDsuch thatT2W_TXNID=T2W_WRITEID=Open/AbortedTxnId.4.Added new column TC_WRITEID in TXN_COMPONENTS and CTC_WRITEID in COMPLETED_TXN_COMPONENTS to store the write id which should be set as respective values of TC_TXNID and CTC_TXNID from the samerow.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="18196" opendate="2017-12-1 00:00:00" fixdate="2017-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Mini Cluster to run Qtests integrations tests.</summary>
      <description>The overall Goal of this is to add a new Module that can fork a druid cluster to run integration testing as part of the Mini Clusters Qtest suite.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1820" opendate="2010-12-1 00:00:00" fixdate="2010-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive database data center aware</summary>
      <description>In order to support multiple data centers (different DFS, MR clusters) for hive, it is desirable to extend Hive database to be data center aware. Currently Hive database is a logical concept and has no DFS or MR cluster info associated with it. Database has the location property indicating the default warehouse directory, but user cannot specify and change it. In order to make it data center aware, the following info need to be maintained:1) data warehouse root location which is the default HDFS location for newly created tables (default=hive.metadata.warehouse.dir).2) scratch dir which is the HDFS location where MR intermediate files are created (default=hive.exec.scratch.dir)3) MR job tracker URI that jobs should be submitted to (default=mapred.job.tracker)4) hadoop (bin) dir ($HADOOP_HOME/bin/hadoop)These parameters should be saved in database.parameters (key, value) pair and they overwrite the jobconf parameters (so if the default database has no parameter it will get it from the hive-default.xml or hive-site.xml as it is now).</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18202" opendate="2017-12-1 00:00:00" fixdate="2017-1-1 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Automatically migrate hbase.table.name to hbase.mapreduce.hfileoutputformat.table.name for hbase-based table</summary>
      <description>The property name for Hbase table mapping is changed from hbase.table.name to hbase.mapreduce.hfileoutputformat.table.name in HBase 2.We can include such upgrade for existing hbase-based tables in DB upgrade script to automatically change such values.For the new tables, the query will be like:create table hbase_table(key int, val string) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties ('hbase.columns.mapping' = ':key,cf:val') tblproperties ('hbase.mapreduce.hfileoutputformat.table.name' = 'positive_hbase_handler_bulk')</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.046-HIVE-18202.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.047-HIVE-18202-oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.047-HIVE-18202.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.032-HIVE-18202.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.047-HIVE-18202.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18213" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests: YARN Minicluster times out if the disks are &gt;90% full</summary>
      <description>Increase YARN minicluster threshold to timeout only at 99% full instead of 90%.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18230" opendate="2017-12-5 00:00:00" fixdate="2017-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create plan like plan, and replace plan commands for easy modification</summary>
      <description>Given that the plan already on the cluster cannot be altered, it would be helpful to have create plan like plan, and replace plan commands that would make a copy to be modified, and then rename+apply the copy in place of an existing plan, and rename the existing active plan with a versioned name or drop it altogether.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ResourcePlanParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="18232" opendate="2017-12-6 00:00:00" fixdate="2017-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Packaging: add dfs-init script in package target</summary>
      <description>As discussed with Ashutosh Chauhan this change is to include init-hive-dfs.sh in the hive package.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18248" opendate="2017-12-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up parameters</summary>
      <description>Clean up of parameters that need not change at run time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18259" opendate="2017-12-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatic cleanup of invalidation cache for materialized views</summary>
      <description>HIVE-14498 introduces the invalidation cache for materialized views, which keeps track of the transactions executed on a given table to infer whether materialized view contents are outdated or not.Currently, the cache keeps information of transactions in memory to guarantee quick response time, i.e., quick resolution about the view freshness, at query rewriting time. This information can grow large, thus we would like to run a thread that cleans useless transactions from the cache, i.e., transactions that do invalidate any materialized view in the system, at an interval defined by a property.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsInvalidationCache.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18266" opendate="2017-12-12 00:00:00" fixdate="2017-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: /system references wrong file for THP</summary>
      <description>copy paste error in /system endpoint. THP references same files again.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.SystemConfigurationServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="18294" opendate="2017-12-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add switch to make acid table the default</summary>
      <description>it would be convenient for testing to have a switch that enables the behavior where all suitable table tables (currently ORC + not sorted) are automatically created with transactional=true, ie. full acid.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18298" opendate="2017-12-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestReplicationScenarios.testConstraints</summary>
      <description>The test if broken by HIVE-16603. Currently on constraints are created without order on replication destination cluster during bootstrap, after HIVE-16603, it is no longer possible. We need to create foreign keys at last after all primary keys are created.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.ConstraintEventsIterator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1830" opendate="2010-12-6 00:00:00" fixdate="2010-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mappers in group followed by joins may die OOM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18317" opendate="2017-12-19 00:00:00" fixdate="2017-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error messages in TransactionalValidationListerner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">ql.src.test.results.clientnegative.create.not.acid.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="18318" opendate="2017-12-19 00:00:00" fixdate="2017-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP record reader should check interrupt even when not blocking</summary>
      <description>Hive operators don't check interrupts, and may not do blocking operations.LLAP record reader only blocks in IO is slower than processing; so, if IO is fast enough, it will not ever block (at least not interruptibly, the sync w/IO on the object does not check interrupts), and thus never catch interrupts.So, the task would be impossible to terminate.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="18319" opendate="2017-12-20 00:00:00" fixdate="2017-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 3.0.0</summary>
      <description>Hadoop 3.0.0 has been released, we should upgrade to it:http://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/release/3.0.0/RELEASENOTES.3.0.0.htmlhttps://hadoop.apache.org/docs/r3.0.0/index.htmlSome test failures can be found when we tried to upgrade to Hadoop 3.0.0 in HIVE-17684</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18341" opendate="2017-12-26 00:00:00" fixdate="2017-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add repl load support for adding "raw" namespace for TDE with same encryption keys</summary>
      <description>https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#Running_as_the_superuser"a new virtual path prefix, /.reserved/raw/, that gives superusers direct access to the underlying block data in the filesystem. This allows superusers to distcp data without needing having access to encryption keys, and also avoids the overhead of decrypting and re-encrypting data."We need to introduce a new option in "Repl Load" command that will change the files being copied in distcp to have this "/.reserved/raw/" namespace before the file paths.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1835" opendate="2010-12-6 00:00:00" fixdate="2010-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better auto-complete for Hive</summary>
      <description>Add functions and keywords to auto-complete list Make Hive auto-complete aware of Hive delimiters (eg. whitespace, parentheses)</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18352" opendate="2018-1-2 00:00:00" fixdate="2018-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools</summary>
      <description>Introduce a METADATAONLY option as part of the REPL DUMP command which will only try and dump out events for DDL changes, this will be faster as we wont need scan of files on HDFS for DML changes. Additionally since we are only going to dump metadata operations, it might be useful to include acid tables as well via an option as well. This option can be removed when ACID support is complete via HIVE-18320it will be good to support the "WITH" clause as part of REPL DUMP command as well (repl dump already supports it viaHIVE-17757) to achieve the above as that will prevent less changes to the syntax of the statement and provide more flexibility in future to include additional options as well. REPL DUMP [db_name] {FROM [event_id]} {TO [event_id]} {WITH (['key'='value'],.....)}This will enable other tools like security / schema registry / metadata discovery to use replication related subsystem for their needs as well.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.SQLGenerator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.DAGTraversalTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeavesTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExportWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeaves.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.BootstrapEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.ConstraintEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.DatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.ConstraintEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSConstraintEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSFunctionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.FunctionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.PartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.ReplicationState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.TableContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TaskTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.PathInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.PathUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.util.DAGTraversal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MetaDataExportListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18356" opendate="2018-1-2 00:00:00" fixdate="2018-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixing license headers in checkstyle</summary>
      <description>The checkstyle header contains the following ASF header:/** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file[..]Even if we undecided what to do with the already existing headers (HIVE-17952), the new ones should use the proper one with 1 '*' in the first line:/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file[..]</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.checkstyle.asf.header</file>
      <file type="M">standalone-metastore.checkstyle.asf.header</file>
      <file type="M">checkstyle.asf.header</file>
    </fixedFiles>
  </bug>
  <bug id="18361" opendate="2018-1-3 00:00:00" fixdate="2018-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend shared work optimizer to reuse computation beyond work boundaries</summary>
      <description>Follow-up of the work in HIVE-16867.HIVE-16867 introduced an optimization that identifies scans on input tables that can be merged and reuses the computation that is done in the work containing those scans. In particular, we traverse both parts of the plan upstream and reuse the operators if possible.Currently, the optimizer will not go beyond the output edge(s) of that work. This extension removes that limitation.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18365" opendate="2018-1-3 00:00:00" fixdate="2018-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>netty-all jar is not present in the llap tarball</summary>
      <description>netty-all jar should be part of the llap tarball, as it requires it. Earlier it was part of the tez libs, but it was removed from there, so it must be added to it explicitly.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18367" opendate="2018-1-3 00:00:00" fixdate="2018-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe Extended output is truncated on a table with an explicit row format containing tabs or newlines.</summary>
      <description>'Describe Extended' dumps information about a table. The protocol for sending this data relies on tabs and newlines to separate pieces of data. If a table has 'FIELDS terminated by XXX' or 'LINES terminated by XXX' where XXX is a tab or newline then the output seen by the user is prematurely truncated. Fix this by replacing tabs and newlines in the table description with \n and \t.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cteViews.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="18379" opendate="2018-1-4 00:00:00" fixdate="2018-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER TABLE authorization_part SET PROPERTIES ("PARTITIONL_LEVEL_PRIVILEGE"="TRUE"); fails when authorization_part is MicroManaged table.</summary>
      <description>ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE") fails when authorization_part is a Micromanaged table.This is from authorization_2.q qtest.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18384" opendate="2018-1-5 00:00:00" fixdate="2018-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConcurrentModificationException in log4j2.x library</summary>
      <description>In one of the internal testing, observed the following exceptionjava.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) ~[?:1.8.0_152] at java.util.ArrayList$Itr.next(ArrayList.java:859) ~[?:1.8.0_152] at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1042) ~[?:1.8.0_152] at org.apache.logging.log4j.message.ParameterFormatter.appendCollection(ParameterFormatter.java:596) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterFormatter.appendPotentiallyRecursiveValue(ParameterFormatter.java:504) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterFormatter.recursiveDeepToString(ParameterFormatter.java:429) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterFormatter.formatMessage2(ParameterFormatter.java:189) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterizedMessage.formatTo(ParameterizedMessage.java:224) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterizedMessage.getFormattedMessage(ParameterizedMessage.java:200) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEvent.setMessage(RingBufferLogEvent.java:126) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEvent.setValues(RingBufferLogEvent.java:104) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEventTranslator.translateTo(RingBufferLogEventTranslator.java:56) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEventTranslator.translateTo(RingBufferLogEventTranslator.java:34) ~[log4j-core-2.6.2.jar:2.6.2] at com.lmax.disruptor.RingBuffer.translateAndPublish(RingBuffer.java:930) ~[disruptor-3.3.0.jar:?] at com.lmax.disruptor.RingBuffer.tryPublishEvent(RingBuffer.java:456) ~[disruptor-3.3.0.jar:?] at org.apache.logging.log4j.core.async.AsyncLoggerDisruptor.tryPublish(AsyncLoggerDisruptor.java:190) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.AsyncLogger.publish(AsyncLogger.java:160) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.AsyncLogger.logWithThreadLocalTranslator(AsyncLogger.java:156) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.AsyncLogger.logMessage(AsyncLogger.java:126) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:2011) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1884) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.slf4j.Log4jLogger.info(Log4jLogger.java:189) ~[log4j-slf4j-impl-2.6.2.jar:2.6.2] at org.apache.hadoop.hive.druid.security.KerberosHttpClient.inner_go(KerberosHttpClient.java:96) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hadoop.hive.druid.security.KerberosHttpClient.access$100(KerberosHttpClient.java:50) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hadoop.hive.druid.security.KerberosHttpClient$2.onSuccess(KerberosHttpClient.java:144) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hadoop.hive.druid.security.KerberosHttpClient$2.onSuccess(KerberosHttpClient.java:134) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hive.druid.com.google.common.util.concurrent.Futures$4.run(Futures.java:1181) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_152] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_152]The fix for this went into 2.9.1 LOG4J2-1988 onwards. Updating log4j to latest version should have a fix for this issue.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18414" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18415" opendate="2018-1-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower "Updating Partition Stats" Logging Level</summary>
      <description>org.apache.hadoop.hive.metastore.utils.MetaStoreUtilsLOG.warn("Updating partition stats fast for: " + part.getTableName());...LOG.warn("Updated size to " + params.get(StatsSetupConst.TOTAL_SIZE));This logging produces many lines of WARN log messages in my log file and it's not clear to me what the issue is here. Why is this a warning and how should I respond to address this warning?DEBUG is probably more appropriate for a utility class. Please lower.</description>
      <version>1.2.2,2.2.0,2.3.2,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18418" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up plugin between DAGs</summary>
      <description>Noticed this while looking at some other JIRA. We need to make sure we clean up the plugin state between DAGs.Technically HS2 would send us the new token count with the new dag but it's good to make sure it cannot get inherited by accident.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18421" opendate="2018-1-10 00:00:00" fixdate="2018-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized execution handles overflows in a different manner than non-vectorized execution</summary>
      <description>In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.Simple test case to reproduce this issueset hive.vectorized.execution.enabled=true;create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;insert into parquettable values (-104, 25), (-112, 24), (54, 9);select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) &lt; 50 order by diff desc;+-------+-----+-------+| t1 | t2 | diff |+-------+-----+-------+| -104 | 25 | 127 || -112 | 24 | 120 || 54 | 9 | 45 |+-------+-----+-------+When vectorization is turned off the same query produces only one row.</description>
      <version>2.1.1,2.2.0,2.3.2,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorTestCode.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestUnaryMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.java</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestClass.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryMinus.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumn.txt</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedArithmeticBench.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18426" opendate="2018-1-10 00:00:00" fixdate="2018-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in RoutingAppender for every hive operation</summary>
      <description>Each new operation creates new entry in the ConcurrentMap in RoutingAppender but when the operation ends, AppenderControl stored in the map is retrieved and stopped but the entry in ConcurrentMap is never cleaned up.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingLayout.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18433" opendate="2018-1-10 00:00:00" fixdate="2018-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade version of com.fasterxml.jackson</summary>
      <description>Let's upgrade to version 2.9.4</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.PTestClient.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18434" opendate="2018-1-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type is not determined correctly for comparison between decimal column and string constant</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18436" opendate="2018-1-11 00:00:00" fixdate="2018-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Spark 2.3.0</summary>
      <description>Branching has been completed. Release candidates should be published soon. Might be a while before the actual release, but at least we get to identify any issues early.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestRpc.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestKryoMessageCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18437" opendate="2018-1-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use plan parallelism for the default pool if both are present</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="18450" opendate="2018-1-13 00:00:00" fixdate="2018-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TABLE function in CBO</summary>
      <description>Follow-up of HIVE-18416 to support TABLE function in CBO.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tablevalues.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableFunctionScan.java</file>
    </fixedFiles>
  </bug>
  <bug id="18452" opendate="2018-1-13 00:00:00" fixdate="2018-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>work around HADOOP-15171</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18453" opendate="2018-1-16 00:00:00" fixdate="2018-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Add "CREATE TRANSACTIONAL TABLE" syntax to unify ACID ORC &amp; Parquet support</summary>
      <description>The ACID tablemarkers arecurrently done with TBLPROPERTIES which is inherently fragile.The "create transactional table" offers a way to standardize the syntax and allows for future compatibility changes to support Parquet ACIDv2 tables along with ORC tables.The ACIDv2design is format independent, with the ability to add new vectorized input formats withno changes to the design.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="18456" opendate="2018-1-16 00:00:00" fixdate="2018-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some tests for HIVE-18367 to check that the table information contains the query correctly</summary>
      <description>This cannot be tested with a CliDriver test so add a java test to check the output of 'describe extended', which is changed by HIVE-18367</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="18458" opendate="2018-1-16 00:00:00" fixdate="2018-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Workload manager initializes even when interactive queue is not set</summary>
      <description>Workload manager gets initialized even when interactive queue is not defined (however there is an active resource plan in metastore). Active resource plan is used for tez in this case.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="18467" opendate="2018-1-17 00:00:00" fixdate="2018-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support whole warehouse dump / load + create/drop database events</summary>
      <description>A complete hive warehouse might be required to replicate to a DR site for certain use cases and rather than allowing only a database name in the REPL DUMP commands, we should allow dumping of all databases using the "*" option as in REPL DUMP * On the repl load side there will not be an option to specify the database name when loading from a location used to dump multiple databases, hence only REPL LOAD FROM &amp;#91;location&amp;#93; would be supported when dumping via REPL DUMP *Additionally, incremental dumps will go through all events across databases in a warehouse and hence CREATE / DROP Database events have to be serialized correctly to allow repl load to create them correctly.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateDatabaseMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.CreateDatabaseMessage.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
    </fixedFiles>
  </bug>
  <bug id="18469" opendate="2018-1-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2UI: Introduce separate option to show query on web ui</summary>
      <description>currently ConfVars.HIVE_LOG_EXPLAIN_OUTPUT enables 2 features: log the query to the console (even thru beeline) shows the query on the web uiI've enabled it...and ever since then my beeline is always flooded with an explain extended output...which is very verbose; even for simple queries.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.jamon.org.apache.hive.tmpl.QueryProfileTmpl.jamon</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18473" opendate="2018-1-17 00:00:00" fixdate="2018-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Infer timezone information correctly in DruidSerde</summary>
      <description>Currently timezone information is not being processed by DruidSerde (contrary to other SerDes).</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="18489" opendate="2018-1-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically migrate s3n URIs to s3a URIs</summary>
      <description>s3n has been removed from Hadoop 3.x, we should auto-migrate tables with s3n URIs to the s3a URIs</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.048-HIVE-18489.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="1849" opendate="2010-12-13 00:00:00" fixdate="2010-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add more logging to partition pruning</summary>
      <description>In facebook, we are seeing some intermittent errors, where it seems that either all the partitions are not returned by the metastoreor some of them are pruned wrongly.This patch adds more logging for debugging such scenarios.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18493" opendate="2018-1-19 00:00:00" fixdate="2018-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add display escape for CR/LF to Hive CLI and Beeline</summary>
      <description>Add optional display escaping of carriage return and line feed so row output remains one line.</description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFileBeeLineClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="18495" opendate="2018-1-19 00:00:00" fixdate="2018-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit rule to enable Driver level testing</summary>
      <description>I've tried to write a case for a sophisticated check...it worked so well that I've started using it and eventually created a junit rule to make it easier to reuseCurrently it takes ~15-25sec to run a test case with this framework (from which most of the time is the launch time of all kind of stuff which are needed to run a driver command). enable to write JUnit tests which has access to the IDriver level leave out the cli-driver; it sometimes causes problems write tests at the ql module it should also work from the IDE without changing anythingNote: JUnit 5 would be great for this task; but unfortunately junit5 needs maven-surefire 2.19.1 ; which causes all kinds of problems for hive devs using idea...so that's not an option.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCliConfig.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18499" opendate="2018-1-19 00:00:00" fixdate="2018-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Amend point lookup tests to check for data</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup.q</file>
    </fixedFiles>
  </bug>
  <bug id="18513" opendate="2018-1-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query results caching</summary>
      <description>Add a query results cache that can save the results of an executed Hive query for reuse on subsequentqueries. This may be useful in cases wherethe same query is issuedmany times, since Hive can return back the results of a cached query rather than having to execute the full query on the cluster.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-blobstore.src.test.resources.hive-site.xml</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.spark.local.hive-site.xml</file>
      <file type="M">data.conf.rlist.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.tez.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18514" opendate="2018-1-22 00:00:00" fixdate="2018-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add service output for ranger to WM DDL operations</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18516" opendate="2018-1-23 00:00:00" fixdate="2018-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements for ACID Tables</summary>
      <description>load data should rename files consistent with insert statements for ACID Tables.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.data.into.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.7.q</file>
      <file type="M">ql.src.test.queries.clientnegative.load.data.into.acid.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveCopyFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18518" opendate="2018-1-23 00:00:00" fixdate="2018-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade druid version to 0.11.0</summary>
      <description>this task is to upgrade to druid version 0.11.0</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.HiveDruidSerializationModule.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18519" opendate="2018-1-23 00:00:00" fixdate="2018-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>do not create materialized CTEs with ACID/MM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18521" opendate="2018-1-24 00:00:00" fixdate="2018-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: query failing in reducer VectorUDAFAvgDecimalPartial2 java.lang.ClassCastException StructTypeInfo --&gt; DecimalTypeInfo</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimalMerge.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18525" opendate="2018-1-24 00:00:00" fixdate="2018-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explain plan to Hive on Spark Web UI</summary>
      <description>More of an investigation JIRA. The Spark UI has a "long description" of each stage in the Spark DAG. Typically one stage in the Spark DAG corresponds to either a MapWork or ReduceWork object. It would be useful if the long description contained the explain plan of the corresponding work object.I'm not sure how much additional overhead this would introduce. If not the full explain plan, then maybe a modified one that just lists out all the operator tree along with each operator name.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ReduceTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.CacheTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18529" opendate="2018-1-24 00:00:00" fixdate="2018-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Add a debug config option to disable scratch column reuse</summary>
      <description>Debugging scratch column reuse is particularly painful and slow, adding a config allows for this to be done without rebuilds.</description>
      <version>2.3.2,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties.orig</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18531" opendate="2018-1-24 00:00:00" fixdate="2018-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Vectorized PTF operator should not set the initial type infos</summary>
      <description>The Vectorized PTF operator is mistakenly setting the initial type infos for its output VectorizationContext. It should not. It is only creating a projection of the initial column names/type infos from ReduceSink (i.e. keys, values) plus scratch columns for output columns for which the type infos are already known.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18533" opendate="2018-1-25 00:00:00" fixdate="2018-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to use InProcessLauncher to submit spark jobs</summary>
      <description>See discussion in HIVE-16484 for details.I think this will help with reducing the amount of time it takes to open a HoS session + debuggability (no need launch a separate process to run a Spark app).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestJobHandle.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientFactory.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.JobHandleImpl.java</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkLauncherSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkLauncherSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="18546" opendate="2018-1-25 00:00:00" fixdate="2018-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary code introduced in HIVE-14498</summary>
      <description>HIVE-14498 introduced some code to check the invalidation of materialized views that can be simplified, relying instead on existing transaction ids.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetTableMeta.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsInvalidationCache.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.UniqueConstraintsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TxnsSnapshot.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrimaryKeysResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionWithoutSD.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRow.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpecWithSharedSD.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionListComposingSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotNullConstraintsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Materialization.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">metastore.scripts.upgrade.derby.048-HIVE-14498.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.033-HIVE-14498.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.048-HIVE-14498.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.048-HIVE-14498.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.047-HIVE-14498.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-3.0.0.postgres.sql</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddForeignKeyRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddNotNullConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPrimaryKeyRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddUniqueConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AggrStats.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BasicTxnInfo.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ForeignKeysResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="18548" opendate="2018-1-25 00:00:00" fixdate="2018-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix log4j import</summary>
      <description>LLAP server code could run into invalid reference to log4j.MDC due to code importing org.apache.log4j.MDC instead of org.slf4j.MDC</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18551" opendate="2018-1-26 00:00:00" fixdate="2018-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: VectorMapOperator tries to write too many vector columns for Hybrid Grace</summary>
      <description>Code incorrectly usesprojectedColumns.length instead ofsingleRow.length</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="18557" opendate="2018-1-26 00:00:00" fixdate="2018-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>q.outs: fix issues caused by q.out_spark files</summary>
      <description>HIVE-18061 caused some issues in yetuscheckby introducing q.out_spark files.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18558" opendate="2018-1-26 00:00:00" fixdate="2018-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade orc version to 1.4.2</summary>
      <description>Upgrade orc version to latest 1.4.2 release.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18574" opendate="2018-1-29 00:00:00" fixdate="2018-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Ship netty3 as part of LLAP install tarball</summary>
      <description>Removing netty from Tez libs causes2018-01-29T18:28:49,995 ERROR [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exceptionjava.lang.NoClassDefFoundError: org/jboss/netty/channel/group/ChannelGroup at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.serviceStart(LlapDaemon.java:410)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18575" opendate="2018-1-29 00:00:00" fixdate="2018-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID properties usage in jobconf is ambiguous for MM tables</summary>
      <description>Vectorization checks for ACID table trigger for MM tables where they don't apply. Other places seem to set the setting for transactional case while most of the code seems to assume it implies full acid.Overall, many places in the code use the settings directly or set the ACID flag without setting the ACID properties.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.Partish.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingAssert.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18578" opendate="2018-1-30 00:00:00" fixdate="2018-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some class has missed the ASF header</summary>
      <description>Some class has missed the ASF header</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.DruidNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="18589" opendate="2018-1-31 00:00:00" fixdate="2018-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.io.IOException: Not enough history available</summary>
      <description>txnid:17 starts reading T2/P1txnid:20 does insert overwrite T1/P1, creates base_20, commits. txnid:17 still runningtxnid:21 stars reading T1/P1. It's ValidTxnList will txnid:17 as open.before Insert overwrite was supported, only the compactor could produce base_20 by running major compaction. Major compaction erases history and so a reader with txnid:17 open, can't use base_20.Normally, the Cleaner is smart enough to not clean pre-compaction files if it's possible that there is a reader that requires them. There is a safety check that creates "Not enough history.." error if it finds that the current reader can't properly execute based on the files available.with the introduction of IOW on acid tables, there is another way to produce a base. The difference is that here, the base has no history by definition and so the same check is not needed but is triggered in the scenario above.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18590" opendate="2018-1-31 00:00:00" fixdate="2018-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assertion error on transitive join inference in the presence of NOT NULL constraint</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="18601" opendate="2018-2-1 00:00:00" fixdate="2018-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Power platform by updating protoc-jar-maven-plugin version</summary>
      <description>Below is error is seen while buildingstandalone-metastore project[ [1;34mINFO [m] [1m--- [0;32mprotoc-jar-maven-plugin:3.0.0-a3:run [m [1m(default) [m @ [36mhive-standalone-metastore [0;1m --- [m[ [1;34mINFO [m] Protoc version: 2.5.0[ [1;34mINFO [m] Input directories:[ [1;34mINFO [m] /var/lib/jenkins/workspace/hive/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore[ [1;34mINFO [m] Output targets:[ [1;34mINFO [m] java: /var/lib/jenkins/workspace/hive/standalone-metastore/target/generated-sources (add: none, clean: false)[ [1;34mINFO [m] /var/lib/jenkins/workspace/hive/standalone-metastore/target/generated-sources does not exist. Creating...[ [1;34mINFO [m] Processing (java): metastore.protoprotoc-jar: protoc version: 250, detected platform: linux/ppc64leprotoc-jar: executing: [/tmp/protoc1841305810088884216.exe, -I/var/lib/jenkins/workspace/hive/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/var/lib/jenkins/workspace/hive/standalone-metastore/target/generated-sources, /var/lib/jenkins/workspace/hive/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]/tmp/protoc1841305810088884216.exe: 1: /tmp/protoc1841305810088884216.exe: ELF : not found/tmp/protoc1841305810088884216.exe: 1: /tmp/protoc1841305810088884216.exe: C: not found/tmp/protoc1841305810088884216.exe: 2: /tmp/protoc1841305810088884216.exe: : not found/tmp/protoc1841305810088884216.exe: 3: /tmp/protoc1841305810088884216.exe: _c jnP R ?Y@9  Ch  yIk : not found/tmp/protoc1841305810088884216.exe: 2: /tmp/protoc1841305810088884216.exe: Syntax error: Unterminated quoted stringTheprotoc-jar-maven-plugin version used is3.0.0-a3 whereas Power (ppc64le) support was added in3.5.1.1.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18607" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase HFile write does strange things</summary>
      <description>There's some strange code in the output handler that changes output directory into a file because Hive supposedly wants that. If you run insert overwrite with a side directory multiple times, the 2nd insert fails</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.positive.hbase.handler.bulk.q.out</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.bulk.q</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="18610" opendate="2018-2-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance: ListKeyWrapper does not check for hashcode equals, before comparing members</summary>
      <description>ListKeyWrapper::equals() @Override public boolean equals(Object obj) { if (!(obj instanceof ListKeyWrapper)) { return false; } Object[] copied_in_hashmap = ((ListKeyWrapper) obj).keys; return equalComparer.areEqual(copied_in_hashmap, keys); }</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestKeyWrapperFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18611" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid memory allocation of aggregation buffer during stats computation</summary>
      <description>Bloom filter aggregation buffer may result in allocation of upto ~594MB array which is unnecessary.</description>
      <version>2.3.2,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18612" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build subprocesses under Yetus in Ptest use 1.7 jre instead of 1.8</summary>
      <description>As per this jira comment made by Yetusmaven plugins that want to use java executable are seeing a 1.7 java binary. In this particular case Yetus sets JAVA_HOME to a 1.8 JDK installation, and thus maven uses that, but any subsequent java executes will use the JRE which they see on PATH.This should be fixed by adding the proper java/bin (that of JAVA_HOME setting) to PATH.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug id="18614" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sys db creation in Hive</summary>
      <description>Sys db can not be created due to several server side issues.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="18616" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>work around HADOOP-15171 p2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18620" opendate="2018-2-5 00:00:00" fixdate="2018-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message while dropping a table that is part of a materialized view</summary>
      <description>When we want to drop a table used by a materialized view, we prevent dropping that table. However, the message shown is not very meaningful (FK-PK violation).</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.drop.table.used.by.mv.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="18626" opendate="2018-2-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl load "with" clause does not pass config to tasks</summary>
      <description>The "with" clause in repl load suppose to pass custom hive config entries to replication. However, the config is only effective in BootstrapEventsIterator,but not the generated tasks (such as MoveTask, DDLTask).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18627" opendate="2018-2-6 00:00:00" fixdate="2018-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD: Handle FLOAT boxing differently for single/double precision constants</summary>
      <description>Constants like 0.1 and 0.3 are differently boxed based on intermediate precision of the compiler codepath.Disabling CBO produces 0.1BD constants which fail to box correctly to Double/Float.Enabling CBO fixes this issue, but cannot be applied all queries in Hive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18633" opendate="2018-2-6 00:00:00" fixdate="2018-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Service discovery for Active/Passive HA mode</summary>
      <description>HIVE-18281 adds Active/Passive HA mode for HS2. This jira is to extend the feature so that clients will connect only to leader instance via JDBC + service discovery.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2LeadershipStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistryClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
    </fixedFiles>
  </bug>
  <bug id="18637" opendate="2018-2-6 00:00:00" fixdate="2018-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WorkloadManagent Event Summary leaving subscribedCounters and currentCounters fields empty</summary>
      <description>subscribedCounters and currentCounters values are empty when trigger results in MOVE eventWorkloadManager Events SummaryINFO : { "queryId" : "hive_20180205214449_d2955891-e3b2-4ac3-bca9-5d2a53feb8c0", "queryStartTime" : 1517867089060, "queryEndTime" : 1517867144341, "queryCompleted" : true, "queryWmEvents" : [ { "wmTezSessionInfo" : { "sessionId" : "157866e5-ed1c-4abd-9846-db76b91c1124", "poolName" : "pool2", "clusterPercent" : 30.0 }, "eventStartTimestamp" : 1517867094797, "eventEndTimestamp" : 1517867094798, "eventType" : "GET", "elapsedTime" : 1 }, { "wmTezSessionInfo" : { "sessionId" : "157866e5-ed1c-4abd-9846-db76b91c1124", "poolName" : "pool1", "clusterPercent" : 70.0 }, "eventStartTimestamp" : 1517867139886, "eventEndTimestamp" : 1517867139887, "eventType" : "MOVE", "elapsedTime" : 1 }, { "wmTezSessionInfo" : { "sessionId" : "157866e5-ed1c-4abd-9846-db76b91c1124", "poolName" : null, "clusterPercent" : 0.0 }, "eventStartTimestamp" : 1517867144360, "eventEndTimestamp" : 1517867144360, "eventType" : "RETURN", "elapsedTime" : 0 } ], "appliedTriggers" : [ { "name" : "too_large_write_triger", "expression" : { "counterLimit" : { "limit" : 10240, "name" : "HDFS_BYTES_WRITTEN" }, "predicate" : "GREATER_THAN" }, "action" : { "type" : "MOVE_TO_POOL", "poolName" : "pool1" }, "violationMsg" : "Trigger { name: too_large_write_triger, expression: HDFS_BYTES_WRITTEN &gt; 10240, action: MOVE TO pool1 } violated. Current value: 5096345" } ], "subscribedCounters" : [ ], "currentCounters" : { }, "elapsedTime" : 55304}</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.wm.TestTrigger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.WmContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18638" opendate="2018-2-6 00:00:00" fixdate="2018-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Triggers for multi-pool move, failing to initiate the move event</summary>
      <description>Resource plan with multiple pools and trigger set to move job across those pools seems to be failing to do soResource plan:1: jdbc:hive2://ctr-e137-1514896590304-51538-&gt; show resource plan plan_2; INFO : Compiling command(queryId=hive_20180202220823_2fb8bca7-5b7a-48cf-8ff9-8d5f3548d334): show resource plan plan_2 INFO : Semantic Analysis Completed INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:line, type:string, comment:from deserializer)], properties:null) INFO : Completed compiling command(queryId=hive_20180202220823_2fb8bca7-5b7a-48cf-8ff9-8d5f3548d334); Time taken: 0.008 seconds INFO : Executing command(queryId=hive_20180202220823_2fb8bca7-5b7a-48cf-8ff9-8d5f3548d334): show resource plan plan_2 INFO : Starting task [Stage-0:DDL] in serial mode INFO : Completed executing command(queryId=hive_20180202220823_2fb8bca7-5b7a-48cf-8ff9-8d5f3548d334); Time taken: 0.196 seconds INFO : OK +----------------------------------------------------+ | line | +----------------------------------------------------+ | plan_2[status=ACTIVE,parallelism=null,defaultPool=pool2] | | + pool2[allocFraction=0.5,schedulingPolicy=default,parallelism=3] | | | trigger too_large_write_triger: if (HDFS_BYTES_WRITTEN &gt; 10kb) { MOVE TO pool1 } | | | mapped for default | | + pool1[allocFraction=0.3,schedulingPolicy=default,parallelism=5] | | | trigger slow_pool_trigger: if (ELAPSED_TIME &gt; 30000) { MOVE TO pool3 } | | + pool3[allocFraction=0.2,schedulingPolicy=default,parallelism=3] | | + default[allocFraction=0.0,schedulingPolicy=null,parallelism=4] | +----------------------------------------------------+ 8 rows selected (0.25 seconds)Workload Manager Events Summary from query run:INFO : { "queryId" : "hive_20180202213425_9633d7af-4242-4e95-a391-2cd3823e3eac", "queryStartTime" : 1517607265395, "queryEndTime" : 1517607321648, "queryCompleted" : true, "queryWmEvents" : [ { "wmTezSessionInfo" : { "sessionId" : "21f8a4ab-511e-4828-a2dd-1d5f2932c492", "poolName" : "pool2", "clusterPercent" : 50.0 }, "eventStartTimestamp" : 1517607269660, "eventEndTimestamp" : 1517607269661, "eventType" : "GET", "elapsedTime" : 1 }, { "wmTezSessionInfo" : { "sessionId" : "21f8a4ab-511e-4828-a2dd-1d5f2932c492", "poolName" : null, "clusterPercent" : 0.0 }, "eventStartTimestamp" : 1517607321663, "eventEndTimestamp" : 1517607321663, "eventType" : "RETURN", "elapsedTime" : 0 } ], "appliedTriggers" : [ { "name" : "too_large_write_triger", "expression" : { "counterLimit" : { "limit" : 10240, "name" : "HDFS_BYTES_WRITTEN" }, "predicate" : "GREATER_THAN" }, "action" : { "type" : "MOVE_TO_POOL", "poolName" : "pool1" }, "violationMsg" : null } ], "subscribedCounters" : [ "HDFS_BYTES_WRITTEN" ], "currentCounters" : { "HDFS_BYTES_WRITTEN" : 33306829 }, "elapsedTime" : 56284}From the Workload Manager Event Summary it could seen that the 'MOVE' event didn't happen though the limit for counter (10240) HDFS_BYTES_WRITTEN was exceeded</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18643" opendate="2018-2-7 00:00:00" fixdate="2018-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t check for archived partitions for ACID ops</summary>
      <description>This removes the slowness associated with pointless metastore calls when ACID update/delete queries affect a large number of partitions.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1865" opendate="2010-12-23 00:00:00" fixdate="2010-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>redo zookeeper hive lock manager</summary>
      <description>Instead of creating a flat hierarchy, create a hierarchy /database/table/partition1/partition2specifically to speed up show locks &lt;T&gt;</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18654" opendate="2018-2-8 00:00:00" fixdate="2018-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hiveserver2 specific HADOOP_OPTS environment variable</summary>
      <description>HIVE-2665 added support to include metastore specific HADOOP_OPTS variable. This is helpful in debugging especially if you want to add some jvm parameters to metastore's process. A similar setting for Hiveserver2 is missing and could be very helpful in debugging.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.hiveserver2.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18658" opendate="2018-2-8 00:00:00" fixdate="2018-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM: allow not specifying scheduling policy when creating a pool</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18659" opendate="2018-2-8 00:00:00" fixdate="2018-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add acid version marker to acid files/directories</summary>
      <description>add acid version marker to acid files so that we know which version of acid wrote the file</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
    </fixedFiles>
  </bug>
  <bug id="18660" opendate="2018-2-8 00:00:00" fixdate="2018-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PCR doesn&amp;#39;t distinguish between partition and virtual columns</summary>
      <description>As a result transforms a filter INPUT_FILE_NAME is not null; to false causing wrong results.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.boolexpr.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="18663" opendate="2018-2-9 00:00:00" fixdate="2018-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logged Spark Job Id contains a UUID instead of the actual id</summary>
      <description>We have logs like Spark Job&amp;#91;job-id&amp;#93; but the &amp;#91;job-id&amp;#93; is set to a UUID that is created by the RSC ClientProtocol. It should be pretty easy to print out the actual job id instead.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="18668" opendate="2018-2-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Really shade guava in ql</summary>
      <description>After HIVE-15393 a test started to fail in druid; after some investigation it turned out that ql doesn't shade it's guava artifact at all...because it shades 'com.google.guava' instead 'com.google.common'</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18675" opendate="2018-2-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make HIVE_LOCKS.HL_TXNID NOT NULL</summary>
      <description>In Hive 3.0 all statements that may need locks run in a transaction</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18686" opendate="2018-2-12 00:00:00" fixdate="2018-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Installation on Postgres and Oracle broken</summary>
      <description>HIVE-18614 broke the installation and upgrade on Postgres and Oracle. It calls Connection.setSchema in the JDBC driver. But the JDBC drivers for these databases don't support that call.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="18699" opendate="2018-2-13 00:00:00" fixdate="2018-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for duplicate partitions in HiveMetastore.exchange_partitions</summary>
      <description>The HiveMetastore.exchange_partitions method throws the following exception if the dest table already contains a partition with the same values as the partition to be exchanged.org.apache.hadoop.hive.metastore.api.MetaException: Insert of object"org.apache.hadoop.hive.metastore.model.MPartition@4e78fff5" using statement "INSERT INTO PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME,PART_NAME,SD_ID,TBL_ID) VALUES (?,?,?,?,?,?)" failed : The statement wasaborted because it would have caused a duplicate key value in a unique or primary key constraint or unique indexidentified by 'UNIQUEPARTITION' defined on 'PARTITIONS'.This use case could be handled better by checking if the partition already exists in dest table and if it is, throw a MetaException with a proper error message.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="187" opendate="2008-12-18 00:00:00" fixdate="2008-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ODBC driver</summary>
      <description>We need to provide the a small number of functions to get basic queryexecution and retrieval of results. This is based on the tutorial providedhere: http://www.easysoft.com/developer/languages/c/odbc_tutorial.htmlThe minimum set of ODBC functions required are:SQLAllocHandle - for environment, connection, statementSQLSetEnvAttrSQLDriverConnectSQLExecDirectSQLNumResultColsSQLFetchSQLGetDataSQLDisconnectSQLFreeHandleIf required the plan would be to do the following:1. generate c++ client stubs for thrift server2. implement the required functions in c++ by calling the c++ client3. make the c++ functions in (2) extern C and then use those in the odbcSQL* functions4. provide a .so (in linux) which can be used by the ODBC clients.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.include.thrift.TReflectionLocal.h</file>
      <file type="M">service.include.thrift.transport.TZlibTransport.h</file>
      <file type="M">service.include.thrift.transport.TTransportUtils.h</file>
      <file type="M">service.include.thrift.transport.TTransportException.h</file>
      <file type="M">service.include.thrift.transport.TTransport.h</file>
      <file type="M">service.include.thrift.transport.TSocketPool.h</file>
      <file type="M">service.include.thrift.transport.TSocket.h</file>
      <file type="M">service.include.thrift.transport.TServerTransport.h</file>
      <file type="M">service.include.thrift.transport.TServerSocket.h</file>
      <file type="M">service.include.thrift.transport.THttpClient.h</file>
      <file type="M">service.include.thrift.transport.TFileTransport.h</file>
      <file type="M">service.include.thrift.TProcessor.h</file>
      <file type="M">service.include.thrift.TLogging.h</file>
      <file type="M">service.include.thrift.Thrift.h</file>
      <file type="M">service.include.thrift.server.TThreadPoolServer.h</file>
      <file type="M">service.include.thrift.server.TThreadedServer.h</file>
      <file type="M">service.include.thrift.server.TSimpleServer.h</file>
      <file type="M">service.include.thrift.server.TServer.h</file>
      <file type="M">service.include.thrift.server.TNonblockingServer.h</file>
      <file type="M">service.include.thrift.protocol.TProtocolException.h</file>
      <file type="M">service.include.thrift.protocol.TProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TOneWayProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TDenseProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TDebugProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TBinaryProtocol.h</file>
      <file type="M">service.include.thrift.processor.StatsProcessor.h</file>
      <file type="M">service.include.thrift.processor.PeekProcessor.h</file>
      <file type="M">service.include.thrift.fb303.ServiceTracker.h</file>
      <file type="M">service.include.thrift.fb303.fb303.types.h</file>
      <file type="M">service.include.thrift.fb303.FacebookService.h</file>
      <file type="M">service.include.thrift.fb303.FacebookBase.h</file>
      <file type="M">service.include.thrift.config.h</file>
      <file type="M">service.include.thrift.concurrency.Util.h</file>
      <file type="M">service.include.thrift.concurrency.TimerManager.h</file>
      <file type="M">service.include.thrift.concurrency.ThreadManager.h</file>
      <file type="M">service.include.thrift.concurrency.Thread.h</file>
      <file type="M">service.include.thrift.concurrency.PosixThreadFactory.h</file>
      <file type="M">service.include.thrift.concurrency.Mutex.h</file>
      <file type="M">service.include.thrift.concurrency.Monitor.h</file>
      <file type="M">service.include.thrift.concurrency.Exception.h</file>
      <file type="M">service.if.hive.service.thrift</file>
      <file type="M">serde.if.serde.thrift</file>
      <file type="M">serde.build.xml</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1870" opendate="2010-12-28 00:00:00" fixdate="2010-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestRemoteHiveMetaStore.java accidentally deleted during commit of HIVE-1845</summary>
      <description>TestRemoteHiveMetaStore.java was removed by the commit of HIVE-1845. This change was not part ofthe patch for HIVE-1845.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18713" opendate="2018-2-14 00:00:00" fixdate="2018-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize: Transform IN clauses to = when there&amp;#39;s only one element</summary>
      <description>(col1) IN (col2) can be transformed to (col1) = (col2), to avoid the hash-set implementation.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.simple.select.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.simple.select.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.simple.select.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="18717" opendate="2018-2-15 00:00:00" fixdate="2018-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid transitive dependency on jetty 6.x</summary>
      <description>Although Hive is using jetty 9.3, transitive dependencies bring in 6.2.x which should be avoided.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18718" opendate="2018-2-15 00:00:00" fixdate="2018-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integer like types throws error when there is a mismatch</summary>
      <description>If a value is saved with long type and read as int type it results inFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedListColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="18721" opendate="2018-2-15 00:00:00" fixdate="2018-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket Map Join : Handle empty buckets</summary>
      <description>Bucket Map Join needs to bluff Tez by sending empty task list for DataMovementEvent for those buckets for which there is no data.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18745" opendate="2018-2-19 00:00:00" fixdate="2018-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix MetaStore creation in tests, so multiple MetaStores can be started on the same machine</summary>
      <description>janulatha fixed the problem, when multiple MetaStore tests are started on the same machine, then they tried to reserve the same port. This caused flakiness in the MetaStore tests run with the ptest framework. See:HIVE-18147I reviewed theHIVE-17980, and tried to make sure, that the fix remains in every codepath. I was unsuccessfulin it.This Jira aims to go through the MetaStore tests, and make sure all of them is using the startMetaStoreWithRetry method so the different tests will not cause each other to fail. Also there were clashes not only in port numbers, but warehouse directories as well, so this Jira should fix that also.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerWithOldConf.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.RemoteMetaStoreForTests.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.StorageBasedMetastoreTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.metastore.security.TestHadoopAuthBridge23.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="18748" opendate="2018-2-20 00:00:00" fixdate="2018-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename table impacts the ACID behavior as table names are not updated in meta-tables.</summary>
      <description>ACID implementation uses metatables such as TXN_COMPONENTS, COMPLETED_TXN_COMPONENTS, COMPACTION_QUEUE, COMPLETED_COMPCTION_QUEUE etc to manage ACID operations.Per table write ID implementation (HIVE-18192) introduces couple of metatables such as NEXT_WRITE_ID and TXN_TO_WRITE_ID to manage write ids allocated per table.Now, when we rename any tables, it is necessary to update the corresponding table names in these metatables as well. Otherwise, ACID table operations won't work properly.Since, this change is significant and have other side-effects, we propose to disable rename tables on ACID tables until a fixisfigured out.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.AcidEventListener.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnConcatenate.java</file>
    </fixedFiles>
  </bug>
  <bug id="18751" opendate="2018-2-20 00:00:00" fixdate="2018-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID table scan through get_splits UDF doesn&amp;#39;t receive ValidWriteIdList configuration.</summary>
      <description>Per table write ID (HIVE-18192) have replaced global transaction ID with write ID to version data files in ACID/MM tables,To ensure snapshot isolation, need to generate ValidWriteIdList for the given txn/table and use it when scan the ACID/MM tables.In case of get_splits UDF which runs on ACID table scan query won't receive it properly through configuration (hive.txn.tables.valid.writeids)and hence throws exception.TestAcidOnTez.testGetSplitsLocks is the test failing for the same. Need to fix it.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug id="18754" opendate="2018-2-20 00:00:00" fixdate="2018-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL STATUS should support &amp;#39;with&amp;#39; clause</summary>
      <description>We have support for "WITH" clause in "REPL LOAD" command, but we don't have that for "REPL STATUS" command. With the cloud replication model , HiveServer2 is only running in the source on-prem cluster. "REPL LOAD"'s with clause is currently used to pass the remote cloud clusters metastore uri, using "hive.metastore.uri" parameter.Once "REPL LOAD" is run, "REPL STATUS" needs to be run to determine where the next incremental replication should start from. Since "REPL STATUS" is also going to run on source cluster, we need to add support for the "WITH" clause for it.We should also change the privilege required for "REPL STATUS" command to what is required by "REPL LOAD" command as now arbitrary configs can be set for "REPL STATUS" using the WITH clause.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="18757" opendate="2018-2-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO for text fails for empty files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="18764" opendate="2018-2-21 00:00:00" fixdate="2018-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ELAPSED_TIME resource plan setting is not getting honored</summary>
      <description>Trigger validation for ELAPSED_TIME counter should happen even if session is not created. Currently ELAPSED_TIME counter is populated only after session creation but a query can be waiting to get a session for a long time by the time trigger might have been violated.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.WmContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18769" opendate="2018-2-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Disable vectorization of key-less outer joins</summary>
      <description>Left &amp; Right outer joins without keys are valid in SQL and they have different semantics from cross-productscreate temporary table foo(x int) stored as orc;insert into foo values(1),(2);create temporary table bar(y int) stored as orc;select count(*) from bar right outer join foo; -- = 2select count(*) from bar, foo; -- = 0 canSpecializeMapJoin should bail on these cases.</description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18771" opendate="2018-2-22 00:00:00" fixdate="2018-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor tests, so only 1 MetaStore instance will be started per test class and test configuration</summary>
      <description>It takes too much time to start a MetaStore for every test instance.To reduce the running time, start only 1 MetaStore instance, and run every test against this instance</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesList.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesGetExists.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestListPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetTableMeta.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetListIndexes.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestFunctions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitionsFromPartSpec.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddAlterDropIndexes.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.MetaStoreFactoryForTests.java</file>
    </fixedFiles>
  </bug>
  <bug id="18777" opendate="2018-2-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Authorization interface to support information_schema integration with external authorization</summary>
      <description>HIVE-1010 added support for information_schema. However, the authorization information is not integrated when another project such as Ranger is used to do the authorization.We need to add API which Ranger/Sentry can implement, so that it is possible to retrieve authorization policy information from them.The existing API only supports checking if user has a permission on an object and can't be used to retrieve policy details.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AbstractHiveAuthorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18793" opendate="2018-2-24 00:00:00" fixdate="2018-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Round udf should support variable as second argument</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.round.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.round.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.java</file>
    </fixedFiles>
  </bug>
  <bug id="18794" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl load "with" clause does not pass config to tasks for non-partition tables</summary>
      <description>Miss one scenario inHIVE-18626.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18796" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix TestSSL</summary>
      <description>broken by HIVE-18203</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.hadoop.hive.jdbc.SSLTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="188" opendate="2008-12-19 00:00:00" fixdate="2008-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive CLI should quit when stdout is closed</summary>
      <description>bin/hive -e 'select * from a_big_table;' | headThe process does not end after printing out the first 10 lines.This is probably because System.out.println is not checking errors. We need to manually call boolean System.out.checkError() to know if the output is closed or not.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18805" opendate="2018-2-26 00:00:00" fixdate="2018-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ConstantPropagate before stats annotation</summary>
      <description>this seems to also make a few more optimizations identify more cases</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce.groupby.duplicate.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="18819" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Optimize IF statement expression evaluation of THEN/ELSE</summary>
      <description>Currently, all the rows of a batch are evaluated for the THEN and ELSE expressions even though only a value from one of them is needed for any particular row.</description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf.adaptor.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.udf.adaptor.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">data.files.student.2.lines</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18824" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ValidWriteIdList config should be defined on tables which has to collect stats after insert</summary>
      <description>In HIVE-18192, per table write ID was introduced where snapshot isolation is built using ValidWriteIdList on tables which are read with in a txn. ReadEntity list is referred to decide which table is read within a txn.For insert operation, table will be foundonly in WriteEntity, but the table is read to collect stats.So, it is needed to build the ValidWriteIdList for tables/partition part of WriteEntity as well.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18825" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define ValidTxnList before starting query optimization</summary>
      <description>Consider a set of tables used by a materialized view where inserts happened after the materialization was created. To compute incremental view maintenance, weneed tobe able to filter onlynew rowsfrom those base tables. That can be done by inserting afilter operator with condition e.g. ROW&amp;#95;&amp;#95;ID.transactionId &lt; highwatermark andROW&amp;#95;&amp;#95;ID.transactionId NOT IN(&lt;open txns&gt;) on top of the MVs query definition and triggering the rewriting (which should in turn produce a partial rewriting). However, to do that, we need to have a value for ValidTxnList during query compilation so we know the snapshot that we are querying.This patch aims to generate ValidTxnList before query optimization. There should not be any visible changes for end user.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18828" opendate="2018-2-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve error handling for codecs in LLAP IO</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="18835" opendate="2018-3-1 00:00:00" fixdate="2018-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC standalone jar download link in ambari</summary>
      <description>Let HS2 offer the file for download, so that Ambari can create link on it.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="18837" opendate="2018-3-1 00:00:00" fixdate="2018-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a flag and disable some object pools in LLAP until further testing</summary>
      <description>There appears to be some subtle concurrency issue in FixedSizedObjectPool that happens with multiple consumers where some object may be retrieved twice. Unfortunately running a load test for hour(s) does not trigger it for me and overall it happens extremely rarely on non-specific tests; adding debug info at this level is a little bit difficult to determine how it could have happened and interlocked operations in the trace may actually eliminate the issue. I suspect it has something to do with aggressive assumptions made for locking and array elements and the memory model. Maybe that can be simplified without much perf loss.Anyway, for now we will disable the pools where multiple consumers use them.Need to test perf to see if these two pools even matter; if so, we can simplify the model as per above or debug the issue in some way.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18842" opendate="2018-3-2 00:00:00" fixdate="2018-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLUSTERED ON/DISTRIBUTED ON+SORTED ON support for materialized views</summary>
      <description>We should support defining a CLUSTERED ON/DISTRIBUTED ON+SORTED ON specification for materialized views. The syntax should be extended as follows:CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] [CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)] -- NEW! [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] AS select_statement;</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.view.CreateViewDesc.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="18848" opendate="2018-3-2 00:00:00" fixdate="2018-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve readability of filter conditions in explain plan when CBO is run</summary>
      <description>CBO might return comparison operands in any non-deterministic order. Try to show &lt;reference&gt; &lt;cmp&gt; &lt;literal&gt; when possible, i.e., c &lt; 10 rather than 10 &gt; c.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18861" opendate="2018-3-5 00:00:00" fixdate="2018-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>druid-hdfs-storage is pulling in hadoop-aws-2.7.x and aws SDK, creating classpath problems on hadoop 3.x</summary>
      <description>druid-hdfs-storage JAR is transitively pulling in hadoop-aws JAR 2.7.3, which creates classpath problems as a set of aws-sdk 1.10.77 JARs get on the CP, even with Hadoop 3 &amp; its move to a full aws-sdk-bundle JAR.Two options exclude the dependency force it up to whatever ${hadoop.version} is, so make it consistent</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18864" opendate="2018-3-5 00:00:00" fixdate="2018-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ValidWriteIdList snapshot seems incorrect if obtained after allocating writeId by current transaction.</summary>
      <description>For multi-statement txns, it is possible that write on a table happens after a read. Let's see the below scenario. Committed txn=9 writes on table T1 with writeId=5. Opentxn=10.ValidTxnList(open:null, txn_HWM=10), Read table T1 from txn=10. ValidWriteIdList(open:null, write_HWM=5). Open txn=11, writes on table T1 with writeid=6. Read table T1 from txn=10. ValidWriteIdList(open:null, write_HWM=5). Write table T1 from txn=10 with writeId=7. Read table T1 from txn=10. ValidWriteIdList(open:null, write_HWM=7).  This read will able to see rows added by txn=11 which is still open.So, it is needed to rebuild the open/aborted list of ValidWriteIdList based on txn_HWM. Any writeId allocated by txnId &gt;txn_HWM should be marked as open. In this example,ValidWriteIdList(open:6, write_HWM=7) should be generated.cc ekoifman, thejas</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.050-HIVE-18192.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18866" opendate="2018-3-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin and analyze: Implement a Long -&gt; Hash64 vector fast-path</summary>
      <description>A significant amount of CPU is wasted with JMM restrictions on byte[] arrays.To transform from one Long -&gt; another Long, this goes into a byte[] array, which shows up as a hotspot.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.common.ndv.hll.HyperLogLog.java</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18883" opendate="2018-3-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add findbugs to yetus pre-commit checks</summary>
      <description>We should enable FindBugs for our YETUS pre-commit checks, this will help overall code quality and should decrease the overall number of bugs in Hive.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">dev-support.yetus-wrapper.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18888" opendate="2018-3-7 00:00:00" fixdate="2018-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace synchronizedMap with ConcurrentHashMap</summary>
      <description>There are a bunch of places that use Collections.synchronizedMap instead of ConcurrentHashMap which are better. We should search/replace the uses.</description>
      <version>2.3.3,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.java</file>
    </fixedFiles>
  </bug>
  <bug id="18889" opendate="2018-3-7 00:00:00" fixdate="2018-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update all parts of Hive to use the same Guava version</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1889" opendate="2011-1-5 00:00:00" fixdate="2011-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option (hive.index.compact.file.ignore.hdfs) to ignore HDFS location stored in index files.</summary>
      <description>In the current code, we use the whole hdfs path in index files to choose splits to run mapreduce job.If the data got moved or the name of the cluster got changed, the index data should still be usable.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18899" opendate="2018-3-7 00:00:00" fixdate="2018-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate FetchWork required for each query that uses the results cache</summary>
      <description>gopalv found issues when running lots of concurrent queries against HS2 with the query cache. Looks like the FetchWork held by the results cache cannot be shared between multiple queries because it contains a ListSinkOperator that is used to hold the results of a fetch.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="18909" opendate="2018-3-8 00:00:00" fixdate="2018-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metrics for results cache</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="18910" opendate="2018-3-8 00:00:00" fixdate="2018-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate to Murmur hash for shuffle and bucketing</summary>
      <description>Hive uses JAVA hash which is not as good as murmur for better distribution and efficiency in bucketing a table.Migrate to murmur hash but still keep backward compatibility for existing users so that they dont have to reload the existing tables.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.bucket.many.q</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.truncate.column.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.shared.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.nested.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.union.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.delimited.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.db.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.alter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.3.exim.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.2.exim.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.array.null.element.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.named.column.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.insert.into.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skiphf.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.capacity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.count.distinct.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.column.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.multi.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.except.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.with.different.encryption.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.nonascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.defaultformats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.infinity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.num.reducers.acid2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.num.reducers2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.num.reducers.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.basicstat.partval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.excludeHadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.col.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.file.format.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.dynpart.hashjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.update.delete.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.num.reducers.acid2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.num.reducers2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.num.reducers.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestBucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.table.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.directory.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.table.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSerializedImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnAddPartition.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.queries.clientpositive.archive.excludeHadoop20.q</file>
    </fixedFiles>
  </bug>
  <bug id="18915" opendate="2018-3-8 00:00:00" fixdate="2018-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better client logging when a HoS session can&amp;#39;t be opened</summary>
      <description>Users just get a FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session &amp;#91;id&amp;#93; when a HoS session can't be opened, would be better</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="18916" opendate="2018-3-8 00:00:00" fixdate="2018-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SparkClientImpl doesn&amp;#39;t error out if spark-submit fails</summary>
      <description>If spark-submit returns a non-zero exit code, SparkClientImpl will simply log the exit code, but won't throw an error. Eventually, the connection timeout will get triggered and an exception like Timed out waiting for client connection will be logged, which is pretty misleading.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkSubmitSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcServer.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.AbstractSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QOutProcessor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18917" opendate="2018-3-8 00:00:00" fixdate="2018-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add spark.home to hive.conf.restricted.list</summary>
      <description>Add spark.home to hive.conf.restricted.list so its not settable by users.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestRestrictedList.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18953" opendate="2018-3-14 00:00:00" fixdate="2018-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement CHECK constraint</summary>
      <description>Implement column level CHECK constraint</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MConstraint.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableValidWriteIds.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SchemaVersion.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRow.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Materialization.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDefaultConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddForeignKeyRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddNotNullConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPrimaryKeyRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddUniqueConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CreationMetadata.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FindSchemasByColsResp.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="18962" opendate="2018-3-14 00:00:00" fixdate="2018-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add WM task state to Tez AM heartbeat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="18963" opendate="2018-3-14 00:00:00" fixdate="2018-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Provide an option to simplify beeline usage by supporting default and named URL for beeline</summary>
      <description>Currently, after opening Beeline CLI, the user needs to supply a connection string to use the HS2 instance and set up the jdbc driver. Since we plan to replace Hive CLI with Beeline in future (HIVE-10511), it will help the usability if the user can simply type beeline and get start the hive session. The jdbc url can be specified in a beeline-site.xml (which can contain other named jdbc urls as well, and they can be accessed by something like: beeline -c namedUrl. The use of beeline-site.xml can also be potentially expanded later if needed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.BeelineWithHS2ConnectionFileTestBase.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.hs2connection.TestUserHS2ConnectionFileParser.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.HS2ConnectionFileUtils.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.HS2ConnectionFileParser.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.BeelineHS2ConnectionFileParseException.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="18967" opendate="2018-3-15 00:00:00" fixdate="2018-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone metastore SQL upgrade scripts do not properly set schema version</summary>
      <description>The new combined upgrade scripts for Hive 2.3 to 3.0 transition do not properly set the schema version after they have completed the upgrade.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18968" opendate="2018-3-15 00:00:00" fixdate="2018-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: report guaranteed tasks count in AM registry to check for consistency</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistry.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.QueryAllocationManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.TezAmRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.TezAmInstance.java</file>
    </fixedFiles>
  </bug>
  <bug id="18979" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable AggregateReduceFunctionsRule from Calcite</summary>
      <description>The rule rewrites aggregate functions into simpler ones. It should help us gathering more common sub-expressions for a given plan, and it should have a positive effect on materialized view rewriting coverage too.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.binarysetfunctions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.input.format.excludes.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.types.non.dictionary.encoding.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets3.dec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumEmptyIsZero.java</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby3.map.skew.q</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.dist.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.dist.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.input.format.excludes.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18982" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a CLI option to manually trigger failover</summary>
      <description>HIVE-18281 added active-passive HA. There might be a administrative need to trigger a manual failover of HS2 Active server. Add command line tool to view list of all HS2 instances and trigger manual failover (only under force mode). The clients currently connected to active HS2 will be closed. In future, more options to existing clients connections can be handled via configs/options (like wait until timeout, wait until current sessions are closed etc.)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServerPam.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2Peers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2LeadershipStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistry.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">bin.ext.hiveserver2.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18984" opendate="2018-3-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make time window configurable per materialized view</summary>
      <description>Currently, hive.materializedview.rewriting.time.window can be used to specify a time window after which outdated materialized views become invalid for automatic query rewriting (default value is 0). We would like to be able to specify this property for each individual materialized view too via tblproperties.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="18988" opendate="2018-3-19 00:00:00" fixdate="2018-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bootstrap replication of ACID tables</summary>
      <description>Bootstrapping of ACID tables, need special handling to replicate a stable state of data. If ACID feature enables, then perform bootstrap dump for ACID tables with in read txn. -&gt; Dump table/partition metadata. -&gt; Get the list of valid data files for a table using same logic as read txn do. -&gt; Dump latest ValidWriteIdListas per current read txn. Setthe valid last replication state such that itdoesn't miss any open txn started after triggering bootstrap dump. If any txns on-going which was opened before triggering bootstrap dump, then it is not guaranteed that ifopen_txn event captured for these txns. Also, if these txns are opened for streaming ingest case, then dumped ACID table data may include data of open txns which impact snapshot isolation at target. To avoid that, bootstrap dump shouldwait for timeout (new configuration: hive.repl.bootstrap.dump.open.txn.timeout). After timeout, just force abort those txns and continue. If any txns force aborted belongs to a streaming ingest case, then dumped ACID table data may have aborted data too. So, it is necessary to replicate the aborted write ids to target to mark those data invalid forany readers.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CreationMetadata.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidReadTxnList.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableValidWriteIds.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SchemaVersion.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Materialization.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FindSchemasByColsResp.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplTxnTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplTxnWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.TableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.BootStrapReplicationSpecFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FunctionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.OpenTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="18992" opendate="2018-3-19 00:00:00" fixdate="2018-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable synthetic file IDs by default in LLAP</summary>
      <description>The file IDs are much more reliable than they were initially (hash+len+date instead of just one hash of everything) so they should be enabled by default.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18994" opendate="2018-3-19 00:00:00" fixdate="2018-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle client connections on failover</summary>
      <description>When leader failover happens (either automatically or manually), tez sessions are closed. But client connections are not. We need to close the client connections explicitly so that workload manager revokes all the guaranteed slots and upon reconnection client will connect to active HS2 instance (this is to avoid clients reusing the same connection and submitting queries to passive HS2). In future, some timeout or other policies (may be WM will run everything speculatively) can be added.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
    </fixedFiles>
  </bug>
  <bug id="1900" opendate="2011-1-7 00:00:00" fixdate="2011-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>a mapper should be able to span multiple partitions</summary>
      <description>Currently, a mapper only spans a single partition which creates a problem in the presence of manysmall partitions (which is becoming a common usecase in facebook).If the plan is the same, a mapper should be able to span files across multiple partitions</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19007" opendate="2018-3-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support REPL LOAD from primary using replica connection configurations received through WITH clause.</summary>
      <description>Needto support runningREPL LOAD command from primary for different use-cases such as Cloud replication (for efficient use of cloud resources) or workload management.To achieve this, WITH clause of REPL LOAD lets user to pass Hive configssuch ashive.metastore.warehouse.dir,hive.metastore.uris,hive.repl.replica.functions.root.dir etc, which can be used to establish connection with replica warehouse.The configs received from WITH clause of REPL LOAD are not set properly (due to changes by HIVE-18716) to the tasks created.It is also required to re-get the Hive db object if the configs are changed.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="19008" opendate="2018-3-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Spark session id logging</summary>
      <description>HoS users have two session ids, one id for the Hive session and another id for the Spark session, both are UUIDs.I think some improvements could be made here:The Spark session id could just be a counter that is incremented for each new Spark session within a Hive session. Each Spark session is still globally identifiable by its associated Hive session id + its own counter. This may make more sense since the Hive session - Spark session has a 1-to-many relationship, as in a single Hive session can contain multiple Spark sessions, and each Spark session must belong to a Hive session.Furthermore, we should include both the Hive session id and Spark session id in the console logs + the Spark Web UI.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19009" opendate="2018-3-21 00:00:00" fixdate="2018-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain and use runtime statistics during hs2 lifetime</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestCounterMapping.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReOptimizePlugin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReExecutionOverlayPlugin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.IReExecutionPlugin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.SimpleRuntimeStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.EmptyStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19012" opendate="2018-3-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support builds for ARM and PPC arch</summary>
      <description>Hivestandalone metastore usesprotoc-jar-maven-plugin 3.5.1.1 which supports downloading from maven repo. Artifact download should be supported for ARM and PPCarchitecture since some protobuf versions do not exist in ARM/PPC.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19013" opendate="2018-3-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some minor build issues in storage-api</summary>
      <description>Currently, the storage-api tests complain that there isn't a log4j2.xml and the javadoc fails.</description>
      <version>None</version>
      <fixedVersion>storage-2.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidWriteIdList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidCompactorWriteIdList.java</file>
    </fixedFiles>
  </bug>
  <bug id="19014" opendate="2018-3-21 00:00:00" fixdate="2018-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>utilize YARN-8028 (queue ACL check) in Hive Tez session pool</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLoggedInUser.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19016" opendate="2018-3-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization and Parquet: Disable vectorization for nested complex types</summary>
      <description>Original title:Vectorization and Parquet: When vectorized, parquet_nested_complex.q produces RuntimeException: Unsupported type usedAdding "SET hive.vectorized.execution.enabled=true;" to parquet_nested_complex.q triggers this call stack:Caused by: java.lang.RuntimeException: Unsupported type used in list:array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;int&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkListColumnSupport(VectorizedParquetRecordReader.java:589) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:525) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:440) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:401) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:353) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:92) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:360) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]FYI: vihangk1</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19018" opendate="2018-3-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline -e now requires semicolon even when used with query from command line</summary>
      <description>Right now if you execute beeline -u "jdbc:hive2://" -e "select 3", beeline console will wait for you to enter ';". It's a regression from the old behavior.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestCommands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="19021" opendate="2018-3-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM counters are not properly propagated from LLAP to AM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.WmFragmentCounters.java</file>
    </fixedFiles>
  </bug>
  <bug id="19024" opendate="2018-3-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Disable complex type constants for VectorUDFAdaptor</summary>
      <description>Currently, complex type constants are not detected and cause execution failures.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.udf.inline.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.context.aware.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19032" opendate="2018-3-22 00:00:00" fixdate="2018-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Disable GROUP BY aggregations with DISTINCT</summary>
      <description>Vectorized GROUP BY does not support DISTINCT aggregation functions.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.groupby.grouping.sets.grouping.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.groupby.cube1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.rollup1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets.grouping.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.cube1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19033" opendate="2018-3-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to purge LLAP IO cache</summary>
      <description>Provide an API endpoint that will trigger purging of LLAP IO cache. Also CLI tool to invoke the endpoint of all LLAP daemons.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapClusterResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapCacheResourceProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapIoMemoryServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.CacheContentsTracker.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.protocol.LlapManagementProtocolPB.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapIo.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19042" opendate="2018-3-24 00:00:00" fixdate="2018-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>set MALLOC_ARENA_MAX for LLAP</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19046" opendate="2018-3-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor the common parts of the HiveMetastore add_partition_core and add_partitions_pspec_core methods</summary>
      <description>This is a follow-up Jira of the HIVE-18696review.The biggest part of these methods use the same code. It would make sense to move this code part to a common method.This code is almost the same in the two methods: List&lt;Future&lt;Partition&gt;&gt; partFutures = Lists.newArrayList(); final Table table = tbl; for (final Partition part : parts) { if (!part.getTableName().equals(tblName) || !part.getDbName().equals(dbName)) { throw new MetaException("Partition does not belong to target table " + dbName + "." + tblName + ": " + part); } boolean shouldAdd = startAddPartition(ms, part, ifNotExists); if (!shouldAdd) { existingParts.add(part); LOG.info("Not adding partition " + part + " as it already exists"); continue; } final UserGroupInformation ugi; try { ugi = UserGroupInformation.getCurrentUser(); } catch (IOException e) { throw new RuntimeException(e); } partFutures.add(threadPool.submit(new Callable&lt;Partition&gt;() { @Override public Partition call() throws Exception { ugi.doAs(new PrivilegedExceptionAction&lt;Object&gt;() { @Override public Object run() throws Exception { try { boolean madeDir = createLocationForAddedPartition(table, part); if (addedPartitions.put(new PartValEqWrapper(part), madeDir) != null) { // Technically, for ifNotExists case, we could insert one and discard the other // because the first one now "exists", but it seems better to report the problem // upstream as such a command doesn't make sense. throw new MetaException("Duplicate partitions in the list: " + part); } initializeAddedPartition(table, part, madeDir); } catch (MetaException e) { throw new IOException(e.getMessage(), e); } return null; } }); return part; } })); } try { for (Future&lt;Partition&gt; partFuture : partFutures) { Partition part = partFuture.get(); if (part != null) { newParts.add(part); } } } catch (InterruptedException | ExecutionException e) { // cancel other tasks for (Future&lt;Partition&gt; partFuture : partFutures) { partFuture.cancel(true); } throw new MetaException(e.getMessage()); }</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitionsFromPartSpec.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitions.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="19047" opendate="2018-3-26 00:00:00" fixdate="2018-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only the first init file is interpreted</summary>
      <description>I've passed multiple -i optionsto beeline; and I've expected to load both of them...but unfortunately it only parsed the first file (and ignored the second entirely)I think it would be better to: either reject the command if it has multiple "-i" given or load all "-i" scripts</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="19048" opendate="2018-3-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initscript errors are ignored</summary>
      <description>I've been running some queries for a while when I've noticed that my initscript has an error; and beeline stops interpreting the initscript after encountering the first error.echo 'invalid;' &gt; init.sqlecho 'select 1;' &gt; s1.sqlbeeline -u jdbc:hive2://localhost:10000/ -n hive -i init.sql -f s1.sql [...]Running init script init.sql0: jdbc:hive2://localhost:10000/&gt; invalid;Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'invalid' '&lt;EOF&gt;' '&lt;EOF&gt;' (state=42000,code=40000)0: jdbc:hive2://localhost:10000/&gt; select 1;[...]$ echo $?0</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="19049" opendate="2018-3-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Alter table add columns for Druid</summary>
      <description>Add support for Alter table add columns for Druid. Currently it is not supported and throws exception.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.non.native.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">hbase-handler.src.test.results.negative.hbase.ddl.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19052" opendate="2018-3-26 00:00:00" fixdate="2018-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Disable Vector Pass-Thru SMB MapJoin in the presence of old-style MR FilterMaps</summary>
      <description>Pass-Thru VectorSMBMapJoinOperatorwas not designed to handle old-style MR FilterMaps.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19054" opendate="2018-3-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Function replication shall use "hive.repl.replica.functions.root.dir" as root</summary>
      <description>It's wrongly use fs.defaultFS as the root, ignore "hive.repl.replica.functions.root.dir" definition, thus prevent replicating to cloud destination.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19055" opendate="2018-3-26 00:00:00" fixdate="2018-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM alter may fail if the name is not changed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMNullableResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19057" opendate="2018-3-26 00:00:00" fixdate="2018-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query result caching cannot be disabled by client</summary>
      <description>HIVE-18513 introduced query results caching along with some toggles to control enabling/disabling it. We should whiltelist the following configs so that the end user can dynamically control it in their session.hive.query.results.cache.enabledhive.query.results.cache.wait.for.pending.results</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19059" opendate="2018-3-27 00:00:00" fixdate="2018-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DEFAULT keyword with INSERT and UPDATE</summary>
      <description>Support DEFAULT keyword in INSERT e.g.INSERT INTO TABLE t values (DEFAULT, DEFAULT)or with UPDATEUPDATE TABLE t SET col1=DEFAULT WHERE col2 &gt; 4</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19061" opendate="2018-3-27 00:00:00" fixdate="2018-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM needs to output an event for allocation update</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19081" opendate="2018-3-29 00:00:00" fixdate="2018-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add partition should prevent loading acid files</summary>
      <description>similar to HIVE-19029{{Alter Table T add Partition ...} T is acid should check to make sure input files were not copied from another Acid table, i.e. make sure the files don't have Acid metadata columns.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnAddPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="19089" opendate="2018-4-2 00:00:00" fixdate="2018-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create/Replicate Allocate write-id event</summary>
      <description>EVENT_ALLOCATE_WRITE_IDSource Warehouse: Create new event type EVENT_ALLOCATE_WRITE_ID with related message format etc. Capture this event when allocate a table write ID from the sequence table by ACID operation. Repl dump should read this event from EventNotificationTable and dump the message.Target Warehouse: Repl load should read the event from the dump and get the message. Validate if source txn ID from the event is there in the source-target txn ID map. If not there, just noop the event. If valid, then Allocate table write ID from sequence tableExtend listener notify event API to add two new parameter , dbconn and sqlgenerator to addthe events to notification_log table within the same transaction</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.EventMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.events.ListenerEvent.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.events.CommitTxnEvent.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.events.AbortTxnEvent.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplTxnTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplTxnWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.OpenTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CreationMetadata.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FindSchemasByColsResp.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Materialization.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SchemaVersion.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="19091" opendate="2018-4-3 00:00:00" fixdate="2018-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Hive 3.0.0 Release] Rat check failure fixes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19092" opendate="2018-4-3 00:00:00" fixdate="2018-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Somne improvement in bin shell scripts</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.lineage.sh</file>
      <file type="M">bin.ext.fixacidkeyindex.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1912" opendate="2011-1-13 00:00:00" fixdate="2011-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Double escaping special chars when removing old partitions in rmr</summary>
      <description>If a partition column value contains special characters such as ':', it will be escaped to '%3A' in the partition path. However in FsShell.rmr(oldPath.toUri().toString()), toUri() will double escape '%' to '%25'. This will make the removal fail.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19120" opendate="2018-4-5 00:00:00" fixdate="2018-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>catalog not properly set for some tables in SQL upgrade scripts</summary>
      <description>A catalog column is added to the PARTITION_EVENTS and NOTIFICATION_LOG but the upgrade scripts do not include an UPDATE statement to set this to the default value.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="19123" opendate="2018-4-5 00:00:00" fixdate="2018-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestNegativeCliDriver nopart_insert failing</summary>
      <description>Looks like HIVE-19083 caused this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19124" opendate="2018-4-6 00:00:00" fixdate="2018-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement a basic major compactor for MM tables</summary>
      <description>For now, it will run a query directly and only major compactions will be supported.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidReaderWriteIdList.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19128" opendate="2018-4-7 00:00:00" fixdate="2018-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for spark perf tests</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query90.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19133" opendate="2018-4-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 WebUI phase-wise performance metrics not showing correctly</summary>
      <description>The query specific WebUI metrics (go to drilldown-&gt; performance logging) are not showing up in the correct phase and are often mixed up.Attaching screenshot.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithMr.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="19135" opendate="2018-4-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need tool to allow admins to create catalogs and move existing dbs to catalog during upgrade</summary>
      <description>As part of upgrading to Hive 3 admins may wish to create new catalogs and move some existing databases into those catalogs. We can do this by adding options to schematool. This guarantees that only admins can do these operations.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19137" opendate="2018-4-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>orcfiledump doesn&amp;#39;t print hive.acid.version value</summary>
      <description>HIVE-18659 added hive.acid.version in the file footer. orcfiledump prints something like User Metadata: hive.acid.key.index=1,536870912,1; hive.acid.stats=2,0,0 hive.acid.version=probably because public static void setAcidVersionInDataFile(Writer writer) { //so that we know which version wrote the file ByteBuffer bf = ByteBuffer.allocate(4).putInt(ORC_ACID_VERSION); bf.rewind(); //don't ask - some ByteBuffer weridness. w/o this, empty buffer is written writer.addUserMetadata(ACID_VERSION_KEY, bf); }use UTF8.encode()) instead</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19138" opendate="2018-4-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Results cache: allow queries waiting on pending cache entries to check cache again if pending query fails</summary>
      <description>HIVE-18846 allows the results cache to refer to currently executing queries so that another query can wait for these results to become ready in the results cache. If the pending query fails then Hive will automatically skip the cache and do the full query compilation. Make a fix here so that if the pending query fails, Hive will still try to check the cache again in case the cache has another cached/pending result that can be used to answer the query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19141" opendate="2018-4-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestNegativeCliDriver insert_into_notnull_constraint, insert_into_acid_notnull failing</summary>
      <description>These tests have been consistently failing for a while. I suspect HIVE-18727 has caused these failures. HIVE-18727 changed the code to throw ERROR instead of EXCEPTION if constraints are violated. I guess Negative cli driver doesn't handle errors.Following are full list of related failures:TestNegativeCliDriver.alter_notnull_constraint_violationTestNegativeCliDriver.insert_into_acid_notnull TestNegativeCliDriver.insert_into_notnull_constraint TestNegativeCliDriver.insert_multi_into_notnull TestNegativeCliDriver.insert_overwrite_notnull_constraint TestNegativeCliDriver.update_notnull_constraint</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.merge.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.default.constraint.invalid.default.value.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.check.constraint.violation.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreNegativeCliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19143" opendate="2018-4-10 00:00:00" fixdate="2018-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for negative tests</summary>
      <description>Missed in HIVE-18859</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.subquery.subquery.chain.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.table.grant.nosuchrole.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.grant.nosuchrole.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.case.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.dup.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.caseinsensitivity.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19147" opendate="2018-4-10 00:00:00" fixdate="2018-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix PerfCliDrivers: Tpcds30T missed CAT_NAME change</summary>
      <description>it seems the baked metastore dump misses the CAT_NAME field added by some recent metastore change</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.tez.hive-site.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19153" opendate="2018-4-10 00:00:00" fixdate="2018-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for few tests</summary>
      <description>Some golden file updates which were missed since many tests were failing.Following test golden files were updated:acid_table_statsbucket_map_join_tez_emptydefault_constraintinsert_values_orig_table_use_metadatatez_smb_1</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19154" opendate="2018-4-10 00:00:00" fixdate="2018-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Poll notification events to invalidate the results cache</summary>
      <description>Related to the work for HIVE-18609. HIVE-18609 will only invalidate entries in the cache if that query looked up again, which could potentially leave a lot of undetected invalid entries in the cache taking up space which could cause other entries to be evicted. To remove these entries in a more timely fashion, have a background thread to periodically check the notification events for updates to the tables used in the results cache.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19156" opendate="2018-4-10 00:00:00" fixdate="2018-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver.vectorized_dynamic_semijoin_reduction.q is broken</summary>
      <description>Looks like this test has been broken for some time</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.java</file>
    </fixedFiles>
  </bug>
  <bug id="1916" opendate="2011-1-14 00:00:00" fixdate="2011-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Default Alias For Aggregated Columns (_c1)</summary>
      <description>Problem:When running a Hive query that aggregates (does a group by operation), Hive will automatically name this column c0, _c1, _c2, etc.. This is a problem because Hive will not then execute a query against a column that begins with "" and then the user has to manually input back-ticks in order to get the query to run.Potential Solution:Hive should by default call these columns by their query assignment like "sum_active30day_users" or if that is not possible, call it something simple like "column_1" so that users can then query the new column without adding special back-ticks.Example Query:SELECT a.ds, COUNT(a.num_accounts)Example Result:ds, count_num_accounts OR ds, column_1</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19168" opendate="2018-4-11 00:00:00" fixdate="2018-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger changes for llap commands</summary>
      <description>New llap commands "llap cluster -info" and "llap cache -purge" require some changes so that Ranger can log the commands for auditing.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapClusterResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapCacheResourceProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
    </fixedFiles>
  </bug>
  <bug id="1917" opendate="2011-1-15 00:00:00" fixdate="2011-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS (create-table-as-select) throws exception when showing results</summary>
      <description>CTAS throws an exception in CliDriver when showing results at the end of a query. CTAS should not show results because it is not a 'select' query or 'desc'/explain etc. It should be the same as create table/view/index and insert overwrite statements.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19175" opendate="2018-4-11 00:00:00" fixdate="2018-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver.testCliDriver update_access_time_non_current_db failing</summary>
      <description>Caused by HIVE-18060. Instead of generating golden file under clientpositive/llap it is under clientpositive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.update.access.time.non.current.db.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19176" opendate="2018-4-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HoS support to progress bar on Beeline client</summary>
      <description>Make whats was done inHIVE-15473work for HoS.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.ServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.status.TestSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1918" opendate="2011-1-17 00:00:00" fixdate="2011-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add export/import facilities to the hive system</summary>
      <description>This is an enhancement request to add export/import features to hive.With this language extension, the user can export the data of the table - which may be located in different hdfs locations in case of a partitioned table - as well as the metadata of the table into a specified output location. This output location can then be moved over to another different hadoop/hive instance and imported there. This should work independent of the source and target metastore dbms used; for instance, between derby and mysql.For partitioned tables, the ability to export/import a subset of the partition must be supported.Howl will add more features on top of this: The ability to create/use the exported data even in the absence of hive, using MR or Pig. Please see http://wiki.apache.org/pig/Howl/HowlImportExport for these details.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19186" opendate="2018-4-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi Table INSERT statements query has a flaw for partitioned table when INSERT INTO and INSERT OVERWRITE are used</summary>
      <description>One problem test case is: create table intermediate(key int) partitioned by (p int) stored as orc;insert into table intermediate partition(p='455') select distinct key from src where key &gt;= 0 order by key desc limit 2;insert into table intermediate partition(p='456') select distinct key from src where key is not null order by key asc limit 2;insert into table intermediate partition(p='457') select distinct key from src where key &gt;= 100 order by key asc limit 2;create table multi_partitioned (key int, key2 int) partitioned by (p int);from intermediateinsert into table multi_partitioned partition(p=2) select p, keyinsert overwrite table multi_partitioned partition(p=1) select key, p;</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19187" opendate="2018-4-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Druid Storage Handler to Druid 0.12.0</summary>
      <description>Current used Druid Version is 0.11.0This Patch updates the Druid version to the most recent version 0.12.0</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19206" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatic memory management for open streaming writers</summary>
      <description>Problem: When there are 100s of record updaters open, the amount of memory required by orc writers keeps growing because of ORC's internal buffers. This can lead to potential high GC or OOM during streaming ingest.Solution: The high level idea is for the streaming connection to remember all the open record updaters and flush the record updater periodically (at some interval). Records written to each record updater can be used as a metric to determine the candidate record updaters for flushing. If stripe size of orc file is 64MB, the default memory management check happens only after every 5000 rows which may which may be too late when there are too many concurrent writers in a process. Example case would be 100 writers open and each of them have almost full stripe of 64MB buffered data, this would take 100*64MB ~=6GB of memory. When all of the record writers flush, the memory usage drops down to 100*~2MB which is just ~200MB memory usage.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictDelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.ConnectionInfo.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19209" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming ingest record writers should accept input stream</summary>
      <description>Record writers in streaming ingest currently accepts byte[]. Provide an option for clients to pass in input stream directly from which byte[] for record can be constructed.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictDelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.RecordWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19210" opendate="2018-4-13 00:00:00" fixdate="2018-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for streaming ingest</summary>
      <description>This will retain the old hcat streaming API for old clients. The new streaming ingest API will be separate module under hive.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19211" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New streaming ingest API and support for dynamic partitioning</summary>
      <description>New streaming API under new hive sub-module Dynamic partitioning support Auto-rollover transactions Automatic heartbeating</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.QueryFailedException.java</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestDelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionError.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionBatchUnAvailable.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionBatch.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingIOFailure.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingException.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.SerializationError.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.RecordWriter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTableSerializer.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.LockFailureListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClientBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.Transaction.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.HiveConfFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.package.html</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.UgiMetaStoreClientFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolver.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.Mutator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinatorBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.PartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspector.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspectorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.package-info.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.ConnectionError.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.DelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HeartBeatFailure.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveEndPoint.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.ImpersonationFailed.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidColumn.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidPartition.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidTable.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidTrasactionState.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.PartitionCreationFailed.java</file>
    </fixedFiles>
  </bug>
  <bug id="19212" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs yetus pre-commit checks</summary>
      <description>Follow up from HIVE-18883, the committed patch isn't working and Findbugs is still not working.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.YetusPhase.java</file>
      <file type="M">dev-support.yetus-wrapper.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19214" opendate="2018-4-13 00:00:00" fixdate="2018-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>High throughput ingest ORC format</summary>
      <description>Create delta files with all ORC overhead disabled (no index, no compression, no dictionary). Compactor will recreate the orc files with index, compression and dictionary encoding.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.acid3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.exception.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.acid3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.acid3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.exception.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19219" opendate="2018-4-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL DUMP should throw error if requested events are cleaned-up.</summary>
      <description>This is the case where the events were deleted on source because of old event purging and hence min(source event id) &gt; target event id (last replicated event id).Repl dump should fail in this case so that usercan drop the database and bootstrap again.Cleaner thread is concurrently removing the expired events from NOTIFICATION_LOG table.So, it is necessary to check if the current dump missed any event while dumping. After fetching events in batches, we shall check if it is fetched in contiguous sequence of event id.If it is not in contiguous sequence, then likely some events missed in the dump and hence throw error.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19222" opendate="2018-4-16 00:00:00" fixdate="2018-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestNegativeCliDriver tests are failing due to "java.lang.OutOfMemoryError: GC overhead limit exceeded"</summary>
      <description>TestNegativeCliDriver tests are failing with OOM recently. Not sure why. I will try to increase the memory to test out.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19226" opendate="2018-4-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend storage-api to print timestamp values in UTC</summary>
      <description>Related to HIVE-12192. Create new method that prints values in UTC.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestStructColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.TimestampUtils.java</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="19227" opendate="2018-4-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for negative tests</summary>
      <description>Now that we are able to run TestNegativeCliDriver some of golden files are found to be outdated.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.JoinWithAmbigousAlias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19228" opendate="2018-4-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove commons-httpclient 3.x usage</summary>
      <description>Commons-httpclient is not supported well anymore. Remove dependency and move to Apache HTTP client.</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.3.9,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
    </fixedFiles>
  </bug>
  <bug id="1923" opendate="2011-1-21 00:00:00" fixdate="2011-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow any type of stats publisher and aggregator in addition to HBase and JDBC</summary>
      <description>Stats Publisher and Aggregator should be configurable to take any class that implement the publisher and aggregator interface. Currently we only allow HBase and JDBC.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19233" opendate="2018-4-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility for acid 1.0 to 2.0 migration</summary>
      <description>Click to add description</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19235" opendate="2018-4-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for Minimr tests</summary>
      <description>stats update needed for 3 tests.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19243" opendate="2018-4-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop.version to 3.1.0</summary>
      <description>Given that Hadoop 3.1.0 has been released, we need to upgrade hadoop.version to 3.1.0. This change is requiredforHIVE-18037sinceit depends on YARN Service which had its first release in 3.1.0 (and is non-existent in 3.0.0).</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19249" opendate="2018-4-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication: WITH clause is not passing the configuration to Task correctly in all cases</summary>
      <description>When running repl load like following:REPL LOAD `repldb_kms207` FROM 'hdfs://url:8020/apps/hive/repl/f8b057a7-c3f2-43bd-8baa-f7408a9008fc' WITH ('hive.exec.parallel'='true','hive.distcp.privileged.doAs'='beacon','hive.metastore.uris'='thrift://metastore-url:9083','hive.metastore.warehouse.dir'='s3a://s3-warehouse','hive.warehouse.subdir.inherit.perms'='false','hive.repl.replica.functions.root.dir'='s3a://s3-warehouse','fs.s3a.bucket.ss-datasets.endpoint'='s3-bucket-endpoint','fs.s3a.impl.disable.cache'='true','fs.s3a.server-side-encryption-algorithm'='SSE-KMS','fs.s3a.server-side-encryption.key'='encr-key','distcp.options.pp'='','distcp.options.pg'='','distcp.options.pu'='');the task that get created need to use the configs that are passed in the USING clause. However, in some cases the wrong config object gets used.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="19254" opendate="2018-4-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NumberFormatException in MetaStoreUtils.isFastStatsSame</summary>
      <description>I see the following exception under some cases in the logs. This possibly happens when you try to add empty partitions.2018-04-19T19:32:19,260 ERROR [pool-7-thread-7] metastore.RetryingHMSHandler: MetaException(message:java.lang.NumberFormatException: null) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6824) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:4864) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions(HiveMetaStore.java:4801) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy24.alter_partitions(Unknown Source) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_partitions.getResult(ThriftHiveMetastore.java:16046) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_partitions.getResult(ThriftHiveMetastore.java:16030) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NumberFormatException: null at java.lang.Long.parseLong(Long.java:552) at java.lang.Long.parseLong(Long.java:631) at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.isFastStatsSame(MetaStoreUtils.java:632) at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:743) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:4827) ... 21 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19271" opendate="2018-4-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver default_constraint and check_constraint failing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19277" opendate="2018-4-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Active/Passive HA web endpoints does not allow cross origin requests</summary>
      <description>CORS is not allowed with web endpoints added for active/passive HA. Enable CORS by default for all web endpoints.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2Peers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2LeadershipStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="19280" opendate="2018-4-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid error messages for UPDATE/DELETE on insert-only transactional tables</summary>
      <description>UPDATE/DELETE on MM tables fails with "FAILED: SemanticException Error 10297: Attempt to do update or delete on table tpch.tbl_default_mm that is not transactional". This is invalid since the MM table is transactional.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="19281" opendate="2018-4-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect protocol name for LLAP AM plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.protocol.LlapPluginProtocolPB.java</file>
    </fixedFiles>
  </bug>
  <bug id="19282" opendate="2018-4-24 00:00:00" fixdate="2018-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t nest delta directories inside LB directories for ACID tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19302" opendate="2018-4-25 00:00:00" fixdate="2018-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logging Too Verbose For TableNotFound</summary>
      <description>There is way too much logging when a user submits a query against a table which does not exist. In an ad-hoc setting, it is quite normal that a user fat-fingers a table name. Yet, from the perspective of the Hive administrator, there was perhaps a major issue based on the volume and severity of logging. Please change the logging to INFO level, and do not present a stack trace, for such a trivial error.See the attached file for a sample of what logging a single "table not found" query generates.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19306" opendate="2018-4-25 00:00:00" fixdate="2018-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow batch serializer</summary>
      <description>Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19312" opendate="2018-4-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables don&amp;#39;t work with BucketizedHIF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19319" opendate="2018-4-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RuntimeStats fixes</summary>
      <description>there are some minor issues which were found during testing: 0 sized map is persisted if reoptimization occurs write happens twice move entry limit to only apply to the cache ensure that executor not get blocked</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestRuntimeStats.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RuntimeStatsCleanerTask.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetRuntimeStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReOptimizePlugin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSources.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.MetastoreStatsConnector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.CachingStatsSource.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19327" opendate="2018-4-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>qroupby_rollup_empty.q fails for insert-only transactional tables</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19332" opendate="2018-4-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable compute.query.using.stats for external table</summary>
      <description>Hive can use statistics to answer queries like count. This can be problematic on external tables where another tool might add files that Hive doesnt know about. In that case Hive will return incorrect results.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19334" opendate="2018-4-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use actual file size rather than stats for fetch task optimization with external tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19335" opendate="2018-4-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable runtime filtering (semijoin reduction opt with bloomfilter) for external tables</summary>
      <description>Even with good stats runtime filtering can cause issues, if they are out of date things are even worse. Disable by defaultfor external tables.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19336" opendate="2018-4-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable SMB/Bucketmap join for external tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="1934" opendate="2011-1-27 00:00:00" fixdate="2011-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table rename messes the location</summary>
      <description>create table tmptmp(a string) partitioned by (b string);alter table tmptmp add partition (b="1:2:3");alter table tmptmp rename to tmptmp_test;The location for tmptmp_test partition (b="1:2:3) is unescaped due to rename, and hence it cannot be dropped.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter3.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19340" opendate="2018-4-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable timeout of transactions opened by replication task at target cluster</summary>
      <description>The transactions opened by applying EVENT_OPEN_TXN should never be aborted automatically due to time-out. Aborting of transaction started by replication task may leads to inconsistent state at target which needs additional overhead to clean-up. So, it is proposed to mark the transactions opened by replication task as special ones and shouldn't be aborted if heart beat is lost.This helps to ensure all ABORT and COMMIT events will always find the corresponding txn at target to operate.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.0.0-to-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19344" opendate="2018-4-27 00:00:00" fixdate="2018-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of msck.repair.batch.size</summary>
      <description>msck.repair.batch.size default to 0 which means msck will try to add all the partitions in one API call to HMS. This can potentially add huge memory pressure on HMS. The default value should be changed to a reasonable number so that in case of large number of partitions we can batch the addition of partitions. Same goes for msck.repair.batch.max.retries</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19357" opendate="2018-4-29 00:00:00" fixdate="2018-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: assert_true HiveException erroneously gets suppressed to NULL</summary>
      <description>This could be limited to assert exceptions; but might interfere with other exceptions...discovered while "fixing" testreopt after HIVE-19269create table tu(id_uv int,id_uw int,u int);create table tv(id_uv int,v int);create table tw(id_uw int,w int);insert into tu values (10,10,10),(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5),(6,6,6);insert into tv values (10,10),(1,1),(2,2),(3,3);insert into tw values (10,10),(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9);set zzz=0;set hive.vectorized.execution.enabled=false;select assert_true(${hiveconf:zzz}&gt;sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u&lt;10 and v&gt;1;-- fails as expectedset hive.vectorized.execution.enabled=true;select assert_true(${hiveconf:zzz}&gt;sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u&lt;10 and v&gt;1;-- there is a result set</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestUnaryMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFStructField.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TimestampToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StructColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupConcatColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.OctetLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColNotEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumnChecked.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprNullNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprNullCondExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprNullColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprLongColumnLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprDoubleColumnDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCondExprNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCondExprCondExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCondExprColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCondExprBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprColumnNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprColumnCondExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncTimestampToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStructColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateScalarSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalYearMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalDayTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastMillisecondsLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestColumnScalarOperationVectorExpressionEvaluation.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestColumnScalarOperationVectorExpressionCheckedEvaluation.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestColumnScalarFilterVectorExpressionEvaluation.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestColumnColumnOperationVectorExpressionEvaluation.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestColumnColumnOperationVectorExpressionCheckedEvaluation.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestColumnColumnFilterVectorExpressionEvaluation.txt</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestClass.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupScalarCompareStringGroupColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringGroupScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarDivideColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateScalar.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.AbstractExpression.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumnDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideScalarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryFunc.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryMinus.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnArithmeticDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnArithmeticDecimal64Scalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ScalarArithmeticDecimal64Column.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DecimalColumnUnaryFunc.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnCompareScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareDecimalColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnCompareDecimalScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalScalarCompareDecimalColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterScalarCompareColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareStringGroupScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupScalarCompareStringGroupColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprScalarColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprScalarScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19358" opendate="2018-4-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO decorrelation logic should generate Hive operators</summary>
      <description>Decorrelation logic may generate logical instances of the operators in the plan (e.g., LogicalFilter instead of HiveFilter). This leads to errors while costing the tree in the Volcano planner (used in MV rewriting), since logical operators do not have a cost associated to them.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="1936" opendate="2011-1-29 00:00:00" fixdate="2011-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.semantic.analyzer.hook cannot have multiple values</summary>
      <description>It should support comma separated values like hive pre/post execution hooks</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19373" opendate="2018-5-1 00:00:00" fixdate="2018-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test cases that verifies ALTER change owner type on the HMS</summary>
      <description>Subtask to add test cases that check the owner type of a table is changed on the HMS</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.client.builder.TableBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="19374" opendate="2018-5-1 00:00:00" fixdate="2018-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse and process ALTER TABLE SET OWNER command syntax</summary>
      <description>Subtask that parses the new alter table set owner syntax and implements code to call HMS to change the owner of a table to a user or a role.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="19381" opendate="2018-5-1 00:00:00" fixdate="2018-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Function replication in cloud fail when download resource from AWS</summary>
      <description>Another case replication shall use the config in with clause.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="19382" opendate="2018-5-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Acquire locks before generating valid transaction list for some operations</summary>
      <description>To ensure correctness, in particular for operations that require exclusive (INSERT OVERWRITE) and semishared (UPDATE/DELETE) locks.This is a temporary fix till lock acquisition is moved before analyze in HIVE-18948.With this fix, system proceed as follows. The driver will acquire the snapshot, compile the query wrt that snapshot, and then, it will acquire locks. If snapshot is still valid, it will continue as usual. But if snapshot is not valid anymore, it will recompile the query.This is easier to implement than full solution described in HIVE-18948 because we do not need to move the logic to extract the read/write entities from a query before compilation (actually while parsing).</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="19383" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ArrayList$SubList kryo serializer</summary>
      <description>Otherwise failure is encountered while trying to deserialize such a plan.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19385" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional hive env variable to redirect bin/hive to use Beeline</summary>
      <description>With beeline-site and beeline-user-site, the user can easily specify default hs2 urls to connect. We can use an optional env variable, which when set, will enable bin/hive to use beeline.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19387" opendate="2018-5-2 00:00:00" fixdate="2018-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate table for Acid tables conflicts with ResultSet cache</summary>
      <description>How should this work? Should it work like Insert Overwrite T select * from T where 1=2?This should create a new empty base_x/ and thus operate w/o violating Snapshot Isolation semantics.This makes sense for specific partition or unpartitioned table. What about "Truncate T" where T is partitioned? Is the expectation to wipe out all partition info or to make each partition empty?</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19389" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool: For Hive&amp;#39;s Information Schema, use embedded HS2 as default</summary>
      <description>Currently, for initializing/upgrading Hive's information schema, we require a full jdbc url (for HS2). It will be good to have it connect using embedded HS2 by default.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19390" opendate="2018-5-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Useless error messages logged for dummy table stats</summary>
      <description>Queries like INSERT INTO t1 VALUES (20); gets rewritten into insert into t1 select 20 from default_db.default_tblname which later throws as compiler tries to get stats</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19400" opendate="2018-5-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust Hive 1.0 to 2.0 conversion utility to the upgrade</summary>
      <description>Conversion utility should allow specification of the output dir, and create files only if there is actually something to do.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19415" opendate="2018-5-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CORS for all HS2 web endpoints</summary>
      <description>HIVE-19277 changes alone are not sufficient to support CORS. CrossOriginFilter has to be added to jetty which will serve appropriate response for OPTIONS pre-flight request.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19418" opendate="2018-5-4 00:00:00" fixdate="2018-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add background stats updater similar to compactor</summary>
      <description>There's a JIRA HIVE-19416 to add snapshot version to stats for MM/ACID tables to make them usable in a transaction without breaking ACID (for metadata-only optimization). However, stats for ACID tables can still become unusable if e.g. two parallel inserts run - neither sees the data written by the other, so after both finish, the snapshots on either set of stats won't match the current snapshot and the stats will be unusable.Additionally, for ACID and non-ACID tables alike, a lot of the stats, with some exceptions like numRows, cannot be aggregated (i.e. you cannot combine ndvs from two inserts), and for ACID even less can be aggregated (you cannot derive min/max if some rows are deleted but you don't scan the rest of the dataset).Therefore we will add background logic to metastore (similar to, and partially inside, the ACID compactor) to update stats.It will have 3 modes of operation.1) Off.2) Update only the stats that exist but are out of date (generating stats can be expensive, so if the user is only analyzing a subset of tables it should be able to only update that subset). We can simply look at existing stats and only analyze for the relevant partitions and columns.3) On: 2 + create stats for all tables and columns missing stats.There will also be a table parameter to skip stats update. In phase 1, the process will operate outside of compactor, and run analyze command on the table. The analyze command will automatically save the stats with ACID snapshot information if needed, based on HIVE-19416, so we don't need to do any special state management and this will work for all table types. However it's also more expensive.In phase 2, we can explore adding stats collection during MM compaction that uses a temp table. If we don't have open writers during major compaction (so we overwrite all of the data), the temp table stats can simply be copied over to the main table with correct snapshot information, saving us a table scan.In phase 3, we can add custom stats collection logic to full ACID compactor that is not query based, the same way as we'd do for (2). Alternatively we can wait for ACID compactor to become query based and just reuse (2).</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUpdaterThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="19421" opendate="2018-5-4 00:00:00" fixdate="2018-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade version of Jetty to 9.3.20.v20170531</summary>
      <description>Move Jetty up to 9.3.20.v20170531</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="19423" opendate="2018-5-4 00:00:00" fixdate="2018-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD creates staging directory in source dump directory instead of table data location</summary>
      <description>REPL LOAD creates staging directory in source dump directory instead of table data location. In case of replication from on-perm to cloud it can create problem.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="19435" opendate="2018-5-7 00:00:00" fixdate="2018-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental replication cause data loss if a table is dropped followed by create and insert-into with different partition type.</summary>
      <description>If the incremental dump havedrop of partitioned table followed by create/insert on non-partitioned table with same name, doesn't replicate the data. Explained below.Let's say we have a partitioned table T1 which was already replicated to target.DROP_TABLE(T1)-&gt;CREATE_TABLE(T1) (Non-partitioned) -&gt; INSERT(T1)(10)After REPL LOAD, T1 doesn't have any data.Same is valid for non-partitioned to partitioned and partition spec mismatch case as well.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="19460" opendate="2018-5-8 00:00:00" fixdate="2018-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve stats estimations for NOT IN operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.struct.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19462" opendate="2018-5-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix mapping for char_length function to enable pushdown to Druid.</summary>
      <description>currentlychar_length is not push down to Druid because of missing mapping form/to calciteThis patch will add this mapping.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19465" opendate="2018-5-8 00:00:00" fixdate="2018-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ORC to 1.5.0</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.typechangetest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.ConsumerFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.multi.db.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19466" opendate="2018-5-8 00:00:00" fixdate="2018-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update constraint violation error message</summary>
      <description>Currently for both CHECK and NOT NULL constraint violation hive throws NOT NULL Constraint violated.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.update.notnull.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.overwrite.notnull.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.multi.into.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.notnull.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.acid.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.notnull.constraint.violation.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEnforceNotNullConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEnforceNotNullConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="19467" opendate="2018-5-8 00:00:00" fixdate="2018-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make storage format configurable for temp tables created using LLAP external client</summary>
      <description>Temp tables created for complex queries when using the LLAP external client are created using the default storage format. Default to orc, and make configurable.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19474" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal type should be casted as part of the CTAS or INSERT Clause.</summary>
      <description>HIVE-18569 introduced a runtime config variable to allow the indexing of Decimal as Double, this leads to kind of messy state, Hive metadata think the column is still decimal while it is stored as double. Since the Hive metadata of the column is Decimal the logical optimizer will not push down aggregates.i tried to fix this by adding some logic to the application but it makes the code very clumsy with lot of branches. Instead i propose to revert HIVE-18569 and let the user introduce an explicit cast this will be better since the metada reflects actual storage type and push down aggregates will kick in and there is no config needed without adding any code or bug.cc ashutoshc and nishantbangarwaYou can see the difference with the following DDLcreate table test_base_table(`timecolumn` timestamp, `interval_marker` string, `num_l` DECIMAL(10,2));insert into test_base_table values ('2015-03-08 00:00:00', 'i1-start', 4.5);set hive.druid.approx.result=true;CREATE TABLE druid_test_tableSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'TBLPROPERTIES ("druid.segment.granularity" = "DAY")ASselect cast(`timecolumn` as timestamp with local time zone) as `__time`, `interval_marker`, cast(`num_l` as double)FROM test_base_table;describe druid_test_table;explain select sum(num_l), min(num_l) FROM druid_test_table;CREATE TABLE druid_test_table_2STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'TBLPROPERTIES ("druid.segment.granularity" = "DAY")ASselect cast(`timecolumn` as timestamp with local time zone) as `__time`, `interval_marker`, `num_l`FROM test_base_table;describe druid_test_table_2;explain select sum(num_l), min(num_l) FROM druid_test_table_2;</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19477" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiveserver2 in http mode not emitting metric default.General.open_connections</summary>
      <description>Instances in binary mode are emitting the metric default.General.open_connections but the instances operating in http mode are not emitting this metric.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="1948" opendate="2011-2-2 00:00:00" fixdate="2011-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Have audit logging in the Metastore</summary>
      <description>It would be good to have audit logging in the metastore, similar to Hadoop's NameNode audit logging. This would allow administrators to dig into details about which user performed metadata operations (like create/drop tables/partitions) and from where (IP address).</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19483" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore cleaner tasks that run periodically are created more than once</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="19494" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accept shade prefix during reflective instantiation of output format</summary>
      <description>Hive Streaming API jars are sometimes shaded with a different prefix when used in environments where another version of hive already exists (spark for example). In most cases, shading is done with rename of classes with some prefix. If an uber/assembly jar is generated with renamed prefix, Hive Streaming API will not work as Hive Streaming API will reflectively instantiate outputformat class using FQCN string provided by metastore table storage descriptor object. For example: RecordWriter will create instance of OutputFormat using string "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat". When a shaded jar with renamed class references are used, this class will not be found by the classloader. We can optionally accept a shade prefix from user via config which will be tried (as fallback) when ClassNotFoundException is thrown.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19495" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow SerDe itest failure</summary>
      <description>"You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowColumnarBatchSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="19496" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check untar folder</summary>
      <description>We need to check if the file is under untar folder.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CompressionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19498" opendate="2018-5-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: CAST expressions produce wrong results</summary>
      <description>Wrong results for:DATE --&gt; BOOLEAN DOUBLE --&gt; DECIMAL STRING|CHAR|VARCHAR --&gt; DECIMAL TIMESTAMP --&gt; LONG</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastFloatToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToBoolean.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal64ToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimalMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimal.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19499" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap REPL LOAD shall add tasks to create checkpoints for db/tables/partitions.</summary>
      <description>Currently. bootstrap REPL LOAD expect the target database to be empty or not exist to start bootstrap load.But, this adds overhead when there is a failure in between bootstrap load and there is no way to resume it from where it fails. So, it is needed to create checkpoints in table/partitions to skip the completely loaded objects.Use the fully qualified path of the dump directory as a checkpoint identifier. This should be added to the table / partition properties in hive via a task, as the last task in the DAG for table / partition creation.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TaskTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="195" opendate="2008-12-23 00:00:00" fixdate="2008-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>map-side joins are not supported</summary>
      <description>map-side joins are not supported.If all the tables but one are small and can fit in memory, the join should be performed on the map-side. Ideally, it should be all cost-based, butto start with, the user should be given an option to specify that he needs a map-side join.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">data.scripts.dumpdata.script.py</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExtractOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.fetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.selectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19504" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value for hive.auto.convert.join.shuffle.max.size property</summary>
      <description>The property default value is too low by mistake (10MB), it is missing three trailing zeros. HIVECONVERTJOINMAXSHUFFLESIZE("hive.auto.convert.join.shuffle.max.size", 10000000L, "If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. \n" + "However, if it is on, and the predicted size of the larger input for a given join is greater \n" + "than this number, the join will not be converted to a dynamically partitioned hash join. \n" + "The value \"-1\" means no limit."),</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19529" opendate="2018-5-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Date/Timestamp NULL issues</summary>
      <description>Wrong results found for: date_add/date_subUT areas: date_add/date_subdatediffto_dateinterval_year_month + interval_year_month interval_day_time + interval_day_time interval_day_time + timestamp timestamp + interval_day_time date + interval_day_time interval_day_time + date interval_year_month + date date + interval_year_month interval_year_month + interval_year_month timestamp + interval_year_monthdate - date interval_year_month - interval_year_month interval_day_time - interval_day_time timestamp - interval_day_time timestamp - timestamp date - timestamp timestamp - date date - interval_day_time date - interval_year_month timestamp - interval_year_month</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomBatchSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareTruncStringScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19530" opendate="2018-5-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix JDBCSerde and re-enable vectorization</summary>
      <description>According tojcamachorodriguezthere is a big switch statement in the code that has might have missing types. This can lead to the string types seen.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19532" opendate="2018-5-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>merge master-txnstats branch</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-common.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">ql.src.test.queries.clientpositive.stats.part2.q</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.sizebug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.transactional.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.exim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.TxnIdUtils.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidTxnWriteIdList.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.1.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.1.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.1.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.1.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.1.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ColStatsProcessor.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AggrStats.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IHMSHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartition.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="19569" opendate="2018-5-16 00:00:00" fixdate="2018-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table db1.t1 rename db2.t2 generates MetaStoreEventListener.onDropTable()</summary>
      <description>When renaming a table within the same DB, this operation causes MetaStoreEventListener.onAlterTable() to fire but when changing DB name for a table it causes MetaStoreEventListener.onDropTable() + MetaStoreEventListener.onCreateTable().The files from original table are moved to new table location. This creates confusing semantics since any logic in onDropTable() doesn't know about the larger context, i.e. that there will be a matching onCreateTable().In particular, this causes a problem for Acid tables since files moved from old table use WriteIDs that are not meaningful with the context of new table.Current implementation is due to replication. This should ideally be changed to raise a "not supported" error for tables that are marked for replication.cc sankarh</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnConcatenate.java</file>
    </fixedFiles>
  </bug>
  <bug id="19577" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TEMPORARY TABLE LIKE and INSERT generate output format mismatch errors</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19578" opendate="2018-5-16 00:00:00" fixdate="2018-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HLL merges tempList on every add</summary>
      <description>See comments on HIVE-18866; this has significant perf overhead after the even bigger overhead from hashing is removed.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.common.ndv.hll.HyperLogLog.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.common.ndv.hll.HLLSparseRegister.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.context.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.vector.nohybridgrace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19583" opendate="2018-5-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some yetus working dirs are left on hivepest-server-upstream disk after test</summary>
      <description>PTest's PrepPhase is creating a yetus working folder for each build after checking out source code. The source code is then copied into that for yetus. This folder is cleaned up after the test executed, so if that doesn't happen e.g. due to patch not being applicable the folder is left on the disk. We need to remove it in this case too.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug id="19598" opendate="2018-5-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Acid V1 to V2 upgrade module</summary>
      <description>The on-disk layout for full acid (transactional) tables has changed 3.0.Any transactional table that has any update/delete events in any deltas that have not been Major compacted, must go through a Major compaction before upgrading to 3.0. No more update/delete/merge should be run after/during major compaction.Not doing so will result in data corruption/loss.Need to create a utility tool to help with this process.HIVE-19233 started this but it needs more work.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="196" opendate="2008-12-23 00:00:00" fixdate="2008-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure when doing 2 tests with the same user on the same machine</summary>
      <description>org.apache.hadoop.util.Shell$ExitCodeException: chmod: cannot access `/tmp/zshao/kv1.txt': No such file or directoryWe should make a unique directory for each of the test runs, instead of sharing /tmp/${username}.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestFlatFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">data.conf.hive-log4j.properties</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19605" opendate="2018-5-18 00:00:00" fixdate="2018-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TAB_COL_STATS table has no index on db/table name</summary>
      <description>The TAB_COL_STATS table is missing an index on (CAT_NAME, DB_NAME, TABLE_NAME). The getTableColumnStatistics call queries based on this tuple. This makes those queries take a significant amount of time in large metastores since they do a full table scan.</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.0.0-to-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="19659" opendate="2018-5-22 00:00:00" fixdate="2018-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update master to version 4.0</summary>
      <description>Currently we have branch-3.0 for presumably branch-3.0.X, but branch-3 is marked as 3.0 and master as 3.1-SNAPSHOT.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">classification.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-util.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1966" opendate="2011-2-7 00:00:00" fixdate="2011-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin operator should not load hashtable for each new inputfile if the hashtable to be loaded is already there.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19661" opendate="2018-5-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>switch Hive UDFs to use Re2J regex engine</summary>
      <description>Java regex engine can be very slow in some cases e.g. https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8203458</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRegExp.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">LICENSE</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19663" opendate="2018-5-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor LLAP IO report generation</summary>
      <description>Follow-up from HIVE-19642.Instead of each component calling some other component in a chain, all the parts of the state dump should be called in one place to avoid weird dependencies/sequences that need to be accounted for to generate the report.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.MetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SimpleBufferManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapOomDebugDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionDispatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.CacheContentsTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19674" opendate="2018-5-23 00:00:00" fixdate="2018-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by Decimal Constants push down to Druid tables.</summary>
      <description>Queries like following gets generated by Tableau.SELECT SUM(`ssb_druid_100`.`lo_revenue`) AS `sum_lo_revenue_ok` FROM `druid_ssb`.`ssb_druid_100` `ssb_druid_100`GROUP BY 1.1000000000000001;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stat.estimate.drill.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constGby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19680" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push down limit is not applied for Druid storage handler.</summary>
      <description>Query like select `__time` from druid_test_table limit 1;returns more than one row.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DruidStorageHandlerUtilsTest.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19687" opendate="2018-5-23 00:00:00" fixdate="2018-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export table on acid partitioned table is failing</summary>
      <description>Reproducercreate table exportPartitionTable(id int, name string) partitioned by(country string) clustered by (id) into 2 buckets stored as orc tblproperties ("transactional"="true");export table exportPartitionTable PARTITION (country='india') to '/tmp/exportDataStore';ErrorFAILED: SemanticException [Error 10004]: Line 1:165 Invalid table alias or column reference 'india': (possible column names are: id, name, country)</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19688" opendate="2018-5-24 00:00:00" fixdate="2018-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make catalogs updatable</summary>
      <description>The initial changes for catalogs did not include an ability to alter catalogs. We need to add that.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestCatalogs.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.EventMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaToolCatalogOps.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19691" opendate="2018-5-24 00:00:00" fixdate="2018-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start SessionState in materialized views registry</summary>
      <description>SessionState is not initialized when we load the materialized views, which leads to a NullPointerException and other issues.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="19694" opendate="2018-5-24 00:00:00" fixdate="2018-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Materialized View statement should check for MV name conflicts before running MV&amp;#39;s SQL statement.</summary>
      <description>If the CREATE MATERIALIZE VIEW statement refers to a mv name that already exists, the statement runs the SQL on cluster and Move task returns an error at the very end.This unnecessarily uses up cluster resources and user time.0: jdbc:hive2://localhost:10007/tpcds_bin_par&gt; CREATE MATERIALIZED VIEW mv_store_sales_item_store. . . . . . . . . . . . . . . . . . . . . . .&gt; ENABLE REWRITE AS (. . . . . . . . . . . . . . . . . . . . . . .&gt; select ss_item_sk,. . . . . . . . . . . . . . . . . . . . . . .&gt; ss_store_sk,. . . . . . . . . . . . . . . . . . . . . . .&gt; sum(ss_quantity) as ss_quantity,. . . . . . . . . . . . . . . . . . . . . . .&gt; sum(ss_ext_wholesale_cost) as ss_ext_wholesale_cost,. . . . . . . . . . . . . . . . . . . . . . .&gt; sum(ss_net_paid) as ss_net_paid,. . . . . . . . . . . . . . . . . . . . . . .&gt; sum(ss_net_profit) as ss_net_profit. . . . . . . . . . . . . . . . . . . . . . .&gt; from store_sales. . . . . . . . . . . . . . . . . . . . . . .&gt; group by ss_item_sk,ss_store_sk. . . . . . . . . . . . . . . . . . . . . . .&gt; );INFO : Compiling command(queryId=root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037): CREATE MATERIALIZED VIEW mv_store_sales_item_storeENABLE REWRITE AS (select ss_item_sk,| `ss_store_sk` bigint, || `ss_quantity` bigint, || `ss_ext_wholesale_cost` double, || `ss_net_paid` double, || `ss_net_profit` double) |. . . . . . . . . . . . . . . . . . . . . . .&gt; from store_sales. . . . . . . . . . . . . . . . . . . . . . .&gt; group by ss_item_sk,ss_store_sk. . . . . . . . . . . . . . . . . . . . . . .&gt; );INFO : Compiling command(queryId=root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037): CREATE MATERIALIZED VIEW mv_store_sales_item_storeENABLE REWRITE AS (select ss_item_sk,ss_store_sk,sum(ss_quantity) as ss_quantity,sum(ss_ext_wholesale_cost) as ss_ext_wholesale_cost,sum(ss_net_paid) as ss_net_paid,sum(ss_net_profit) as ss_net_profitfrom store_salesgroup by ss_item_sk,ss_store_sk)INFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:ss_item_sk, type:bigint, comment:null), FieldSchema(name:ss_store_sk, type:bigint, comment:null), FieldSchema(name:ss_quantity, type:bigint, comment:null), FieldSchema(name:ss_ext_wholesale_cost, type:double, comment:null), FieldSchema(name:ss_net_paid, type:double, comment:null), FieldSchema(name:ss_net_profit, type:double, comment:null)], properties:null)INFO : Completed compiling command(queryId=root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037); Time taken: 3.652 secondsINFO : Executing command(queryId=root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037): CREATE MATERIALIZED VIEW mv_store_sales_item_storeENABLE REWRITE AS (select ss_item_sk,ss_store_sk,sum(ss_quantity) as ss_quantity,sum(ss_ext_wholesale_cost) as ss_ext_wholesale_cost,sum(ss_net_paid) as ss_net_paid,sum(ss_net_profit) as ss_net_profitfrom store_salesgroup by ss_item_sk,ss_store_sk)INFO : Query ID = root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037INFO : Total jobs = 1INFO : Launching Job 1 out of 1INFO : Starting task [Stage-1:MAPRED] in serial modeINFO : Subscribed to counters: [] for queryId: root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037INFO : Session is already openINFO : Dag name: CREATE MATERIALIZED V...tem_sk,ss_store_sk) (Stage-1)INFO : Status: Running (Executing on YARN cluster with App id application_1525123931791_0151)---------------------------------------------------------------------------------------------- VERTICES MODE STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED----------------------------------------------------------------------------------------------Map 1 .......... llap SUCCEEDED 1682 1682 0 0 0 0Reducer 2 ...... llap SUCCEEDED 1009 1009 0 0 0 7----------------------------------------------------------------------------------------------VERTICES: 02/02 [==========================&gt;&gt;] 100% ELAPSED TIME: 1734.00 s----------------------------------------------------------------------------------------------INFO : Status: DAG finished successfully in 1731.89 secondsINFO :INFO : Query Execution SummaryINFO : ----------------------------------------------------------------------------------------------INFO : OPERATION DURATIONINFO : ----------------------------------------------------------------------------------------------INFO : Compile Query 3.65sINFO : Prepare Plan 0.45sINFO : Get Query Coordinator (AM) 0.00sINFO : Submit Plan 0.17sINFO : Start DAG 0.60sINFO : Run DAG 1731.89sINFO : ----------------------------------------------------------------------------------------------INFO :INFO : Task Execution SummaryINFO : ----------------------------------------------------------------------------------------------INFO : VERTICES DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDSINFO : ----------------------------------------------------------------------------------------------INFO : Map 1 928170.00 0 0 28,800,426,268 28,760,206,232INFO : Reducer 2 1099992.00 0 0 28,760,206,232 0INFO : ----------------------------------------------------------------------------------------------INFO :INFO : LLAP IO SummaryINFO : ----------------------------------------------------------------------------------------------INFO : VERTICES ROWGROUPS META_HIT META_MISS DATA_HIT DATA_MISS ALLOCATION USED TOTAL_IOINFO : ----------------------------------------------------------------------------------------------INFO : Map 1 2890797 33830 1888 40.57GB 401.64GB 832.29GB 777.79GB 117756.65sINFO : ----------------------------------------------------------------------------------------------INFO :INFO : FileSystem Counters SummaryINFO :INFO : Scheme: HDFSINFO : ----------------------------------------------------------------------------------------------INFO : VERTICES BYTES_READ READ_OPS LARGE_READ_OPS BYTES_WRITTEN WRITE_OPSINFO : ----------------------------------------------------------------------------------------------INFO : Map 1 401.65GB 29867 0 0B 0INFO : Reducer 2 0B 4036 0 5.95GB 3027INFO : ----------------------------------------------------------------------------------------------INFO :INFO : Scheme: FILEINFO : ----------------------------------------------------------------------------------------------INFO : VERTICES BYTES_READ READ_OPS LARGE_READ_OPS BYTES_WRITTEN WRITE_OPSINFO : ----------------------------------------------------------------------------------------------INFO : Map 1 0B 0 0 691.51GB 0INFO : Reducer 2 490.46GB 0 0 0B 0INFO : ----------------------------------------------------------------------------------------------INFO :INFO : org.apache.tez.common.counters.DAGCounter:INFO : NUM_KILLED_TASKS: 7INFO : NUM_SUCCEEDED_TASKS: 2691INFO : TOTAL_LAUNCHED_TASKS: 2698INFO : AM_CPU_MILLISECONDS: 884400INFO : AM_GC_TIME_MILLIS: 1052INFO : File System Counters:INFO : FILE_BYTES_READ: 490462333002INFO : FILE_BYTES_WRITTEN: 691512269321INFO : FILE_READ_OPS: 0INFO : FILE_LARGE_READ_OPS: 0INFO : FILE_WRITE_OPS: 0INFO : HDFS_BYTES_READ: 401649861525INFO : HDFS_BYTES_WRITTEN: 5953929405INFO : HDFS_READ_OPS: 33903INFO : HDFS_LARGE_READ_OPS: 0INFO : HDFS_WRITE_OPS: 3027INFO : org.apache.tez.common.counters.TaskCounter:INFO : REDUCE_INPUT_GROUPS: 301902000INFO : REDUCE_INPUT_RECORDS: 28760206232INFO : COMBINE_INPUT_RECORDS: 0INFO : SPILLED_RECORDS: 57520412464INFO : NUM_SHUFFLED_INPUTS: 1697138INFO : NUM_SKIPPED_INPUTS: 0INFO : NUM_FAILED_SHUFFLE_INPUTS: 0INFO : MERGED_MAP_OUTPUTS: 1697138INFO : INPUT_RECORDS_PROCESSED: 28802064INFO : INPUT_SPLIT_LENGTH_BYTES: 1423169426915INFO : OUTPUT_RECORDS: 28760206232INFO : OUTPUT_LARGE_RECORDS: 0INFO : OUTPUT_BYTES: 1251477312279INFO : OUTPUT_BYTES_WITH_OVERHEAD: 1249819249865INFO : OUTPUT_BYTES_PHYSICAL: 691471524553INFO : ADDITIONAL_SPILLS_BYTES_WRITTEN: 490687645770INFO : ADDITIONAL_SPILLS_BYTES_READ: 490687645770INFO : ADDITIONAL_SPILL_COUNT: 0INFO : SHUFFLE_CHUNK_COUNT: 1682INFO : SHUFFLE_BYTES: 691471524553INFO : SHUFFLE_BYTES_DECOMPRESSED: 1249819249865INFO : SHUFFLE_BYTES_TO_MEM: 691471524553INFO : SHUFFLE_BYTES_TO_DISK: 0INFO : SHUFFLE_BYTES_DISK_DIRECT: 0INFO : NUM_MEM_TO_DISK_MERGES: 0INFO : NUM_DISK_TO_DISK_MERGES: 0INFO : SHUFFLE_PHASE_TIME: 208623266INFO : MERGE_PHASE_TIME: 248665618INFO : FIRST_EVENT_RECEIVED: 6992INFO : LAST_EVENT_RECEIVED: 66502682INFO : HIVE:INFO : CREATED_FILES: 1009INFO : DESERIALIZE_ERRORS: 0INFO : RECORDS_IN_Map_1: 28800426268INFO : RECORDS_OUT_1_tpcds_bin_partitioned_orc_10000.mv_store_sales_it: 301902000INFO : RECORDS_OUT_INTERMEDIATE_Map_1: 28760206232INFO : RECORDS_OUT_INTERMEDIATE_Reducer_2: 0INFO : RECORDS_OUT_OPERATOR_FS_11: 301902000INFO : RECORDS_OUT_OPERATOR_GBY_10: 301902000INFO : RECORDS_OUT_OPERATOR_GBY_8: 28760206232INFO : RECORDS_OUT_OPERATOR_MAP_0: 0INFO : RECORDS_OUT_OPERATOR_RS_9: 28760206232INFO : RECORDS_OUT_OPERATOR_SEL_7: 28800426268INFO : RECORDS_OUT_OPERATOR_TS_0: 28800426268INFO : Shuffle Errors:INFO : BAD_ID: 0INFO : CONNECTION: 0INFO : IO_ERROR: 0INFO : WRONG_LENGTH: 0INFO : WRONG_MAP: 0INFO : WRONG_REDUCE: 0INFO : Shuffle Errors_Reducer_2_INPUT_Map_1:INFO : BAD_ID: 0INFO : CONNECTION: 0INFO : IO_ERROR: 0INFO : WRONG_LENGTH: 0INFO : WRONG_MAP: 0INFO : WRONG_REDUCE: 0INFO : TaskCounter_Map_1_INPUT_store_sales:INFO : INPUT_RECORDS_PROCESSED: 28802064INFO : INPUT_SPLIT_LENGTH_BYTES: 1423169426915INFO : TaskCounter_Map_1_OUTPUT_Reducer_2:INFO : ADDITIONAL_SPILLS_BYTES_READ: 0INFO : ADDITIONAL_SPILLS_BYTES_WRITTEN: 0INFO : ADDITIONAL_SPILL_COUNT: 0INFO : OUTPUT_BYTES: 1251477312279INFO : OUTPUT_BYTES_PHYSICAL: 691471524553INFO : OUTPUT_BYTES_WITH_OVERHEAD: 1249819249865INFO : OUTPUT_LARGE_RECORDS: 0INFO : OUTPUT_RECORDS: 28760206232INFO : SHUFFLE_CHUNK_COUNT: 1682INFO : SPILLED_RECORDS: 28760206232INFO : TaskCounter_Reducer_2_INPUT_Map_1:INFO : ADDITIONAL_SPILLS_BYTES_READ: 490687645770INFO : ADDITIONAL_SPILLS_BYTES_WRITTEN: 490687645770INFO : COMBINE_INPUT_RECORDS: 0INFO : FIRST_EVENT_RECEIVED: 6992INFO : LAST_EVENT_RECEIVED: 66502682INFO : MERGED_MAP_OUTPUTS: 1697138INFO : MERGE_PHASE_TIME: 248665618INFO : NUM_DISK_TO_DISK_MERGES: 0INFO : NUM_FAILED_SHUFFLE_INPUTS: 0INFO : NUM_MEM_TO_DISK_MERGES: 0INFO : NUM_SHUFFLED_INPUTS: 1697138INFO : NUM_SKIPPED_INPUTS: 0INFO : REDUCE_INPUT_GROUPS: 301902000INFO : REDUCE_INPUT_RECORDS: 28760206232INFO : SHUFFLE_BYTES: 691471524553INFO : SHUFFLE_BYTES_DECOMPRESSED: 1249819249865INFO : SHUFFLE_BYTES_DISK_DIRECT: 0INFO : SHUFFLE_BYTES_TO_DISK: 0INFO : SHUFFLE_BYTES_TO_MEM: 691471524553INFO : SHUFFLE_PHASE_TIME: 208623266INFO : SPILLED_RECORDS: 28760206232INFO : TaskCounter_Reducer_2_OUTPUT_out_Reducer_2:INFO : OUTPUT_RECORDS: 0INFO : org.apache.hadoop.hive.llap.counters.LlapIOCounters:INFO : ALLOCATED_BYTES: 832288587776INFO : ALLOCATED_USED_BYTES: 777785688450INFO : CACHE_HIT_BYTES: 40573598684INFO : CACHE_MISS_BYTES: 401640241544INFO : CONSUMER_TIME_NS: 103780723121700INFO : DECODE_TIME_NS: 67854956903872INFO : HDFS_TIME_NS: 40407374232025INFO : METADATA_CACHE_HIT: 33830INFO : METADATA_CACHE_MISS: 1888INFO : NUM_DECODED_BATCHES: 2890797INFO : NUM_VECTOR_BATCHES: 28802069INFO : ROWS_EMITTED: 28800426268INFO : SELECTED_ROWGROUPS: 2890797INFO : TOTAL_IO_TIME_NS: 117756651175367INFO : org.apache.hadoop.hive.llap.counters.LlapWmCounters:INFO : GUARANTEED_QUEUED_NS: 0INFO : GUARANTEED_RUNNING_NS: 0INFO : SPECULATIVE_QUEUED_NS: 127227283691687INFO : SPECULATIVE_RUNNING_NS: 475493970727392INFO : org.apache.hadoop.hive.ql.exec.tez.HiveInputCounters:INFO : GROUPED_INPUT_SPLITS_Map_1: 1682INFO : INPUT_DIRECTORIES_Map_1: 1824INFO : INPUT_FILES_Map_1: 4323INFO : RAW_INPUT_SPLITS_Map_1: 8292INFO : Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial modeINFO : Starting task [Stage-0:MOVE] in serial modeINFO : Moving data to directory hdfs://ctr-e138-1518143905142-92974-01-000002.hwx.site:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/mv_store_sales_item_store from hdfs://ctr-e138-1518143905142-92974-01-000002.hwx.site:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/.hive-staging_hive_2018-05-24_03-43-30_960_8962535148229486995-408/-ext-10002INFO : Starting task [Stage-4:DDL] in serial modeERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Table already exists: tpcds_bin_partitioned_orc_10000.mv_store_sales_item_storeINFO : Completed executing command(queryId=root_20180524034330_21fca7f6-ed5a-492c-88e9-913d4120b037); Time taken: 1734.952 secondsError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Table already exists: tpcds_bin_partitioned_orc_10000.mv_store_sales_item_store (state=08S01,code=1</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="19713" opendate="2018-5-25 00:00:00" fixdate="2018-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>itests/hive-jmh should not reference a concreate storage-api version</summary>
      <description>this is a bigger problem on branch-3; where storage-api is 2.6.1; but hive-jmh references 2.7.0 (which is for master)</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19734" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: When beeline-site.xml is present, beeline does not honor -n (username) and -p (password) arguments</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="19739" opendate="2018-5-30 00:00:00" fixdate="2018-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap REPL LOAD to use checkpoints to validate and skip the loaded data/metadata.</summary>
      <description>Currently. bootstrap REPL LOAD have added checkpoint identifiers in DB/table/partition object properties once the data/metadata related to the object is successfully loaded.If the Db exist and is not empty, then currently we are throwing exception. But need to support it for the retry scenario after a failure.If there is a retry of bootstrap load using the same dump, then instead of throwing error, we should check if any of the tables/partitions are completely loaded using the checkpoint identifiers. If yes, then skip it or else drop/create them again.If the bootstrap load is performed using different dump, then it should throw exception.Allow bootstrap on empty Db only if ckpt property is not set. Also, if bootstrap load is completed on the target Db, then shouldn't allow bootstrap retry at all.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="19758" opendate="2018-5-31 00:00:00" fixdate="2018-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set hadoop.version=3.1.0 in standalone-metastore</summary>
      <description>When HIVE-19243set hadoop.version=3.1.0 it did not change the value used in standalone-metastore which still uses 3.0.0-beta1. At the moment standalone-metastore is still a module of hive and so this can suck in the wrong code.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19768" opendate="2018-6-1 00:00:00" fixdate="2018-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Utility to convert tables to conform to Hive strict managed tables mode</summary>
      <description>Create a utility that can check existing hive tables and convert them if necessary to conform to strict managed tables mode. Managed non-transactional ORC tables will be converted to full transactional tables Managed non-transactional tables of other types will be converted to insert-only transactional tables Tables with non-native storage/schema will be converted to external tables.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19769" opendate="2018-6-1 00:00:00" fixdate="2018-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create dedicated objects for DB and Table names</summary>
      <description>Currently table names are always strings. Sometimes that string is just tablename, sometimes it is dbname.tablename. Sometimes the code expects one or the other, sometimes it handles either. This is burdensome for developers and error prone. With the addition of catalog to the hierarchy, this becomes even worse.I propose to add two objects, DatabaseName and TableName. These will track full names of each object. They will handle inserting default catalog and database names when those are not provided. They will handle the conversions to and from strings.These will need to be added to storage-api because ValidTxnList will use it.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="1977" opendate="2011-2-8 00:00:00" fixdate="2011-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DESCRIBE TABLE syntax doesn&amp;#39;t support specifying a database qualified table name</summary>
      <description>The syntax for DESCRIBE is broken. It should be:DESCRIBE [EXTENDED] [database DOT]table [column]but is actuallyDESCRIBE [EXTENDED] table[DOT col_name]Ref: http://dev.mysql.com/doc/refman/5.0/en/describe.html</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="19800" opendate="2018-6-5 00:00:00" fixdate="2018-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate submodules for pre and post upgrade and add rename file logic</summary>
      <description>this is a followup toHIVE-19751 which includesHIVE-19751 since it hasn't landed yetthis includes file rename logic andHIVE-19750 since it hasn't landed yet eithercc jdere</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.src.test.java.org.apache.hadoop.hive.upgrade.acid.TestUpgradeTool.java</file>
      <file type="M">upgrade-acid.src.main.java.org.apache.hadoop.hive.upgrade.acid.UpgradeTool.java</file>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.FileUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnConcatenate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnAddPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19801" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Add some missing classes to jdbc standalone jar and remove hbase classes</summary>
      <description></description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19806" opendate="2018-6-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests do not properly sort their output</summary>
      <description>A number of the tests produce unsorted output that happens to come out the same on people's laptops and the ptest infrastructure. But when run on a separate linux box the sort differences show up.</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.schema.evolution.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.interval.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.case.when.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.bround.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.parquet.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.selectindate.q</file>
      <file type="M">ql.src.test.queries.clientpositive.parquet.ppd.multifiles.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.schema.evolution.float.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.insert.overwrite.dynamic.partitions.q</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.insert.overwrite.directory.q</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.insert.into.dynamic.partitions.q</file>
    </fixedFiles>
  </bug>
  <bug id="19810" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StorageHandler fail to ship jars in Tez intermittently</summary>
      <description>Hive relies on StorageHandler to ship jars to backend automatically in several cases: JdbcStorageHandler, HBaseStorageHandler, AccumuloStorageHandler. This does not work reliably, in particular, the first dag in the session will have those jars, the second will not unless container is reused. In the later case, the containers allocated to first dag will be reused in the second dag so the container will have additional resources.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="19826" opendate="2018-6-7 00:00:00" fixdate="2018-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OrcRawRecordMerger doesn&amp;#39;t work for more than one file in non vectorized case</summary>
      <description>Key object in the map is reused and reset, leading to bizarre merges and wrong results.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19862" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgres init script has a glitch around UNIQUE_DATABASE</summary>
      <description>ALTER TABLE ONLY "DBS" ADD CONSTRAINT "UNIQUE_DATABASE" UNIQUE ("NAME");Should also include "CTLG_NAME".</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="19871" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add_partitions, create constraint calls and create_table_with_constraints do not properly handle client being configured with a non-Hive catalog.</summary>
      <description>If a client calls add_partitions(List&lt;Partition&gt; parts, boolean ifNotExists, boolean needResults)and the catalog name is set to a non-default value in the config file but unset in the partitions, the request to add the partition will fail with an error message "table not found" even when the table is valid.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.NonCatCallsWithCatalog.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="19890" opendate="2018-6-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Inherit bucket-id from original ROW_ID for delete deltas</summary>
      <description>The ACID delete deltas for unbucketed tables are written to arbitrary files, which should instead be shuffled using the bucket-id instead of hash(ROW__ID).</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="19892" opendate="2018-6-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable query results cache for for HiveServer2 doAs=true</summary>
      <description>If running HS2 with doAs=true, the temp query results directory will have ownership/permissions based on the doAs user. A subsequent query running as a different user may not be able to access this query results directory. Results caching will have to be disabled in this case.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="19899" opendate="2018-6-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support stored as JsonFile</summary>
      <description>This is to support "Create table ... stored as JsonFile" syntax.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.json.serde1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.json.serde1.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.java</file>
      <file type="M">ql.src.main.resources.META-INF.services.org.apache.hadoop.hive.ql.io.StorageFormatDescriptor</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOConstants.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.AbstractHCatStorerTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19908" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Block Insert Overwrite with Union All on full CRUD ACID tables using HIVE_UNION_SUBDIR_</summary>
      <description>This currently results in data loss. Will block and suggest using truncate + insert.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="1991" opendate="2011-2-14 00:00:00" fixdate="2011-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Shell to output number of mappers and number of reducers</summary>
      <description>Number of mappers and number of reducers are nice information to be outputted for users to know.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19917" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export of full CRUD transactional table fails if table is not in default database</summary>
      <description>The actual issues is fixed by HIVE-19861.This is a follow up to add a test case.Issue:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Can not create a Path from a null string at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:940) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:945) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.exec.DDLTask.createTableLike(DDLTask.java:5099) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:433) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeAcidExport(UpdateDeleteSemanticAnalyzer.java:195) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:106) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:288) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:658) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1813) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1760) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1755) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:194) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:257) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.operation.Operation.run(Operation.java:243) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:312) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]Caused by: java.lang.IllegalArgumentException: Can not create a Path from a null string at org.apache.hadoop.fs.Path.checkPathArg(Path.java:164) ~[hadoop-common-3.0.0.3.0.0.0-1485.jar:?] at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:180) ~[hadoop-common-3.0.0.3.0.0.0-1485.jar:?] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.createTempTable(SessionHiveMetaStoreClient.java:459) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:117) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:831) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:816) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at sun.reflect.GeneratedMethodAccessor124.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at com.sun.proxy.$Proxy55.createTable(Unknown Source) ~[?:?] at sun.reflect.GeneratedMethodAccessor124.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2768) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] at com.sun.proxy.$Proxy55.createTable(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:929) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485] ... 27 more2018-06-14T17:53:32,112 ERROR [07758225-f4e7-4fc2-a9e5-c6ed19e9fcfd HiveServer2-Handler-Pool: Thread-143]: metadata.Hive (:()) - Table tpch.tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39 not found: hive.tpch.tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39 table not found2018-06-14T17:53:32,113 ERROR [07758225-f4e7-4fc2-a9e5-c6ed19e9fcfd HiveServer2-Handler-Pool: Thread-143]: ql.Driver (:()) - FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39 at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeAcidExport(UpdateDeleteSemanticAnalyzer.java:198) at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:106) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:288) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:658) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1813) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1760) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1755) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:194) at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:257) at org.apache.hive.service.cli.operation.Operation.run(Operation.java:243) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527) at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:312) at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39 at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1141) at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1092) at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1079) at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeAcidExport(UpdateDeleteSemanticAnalyzer.java:196) ... 23 more</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
    </fixedFiles>
  </bug>
  <bug id="19967" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB Join : Need Optraits for PTFOperator ala GBY Op</summary>
      <description>The SMB join on one or more PTF Ops should reset the optraits keys just like GBY Op does.Currently there is no implementation of PTFOp optraits.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19969" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency order (dirlist) assessment fails in yetus run</summary>
      <description>As seen here, the dirlist step of yetus fails to determine order of modules to be built. It silently falls back to alphabetical order which may or may not work depending on the patch.Thu Jun 21 02:43:04 UTC 2018cd /data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958mvn -q exec:exec -Dexec.executable=pwd -Dexec.args=''/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/storage-api/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/upgrade-acid/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/classification/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/common/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/0.23/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/scheduler/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/aggregator/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/common/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/service-rpc/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/serdeUsage: java [-options] class [args...] (to execute a class) or java [-options] -jar jarfile [args...] (to execute a jar file)where options include:The problem is in standalone-metastore module: maven plugin 'exec' has a global config set executable=javadisregarding the dirlist task's -Dexec.executable=pwd and causing the above error.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19985" opendate="2018-6-25 00:00:00" fixdate="2018-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Skip decoding the ROW__ID sections for read-only queries</summary>
      <description>For a base_n file there are no aborted transactions within the file and if there are no pending delete deltas, the entire ACID ROW__ID can be skipped for all read-only queries (i.e SELECT), though it still needs to be projected out for MERGE, UPDATE and DELETE queries.This patch tries to entirely ignore the ACID ROW__ID fields for all tables where there are no possible deletes or aborted transactions for an ACID split.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19986" opendate="2018-6-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging of runtime statistics indicating when Hdfs Erasure Coding is used by MR</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ErasureProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.MapRedStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2ErasureCoding.java</file>
    </fixedFiles>
  </bug>
  <bug id="19989" opendate="2018-6-25 00:00:00" fixdate="2018-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore uses wrong application name for HADOOP2 metrics</summary>
      <description>Right now it is hardcoded as 'metastore'. It should instead be fetched from config like it was previously.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.metrics.Metrics.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20002" opendate="2018-6-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shipping jdbd-storage-handler dependency jars in LLAP</summary>
      <description>Shipping the following jars to LLAP to make jdbc storage-handler work: commons-dbcp, commons-pool, db specific jdbc jar whichever exists in classpath.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20009" opendate="2018-6-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix runtime stats for merge statement</summary>
      <description>pushed to branch-3 as well.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20021" opendate="2018-6-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fall back to Synthetic File-ids when getting a HdfsConstants.GRANDFATHER_INODE_ID</summary>
      <description>HDFS client implementations have multiple server implementations, which do not all support the inodes for file locations.If the client returns a 0 InodeId, fall back to the synthetic ones.</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="20037" opendate="2018-6-29 00:00:00" fixdate="2018-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print root cause exception&amp;#39;s toString() rather than getMessage()</summary>
      <description>When we run HoS job and if it fails for some errors, we are printing the exception message rather than exception toString(), for some exceptions, e.g., this java.lang.NoClassDefFoundError, we are missing the exception type information. Failed to execute Spark task Stage-1, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create Spark client for Spark session cf054497-b073-4327-a315-68c867ce3434: org/apache/spark/SparkConf)'If we use exception's toString(), it will be as follows and make more sense.Failed to execute Spark task Stage-1, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create Spark client for Spark session cf054497-b073-4327-a315-68c867ce3434: java.lang.NoClassDefFoundError: org/apache/spark/SparkConf)'</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="20039" opendate="2018-6-30 00:00:00" fixdate="2018-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning: Left Outer Join on bucketed table gives wrong result</summary>
      <description>Left outer join on bucketed table on certain cases gives wrong results.Depending on the order in which the table-scans are walked through, the FilterPruner might end up using the wrong table scan's table properties on the other table.</description>
      <version>2.3.2,3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20040" opendate="2018-6-30 00:00:00" fixdate="2018-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HTTP listen queue is 50 and SYNs are lost</summary>
      <description>When testing with 5000 concurrent users, the JDBC HTTP port ends up overflowing on SYNs when the HS2 gc pauses.This is because each getQueryProgress request is an independent HTTP request, so unlike the BINARY mode, there are more connections being established &amp; closed in HTTP mode.LISTEN 0 50 *:10004 *:* This turns into connection errors when enabling net.ipv4.tcp_abort_on_overflow=1, but the better approach is to enqueue the connections until the HS2 is done with its GC pause.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="20044" opendate="2018-7-1 00:00:00" fixdate="2018-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow Serde should pad char values and handle empty strings correctly</summary>
      <description>When Arrow Serde serializes char values, it loses padding. Also when it counts empty strings, sometimes it makes a smaller number. It should pad char values and handle empty strings correctly.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20045" opendate="2018-7-1 00:00:00" fixdate="2018-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hidden config list</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20046" opendate="2018-7-2 00:00:00" fixdate="2018-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove NUM_FILES check</summary>
      <description>// Since newly initialized empty table has 0 for the parameter. if (Long.parseLong(statsParams.get(StatsSetupConst.NUM_FILES)) == 0) { return true; }This doesn't look safe; # of files could be set to 0 by an invalid update, or potentially a parallel update that we cannot see (not sure if this is possible; there's some code in metastore that updates basic stats outside of the scope of the query).It would be better to remove this, and see if it breaks some tests. If we do need this, there should be a negative test at some point</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20047" opendate="2018-7-2 00:00:00" fixdate="2018-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove txnID argument for txn stats methods</summary>
      <description>Followup from HIVE-19975.W.r.t. write IDs and txn IDs, stats validity check currently verifies one of two things - that stats write ID is valid for query write ID list, or that stats txn ID (derived from write ID) is the same as the query txn ID.I'm not sure the latter check is needed; removing it would allow us to make a bunch of APIs a little bit simpler.ekoifman do you have any feedback? Can any stats reader (e.g. compile) observe stats written by the same txn; but in such manner that it doesn't have the write ID of the same-txn stats writer, in its valid write ID list? I'm assuming it's not possible, e.g. in multi statement txn each query would have the previous same-txn writer for the same table in its valid write ID list?</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IHMSHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TruncateTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RenamePartitionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ColStatsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.metastore.SynchronizedMetaStoreClient.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="20065" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore should not rely on jackson 1.x</summary>
      <description>somehow jackson 1.x is on the classpath in some ide-s ...and somehow 1.x org.codehaus jackson is being used from a dozen of classes - meanwhile the pom.xml doesn't mention it at all; but only a fasterxml's 2.9.0....I don't know where it gets the jackson 1.x implementation; but I think it shouldn't rely on that...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.messaging.json.TestJSONMessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.PartitionFiles.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONOpenTxnMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropFunctionMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropDatabaseMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropConstraintMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropCatalogMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateFunctionMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateDatabaseMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateCatalogMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCommitTxnMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterDatabaseMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAllocWriteIdMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddUniqueConstraintMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPrimaryKeyMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddNotNullConstraintMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddForeignKeyMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAcidWriteMessage.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAbortTxnMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="20073" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional tests for to_utc_timestamp function based on HIVE-20068</summary>
      <description>I have the following script and I'm at loss to explain the behavior. Possibly it's an older bug as we are using the 2.1.1 drivers . We noticed this issue when converting from US/Eastern into UTC and then back to US/Eastern. Everything that was in Status Date / Status Hour on 3/11/17 21:00:00 shifted 6 hours ahead into UTC ... then shifted back to 3/11/17 22:00:00 back in US/Eastern. The behavior appears to be the same using the constant EST5EDT. EDT was effective on 3/12 2 am, so the issue appears only at this boundary condition when we "spring ahead", but it at least on the surface seems incorrect.--------------------------------------------------------------------------------------------------------------------------&amp;#8211; Potential Issue with to_utc_timestamp---------------------------------------------------------------------------------------------------------------------------SELECT '2017-03-11 18:00:00', to_utc_timestamp(timestamp '2017-03-11 18:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 19:00:00', to_utc_timestamp(timestamp '2017-03-11 19:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 20:00:00', to_utc_timestamp(timestamp '2017-03-11 20:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 21:00:00', to_utc_timestamp(timestamp '2017-03-11 21:00:00','US/Eastern'); &amp;#8211; Shifts ahead 6 hours (???)_c0                 _c12017-03-11 21:00:00   2017-03-12 03:00:00SELECT '2017-03-11 22:00:00', to_utc_timestamp(timestamp '2017-03-11 22:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 23:00:00', to_utc_timestamp(timestamp '2017-03-11 23:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 00:00:00', to_utc_timestamp(timestamp '2017-03-12 00:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 01:00:00', to_utc_timestamp(timestamp '2017-03-12 01:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 02:00:00', to_utc_timestamp(timestamp '2017-03-12 02:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 03:00:00', to_utc_timestamp(timestamp '2017-03-12 03:00:00','US/Eastern'); &amp;#8211; Shifts ahead 4 hours as expectedSELECT '2017-03-12 04:00:00', to_utc_timestamp(timestamp '2017-03-12 04:00:00','US/Eastern'); &amp;#8211; Shifts ahead 4 hours as expectedSELECT '2017-03-12 05:00:00', to_utc_timestamp(timestamp '2017-03-12 05:00:00','US/Eastern'); &amp;#8211; Shifts ahead 4 hours as expected</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20103" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM: Only Aggregate DAG counters if at least one is used</summary>
      <description>status = dagClient.getDAGStatus(EnumSet.of(StatusGetOpts.GET_COUNTERS), checkInterval); TezCounters dagCounters = status.getDAGCounters();... if (dagCounters != null &amp;&amp; wmContext != null) { Set&lt;String&gt; desiredCounters = wmContext.getSubscribedCounters(); if (desiredCounters != null &amp;&amp; !desiredCounters.isEmpty()) { Map&lt;String, Long&gt; currentCounters = getCounterValues(dagCounters, vertexNames, vertexProgressMap, desiredCounters, done);Skip collecting DAG counters unless there at least one desired counter in wmContext.The AM has a hard-lock around the counters, so the current jstacks are full of java.lang.Thread.State: RUNNABLE at java.lang.String.intern(Native Method) at org.apache.hadoop.util.StringInterner.weakIntern(StringInterner.java:71) at org.apache.tez.common.counters.GenericCounter.&lt;init&gt;(GenericCounter.java:50) at org.apache.tez.common.counters.TezCounters$GenericGroup.newCounter(TezCounters.java:65) at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:92) at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104) - locked &lt;0x00007efb3ac7af38&gt; (a org.apache.tez.common.counters.TezCounters$GenericGroup) at org.apache.tez.common.counters.AbstractCounterGroup.aggrAllCounters(AbstractCounterGroup.java:204) at org.apache.tez.common.counters.AbstractCounters.aggrAllCounters(AbstractCounters.java:372) - eliminated &lt;0x00007efb3ac64ee8&gt; (a org.apache.tez.common.counters.TezCounters) at org.apache.tez.common.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:357) - locked &lt;0x00007efb3ac64ee8&gt; (a org.apache.tez.common.counters.TezCounters) at org.apache.tez.dag.app.dag.impl.TaskImpl.getCounters(TaskImpl.java:462) at org.apache.tez.dag.app.dag.impl.VertexImpl.aggrTaskCounters(VertexImpl.java:1342) at org.apache.tez.dag.app.dag.impl.VertexImpl.getAllCounters(VertexImpl.java:1202) at org.apache.tez.dag.app.dag.impl.DAGImpl.aggrTaskCounters(DAGImpl.java:755) at org.apache.tez.dag.app.dag.impl.DAGImpl.getAllCounters(DAGImpl.java:704) at org.apache.tez.dag.app.dag.impl.DAGImpl.getDAGStatus(DAGImpl.java:901) at org.apache.tez.dag.app.dag.impl.DAGImpl.getDAGStatus(DAGImpl.java:940) at org.apache.tez.dag.api.client.DAGClientHandler.getDAGStatus(DAGClientHandler.java:73)</description>
      <version>3.0.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="20111" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</summary>
      <description>Similar to HIVE-20085. HBase-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.viewjoins.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.format.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseMetaHook.java</file>
      <file type="M">hbase-handler.src.test.queries.negative.cascade.dbdrop.q</file>
      <file type="M">hbase-handler.src.test.queries.negative.generatehfiles.require.family.path.q</file>
      <file type="M">hbase-handler.src.test.queries.negative.hbase.ddl.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.external.table.ppd.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbasestats.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.binary.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.map.queries.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.map.queries.prefix.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.storage.queries.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.custom.key.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.custom.key2.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.custom.key3.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.ddl.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.decimal.decimal.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.bulk.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.joins.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.null.first.col.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.ppd.join.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.ppd.key.range.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.pushdown.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.queries.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.scan.params.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.single.sourced.multi.insert.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.timestamp.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.timestamp.format.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.viewjoins.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.ppd.key.ranges.q</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.generatehfiles.require.family.path.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.binary.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.map.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.map.queries.prefix.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key2.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.decimal.decimal.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.handler.bulk.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.joins.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.null.first.col.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.join.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.scan.params.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20112" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accumulo-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</summary>
      <description>Similar to HIVE-20085 and HIVE-20111. Accumulo-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.joins.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.index.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.custom.key2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.custom.key.q.out</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.single.sourced.multi.insert.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.queries.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.predicate.pushdown.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.joins.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.index.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.custom.key2.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.custom.key.q</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20113" opendate="2018-7-6 00:00:00" fixdate="2018-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shuffle avoidance: Disable 1-1 edges for sorted shuffle</summary>
      <description>The sorted shuffle avoidance can have some issues when the shuffle data gets broken up into multiple chunks on disk.The 1-1 edge cannot skip the tez final merge - there's no reason for 1-1 to have a final merge at all, it should open a single compressed file and write a single index entry.Until the shuffle issue is resolved &amp; a lot more testing, it is prudent to disable the optimization for sorted shuffle edges and stop rewriting the RS(sorted) = = = RS(sorted) into RS(sorted) = = = RS(FORWARD).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkJoinDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.no.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.is.not.distinct.from.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partition.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.shared.scan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.reddedup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedwork.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20123" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix masking tests after HIVE-19617</summary>
      <description>Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed.</description>
      <version>3.1.0,3.0.0,3.2.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20131" opendate="2018-7-10 00:00:00" fixdate="2018-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Script changes for creating txn write notification in 3.2.0 files</summary>
      <description>1. Changepartition name size from 1024 to 767 . (mySQL 5.6 and before that supports max 767 length keys)2. Remove the create txn_write_notification_log table creation from 3.1.0 scripts and add a new scripts for 3.2.03. Remove the file 3.1.0-to-4.0.0 and instead add file for 3.2.0-to-4.0.0 and 3.1.0-to-3.2.04.Change in metastore init schema xml file to take 4.0.0 instead of 3.1.0 as current version.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade.order.postgres</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.1.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade.order.oracle</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.1.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade.order.mysql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.1.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade.order.mssql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.1.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade.order.derby</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.1.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.0.0-to-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
    </fixedFiles>
  </bug>
  <bug id="20152" opendate="2018-7-12 00:00:00" fixdate="2018-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>reset db state, when repl dump fails, so rename table can be done</summary>
      <description>If a repl dump command is run and it fails for some reason while doing table level dumps, the state set on the db parameters is not reset and hence no table / partition renames can be done. the property to be reset is prefixed with key bootstrap.dump.state and it should be unset. meanwhile the workaround is describe database extended [db_name]; assuming property is 'bootstrap.dump.state.something'alterdatabase [db_name] set dbproperties ('bootstrap.dump.state.something'='idle');"</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20164" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Murmur Hash : Make sure CTAS and IAS use correct bucketing version</summary>
      <description>With the migration to Murmur hash, CTAS and IAS from old table version to new table version does not work as intended and data is hashed using old hash logic.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20183" opendate="2018-7-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inserting from bucketed table can cause data loss, if the source table contains empty buckets</summary>
      <description>Could be reproduced by the following:set hive.enforce.bucketing=true;set hive.enforce.sorting=true;set hive.optimize.bucketingsorting=true;create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS;insert into bucket1 values (1, 'abc'), (3, 'abc');select * from bucket1;+-------------+--------------+| bucket1.id | bucket1.val |+-------------+--------------+| 3 | abc || 1 | abc |+-------------+--------------+create table bucket2 like bucket1;insert overwrite table bucket2 select * from bucket1;select * from bucket2;+-------------+--------------+| bucket2.id | bucket2.val |+-------------+--------------+| 1 | abc |+-------------+--------------+</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20191" opendate="2018-7-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit patch application doesn&amp;#39;t fail if patch is empty</summary>
      <description>I've created some backport tickets to branch-3 (e.g. HIVE-20181) and made the mistake of uploading the patch files with wrong filename (. instead of - between version and branch).These get applied on master, where they're already present, since git apply with -3 won't fail if patch is already there. Tests are run on master instead of failing.I think the patch application should fail if the patch is empty and branch selection logic should probably fail too if the patch name is malformed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20193" opendate="2018-7-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cboInfo is not present in the explain plan json</summary>
      <description>cboInfo attribute is not present in the explain plan json that is provided to the pre exec hook in hive.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20221" opendate="2018-7-23 00:00:00" fixdate="2018-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase column width for partition_params</summary>
      <description>HIVE-12274 have addressed almost all metastore columns; however it have left out PARTITION_PARAMS; so in case of a partitioned tables the limits are still there.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.1.0-to-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.1.0-to-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.1.0-to-3.2.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.2.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.1.0-to-3.2.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.2.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.1.0-to-3.2.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.2.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="20228" opendate="2018-7-24 00:00:00" fixdate="2018-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>configure repl configuration directories based on user running hiveserver2</summary>
      <description>If a custom uesr is used to run hive server 2, then repl subsystem should use the directories within the user's home directory for various configurations rather than use the default /user/hive/</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20241" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partitioning spec in CTAS statements</summary>
      <description>Currently, for partitioned tables we will declare the table and insert the data in different operations. This issue is to extend CTAS statement to support specifying partition columns.For instance:CREATE TABLE partition_ctas_1 PARTITIONED BY (key) ASSELECT value, key FROM src where key &gt; 200 and key &lt; 300;</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20242" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query results cache: Improve ability of queries to use pending query results</summary>
      <description>HIVE-19138 allowed a currently running query to wait on the pending results of an already running query. gopalv, after testing with high concurrency, suggested further improving this by having a way to use the switch to using the results cache even at the end of query compilation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20244" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>forward port HIVE-19704 to master</summary>
      <description>Apparently this logic is still there and can be engaged in some cases, like when one file takes the entire cache from a single large read.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.MetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20245" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix NULL / Wrong Results issues in BETWEEN / IN</summary>
      <description>Write new UT tests that use random data and intentional isRepeating batches to checks for NULL and Wrong Results for vectorized BETWEEN and IN.Add, missing vectorization classes for BETWEEN PROJECTION.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetween.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringUnary.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorSubStr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20246" opendate="2018-7-26 00:00:00" fixdate="2018-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable collecting stats by using DO_NOT_UPDATE_STATS table property</summary>
      <description>By default, Hive collects stats when running operations like alter table partition(s), create table, and create external table. However, collecting stats requires Metastore lists all files under the table directory and the file listing operation can be very expensive particularly on filesystems like S3.HIVE-18743 made DO_NOT_UPDATE_STATS table property could be selectively prevent stats collection. This Jira aims at introducing DO_NOT_UPDATE_STATS table property into the MetaStoreUtils.updatePartitionStatsFast. By adding this, user can be selectively prevent stats collection when doing alter table partition(s) operation at table level. For example, set 'Alter Table S3_Table set tblproperties('DO_NOT_UPDATE_STATS'='TRUE');' MetaStore will not collect stats for the specified S3_Table when alter table add partition(key1=val1, key2=val2);</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20247" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cleanup issues in LLAP IO after cache OOM</summary>
      <description>LLAP IO creates unallocated buffer objects inside the read-related data structures, then allocates them in bulk, then decompresses into them and increfs them.If allocate or decompress steps fail, it's hard for the higher-level cleanup to tell what the state of the buffers in the read-related structures is - they may be unallocated, allocated but not incref-ed, or incref-ed.Some cleanup paths only deal with the latter case, resulting in bugs.Moreover, currently allocator returns partial results on such error. The allocation should be all-or-nothing.This only happens on one path, others allocate and use buffers in a single place.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20248" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up some TODOs after txn stats merge</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
    </fixedFiles>
  </bug>
  <bug id="20262" opendate="2018-7-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stats annotation rule for the UDTFOperator</summary>
      <description>User Defined Table Functions (UDTFs) change the number of rows of the output. A common UDTF is the explode() method that createsa row for each element for each array in the input column.Right now, the number of output rows is equal to the number of input rows. But if the average number of output rows is bigger than 1, the resulting number of rows is underestimated in theexecution plan.Implement arulethat can have a factor X as a parameterand for each UDTF function predict that:number of output rows = X * number of input rows</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20306" opendate="2018-8-3 00:00:00" fixdate="2018-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement projection spec for fetching only requested fields from partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java.orig</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java.orig</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MSerDeInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="20307" opendate="2018-8-3 00:00:00" fixdate="2018-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for filterspec to the getPartitions with projection API</summary>
      <description>Implement the BY_EXPR, BY_NAMES and BY_VALUES filter modes for the projection API to filter the partitions returned as discussed in the design doc</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestGetPartitionsUsingProjection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="20338" opendate="2018-8-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Force synthetic file-id for filesystems which have HDFS protocol impls with POSIX mutation semantics</summary>
      <description>HDFS client protocol is not a guarantee of the immutability of files - the synthetic file-id includes the mtime of the file as well, which is a fail-safe for filesystems which implement the client wire protocol without offering the same storage side restrictions on immutability (i.e allow NFS read-write-modify on the backend).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20339" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Lift unneeded restriction causing some PTF with RANK not to be vectorized</summary>
      <description>Unnecessary: "PTF operator: More than 1 argument expression of aggregation function rank"</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2034" opendate="2011-3-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport HIVE-1991 after overridden by HIVE-1950</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="20340" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Needs Explicit CASTs from Timestamp to STRING when the output of timestamp function is used as String</summary>
      <description>Druid time expressions return numeric values in form of ms (instead of formatted timestamp). Because of this expressions/function which expects its argument as string type ended up returning different values for time expressions input.e.g.SELECT SUBSTRING(to_date(datetime0),4) FROM tableau_orc.calcs;| 4-07-25 |SELECT SUBSTRING(to_date(datetime0),4) FROM druid_tableau.calcs;| 0022400000 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM tableau_orc.calcs;| 2004-07-17 00:00:00 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM druid_tableau.calcs;| 1090454400000 00:00:00 |Druid needs explicit cast to make this work</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="20410" opendate="2018-8-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>aborted Insert Overwrite on transactional table causes "Not enough history available for..." error</summary>
      <description>suppose insert overwrite T values(1)is aborted.this creates a base_x directory (for insert-only transactional tables currently and for full CRUD once 'rename' in the MoveTask is eliminated) but subsequent read fails with "Not enough history available for..." error.The problem is that the logic to produce this exception finds this base_x but treats it as if it was produced by a compactor, in which case the error would'v been appropriate.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20413" opendate="2018-8-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"cannot insert NULL" for TXN_WRITE_NOTIFICATION_LOG in Oracle</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.1.0-to-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug id="20437" opendate="2018-8-22 00:00:00" fixdate="2018-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle schema evolution from float, double and decimal</summary>
      <description>When data created as float, double or decimal in parquet format is read back using some other type, errors are seen. Parquet should behave just like any other format. If the value is valid for the new type, data is retuned otherwise null has to be returned.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.type.change.test.int.vectorized.q</file>
      <file type="M">ql.src.test.queries.clientpositive.type.change.test.int.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20439" opendate="2018-8-22 00:00:00" fixdate="2018-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the inflated memory limit during join selection for llap</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.unionDistinct.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join32.lessSize.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20480" opendate="2018-8-28 00:00:00" fixdate="2018-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement column stats annotation rules for the UDTFOperator: Follow up for HIVE-20262</summary>
      <description>Implementing the rule for column stats: Follow-up task forHIVE-20262</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20521" opendate="2018-9-8 00:00:00" fixdate="2018-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 doAs=true has permission issue with hadoop.tmp.dir, with MR and S3A filesystem</summary>
      <description>This is a result of changes in HIVE-18858. As described by puneetj in HIVE-18858 -This seems to have broken working scenarios with Hive MR. We now see hadoop.tmp.dir is always set to /tmp/hadoop-hive (in job.xml). This creates problems on a multi-tenant hadoop cluster since ownership of tmp folder is set to the user who executes the jobs first and other users fails to write to tmp folder.E.g. User1 run job and /tmp/hadoop-hive is created on worker node with ownership to user1 and sibsequently user2 tries to run a job and job fails due to no write permission on /tmp/hadoop-hive/Old behavior allowed multiple tenants to write to their respective tmp folders which was secure and contention free. User1 - /tmp/hadoop-user1, User2 - /tmp/hadoop-user2.The change in HIVE-18858 causes variable expansion to happen in HiveServer2, while it was happening in the tasks (ExecMapper, ExecReducer) before that change. THis causes "/tmp/hadoop-{user.name}" to be expanded as /tmp/hadoop-hive instead of /tmp/hadoop-user1</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.UtilsForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20659" opendate="2018-9-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21028" opendate="2018-12-11 00:00:00" fixdate="2018-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_table_meta should use a fetch plan to avoid race conditions ending up in NucleusObjectNotFoundException</summary>
      <description>The getTableMeta call retrieves the tables, loops through the tables and during this loop it retrieves the database object to get the containing database name. DataNuclues does a lazy retrieval and so, when the first call to get all the tables is done, it does not retrieve the database objects.When this query is executedquery = pm.newQuery(MTable.class, filterBuilder.toString());it loads all the tables, and when you dotable.getDatabase().getName()it then goes and retrieves the database object.However, there could be another thread which actually has deleted the database!! If this happens, we end up with exceptions such as2018-12-04 22:25:06,525 INFO DataNucleus.Datastore.Retrieve: [pool-7-thread-191]: Object with id "6930391[OID]org.apache.hadoop.hive.metastore.model.MTable" not found !2018-12-04 22:25:06,527 WARN DataNucleus.Persistence: [pool-7-thread-191]: Exception thrown by StateManager.isLoadedNo such database roworg.datanucleus.exceptions.NucleusObjectNotFoundException: No such database rowWe see this happen especially with calls which retrieve all the tables in all the databases (basically a call to get_table_meta with dbNames="&amp;#42;" and tableNames="&amp;#42;").To avoid this, we can define a custom fetch plan and activate it only for the get_table_meta query. This fetch plan would fetch the database object along with the MTable object.We would first create a fetch plan on the pmfpmf.getFetchGroup(MTable.class, "mtable_db_fetch_group").addMember("database");Then we use it just before calling the querypm.getFetchPlan().addGroup("mtable_db_fetch_group");query = pm.newQuery(MTable.class, filterBuilder.toString());Collection&lt;MTable&gt; tables = (Collection&lt;MTable&gt;) query.executeWithArray(...);...Before the API call ends, we can remove the fetch plan bypm.getFetchPlan().removeGroup("mtable_db_fetch_group");</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="21029" opendate="2018-12-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External table replication for existing deployments running incremental replication.</summary>
      <description>Existing deployments using hive replication do not get external tables replicated. For such deployments to enable external table replication they will have to provide a specific switch to first bootstrap external tables as part of hive incremental replication, following which the incremental replication will take care of further changes in external tables.The switch will be provided by an additional hive configuration (for ex: hive.repl.bootstrap.external.tables) and is to be used in WITH clause of REPL DUMP command. Additionally the existing hive config hive.repl.include.external.tables will always have to be set to "true" in the above clause. Proposed usage for enabling external tables replication on existing replication policy.1. Consider an ongoing repl policy &lt;db1&gt; in incremental phase.Enable hive.repl.include.external.tables=true and hive.repl.bootstrap.external.tables=true in next incremental REPL DUMP. Dumps all events but skips events related to external tables. Instead, combine bootstrap dump for all external tables under _bootstrap directory. Also, includes the data locations file "_external_tables_info. LIMIT or TO clause shouldnt be there to ensure the latest events are dumped before bootstrap dumping external tables.2. REPL LOAD on this dump applies all the events first, copies external tables data and then bootstrap external tables (metadata). It is possible that the external tables (metadata) are not point-in time consistent with rest of the tables. But, it would be eventually consistent when the next incremental load is applied. This REPL LOAD is fault tolerant and can be retried if failed.3. All future REPL DUMPs on this repl policy should set hive.repl.bootstrap.external.tables=false. If not set to false, then target might end up having inconsistent set of external tables as bootstrap wouldnt clean-up any dropped external tables.</description>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.ConstraintEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21034" opendate="2018-12-12 00:00:00" fixdate="2018-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to schematool to drop Hive databases</summary>
      <description>An option to remove all Hive managed data could be a useful addition to schematool.I propose to introduce a new flag -dropAllDatabases that would drop all databases with CASCADE to remove all data of managed tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2106" opendate="2011-4-11 00:00:00" fixdate="2011-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the number of operator counter</summary>
      <description>Currently Hadoop counters have to be defined as enum (hardcoded) and we support up to 400 counters now. This limit the number of operators to 100 (each operator has 4 counters). We need to increase the hadoop counters or change the Hive code to use Hadoop 0.20 API.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21095" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Show create table&amp;#39; should not display a time zone for timestamp with local time zone</summary>
      <description>SHOW CREATE TABLE shows the time zone that the table was created in (if it contains a TIMESTAMPTZ column). This is also misleading, since it has nothing to do with the actual data or server or user time zone.e.g.hive&gt; set time zone America/Los_Angeles;hive&gt; create table text_local (ts timestamp with local time zone) stored as textfile;hive&gt; show create table text_local;CREATE TABLE `text_local`( `ts` timestamp with local time zone('America/Los_Angeles'))should be:hive&gt; show create table text_local;CREATE TABLE `text_local`( `ts` timestamp with local time zone)This was discussed in the community doc Consistent timestamp types in Hadoop SQL engines</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21107" opendate="2019-1-9 00:00:00" fixdate="2019-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot find field" error during dynamically partitioned hash join</summary>
      <description>This occurs in non-CBO path with dynamic partitioned join + constant propagation ON.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21109" opendate="2019-1-9 00:00:00" fixdate="2019-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support stats replication for ACID tables.</summary>
      <description>Transactional tables require a writeID associated with the stats update. This writeId needs to be in sync with the writeId on the source and henceneeds to be replicated from the source.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.CreateTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.CreateTableDesc.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestStatsReplicationScenariosNoAutogather.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestStatsReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="21159" opendate="2019-1-24 00:00:00" fixdate="2019-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify Merge statement logic to perform Update split early</summary>
      <description></description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MergeSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21226" opendate="2019-2-7 00:00:00" fixdate="2019-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude read-only transactions from ValidTxnList</summary>
      <description>Once HIVE-21114 is done, we should make sure that ValidTxnList doesn't contain any read-only txns in the exceptions list since by definition there is no data tagged with such txnid.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnCommonUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21239" opendate="2019-2-11 00:00:00" fixdate="2019-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline help LDAP connection example incorrect</summary>
      <description>There'sthe following connection example string in the beeline -h command output:5. Connect using LDAP authentication$ beeline -u jdbc:hive2://hs2.local:10013/default &lt;ldap-username&gt; &lt;ldap-password&gt;When a user attempts to connect like above, it'll fail with LDAP authentication failure. This is because username and passwords are not picked up in the shown form. A working example would be:$ beeline -n &lt;ldap-username&gt; -p &lt;ldap-password&gt; -u jdbc:hive2://hs2.local:10013/default</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21363" opendate="2019-2-28 00:00:00" fixdate="2019-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ldap auth issue: group filter match should be case insensitive</summary>
      <description>Configure HiveServer2 with LDAP auth with (enable ldap, ldap URI, baseDN, userDNPattern, groupDNPattern and groupFilter). If the specified groupFilter case is different than the actual one in directory, then Hive cannot find a match and errors out.For example:groupFilter value=&lt;groupTest&gt;group name in directory server=grouptest.Similar search works by using other ldap clients like ldapsearch (ldap searches are case insensitive). While it is not a major issue as the workaround would be to configure the exact name, it is an easy fix that we should support out of box.</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestGroupFilter.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="21645" opendate="2019-4-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include CBO json plan in explain formatted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21651" opendate="2019-4-26 00:00:00" fixdate="2019-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move protobuf serde into hive-exec.</summary>
      <description>The serde and input format is not accessible without doing an add jar or modifying hive aux libs. Moving it to hive-exec will let us use the serde.Can't move the serde to hive/serde since it depends on ProtobufMessageWriter which is in hive-exec.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.HiveEvents.proto</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.io.encoded.TestVectorDeserializeOrcWriter.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.serde2.TestProtoMessageSerDe.java</file>
      <file type="M">contrib.src.protobuf-test.SampleProtos.proto</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufMessageSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufBytesWritableSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.input.ProtobufMessageInputFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.input.package-info.java</file>
      <file type="M">contrib.src.gen-test.protobuf.gen-java.org.apache.hadoop.hive.contrib.serde2.SampleProtos.java</file>
      <file type="M">contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21832" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New metrics to get the average queue/serving/response time</summary>
      <description>SimpleDescriptiveStatisticswith window size would do here. Time is not important in this case.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.WmFragmentCounters.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21833" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Authorization in Hive based on object ownership</summary>
      <description>Background: Currently Hive Authorizer for Ranger does not provide owner information for Hive objects as part of AuthZ calls. This has resulted in gaps with respect to Sentry AuthZ and customers/partners cannot leverage privileges for owners in their authorization model.User Story: As an enterprise security admin, I need to be able to set privileges based on Hive object ownership for setting up access controls in Ranger so that I can provide appropriate protections and permissions for my enterprise users.Acceptance criteria:1) Owner information is available in Hive -Ranger AuthZ calls2) Ranger admin users can use owner information to set policies based on object ownership in Ranger UI and APIs3) OWNER Macro based policies continue to work for Hive objects</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21834" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid unnecessary calls to simplify filter conditions</summary>
      <description>Every time we create a filter, we try to simplify its condition. However, we already have a rule that simplifies the expressions and it is within the same loop as most of the rules that end up creating new filters. Hence, it may seem like we could remove some of the calls to simplify those conditions.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="21836" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache directory server version to 1.5.7</summary>
      <description>I've bumped into some issues when downloading 1.5.6 artifacts...changing it to 1.5.7 worked fineit seems apacheds is only used during testing</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21952" opendate="2019-7-3 00:00:00" fixdate="2019-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should allow to delete serde properties too, not just add them</summary>
      <description>Hive should allow to delete serde properties not just add/change themWe have a use case when a presence of certain serde propertiescauses issues and we want to delete just that one serde property.It's not currently possible.Thanks.</description>
      <version>2.3.5,3.0.0,4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.table.storage.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.table.storage.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.storage.serde.AlterTableSetSerdePropsAnalyzer.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21998" opendate="2019-7-16 00:00:00" fixdate="2019-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-21823 commit message is wrong</summary>
      <description>https://github.com/apache/hive/commit/4853a44b2fcfa702d23965ab0d3835b6b57954c4The Jira message is wrong. Reuses previous commit message.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22072" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Altering table to make a column change does not update constraints references</summary>
      <description>The constraint will still point to old column descriptor incorrectly.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22089" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson to 2.9.9</summary>
      <description></description>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22090" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jetty to 9.3.27</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22204" opendate="2019-9-13 00:00:00" fixdate="2019-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline option to show/not show execution report</summary>
      <description>Currently, &amp;#45;&amp;#45;silent=true will also remove the short report about execution (includes number of rows returned by a query and execution time). It would be interesting to control whether we want to show that report even if &amp;#45;&amp;#45;silent=true, e.g., using an option &amp;#45;&amp;#45;report=true. Default (existing) behavior should not change.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="2225" opendate="2011-6-16 00:00:00" fixdate="2011-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Purge expired events</summary>
      <description>HIVE-2215 adds the ability to add events in metastore. These events needs to be purged as they have limited life.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMarkPartition.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22416" opendate="2019-10-28 00:00:00" fixdate="2019-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR-related operation logs missing when parallel execution is enabled</summary>
      <description>Repro steps: 1. Happy path, parallel execution disabled0: jdbc:hive2://localhost:10000&gt; set hive.exec.parallel=false;No rows affected (0.023 seconds)0: jdbc:hive2://localhost:10000&gt; select count (*) from t1;INFO : Compiling command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d): select count (*) from t1INFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)INFO : Completed compiling command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d); Time taken: 0.309 secondsINFO : Executing command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d): select count (*) from t1WARN : INFO : Query ID = karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7dINFO : Total jobs = 1INFO : Launching Job 1 out of 1INFO : Starting task [Stage-1:MAPRED] in serial modeINFO : Number of reduce tasks determined at compile time: 1INFO : In order to change the average load for a reducer (in bytes):INFO : set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;INFO : In order to limit the maximum number of reducers:INFO : set hive.exec.reducers.max=&lt;number&gt;INFO : In order to set a constant number of reducers:INFO : set mapreduce.job.reduces=&lt;number&gt;DEBUG : Configuring job job_local495362389_0008 with file:/tmp/hadoop/mapred/staging/karencoppage495362389/.staging/job_local495362389_0008 as the submit dirDEBUG : adding the following namenodes' delegation tokens:[file:///]DEBUG : Creating splits at file:/tmp/hadoop/mapred/staging/karencoppage495362389/.staging/job_local495362389_0008INFO : number of splits:0INFO : Submitting tokens for job: job_local495362389_0008INFO : Executing with tokens: []INFO : The url to track the job: http://localhost:8080/INFO : Job running in-process (local Hadoop)INFO : 2019-10-28 15:26:22,537 Stage-1 map = 0%, reduce = 100%INFO : Ended Job = job_local495362389_0008INFO : MapReduce Jobs Launched: INFO : Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSINFO : Total MapReduce CPU Time Spent: 0 msecINFO : Completed executing command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d); Time taken: 6.497 secondsINFO : OKDEBUG : Shutting down query select count (*) from t1+-----+| c0 |+-----+| 0 |+-----+1 row selected (11.874 seconds)2. Faulty path, parallel execution enabled0: jdbc:hive2://localhost:10000&gt; set hive.server2.logging.operation.level=EXECUTION;No rows affected (0.236 seconds)0: jdbc:hive2://localhost:10000&gt; set hive.exec.parallel=true;No rows affected (0.01 seconds)0: jdbc:hive2://localhost:10000&gt; select count (*) from t1;INFO : Compiling command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77): select count (*) from t1INFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)INFO : Completed compiling command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77); Time taken: 4.707 secondsINFO : Executing command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77): select count (*) from t1WARN : INFO : Query ID = karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77INFO : Total jobs = 1INFO : Launching Job 1 out of 1INFO : Starting task [Stage-1:MAPRED] in parallelINFO : MapReduce Jobs Launched: INFO : Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSINFO : Total MapReduce CPU Time Spent: 0 msecINFO : Completed executing command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77); Time taken: 44.577 secondsINFO : OKDEBUG : Shutting down query select count (*) from t1+-----+| c0 |+-----+| 0  |+-----+1 row selected (54.665 seconds)The issue is that Log4J stores the session ID and query ID in some atomic thread metadata (org.apache.logging.log4j.ThreadContext.getImmutableContext()). If the queryId is missing from this metadata, then the RoutingAppender (which is defined programmatically in LogDivertAppender) will route the log to a NullAppender, which logs nothing. If the queryId is present, then the RoutingAppender routes the event to the "query-appender", which will log the line in the operation log/Beeline. This is not happening in a multi-threaded context since new threads created for parallel query execution do not have the queryId/sessionId metadata.The solution is to addthe queryId/sessionId metadata to any new threads created for MR work.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="2246" opendate="2011-6-30 00:00:00" fixdate="2011-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dedupe tables&amp;#39; column schemas from partitions in the metastore db</summary>
      <description>Note: this patch proposes a schema change, and is therefore incompatible with the current metastore.We can re-organize the JDO models to reduce space usage to keep the metastore scalable for the future. Currently, partitions are the fastest growing objects in the metastore, and the metastore keeps a separate copy of the columns list for each partition. We can normalize the metastore db by decoupling Columns from Storage Descriptors and not storing duplicate lists of the columns for each partition. An idea is to create an additional level of indirection with a "Column Descriptor" that has a list of columns. A table has a reference to its latest Column Descriptor (note: a table may have more than one Column Descriptor in the case of schema evolution). Partitions and Indexes can reference the same Column Descriptors as their parent table.Currently, the COLUMNS table in the metastore has roughly (number of partitions + number of tables) * (average number of columns pertable) rows. We can reduce this to (number of tables) * (average number of columns per table) rows, while incurring a small cost proportional to the number of tables to store the Column Descriptors.Please see the latest review board for additional implementation details.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22580" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakyness in TestDbTxnManager2</summary>
      <description>Different TestDbTxnManager2 tests are failing intermittently with:2019-12-03T20:18:49,269 DEBUG [main] pool.PoolBase: HikariPool-1 - Reset (autoCommit) on connection org.apache.derby.impl.jdbc.EmbedConnection@912054991 (XID = 347), (SESSIONID = 7), (DATABASE = memory:/data/jenkins/workspace/hive-TestDbTxnManager2/hive/ql/target/tmp/junit_metastore_db), (DRDAID = null) 2019-12-03T20:18:49,270 ERROR [main] metastore.RetryingHMSHandler: MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: Table/View 'NEXT_TXN_ID' does not exist. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source) at org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source) at com.zaxxer.hikari.pool.ProxyStatement.executeQuery(ProxyStatement.java:108) at com.zaxxer.hikari.pool.HikariProxyStatement.executeQuery(HikariProxyStatement.java) at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:604) at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:8095) at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy38.open_txns(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxnsIntr(HiveMetaStoreClient.java:3139) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxn(HiveMetaStoreClient.java:3103) at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) at com.sun.proxy.$Proxy39.openTxn(Unknown Source) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:250) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:234) at org.apache.hadoop.hive.ql.Driver.openTransaction(Driver.java:915) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:640) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1504) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1564) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1387) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1376) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.dropTable(TestDbTxnManager2.java:974) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testLockBlockedBy(TestDbTxnManager2.java:396) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)Caused by: ERROR 42X05: Table/View 'NEXT_TXN_ID' does not exist. at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.impl.sql.compile.FromBaseTable.bindTableDescriptor(Unknown Source) at org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(Unknown Source) at org.apache.derby.impl.sql.compile.FromList.bindTables(Unknown Source) at org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(Unknown Source) at org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(Unknown Source) at org.apache.derby.impl.sql.compile.DMLStatementNode.bind(Unknown Source) at org.apache.derby.impl.sql.compile.CursorNode.bindStatement(Unknown Source) at org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source) at org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source) at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source) ... 59 more) at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:570) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:8095) at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy38.open_txns(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxnsIntr(HiveMetaStoreClient.java:3139) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxn(HiveMetaStoreClient.java:3103) at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) at com.sun.proxy.$Proxy39.openTxn(Unknown Source) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:250) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:234) at org.apache.hadoop.hive.ql.Driver.openTransaction(Driver.java:915) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:640) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1504) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1564) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1387) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1376) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.dropTable(TestDbTxnManager2.java:974) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testLockBlockedBy(TestDbTxnManager2.java:396) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)Before this failure we are seeing this:2019-12-03T20:18:49,180 WARN [Heartbeater-1] pool.ProxyConnection: HikariPool-3 - Connection org.apache.derby.impl.jdbc.EmbedConnection@1077215639 (XID = 389), (SESSIONID = 49), (DATABASE = memory:/data/jenkins/workspace/hive-TestDbTxnManager2/hive/ql/target/tmp/junit_metastore_db), (DRDAID = null) marked as broken because of SQLSTATE(08000), ErrorCode(40000)java.sql.SQLNonTransientConnectionException: Connection closed by unknown interrupt. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedResultSet.closeOnTransactionError(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedResultSet.movePosition(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedResultSet.next(Unknown Source) ~[derby-10.14.2.0.jar:?] at com.zaxxer.hikari.pool.HikariProxyResultSet.next(HikariProxyResultSet.java) ~[HikariCP-2.6.1.jar:?] at org.datanucleus.store.rdbms.query.ForwardQueryResult.initialise(ForwardQueryResult.java:99) ~[datanucleus-rdbms-4.1.19.jar:?] at org.datanucleus.store.rdbms.query.JDOQLQuery.performExecute(JDOQLQuery.java:703) ~[datanucleus-rdbms-4.1.19.jar:?] at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?] at org.datanucleus.store.query.Query.executeWithArray(Query.java:1744) ~[datanucleus-core-4.1.17.jar:?] at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?] at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:216) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:246) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:188) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:513) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:432) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:385) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77) ~[hadoop-common-3.1.1.7.1.0.0-SNAPSHOT.jar:?] at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137) ~[hadoop-common-3.1.1.7.1.0.0-SNAPSHOT.jar:?] at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:59) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:781) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:749) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:743) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:605) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_202] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_202] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:80) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:9820) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:172) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:112) ~[classes/:?] at sun.reflect.GeneratedConstructorAccessor281.newInstance(Unknown Source) ~[?:?] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_202] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_202] at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:95) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:119) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4842) ~[classes/:?] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4910) ~[classes/:?] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4890) ~[classes/:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getMS(DbTxnManager.java:192) ~[classes/:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:632) ~[classes/:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.lambda$run$0(DbTxnManager.java:1038) ~[classes/:?] at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_202] at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_202] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) [hadoop-common-3.1.1.7.1.0.0-SNAPSHOT.jar:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:1037) [classes/:?] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_202] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_202] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_202] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_202] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202]Caused by: org.apache.derby.iapi.error.StandardException: Connection closed by unknown interrupt. at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.iapi.util.InterruptStatus.setInterrupted(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.iapi.util.InterruptStatus.throwIf(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.checkCancellationFlag(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.BulkTableScanResultSet.getNextRowCore(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.getNextRow(Unknown Source) ~[derby-10.14.2.0.jar:?] ... 57 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="22588" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flush the remaining rows for the rest of the grouping sets when switching the vector groupby mode</summary>
      <description>Flush the remaining rows for the rest of the grouping sets when switching the vector groupby mode</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2259" opendate="2011-7-5 00:00:00" fixdate="2011-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip comments in hive script</summary>
      <description>If you specify something like:&amp;#8211; This is a commentadd jar jar_path;select * from my_table;This fails.I have created a fix to skip the commented lines.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22652" opendate="2019-12-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey push through Group by with Grouping sets</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.topnkey.TopNKeyPushdownProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22653" opendate="2019-12-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove commons-lang leftovers</summary>
      <description>HIVE-7145 removed commons-lang - in favor of commons-lang3 - as a direct dependency, however a high number of files still refer to commons-lang, which is transitively brought in either way.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestResultProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.UnitTestPropertiesParser.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.CacheTag.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.DatabaseRule.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskValidate.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DBTokenStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStoreProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PersistenceManagerProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDirectSqlUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.client.builder.ConstraintBuilder.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CacheUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AuthFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.HdfsUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreLdapAuthenticationProviderImpl.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ldap.LdapUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ldap.GroupFilterFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestHiveSQLException.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.http.JdbcJarDownloadServlet.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestReflectionObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataOutputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataInputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyObjectInspectorParametersImpl.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.VectorizedColumnReaderTestBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedMapColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedListColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.TestMapJoinOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.MapJoinTestConfig.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSubstringIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInitCap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUnixTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.DropTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.DropPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.DropDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.CreateTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.CreateDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AlterTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AlterDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AddPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ListResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainLockDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.StorageFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MergeSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePartitionPruneRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.JarUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnOrderedMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringInitCap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.debug.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableSetPropertiesOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.info.DescTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.create.show.ShowCreateTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.function.desc.DescFunctionOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.DDLUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.alter.location.AlterDatabaseSetLocationOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveClientCache.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.TestRow.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestSyntaxUtil.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloHiveRow.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.ColumnMapper.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.HiveFileProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ColumnMappings.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HiveClientCache.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatBaseStorer.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.jms.MessagingUtils.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Copy.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.session.TestClearDanglingScratchDir.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.schematool.TestSchemaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCliConfig.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckTableAccessHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="22770" opendate="2020-1-24 00:00:00" fixdate="2020-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip interning of MapWork fields during deserialization</summary>
      <description>HIVE-19937 introduced some interning logic into mapwork deserialization process, but it's only related to spark, maybe we should skip this for tez, reducing the cpu pressure in tez tasks.UPDATE: Hive on spark is not supported anymore, the MapWorkSerializer can be completely removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="23035" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="23607" opendate="2020-6-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permission Issue: Create view on another view succeeds but alter view fails</summary>
      <description>create table test_table (id int); create view test_view as select * from test_table;-- user "abc" has read access on test_view-- Create view succeedscreate view test_view_1 as select * from test_view;-- Alter view failsalter view test_view_1 as select * from test_viewError: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [abc] does not have [SELECT] privilege on [test/test_table] (state=42000,code=40000)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestViewEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23702" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metastore metrics to show age of the oldest initiated compaction</summary>
      <description>It would be good to have a metrics which will show the age of the oldest initiated compaction</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.metrics.MetricsConstants.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2611" opendate="2011-11-28 00:00:00" fixdate="2011-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make index table output of create index command if index is table based</summary>
      <description>If an index is table based, when that index is created a table is created to contain that index. This should be listed in the output of the command.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.edge.cases.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.binary.search.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.merge.negative.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.bitmap.no.map.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="2674" opendate="2011-12-22 00:00:00" fixdate="2011-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_partitions_ps throws TApplicationException if table doesn&amp;#39;t exist</summary>
      <description>If the table passed to get_partition_ps doesn't exist, a NPE is thrown by getPartitionPsQueryResults. There should be a check here, which throws a NoSuchObjectException if the table doesn't exist.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="8472" opendate="2014-10-15 00:00:00" fixdate="2014-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ALTER DATABASE SET LOCATION</summary>
      <description>Similarly to ALTER TABLE tablename SET LOCATION, it would be helpful if there was an equivalent for databases.</description>
      <version>2.2.0,2.4.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
