<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1537" opendate="2010-8-13 00:00:00" fixdate="2010-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow users to specify LOCATION in CREATE DATABASE statement</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15921" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-order the slider stop command to avoid a force if possible</summary>
      <description>A graceful stop is required for slider --service llapstatus to work properly</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="16340" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Kerberos + SSL connections to HMS</summary>
      <description>It should be possible to connect to HMS with Kerberos authentication and SSL enabled, at the same time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
    </fixedFiles>
  </bug>
  <bug id="16341" opendate="2017-3-31 00:00:00" fixdate="2017-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez Task Execution Summary has incorrect input record counts on some operators</summary>
      <description>Task Execution Summary-------------------------------------------------------------------------------------------------------------------------------- VERTICES TOTAL_TASKS FAILED_ATTEMPTS KILLED_TASKS DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS-------------------------------------------------------------------------------------------------------------------------------- Map 1 167 0 0 17640.00 2,109,200 23,068 150,000,004 11,995,136 Map 11 5 0 0 10559.00 71,960 633 4,023,690 799,900 Map 13 1 0 0 2244.00 6,090 29 25 3 Map 3 1 0 0 2849.00 7,080 99 25 3 Map 5 271 0 0 55834.00 12,934,890 358,376 1,500,000,001 1,500,000,161 Map 7 241 0 0 91243.00 5,020,860 71,182 1,827,250,341 652,413,443Reducer 10 1 0 0 1010.00 1,900 0 4 0Reducer 12 1 0 0 3854.00 1,320 0 799,900 1Reducer 14 1 0 0 1420.00 3,790 45 3 1 Reducer 2 1 0 0 9720.00 6,220 122 11,995,136 1 Reducer 4 1 0 0 810.00 2,100 105 3 1 Reducer 6 1 0 0 24863.00 3,260 5 1,500,000,161 1 Reducer 8 412 0 0 88215.00 17,106,440 184,524 2,165,208,640 1,864 Reducer 9 2 0 0 29752.00 3,980 0 1,864 4--------------------------------------------------------------------------------------------------------------------Seeing this on queries using runtime filtering. Noticed the INPUT_RECORDS look incorrect for the reducers that are responsible for aggregating the min/max/bloomfilter (Reducers 12, 14, 2, 6). For example Reducer 2 shows 12M input records. However looking at the task logs for Reducer 2, there were only 167 input records.It looks like Map 1 has 2 different output vertices (Reducer 2 and Reducer 8), but the total output rows for Map 1 (rather than just the rows going to each specific vertex) is being counted in the input rows for both Reducer 2 and Reducer 8.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
    </fixedFiles>
  </bug>
  <bug id="18150" opendate="2017-11-27 00:00:00" fixdate="2017-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spark Version to 2.2.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18151" opendate="2017-11-27 00:00:00" fixdate="2017-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external client: Better error message propagation during submission failures</summary>
      <description>During failed submissions, the original error message should be added to the error message that is eventually propagated to the user.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19594" opendate="2018-5-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add custom tmp folders to tests to avoid collisions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.server.TestInformationSchemaWithPrivilege.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20011" opendate="2018-6-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move away from append mode in proto logging hook</summary>
      <description>Proto logging hook uses append mode with SequenceFile which does not work on filesystems which does not support append. Move away to using sequence file with flush. And use the reader with Long.MAX_VALUE to ensure it reads beyond the length in NameNode.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.tez.dag.history.logging.proto.ProtoMessageWriter.java</file>
      <file type="M">ql.src.java.org.apache.tez.dag.history.logging.proto.ProtoMessageReader.java</file>
      <file type="M">ql.src.java.org.apache.tez.dag.history.logging.proto.DatePartitionedLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="20020" opendate="2018-6-28 00:00:00" fixdate="2018-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive contrib jar should not be in lib</summary>
      <description>Currently the way hive is packaged it includes hive-contrib-&lt;version&gt;.jar in lib, we should not include it here because it is picked up by services like HS2. This creates a situation in which experimental features such as the MultiDelimitSerDe are accessible without understanding how to really install and use it. For example you can create a table using HS2 via beeline with the aforementioned SerDe and it will work as long you do not do M/R jobs. The M/R jobs do not work because the SerDe is not in aux to get shipped into distcache. I propose we do not package it this way and if someone would like to leverage an experimental feature they can add it manually to their environment. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20090" opendate="2018-7-4 00:00:00" fixdate="2018-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend creation of semijoin reduction filters to be able to discover new opportunities</summary>
      <description>Assume the following plan:TS[0] - RS[1] - JOIN[4] - RS[5] - JOIN[8] - FS[9]TS[2] - RS[3] - JOIN[4] TS[6] - RS[7] - JOIN[8]Currently, TS[6] may only be reduced with the output of RS[5], i.e., input to join between both subplans.However, it may be useful to consider other possibilities too, e.g., reduced by the output of RS[1] or RS[3]. For instance, this is important when, given a large plan, an edge between RS&amp;#91;5&amp;#93; and TS&amp;#91;0&amp;#93; would create a cycle, while an edge between RS&amp;#91;1&amp;#93; and TS&amp;#91;6&amp;#93; would not.This patch comprises two parts. First, it creates additional predicates when possible. Secondly, it removes duplicate semijoin reduction branches/predicates, e.g., if another semijoin that consumes the output of the same expression already reduces a certain table scan operator (heuristic, since this may not result in most efficient plan in all cases). Ultimately, the decision on whether to use one or another should be cost-driven (follow-up).</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20322" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlakyTest: TestMiniDruidCliDriver</summary>
      <description>TestMiniDruidCliDriver is failing intermittently but I'm seeing it fail a significant percentage of the time.druid_timestamptzdruidmini_joinsdruidmini_maskingdruidmini_test1</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="2060" opendate="2011-3-17 00:00:00" fixdate="2011-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI local mode hit NPE when exiting by ^D</summary>
      <description>CLI gets an NPE when running in local mode and hit an ^D to exit it.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2069" opendate="2011-3-22 00:00:00" fixdate="2011-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException on getSchemas</summary>
      <description>Calling getSchemas will cause a nullpointerexception</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="20692" opendate="2018-10-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable folding of NOT x IS (NOT) [TRUE|FALSE] expressions</summary>
      <description>Expressions like: not ((a&gt;0) is not true) could be rewritten to (a&gt;0) is true.However currently this doesn't happen because some of these functions are not translated for Calcite.create table t (a integer);explain select not ((a&gt;0) is not true) from t group by a;[...]expressions: (not (_col0 &gt; 0) is not true) (type: boolean) |[...]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2096" opendate="2011-4-6 00:00:00" fixdate="2011-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>throw a error if the input is larger than a threshold for index input format</summary>
      <description>This can hang for ever.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2128" opendate="2011-4-23 00:00:00" fixdate="2011-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatic Indexing with multiple tables</summary>
      <description>Make automatic indexing work with jobs which access multiple tables. We'll probably need to modify the way that the index input format works in order to associate index formats/files with specific tables.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="21580" opendate="2019-4-4 00:00:00" fixdate="2019-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce ISO 8601 week numbering SQL:2016 formats</summary>
      <description>Enable Hive to parse the following datetime formats when any combination/subset of these or previously implemented patterns is provided in one string. Also catch combinations that conflict. IYYY IYY IY I IWhttps://docs.google.com/document/d/1V7k6-lrPGW7_uhqM-FhKl3QsxwCRy69v2KIxPsGjc1k/edit</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="21670" opendate="2019-4-30 00:00:00" fixdate="2019-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replacing mockito-all with mockito-core dependency</summary>
      <description>The mockito-all dependency contains an old version of Hamcrest core which can collide with other Hamcrest dependencies. Replacint it with mockito-core should be straightforward.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2178" opendate="2011-5-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log related Check style Comments fixes</summary>
      <description>Fix Log related Check style Comments</description>
      <version>0.5.0,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.SimpleCharStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="21801" opendate="2019-5-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests using miniHS2 with HTTP as transport are creating miniHS2 with binary transport</summary>
      <description>Even though tests using miniHS2 set the config hive.server2.transport.mode is set to http, miniHS2 is created with binary transport.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.MiniHiveKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="2181" opendate="2011-5-24 00:00:00" fixdate="2011-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up the scratch.dir (tmp/hive-root) while restarting Hive server.</summary>
      <description>Now queries leaves the map outputs under scratch.dir after execution. If the hive server is stopped we need not keep the stopped server's map oputputs. So whle starting the server we can clear the scratch.dir. This can help in improved disk usage.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21981" opendate="2019-7-10 00:00:00" fixdate="2019-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When LlapDaemon capacity is set to 0 and the waitqueue is not empty then the queries are stuck</summary>
      <description>When an LlapDaemon executor capacity is set to 0 then the already queued tasks are not handled causing the queries to stuck</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="2204" opendate="2011-6-7 00:00:00" fixdate="2011-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unable to get column names for a specific table that has &amp;#39;_&amp;#39; as part of its table name</summary>
      <description>I have a table age_group and I am trying to get list of columns for this table name. As underscore and '%' have special meaning in table search pattern according to JDBC searchPattern string specification, I escape the '_' in my table name when I call getColumns for this single table. But HIVE does not return any columns. My call to getColumns is as followscatalog &lt;null&gt;schemaPattern "%"tableNamePattern "age&amp;#95;group"columnNamePattern "%"If I don't escape the '_' in my tableNamePattern, I am able to get the list of columns.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="2215" opendate="2011-6-10 00:00:00" fixdate="2011-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add api for marking / querying set of partitions for events</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyListener.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="22270" opendate="2019-9-30 00:00:00" fixdate="2019-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-io to 2.6</summary>
      <description>Hive's currently using commons-io 2.4 and according to HIVE-21273, a number of issues are present in it, which can be resolved by upgrading to 2.6:IOUtils copyLarge() and skip() methods are performance hogs affectsVersions:2.3;2.4 https://issues.apache.org/jira/projects/IO/issues/IO-355?filter=allopenissues CharSequenceInputStream#reset() behaves incorrectly in case when buffer size is not dividable by data size affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-356?filter=allopenissues &amp;#91;Tailer&amp;#93; InterruptedException while the thead is sleeping is silently ignored affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-357?filter=allopenissues IOUtils.contentEquals* methods returns false if input1 == input2; should return true affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-362?filter=allopenissues Apache Commons - standard links for documents are failing affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-369?filter=allopenissues FileUtils.sizeOfDirectoryAsBigInteger can overflow affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-390?filter=allopenissues Regression in FileUtils.readFileToString from 2.0.1 affectsVersions:2.1;2.2;2.3;2.4 https://issues.apache.org/jira/projects/IO/issues/IO-453?filter=allopenissues Correct exception message in FileUtils.getFile(File; String...) affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-479?filter=allopenissues org.apache.commons.io.FileUtils#waitFor waits too long affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-481?filter=allopenissues FilenameUtils should handle embedded null bytes affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-484?filter=allopenissues Exceptions are suppressed incorrectly when copying files. affectsVersions:2.4;2.5 https://issues.apache.org/jira/projects/IO/issues/IO-502?filter=allopenissues </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22391" opendate="2019-10-22 00:00:00" fixdate="2019-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE while checking Hive query results cache</summary>
      <description>NPE when results cache was enabled:2019-10-21T14:51:55,718 ERROR [b7d7bea8-eef0-4ea4-ae12-951cb5dc96e3 HiveServer2-Handler-Pool: Thread-210]: ql.Driver (:()) - FAILED: NullPointerException nulljava.lang.NullPointerException at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkResultsCache(SemanticAnalyzer.java:15061) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12320) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:360) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:289) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1869) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1816) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1811) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197) at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:262) at org.apache.hive.service.cli.operation.Operation.run(Operation.java:247) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:575) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:561) at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315) at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:566) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="22680" opendate="2019-12-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in druid-handler Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.DruidKerberosUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="22681" opendate="2019-12-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hcatalog-webhcat Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2296" opendate="2011-7-20 00:00:00" fixdate="2011-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bad compressed file names from insert into</summary>
      <description>When INSERT INTO is run on a table with compressed output (hive.exec.compress.output=true) and existing files in the table, it may copy the new files in bad file names:Before INSERT INTO:000000_0.gzAfter INSERT INTO:000000_0.gz000000_0.gz_copy_1This causes corrupted output when doing a SELECT * on the table.Correct behavior should be to pick a valid filename such as:000000_0_copy_1.gz</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="23132" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test of Explain CBO of Merge statements</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sort.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.acid.q</file>
    </fixedFiles>
  </bug>
  <bug id="23134" opendate="2020-4-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive &amp; Kudu interaction not available on ARM</summary>
      <description>Currently, we have set up an ARM CI to test out how Hive works on ARM platform:https://builds.apache.org/view/H-L/view/Hive/job/Hive-linux-ARM-trunk/According to the results, Hive &amp; Kudu interaction is not available on ARM platform:https://builds.apache.org/view/H-L/view/Hive/job/Hive-linux-ARM-trunk/25/testReport/org.apache.hadoop.hive.kudu/this is because that we use Kudu version 1.10 and that version does not come with ARM workable packages.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">itests.qtest-kudu.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23136" opendate="2020-4-4 00:00:00" fixdate="2020-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not compare q test result if test.output.overwrite is specified</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="23139" opendate="2020-4-4 00:00:00" fixdate="2020-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve q test result masking</summary>
      <description>Q Test result masking can take a long time, in extreme cases up to 5 times as much as the test itself, because it is inefficient. Use contains, startsWith instead of patterns for masking lines containing or starting with specified strings.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QOutProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23211" opendate="2020-4-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix metastore schema differences between init scripts, and upgrade scripts</summary>
      <description>There are some differences (character encoding, defaults etc..) in metastore schema if we initialize using the init scripts, or upgrade using the upgrade scripts. The schema should be identical.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.0.0-to-2.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.0.0-to-2.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-2.0.0-to-2.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.0.0-to-2.1.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.034-HIVE-13395.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.039-HIVE-12274.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.035-HIVE-13395.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.0.0-to-2.1.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.035-HIVE-13395.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.020-HIVE-13395.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="2358" opendate="2011-8-8 00:00:00" fixdate="2011-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC DatabaseMetaData and ResultSetMetaData need to match for particular types</summary>
      <description>My patch for HIVE-1631 did not ensure the following (from comment on 1631):-------------Mythili Gopalakrishnan added a comment - 08/Aug/11 08:42Just tested this fix and does NOT work correctly. Here are my findings on a FLOAT columnWithout Patch on a FLOAT Column--------------------------------DatabaseMetaData.getColumns () COLUMN_SIZE returns 12DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0ResultSetMetaData.getPrecision() returns 0ResultSetMetaData.getScale() returns 0With Patch on a FLOAT Column----------------------------DatabaseMetaData.getColumns () COLUMN_SIZE returns 24DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0ResultSetMetaData.getPrecision() returns 7ResultSetMetaData.getScale() returns 7Also both DatabaseMetadata and ResultSetMetaData must return the same information for Precision and Scale for FLOAT,DOUBLE types.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="2398" opendate="2011-8-19 00:00:00" fixdate="2011-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive server doesn&amp;#39;t return schema for &amp;#39;set&amp;#39; command</summary>
      <description>The Hive server does process the CLI commands like 'set', 'set -v' sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.</description>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="243" opendate="2009-1-23 00:00:00" fixdate="2009-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>^C breaks out of running query, but not whole CLI</summary>
      <description>It would be lovely if, when I know a query is bad, I could just ^C out of it. I can do that now, but the whole CLI quits.It'd be quite nice if it took an extra ^C to break the CLI, or if there was some control character to break out of a query without breaking out of the CLI.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2431" opendate="2011-9-8 00:00:00" fixdate="2011-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrading thrift version didn&amp;#39;t upgrade libthrift.jar symlink correctly</summary>
      <description>libthrift.jar and libfb303.jar are symlinks to the current thrift version. With the upgrade to 0.7, there's a bug in the symlink creation.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2439" opendate="2011-9-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade antlr version to 3.4</summary>
      <description>Upgrade antlr version to 3.4</description>
      <version>0.8.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.parse.sample6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.union.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf.when.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf.case.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.subq.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample7.q.out</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.test.results.clientnegative.archive.partspec3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.charliteral.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.columns2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.cast1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input20.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input8.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input9.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.part1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join8.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample5.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="2440" opendate="2011-9-9 00:00:00" fixdate="2011-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make hive mapper initialize faster when having tons of input files</summary>
      <description>when one hive job has tons of input files, a lot of mappers may fail because of slow initialization.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2442" opendate="2011-9-12 00:00:00" fixdate="2011-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore upgrade script and schema DDL for Hive 0.8.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.008-HIVE-2246.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="2455" opendate="2011-9-18 00:00:00" fixdate="2011-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass correct remoteAddress in proxy user authentication</summary>
      <description></description>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2457" opendate="2011-9-20 00:00:00" fixdate="2011-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Files in Avro-backed Hive tables do not have a ".avro" extension</summary>
      <description>When using the Avro SerDe (see HIVE-895, https://github.com/jghoman/haivvreo) the files created for an Avro table do not have a ".avro" extension, which causes problems for tools like Avro MapReduce or Sqoop which expect the extension.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2467" opendate="2011-9-26 00:00:00" fixdate="2011-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HA Support for Metastore Server</summary>
      <description>We require HA deployment for metastore server for HCatalog: Multiple server instances run behind VIP Database provides HAMetastore server instances will need to be able to share any state required for VIP outside RDBMS. As of Hive 0.8 affected conversational state that needs to support VIP/HA setup is limited to current delegation tokens. Is this correct?We are planning to use ZooKeeper to share current delegation tokens and master keys between nodes of the VIP. ZK is already (optionally) used by Hive for concurrency control. Access to ZK would be limited on the network level or in the future, when ZooKeeper supports security, through Kerberos, similar to NN access.Currently Hive taps into Hadoop core security delegation token support through extension oforg.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager&lt;TokenIdent&gt;A solution could amend the Hive specific extension to support: Pluggable delegation token and master key store (ZooKeeper as alternative for in-memory AbstractDelegationTokenSecretManager) Delegation token retrieval from token store when not found in memory (wrap/extend retrievePassword(...)) Cancellation of token in token store Purging of expired tokens from token storehttp://www.mail-archive.com/hcatalog-user@incubator.apache.org/msg00053.html</description>
      <version>0.8.0,0.9.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">shims.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2468" opendate="2011-9-26 00:00:00" fixdate="2011-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive compile against Hadoop 0.23</summary>
      <description>Due to restructure of Hadoop 0.22 branch compared to Hadoop 0.20 Hive does not compile against 0.22</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.build.xml</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">service.build.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="2492" opendate="2011-10-7 00:00:00" fixdate="2011-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PDK PluginTest failing on Hudson</summary>
      <description></description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pdk.scripts.build-plugin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2498" opendate="2011-10-12 00:00:00" fixdate="2011-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by operator does not estimate size of Timestamp &amp; Binary data correctly</summary>
      <description>It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.</description>
      <version>0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2519" opendate="2011-10-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema</summary>
      <description>Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:insert overwrite ... partition (p2="...", p1);which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2568" opendate="2011-11-10 00:00:00" fixdate="2011-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-2246 upgrade script needs to drop foreign key in COLUMNS_OLD</summary>
      <description>One more bug in the MySQL metastore upgrade script: the foreign key in COLUMNS needs to be dropped, otherwise drop_partition will fail because the SDS row cannot be deleted due to the foreign key constraint.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.008-HIVE-2246.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="2569" opendate="2011-11-10 00:00:00" fixdate="2011-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too much debugging info on console if a job failed</summary>
      <description>When a job failed and Hive client tries to get the error message from failed task, it printed the following info on console for each task:Examining task ID: task_201110112120_773499_m_000037 from job job_201110112120_773499This should be shorten significantly.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2572" opendate="2011-11-10 00:00:00" fixdate="2011-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-2246 upgrade script changed the COLUMNS_V2.COMMENT length</summary>
      <description>This changes from varchar(4000) to varchar(128) is backward incompatible.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.008-HIVE-2246.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="2574" opendate="2011-11-11 00:00:00" fixdate="2011-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ivy offline mode broken by changingPattern and checkmodified attributes</summary>
      <description>As described here:http://www.mail-archive.com/ivy-user@ant.apache.org/msg03534.htmlThis wasn't the case formerly (maybe the upgrade to ivy 1.2?)</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2578" opendate="2011-11-14 00:00:00" fixdate="2011-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug mode in some situations doesn&amp;#39;t work properly when child JVM is started from MapRedLocalTask</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">bin.ext.debug.sh</file>
    </fixedFiles>
  </bug>
  <bug id="258" opendate="2009-1-29 00:00:00" fixdate="2009-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support IF(test, valueTrue, valueFalseOrNull)</summary>
      <description>See http://dev.mysql.com/doc/refman/5.0/en/control-flow-functions.html for details.Syntax:IF(expr1,expr2,expr3)</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseNumericOp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseCompare.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AmbiguousMethodException.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2581" opendate="2011-11-15 00:00:00" fixdate="2011-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>explain task: getJSONPlan throws a NPE if the ast is null</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="2599" opendate="2011-11-19 00:00:00" fixdate="2011-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Composit/Compound Keys with HBaseStorageHandler</summary>
      <description>It would be really nice for hive to be able to understand composite keys from an underlying HBase schema. Currently we have to store key fields twice to be able to both key and make data available. I noticed John Sichi mentioned in HIVE-1228 that this would be a separate issue but I cant find any follow up. How feasible is this in the HBaseStorageHandler?</description>
      <version>0.8.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="2631" opendate="2011-12-7 00:00:00" fixdate="2011-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive work with Hadoop 1.0.0</summary>
      <description>With Hadoop 1.0.0 around the corner ( http://mail-archives.apache.org/mod_mbox/hadoop-general/201111.mbox/%3C9D6B6144-F4E0-4A31-883F-2AC504727A1F%40hortonworks.com%3E ), it will be useful to make Hive work with it.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="2646" opendate="2011-12-12 00:00:00" fixdate="2011-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Ivy dependencies on Hadoop should depend on jars directly, not tarballs</summary>
      <description>The current Hive Ivy dependency logic for its Hadoop dependencies is problematic - depending on the tarball and extracting the jars from there, rather than depending on the jars directly. It'd be great if this was fixed to actually have the jar dependencies defined directly.</description>
      <version>0.8.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">shims.build.xml</file>
      <file type="M">service.ivy.xml</file>
      <file type="M">service.build.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">pdk.scripts.build-plugin.xml</file>
      <file type="M">pdk.ivy.xml</file>
      <file type="M">pdk.build.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">jdbc.ivy.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">ivy.common-configurations.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
      <file type="M">builtins.ivy.xml</file>
      <file type="M">builtins.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2703" opendate="2012-1-10 00:00:00" fixdate="2012-1-10 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>ResultSetMetaData.getColumnType() always returns VARCHAR(string) for partition columns irrespective of partition column type</summary>
      <description>ResultSetMetaData.getColumnType() always returns VARCHAR(string) as column type, no matter what the column type is for the partition column.However DatabaseMetadata.getColumnType() returns correct type. Create a table with a partition column having a type other than string, you will see that ResultSet.getColumnType() always returns string as the type for int or boolean or float columns...</description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2712" opendate="2012-1-12 00:00:00" fixdate="2012-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make ZooKeeper token store ACL configurable</summary>
      <description>ACL needs to be set to secure the token store with ZK 3.4.The patch will also include the review changes from HIVE-2467 that were not committed.</description>
      <version>0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2723" opendate="2012-1-17 00:00:00" fixdate="2012-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>should throw "Ambiguous column reference key" Exception in particular join condition</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2735" opendate="2012-1-22 00:00:00" fixdate="2012-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2746" opendate="2012-1-24 00:00:00" fixdate="2012-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore client doesn&amp;#39;t log properly in case of connection failure to server</summary>
      <description>LOG.error(e.getStackTrace()) in current code prints memory location of StackTraceElement[] instead of message.</description>
      <version>0.8.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2748" opendate="2012-1-25 00:00:00" fixdate="2012-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2762" opendate="2012-1-30 00:00:00" fixdate="2012-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter Table Partition Concatenate Fails On Certain Characters</summary>
      <description>Alter table partition concatenate creates a Java URI object for the location of a partition. If the partition name contains certain characters, such as } or space ' ', the object constructor fails, causing the query to fail.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2765" opendate="2012-1-31 00:00:00" fixdate="2012-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase handler uses ZooKeeperConnectionException which is not compatible with HBase versions other than 0.89</summary>
      <description>It cannot integrate with HBase0.21 and may not be able to integrate with hbase0.9x</description>
      <version>0.7.0,0.8.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2771" opendate="2012-2-1 00:00:00" fixdate="2012-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for filter pushdown for key ranges in hbase for keys of type string</summary>
      <description>This is a subtask of HIVE-1643</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2792" opendate="2012-2-9 00:00:00" fixdate="2012-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR(CAST(&lt;string&gt; AS BINARY)) produces unexpected results</summary>
      <description></description>
      <version>0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.udfs.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.substr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ba.table.udfs.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
    </fixedFiles>
  </bug>
  <bug id="2794" opendate="2012-2-9 00:00:00" fixdate="2012-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregations without grouping should return NULL when applied to partitioning column of a partitionless table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2795" opendate="2012-2-9 00:00:00" fixdate="2012-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>View partitions do not have a storage descriptor</summary>
      <description>Besides being an inconsistency, it causes errors.Calling describe formatted on a view partition throws an exceptionjava.lang.NullPointerException at org.apache.hadoop.hive.ql.metadata.Partition.getCols(Partition.java:505) at org.apache.hadoop.hive.ql.exec.DDLTask.describeTable(DDLTask.java:2570)because it does not have a column descriptor, which is part of the storage descriptor.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2804" opendate="2012-2-13 00:00:00" fixdate="2012-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task log retrieval fails on Hadoop 0.23</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2815" opendate="2012-2-22 00:00:00" fixdate="2012-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter pushdown in hbase for keys stored in binary format</summary>
      <description>This patch enables filter pushdown for keys stored in binary format in hbase</description>
      <version>0.6.0,0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="3079" opendate="2012-6-1 00:00:00" fixdate="2012-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2989</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.tablelink.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.table.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.create.tablelink.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure5.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TableType.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableIdentifier.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.010-HIVE-2989.mysql.sql</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="3140" opendate="2012-6-14 00:00:00" fixdate="2012-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comment indenting is broken for "describe" in CLI</summary>
      <description>Just go into the CLI and type "describe &amp;#91;TABLE_NAME&amp;#93;". If a comment has multiple lines, it is completely unreadable due to poor comment indenting. For example:birthdayParam string 1 = comment12 = comment23 = comment3But it supposed to display as:birthdayParam string 1 = comment1 2 = comment2 3 = comment3Comments should be indented the same amount on each line, i.e., if the comment starts at row k for the first line of the comment, it should be indented by k on line 2.</description>
      <version>None</version>
      <fixedVersion>0.10.0,0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.split.sample.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.14.managed.location.over.existing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.13.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.12.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.11.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.10.external.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.08.nonpart.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.03.nonpart.over.compat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.01.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.00.nonpart.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.filter.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.hiveconf.validation0.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.hiveconf.validation1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.corrupt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.change.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.evolved.schemas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.joins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.sanity.test.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.error.message.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.bincolserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.colserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="3149" opendate="2012-6-16 00:00:00" fixdate="2012-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamically generated paritions deleted by Block level merge</summary>
      <description>When creating partitions in a table using dynamic partitions and a Block level merge is executed at the end of the query, some partitions may be lost. Specifically if the values of two or more dynamic partition keys end in the same sequence of numbers, all but the largest will be dropped.I was not able to confirm it, but I suspect that if a map reduce job is speculated as part of the merge, the duplicate data will not be deleted either.E.g.insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)select key, value, if(key % 2 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';In this query, if a Block level merge is executed at the end, only one of the partitions ds=2008-04-08/hr=a1 and ds=2008-04-08/hr=b1 will appear in the final table.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3189" opendate="2012-6-24 00:00:00" fixdate="2012-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cast ( &lt;string type&gt; as bigint) returning null values</summary>
      <description>select rnum, c1, cast(c1 as bigint) from cert.tsdchar tsdchar where rnum in (0,1,2)create table if not exists CERT.TSDCHAR ( RNUM int , C1 string)row format sequencefilernum c1 _c20 -1 &lt;null&gt;1 0 &lt;null&gt;2 10 &lt;null&gt;</description>
      <version>0.8.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFDateAdd.java</file>
    </fixedFiles>
  </bug>
  <bug id="3191" opendate="2012-6-24 00:00:00" fixdate="2012-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>timestamp - timestamp causes null pointer exception</summary>
      <description>select tts.rnum, tts.cts - tts.cts from cert.tts ttsError: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)SQLState: 42000ErrorCode: 12create table if not exists CERT.TTS ( RNUM int , CTS timestamp) stored as sequencefile;</description>
      <version>0.8.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3251" opendate="2012-7-11 00:00:00" fixdate="2012-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive doesn&amp;#39;t remove scrach directories while killing running MR job</summary>
      <description>While killing running MR job, hive doesn't clean up scratch directory (mapred.cache.files). So that, afterwards, scratch directory is left there in hdfs. HDFS name node doesn't know it and try to do lease recovery. while such instances happen more, it will eventually crash namenode.The fix is to leverage hdfs clean up functionality. While creating scratch dirs, hive registers it to hdfs cleanup hook. While killing happens, hdfs will clean them up.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="3261" opendate="2012-7-16 00:00:00" fixdate="2012-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter the number of buckets for a non-empty partitioned table should not be allowed</summary>
      <description>This is dangerous since the code uses the table metadata everywhere to get the number of buckets</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.exim.04.evolved.parts.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.groupby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3693" opendate="2012-11-9 00:00:00" fixdate="2012-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression introduced by HIVE-3483</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-3483 introduced a performance regression in the client side during split computation.The client side spends a lot more time in the split computation phase. The problem is checkFilterPathContains method.While investigating, can you create a config to disable it by default?thanks</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="3764" opendate="2012-12-3 00:00:00" fixdate="2012-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support metastore version consistency check</summary>
      <description>Today there's no version/compatibility information stored in hive metastore. Also the datanucleus configuration property to automatically create missing tables is enabled by default. If you happen to start an older or newer hive or don't run the correct upgrade scripts during migration, the metastore would end up corrupted. The autoCreate schema is not always sufficient to upgrade metastore when migrating to newer release. It's not supported with all databases. Besides the migration often involves altering existing table, changing or moving data etc.Hence it's very useful to have some consistency check to make sure that hive is using correct metastore and for production systems the schema is not automatically by running hive.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.11.0-to-0.12.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.12.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.11.0-to-0.12.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.12.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.11.0-to-0.12.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.12.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.11.0-to-0.12.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.10.0-to-0.11.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.12.0.derby.sql</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3937" opendate="2013-1-24 00:00:00" fixdate="2013-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Profiler</summary>
      <description>Adding a Hive Profiler implementation which tracks inclusive wall times and call counts of the operators</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3995" opendate="2013-2-7 00:00:00" fixdate="2013-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL upgrade scripts are not valid</summary>
      <description>I've noticed that scripts for upgrading metastore backed up on PostgreSQL are not valid.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.012-HIVE-1362.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.011-HIVE-3649.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.010-HIVE-3072.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="4807" opendate="2013-7-2 00:00:00" fixdate="2013-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore hangs</summary>
      <description>Hive metastore hangs (does not accept any new connections) due to a bug in DBCP. The root cause analysis is here https://issues.apache.org/jira/browse/DBCP-398. The fix is to change Hive connection pool to BoneCP which is natively supported by DataNucleus.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5946" opendate="2013-12-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6338" opendate="2014-1-30 00:00:00" fixdate="2014-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception handling in createDefaultDb() in Metastore</summary>
      <description>There is a suggestion on HIVE-5959 comment list on possible improvements.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
