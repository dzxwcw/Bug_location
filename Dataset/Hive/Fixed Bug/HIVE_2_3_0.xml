<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="15160" opendate="2016-11-8 00:00:00" fixdate="2016-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t order by an unselected column</summary>
      <description>If a grouping key hasn't been selected, Hive complains. For comparison, Postgres does not.Example. Notice i_item_id is not selected:select i_item_desc ,i_category ,i_class ,i_current_price ,sum(cs_ext_sales_price) as itemrevenue ,sum(cs_ext_sales_price)*100/sum(sum(cs_ext_sales_price)) over (partition by i_class) as revenueratio from catalog_sales ,item ,date_dim where cs_item_sk = i_item_sk and i_category in ('Jewelry', 'Sports', 'Books') and cs_sold_date_sk = d_date_sk and d_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30 days) group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratiolimit 100;</description>
      <version>2.0.0,2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16301" opendate="2017-3-27 00:00:00" fixdate="2017-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preparing for 2.3 development.</summary>
      <description>branch-2 is now being used for 2.3.0 development. The build files will need to reflect this change.</description>
      <version>2.3.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-util.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16302" opendate="2017-3-27 00:00:00" fixdate="2017-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add junit dependency to hive-shims-common to compile with Hadoop 2.8+</summary>
      <description>Compile error when setting hadoop.version to 2.9.0-SNAPSHOT:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:testCompile (default-testCompile) on project hive-shims-common: Compilation failure: Compilation failure:[ERROR] /Users/ajisaka/git/hive/shims/common/src/main/test/org/apache/hadoop/hive/io/TestHdfsUtils.java:[34,17] package org.junit does not exist[ERROR] /Users/ajisaka/git/hive/shims/common/src/main/test/org/apache/hadoop/hive/io/TestHdfsUtils.java</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16387" opendate="2017-4-5 00:00:00" fixdate="2017-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failing test org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData</summary>
      <description></description>
      <version>2.3.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16440" opendate="2017-4-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failing test columnstats_partlvl_invalid_values when autogather column stats is on</summary>
      <description></description>
      <version>2.1.0,2.3.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16441" opendate="2017-4-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>De-duplicate semijoin branches in n-way joins</summary>
      <description>Currently in n-way joins, semi join optimization creates n branches on same key. Instead it should reuse one branch for all the joins.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug id="16504" opendate="2017-4-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addition of binary licenses broke rat check</summary>
      <description>The clean up of Hive's licenses (HIVE-15035) broke the rat check, as all the license files get reported as invalid licenses. The rat check needs to be modified to ignore those files.</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16505" opendate="2017-4-21 00:00:00" fixdate="2017-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "unknown" boolean truth value</summary>
      <description>according to the standard, boolean truth value might be: TRUE|FALSE|UNKNOWN.similar queries to the following should be supported:select 1 where null is unknown;select 1 where (select cast(null as boolean) ) is unknown;"unknown" behaves similarily to null. (null=null) is null "All boolean values and SQL truth values are comparable and all are assignable to a site of type boolean. The value True is greater than the value False, and any comparison involving the null value or an Unknown truth value will return an Unknown result. The values True and False may be assigned to any site having a boolean data type; assignment of Unknown, or the null value, is subject to the nullability characteristic of the target." Truth table for the AND boolean operatorAND True False UnknownTrue True False UnknownFalse False False FalseUnknown Unknown False UnknownTruth table for the OR boolean operatorOR True False UnknownTrue True True TrueFalse True False UnknownUnknown True Unknown UnknownTruth table for the IS boolean operatorIS TRUE FALSE UNKNOWNTrue True False FalseFalse False True FalseUnknown False False True </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="16511" opendate="2017-4-22 00:00:00" fixdate="2017-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO looses inner casts on constants of complex type</summary>
      <description>type for map &lt;10, cast(null as int)&gt; becomes map &lt;int,string&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16536" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various improvements in TestPerfCliDriver</summary>
      <description>Goal is to reduce the size of stats file used to import stats in metastore. This will help to run this tests on partitioned tables.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.files.tpcds-perf.metastore.export.csv.TAB.COL.STATS.txt</file>
      <file type="M">data.files.tpcds-perf.metastore.export.csv.TABLE.PARAMS.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16537" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing AL files</summary>
      <description></description>
      <version>2.3.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.cloudhost.properties.example</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TezProgressMonitorStatusMapper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ProgressMonitorStatusMapper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.JobProgressUpdate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.StringToDouble.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFWidthBucket.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.monitoring.TestTezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFWidthBucket.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapPreVectorizationPass.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookWithParseHooks.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.RenderStrategy.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.QueryExecutionBreakdownSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.PrintSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.LLAPioSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.FSCountersSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.Constants.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializerTest.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.event.filters.MessageFormatFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.event.filters.EventBoundaryFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.event.filters.BasicFilter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.event.filters.AndFilter.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.logs.InPlaceUpdateStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.ProgressMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.logs.BeelineInPlaceUpdateStream.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloIndexLexicoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="16764" opendate="2017-5-26 00:00:00" fixdate="2017-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support numeric as same as decimal</summary>
      <description>for example numeric(12,2) -&gt; decimal(12,2) This will make Numeric reserved keyword</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.perf.query69.q</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query98.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query97.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query96.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query95.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query94.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query93.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query92.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query91.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query90.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query89.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query88.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query87.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query86.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query85.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query84.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query83.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query82.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query81.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query80.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query79.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query76.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query75.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query73.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query72.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query71.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query70.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query7.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query22.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query24.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query25.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query26.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query27.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query28.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query29.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query30.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query31.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query32.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query33.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query34.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query36.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query37.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query39.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query42.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query43.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query46.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query48.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query50.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query51.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query52.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query54.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query55.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query56.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query58.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query60.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query64.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query65.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query66.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query67.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query68.q</file>
    </fixedFiles>
  </bug>
  <bug id="16765" opendate="2017-5-26 00:00:00" fixdate="2017-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ParquetFileReader should be closed to avoid resource leak</summary>
      <description>ParquetFileReader should be closed to avoid resource leak</description>
      <version>2.3.0,3.0.0</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17125" opendate="2017-7-19 00:00:00" fixdate="2017-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage: Generate lineage information on need basis when atlas hook is enabled</summary>
      <description>When atlas hook is enabled, it would be good to generate lineage information only for insert/create/view statements and skip for select statements. Currently, in some of the complex select queries, generating lineage information turns out to in 100s of seconds, which can be avoided with this change.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.Generator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17150" opendate="2017-7-21 00:00:00" fixdate="2017-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE INDEX execute HMS out-of-transaction listener calls inside a transaction</summary>
      <description>The problem with CREATE INDEX is that it calls a CREATE TABLE operation inside the same CREATE INDEX transaction. During listener calls, there are some listeners that should run in an out-of-transaction context, for instance, Sentry blocks the HMS operation until the DB log notification is processed, but if the transaction has not finished, then the out-of-transaction listener will block forever (or until a read-time out happens).A fix would be to add a parameter to the out-of-transaction listener that alerts the listener if HMS is in an active transaction. If so, then is up to the listener plugin to return immediately and avoid blocking the HMS operation.</description>
      <version>2.3.0</version>
      <fixedVersion>2.3.2,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.MetaStoreEventListenerConstants.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="17391" opendate="2017-8-25 00:00:00" fixdate="2017-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction fails if there is an empty value in tblproperties</summary>
      <description>create table t1 (a int) tblproperties ('serialization.null.format'='');alter table t1 compact 'major';fails</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StringableMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="17399" opendate="2017-8-28 00:00:00" fixdate="2017-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin: Do not remove semijoin branch if it feeds to TS-&gt;DPP_EVENT</summary>
      <description>If there is an incoming semijoin branch to a TS which has DPP event, then try to keep it as it may serve as an excellent filter for DPP thus reducing the input to join drastically.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="17429" opendate="2017-9-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC doesn&amp;#39;t return rows when querying Impala</summary>
      <description>The Hive JDBC driver used to return a result set when querying Impala. Now, instead, it gets data back but interprets the data as query logs instead of a resultSet. This causes many issues (we see complaints about beeline as well as test failures).This appears to be a regression introduced with asynchronous operation against Hive.Ideally, we could make both behaviors work. I have a simple patch that should fix the problem.</description>
      <version>2.1.0,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>2.1.0,2.1.1,2.2.1,2.3.4,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="1743" opendate="2010-10-22 00:00:00" fixdate="2010-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group-by to determine equals of Keys in reverse order</summary>
      <description>When processing group-by, in reduce side, keys are ordered. Comparing equality of two keys can be more efficient in reverse order.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17604" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add druid properties to conf white list</summary>
      <description>Currently throws:Error: Error while processing statement: Cannot modify hive.druid.select.distribute at runtime. It is not in list of params that are allowed to be modified at runtime (state=42000,code=1)</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17606" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve security for DB notification related APIs</summary>
      <description>The purpose is to make sure only the superusers which are specified in the proxyuser settings can make the db notification related API calls, since this is supposed to be called by superuser/admin instead of any end user.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestCopyUtils.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17607" opendate="2017-9-26 00:00:00" fixdate="2017-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove ColumnStatsDesc usage from columnstatsupdatetask</summary>
      <description>it's not entirely connected to this task...it should either has its own descriptor; or work sould take on the: tablename/coltype/colname payload</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="17664" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description></description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17682" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: IF stmt produces wrong results</summary>
      <description>A query using with a vectorized IF(condition, thenExpr, elseExpr) function can produce wrong results.</description>
      <version>1.2.2,2.3.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17684" opendate="2017-10-3 00:00:00" fixdate="2017-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS memory issues with MapJoinMemoryExhaustionHandler</summary>
      <description>We have seen a number of memory issues due the HashSinkOperator use of the MapJoinMemoryExhaustionHandler. This handler is meant to detect scenarios where the small table is taking too much space in memory, in which case a MapJoinMemoryExhaustionError is thrown.The configs to control this logic are:hive.mapjoin.localtask.max.memory.usage (default 0.90)hive.mapjoin.followby.gby.localtask.max.memory.usage (default 0.55)The handler works by using the MemoryMXBean and uses the following logic to estimate how much memory the HashMap is consuming: MemoryMXBean#getHeapMemoryUsage().getUsed() / MemoryMXBean#getHeapMemoryUsage().getMax()The issue is that MemoryMXBean#getHeapMemoryUsage().getUsed() can be inaccurate. The value returned by this method returns all reachable and unreachable memory on the heap, so there may be a bunch of garbage data, and the JVM just hasn't taken the time to reclaim it all. This can lead to intermittent failures of this check even though a simple GC would have reclaimed enough space for the process to continue working.We should re-think the usage of MapJoinMemoryExhaustionHandler for HoS. In Hive-on-MR this probably made sense to use because every Hive task was run in a dedicated container, so a Hive Task could assume it created most of the data on the heap. However, in Hive-on-Spark there can be multiple Hive Tasks running in a single executor, each doing different things.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1774" opendate="2010-11-6 00:00:00" fixdate="2010-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>merge_dynamic_part&amp;#39;s result is not deterministic</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.merge.dynamic.partition.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17988" opendate="2017-11-6 00:00:00" fixdate="2017-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace patch utility usage with git apply in ptest</summary>
      <description>It would be great to replace the standard diff util because git can do a 3-way merge - which in most cases successfull.This could reduce the ptest results which are erroring out because of build failure.error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:7003Falling back to three-way merge...Applied patch to 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java' cleanly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17994" opendate="2017-11-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Serialization bottlenecked on irrelevant hashmap lookup</summary>
      <description>On machines with slower NUMA, the hashmap lookup for TypeInfo::getPrimitiveCategory is the slowest part of the vectorized serialization loops. The static object references run hot with the NUMA access speeds penalizing half the threads.This lookup is done for every column, for every row - though vectorization enforces that this type cannot change at all.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
    </fixedFiles>
  </bug>
  <bug id="17996" opendate="2017-11-7 00:00:00" fixdate="2017-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ASF headers</summary>
      <description>Yetus check reports some ASF header related issues in Hive code. Let's fix them up.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.resources.log4j2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumEmptyIsZeroAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="18108" opendate="2017-11-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>in case basic stats are missing; rowcount estimation depends on the selected columns size</summary>
      <description>in case basicstats are not available (especially rowcount):set hive.stats.autogather=false;create table t (a integer, b string);insert into t values (1,'asd1');insert into t values (2,'asd2');insert into t values (3,'asd3');insert into t values (4,'asd4');insert into t values (5,'asd5');explain select a,count(1) from t group by a;-- estimated to read 8 rows from table texplain select b,count(1) from t group by b;-- estimated: 1 rowsexplain select a,b,count(1) from t group by a,b;-- estimated: 1 rowsit may not depend on the actually selected column set.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.gather.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.parquet.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.ppr.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.parquet.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.reordering.no.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18166" opendate="2017-11-28 00:00:00" fixdate="2017-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Result of hive.query.string is encoded.</summary>
      <description>set hive.query.string returns encoded string.hive.query.string=%0A%0Aselect+*+from+t1</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18251" opendate="2017-12-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loosen restriction for some checks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18273" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LLAP-level counters for WM</summary>
      <description>On query fragment level (like IO counters)time queued as guaranteed;time running as guaranteed;time running as speculative.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18274" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add AM level metrics for WM</summary>
      <description>Unused guaranteed tasks (1 metric); guaranteed/speculative tasks x updated/update in progress (4 metrics).It should be possible to view those over time as the query is (was) running, to detect any anomalies. This jira is just to save the correct metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18275" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HS2-level WM metrics</summary>
      <description>E.g. time spent in pool queue. Some existing UIs use perflogger output, so we should also include that.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.QueryExecutionBreakdownSummary.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="1828" opendate="2010-12-3 00:00:00" fixdate="2010-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>show locks should not use getTable()/getPartition</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lock1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lock1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18443" opendate="2018-1-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure git gc finished in ptest prep phase before copying repo</summary>
      <description>In ptest's prep phase script first we checkout the latest Hive code from git, and then we make copy of its contents (along .git folder) for that will serve as Yetus' working directory.In some cases we can see errors such as+ cp -R . ../yetuscp: cannot stat ?./.git/gc.pid?: No such file or directorye.g. hereThis is caused by git running its gc feature in the background when our prep script has already started copying. In cases where gc finishes while cp is running, we'll get this error</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug id="18607" opendate="2018-2-2 00:00:00" fixdate="2018-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase HFile write does strange things</summary>
      <description>There's some strange code in the output handler that changes output directory into a file because Hive supposedly wants that. If you run insert overwrite with a side directory multiple times, the 2nd insert fails</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.positive.hbase.handler.bulk.q.out</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.bulk.q</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19083" opendate="2018-3-30 00:00:00" fixdate="2018-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19639" opendate="2018-5-21 00:00:00" fixdate="2018-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>a transactional Hive table cannot be imported as an external table</summary>
      <description>When transactional table is imported to a external table, the table should be imported as a non transactional table, as external tables cannot be transactional.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.exim.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.exim.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20212" opendate="2018-7-19 00:00:00" fixdate="2018-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiveserver2 in http mode emitting metric default.General.open_connections incorrectly</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="20213" opendate="2018-7-19 00:00:00" fixdate="2018-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite to 1.17.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stat.estimate.related.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="21009" opendate="2018-12-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP - Specify binddn for ldap-search</summary>
      <description>When user accounts cannot do an LDAP search, there is currently no way of specifying a custom binddn to use for the ldap-search.So I'm missing something like that:hive.server2.authentication.ldap.bindn=cn=ldapuser,ou=user,dc=examplehive.server2.authentication.ldap.bindnpw=password</description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21874" opendate="2019-6-14 00:00:00" fixdate="2019-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement add partitions related methods on temporary table</summary>
      <description>IMetaStoreClient exposes the following add partition related methods:Partition add_partition(Partition partition);int add_partitions(List&lt;Partition&gt; partitions);int add_partitions_pspec(PartitionSpecProxy partitionSpec);List&lt;Partition&gt; add_partitions(List&lt;Partition&gt; partitions, boolean ifNotExists, boolean needResults);These methods should be implemented in order to handle addition of partitions to temporary tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitionsFromPartSpec.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21875" opendate="2019-6-14 00:00:00" fixdate="2019-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement drop partition related methods on temporary tables</summary>
      <description>IMetaStoreClient exposes the following methods related to dropping partitions:boolean dropPartition(String db_name, String tbl_name, List&lt;String&gt; part_vals, boolean deleteData);boolean dropPartition(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, boolean deleteData);boolean dropPartition(String db_name, String tbl_name, List&lt;String&gt; part_vals, PartitionDropOptions options);boolean dropPartition(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, PartitionDropOptions options);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists, boolean needResults);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists, boolean needResults);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, PartitionDropOptions options);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, PartitionDropOptions options);boolean dropPartition(String db_name, String tbl_name, String name, boolean deleteData);boolean dropPartition(String catName, String db_name, String tbl_name, String name, boolean deleteData)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2191" opendate="2011-6-3 00:00:00" fixdate="2011-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow optional [inner] on equi-join.</summary>
      <description>Lot's of databases including mysql support an optional "inner" keyword to explicitely select an equi-join.As shown in the mysql docs: http://dev.mysql.com/doc/refman/5.1/en/join.htmlFor completeness/portability we should allow this.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">docs.xdocs.language.manual.joins.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21911" opendate="2019-6-21 00:00:00" fixdate="2019-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pluggable LlapMetricsListener on Tez side to disable / resize Daemons</summary>
      <description>We need to have a way to plug in different listeners which act upon the LlapDaemon statistics.This listener should be able to disable / resize the LlapDaemons based on health data.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestLlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21912" opendate="2019-6-21 00:00:00" fixdate="2019-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement BlacklistingLlapMetricsListener</summary>
      <description>We should implement a DaemonStatisticsHandler which: If a node average response time is bigger than 150% (configurable) of the other nodes If the other nodes has enough empty executors to handle the requestsThen disables the limping node.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestLlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapMetricsCollector.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22185" opendate="2019-9-10 00:00:00" fixdate="2019-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HADOOP-15832 will cause problems with tests using MiniYarn clusters</summary>
      <description>HADOOP-15832 changed the way to depend on bouncycastle jars.This will break tests depending on it like: org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormatExample exception:java.lang.NoClassDefFoundError: org/bouncycastle/operator/OperatorCreationException at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:814) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1245) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:321) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164) at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:348) at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:128) at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:497) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164) at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108) at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:316) at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:206) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164) at org.apache.hadoop.mapred.MiniMRClientClusterFactory.create(MiniMRClientClusterFactory.java:79) at org.apache.hadoop.mapred.MiniMRCluster.&lt;init&gt;(MiniMRCluster.java:188) at org.apache.hadoop.mapred.MiniMRCluster.&lt;init&gt;(MiniMRCluster.java:176) at org.apache.hadoop.mapred.MiniMRCluster.&lt;init&gt;(MiniMRCluster.java:168) at org.apache.hadoop.mapred.MiniMRCluster.&lt;init&gt;(MiniMRCluster.java:129)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
