<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10685" opendate="2015-5-12 00:00:00" fixdate="2015-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table concatenate oparetor will cause duplicate data</summary>
      <description>"Orders" table has 1500000000 rows and stored as ORC. hive&gt; select count(*) from orders;OK1500000000Time taken: 37.692 seconds, Fetched: 1 row(s)The table contain 14 files,the size of each file is about 2.1 ~ 3.2 GB.After executing command : ALTER TABLE orders CONCATENATE;The table is already 1530115000 rows.My hive version is 1.1.0.</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.2.1,1.3.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="10686" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.IndexOutOfBoundsException for query with rank() over(partition ...)</summary>
      <description>CBO throws Index out of bound exception for TPC-DS Q70.Query explainselect sum(ss_net_profit) as total_sum ,s_state ,s_county ,grouping__id as lochierarchy , rank() over(partition by grouping__id, case when grouping__id == 2 then s_state end order by sum(ss_net_profit)) as rank_within_parentfrom store_sales ss join date_dim d1 on d1.d_date_sk = ss.ss_sold_date_sk join store s on s.s_store_sk = ss.ss_store_sk where d1.d_month_seq between 1193 and 1193+11 and s.s_state in ( select s_state from (select s_state as s_state, sum(ss_net_profit), rank() over ( partition by s_state order by sum(ss_net_profit) desc) as ranking from store_sales, store, date_dim where d_month_seq between 1193 and 1193+11 and date_dim.d_date_sk = store_sales.ss_sold_date_sk and store.s_store_sk = store_sales.ss_store_sk group by s_state ) tmp1 where ranking &lt;= 5 ) group by s_state,s_county with rolluporder by lochierarchy desc ,case when lochierarchy = 0 then s_state end ,rank_within_parent limit 100Original plan (correct) HiveSort(fetch=[100]) HiveSort(sort0=[$3], sort1=[$5], sort2=[$4], dir0=[DESC], dir1=[ASC], dir2=[ASC]) HiveProject(total_sum=[$4], s_state=[$0], s_county=[$1], lochierarchy=[$5], rank_within_parent=[rank() OVER (PARTITION BY $5, when(==($5, 2), $0) ORDER BY $4 ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)], (tok_function when (= (tok_table_or_col lochierarchy) 0) (tok_table_or_col s_state))=[when(=($5, 0), $0)]) HiveAggregate(group=[{0, 1}], groups=[[{0, 1}, {0}, {}]], indicator=[true], agg#0=[sum($2)], GROUPING__ID=[GROUPING__ID()]) HiveProject($f0=[$7], $f1=[$6], $f2=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_county=[$1], s_state=[$2]) SemiJoin(condition=[=($2, $3)], joinType=[inner]) HiveProject(s_store_sk=[$0], s_county=[$23], s_state=[$24]) HiveTableScan(table=[[tpcds.store]]) HiveProject(s_state=[$0]) HiveFilter(condition=[&lt;=($1, 5)]) HiveProject((tok_table_or_col s_state)=[$0], rank_window_0=[rank() OVER (PARTITION BY $0 ORDER BY $1 DESC ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)]) HiveAggregate(group=[{0}], agg#0=[sum($1)]) HiveProject($f0=[$6], $f1=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_state=[$24]) HiveTableScan(table=[[tpcds.store]])Plan after fixTopOBSchema (incorrect) HiveSort(fetch=[100]) HiveSort(sort0=[$3], sort1=[$5], sort2=[$4], dir0=[DESC], dir1=[ASC], dir2=[ASC]) HiveProject(total_sum=[$4], s_state=[$0], s_county=[$1], lochierarchy=[$5], rank_within_parent=[rank() OVER (PARTITION BY $5, when(==($5, 2), $0) ORDER BY $4 ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)]) HiveAggregate(group=[{0, 1}], groups=[[{0, 1}, {0}, {}]], indicator=[true], agg#0=[sum($2)], GROUPING__ID=[GROUPING__ID()]) HiveProject($f0=[$7], $f1=[$6], $f2=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_county=[$1], s_state=[$2]) SemiJoin(condition=[=($2, $3)], joinType=[inner]) HiveProject(s_store_sk=[$0], s_county=[$23], s_state=[$24]) HiveTableScan(table=[[tpcds.store]]) HiveProject(s_state=[$0]) HiveFilter(condition=[&lt;=($1, 5)]) HiveProject((tok_table_or_col s_state)=[$0], rank_window_0=[rank() OVER (PARTITION BY $0 ORDER BY $1 DESC ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)]) HiveAggregate(group=[{0}], agg#0=[sum($1)]) HiveProject($f0=[$6], $f1=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_state=[$24]) HiveTableScan(table=[[tpcds.store]])Exception 15/04/14 02:42:52 [main]: ERROR parse.CalcitePlanner: CBO failed, skipping CBO.java.lang.IndexOutOfBoundsException: Index: 5, Size: 5 at java.util.ArrayList.rangeCheck(ArrayList.java:635) at java.util.ArrayList.get(ArrayList.java:411) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitInputRef(ASTConverter.java:395) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitInputRef(ASTConverter.java:372) at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:543) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:372) at org.apache.calcite.rex.RexCall.accept(RexCall.java:107) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:543) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:372) at org.apache.calcite.rex.RexCall.accept(RexCall.java:107) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convertOBToASTNode(ASTConverter.java:252) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:208) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:98) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:607) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:239) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:202) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311) at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409) at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="10982" opendate="2015-6-11 00:00:00" fixdate="2015-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Customizable the value of java.sql.statement.setFetchSize in Hive JDBC Driver</summary>
      <description>The current JDBC driver for Hive hard-code the value of setFetchSize to 50, which will be a bottleneck for performance.Pentaho filed this issue as http://jira.pentaho.com/browse/PDI-11511, whose status is open.Also it has discussion in http://forums.pentaho.com/showthread.php?158381-Hive-JDBC-Query-too-slow-too-many-fetches-after-query-execution-Kettle-Xformhttp://mail-archives.apache.org/mod_mbox/hive-user/201307.mbox/%3CCACQ46vEVGrfqG5rWXNr1PSgYz7dcF07mvLo8MM2qiT3ANM1XvQ@mail.gmail.com%3E</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="11013" opendate="2015-6-15 00:00:00" fixdate="2015-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniTez tez_join_hash test on the branch fails with NPE (initializeOp not called?)</summary>
      <description>Line numbers are shifted due to logging; the NPE is at hashMapRowGetters = new ReusableGetAdaptor[mapJoinTables.length];So looks like mapJoinTables is null.I added logging to see if they could be set to null from cache, but that doesn't seem to be the case.Looks like initializeOp is not called. Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception from MapJoinOperator : null at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:428) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:872) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:872) at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:643) at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:656) at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:659) at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:755) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:315) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:278) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:271) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:257) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:361) ... 17 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:339) ... 29 more</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
    </fixedFiles>
  </bug>
  <bug id="11014" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some MiniTez tests have result changes compared to master</summary>
      <description>vector_binary_join_groupby, vector_outer_join1, vector_outer_join2 and cbo_windowing</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.binary.join.groupby.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11023" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable directSQL if datanucleus.identifierFactory = datanucleus2</summary>
      <description>We hit an interesting bug in a case where datanucleus.identifierFactory = datanucleus2 .The problem is that directSql handgenerates SQL strings assuming "datanucleus1" naming scheme. If a user has their metastore JDO managed by datanucleus.identifierFactory = datanucleus2 , the SQL strings we generate are incorrect.One simple example of what this results in is the following: whenever DN persists a field which is held as a List&lt;T&gt;, it winds up storing each T as a separate line in the appropriate mapping table, and has a column called INTEGER_IDX, which holds the position in the list. Then, upon reading, it automatically reads all relevant lines with an ORDER BY INTEGER_IDX, which results in the list retaining its order. In DN2 naming scheme, the column is called IDX, instead of INTEGER_IDX. If the user has run appropriate metatool upgrade scripts, it is highly likely that they have both columns, INTEGER_IDX and IDX.Whenever they use JDO, such as with all writes, it will then use the IDX field, and when they do any sort of optimized reads, such as through directSQL, it will ORDER BY INTEGER_IDX.An immediate danger is seen when we consider that the schema of a table is stored as a List&lt;FieldSchema&gt; , and while IDX has 0,1,2,3,... , INTEGER_IDX will contain 0,0,0,0,... and thus, any attempt to describe the table or fetch schema for the table can come up mixed up in the table's native hashing order, rather than sorted by the index.This can then result in schema ordering being different from the actual table. For eg:, if a user has a (a:int,b:string,c:string), a describe on this may return (c:string, a:int, b: string), and thus, queries which are inserting after selecting from another table can have ClassCastExceptions when trying to insert data in the wong order - this is how we discovered this bug. This problem, however, can be far worse, if there are no type problems - it is possible, for eg., that if a,b&amp;c were all strings, that that insert query would succeed but mix up the order, which then results in user table data being mixed up. This has the potential to be very bad.We should write a tool to help convert metastores that use "datanucleus2" to "datanucleus1"(more difficult, needs more one-time testing) or change directSql to support both(easier to code, but increases test-coverage matrix significantly and we should really then be testing against both schemes). But in the short term, we should disable directSql if we see that the identifierfactory is "datanucleus2"</description>
      <version>1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="1104" opendate="2010-1-26 00:00:00" fixdate="2010-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Suppress Checkstyle warnings for generated files</summary>
      <description>Suppress Checkstyle warnings for generated files.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">checkstyle.checkstyle.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11040" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Derby dependency version to 10.10.2.0</summary>
      <description>We don't see this on the Apache pre-commit tests because it uses PTest, but running the entire TestCliDriver suite results in failures in some of the partition-related qtests (partition_coltype_literals, partition_date, partition_date2). I've only really seen this on Linux (I was using CentOS).HIVE-8879 changed the Derby dependency version from 10.10.1.1 to 10.11.1.1. Testing with 10.10.1.1 or 10.20.2.0 seems to allow the partition related tests to pass. I'd like to change the dependency version to 10.20.2.0, since that version should also contain the fix for HIVE-8879.</description>
      <version>None</version>
      <fixedVersion>1.2.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11059" opendate="2015-6-19 00:00:00" fixdate="2015-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcatalog-server-extensions tests scope should depend on hive-exec</summary>
      <description>(causes test failures in Windows due to the lack of WindowsPathUtil being available otherwise)</description>
      <version>1.2.1</version>
      <fixedVersion>1.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1106" opendate="2010-1-27 00:00:00" fixdate="2010-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "Alter table t add if not exists partition ..."</summary>
      <description>It will be useful to add such a command so that users don't see an error when adding an existing partition.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.addpart1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11090" opendate="2015-6-24 00:00:00" fixdate="2015-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ordering issues with windows unit test runs</summary>
      <description></description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.orig.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.2.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.orig.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.short.regress.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.casts.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.date.funcs.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.decimal.expressions.q</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.orig.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11103" opendate="2015-6-24 00:00:00" fixdate="2015-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add banker&amp;#39;s rounding BROUND UDF</summary>
      <description>Banker's rounding: the value is rounded to the nearest even number. Also known as "Gaussian rounding", and, in German, "mathematische Rundung".Example 2 digits 2 digitsUnrounded "Standard" rounding "Gaussian" rounding 54.1754 54.18 54.18 343.2050 343.21 343.20+106.2038 +106.20 +106.20 ========= ======= ======= 503.5842 503.59 503.58</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.RoundUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="11104" opendate="2015-6-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select operator doesn&amp;#39;t propagate constants appearing in expressions</summary>
      <description></description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11151" opendate="2015-6-30 00:00:00" fixdate="2015-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calcite transitive predicate inference rule should not transitively add not null filter on non-nullable input</summary>
      <description>Calcite rule will add predicates even if types don't match</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.2.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="11201" opendate="2015-7-8 00:00:00" fixdate="2015-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog is ignoring user specified avro schema in the table definition</summary>
      <description>HCatalog is ignoring user specified avro schema in the table definition , instead generating its own avro based from hive meta store. By generating its own schema will result in mismatch names. For exmple Avro fields name are Case Sensitive. By generating it's own schema will result in incorrect schema written to the avro file , and result select fail on read. And also Even if user specified schema does not allow null , when data is written using Hcatalog , it will write a schema that will allow null. For example in the table , user specified , all CAPITAL letters in the schema , and record name as LINEITEM. The schema should be written as it is. Instead Hcatalog ignores it and generated its own avro schema from the hive table case.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
    </fixedFiles>
  </bug>
  <bug id="11206" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Join translation should update all ExprNode recursively</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11312" opendate="2015-7-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC format: where clause with CHAR data type not returning any rows</summary>
      <description>Test case:Setup: create table orc_test( col1 string, col2 char(10)) stored as orc tblproperties ("orc.compress"="NONE");insert into orc_test values ('val1', '1');Query:select * from orc_test where col2='1'; Query returns no row.Problem is introduced with HIVE-10286, class RecordReaderImpl.java, method evaluatePredicateRange.Old code: Object baseObj = predicate.getLiteral(PredicateLeaf.FileFormat.ORC); Object minValue = getConvertedStatsObj(min, baseObj); Object maxValue = getConvertedStatsObj(max, baseObj); Object predObj = getBaseObjectForComparison(baseObj, minValue);New code:+ Object baseObj = predicate.getLiteral();+ Object minValue = getBaseObjectForComparison(predicate.getType(), min);+ Object maxValue = getBaseObjectForComparison(predicate.getType(), max);+ Object predObj = getBaseObjectForComparison(predicate.getType(), baseObj);The values for min and max are of type String which contain as many characters as the CHAR column indicated. For example if the type is CHAR(10), and the row has value 1, the value of String min is "1 ";Before Hive 1.2, the method getConvertedStatsObj would call StringUtils.stripEnd(statsObj.toString(), null); which would remove the trailing spaces from min and max. Later in the compareToRange method, it was able to compare "1" with "1".In Hive 1.2 with the use getBaseObjectForComparison method, it simply returns obj.String if the data type is String, which means minValue and maxValue are still "1 ".As a result, the compareToRange method will return a wrong value ("1".compareTo("1 ") -9 instead of 0.</description>
      <version>1.2.0,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.parquet.ppd.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.char.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="11352" opendate="2015-7-23 00:00:00" fixdate="2015-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid the double connections with &amp;#39;e&amp;#39; option[beeline-cli branch]</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="11354" opendate="2015-7-23 00:00:00" fixdate="2015-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL extending compatibility with Transact-SQL</summary>
      <description>Although HPL/SQL already supports some Transact-SQL language elements (declarations, flow-of-control stmts, assignments and so on) some other widely used constructs are not supported yet.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.declare.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.function.out.txt</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Signal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionString.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionOra.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionMisc.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Converter.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="11429" opendate="2015-7-31 00:00:00" fixdate="2015-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase default JDBC result set fetch size (# rows it fetches in one RPC call) to 1000 from 50</summary>
      <description>This is in addition to HIVE-10982 which plans to make the fetch size customizable. This just bumps the default to 1000.</description>
      <version>0.14.0,1.0.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="11432" opendate="2015-8-1 00:00:00" fixdate="2015-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive macro give same result for different arguments</summary>
      <description>If you use hive macro more than once while processing same row, hive returns same result for all invocations even if the argument are different. Example : &gt; CREATE TABLE macro_testing( a int, b int, c int)&gt; select * from macro_testing;1 2 34 5 67 8 910 11 12&gt; create temporary macro math_square(x int) x*x;&gt; select math_square(a), b, math_square(c) from macro_testing;9 2 936 5 3681 8 81144 11 144</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="11482" opendate="2015-8-6 00:00:00" fixdate="2015-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add retrying thrift client for HiveServer2</summary>
      <description>Similar to https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java, this improvement request is to add a retrying thrift client for HiveServer2 to do retries upon thrift exceptions.Here are few commits done on a forked branch that can be picked - https://github.com/InMobi/hive/commit/7fb957fb9c2b6000d37c53294e256460010cb6b7https://github.com/InMobi/hive/commit/11e4b330f051c3f58927a276d562446761c9cd6dhttps://github.com/InMobi/hive/commit/241386fd870373a9253dca0bcbdd4ea7e665406c</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11483" opendate="2015-8-6 00:00:00" fixdate="2015-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add encoding and decoding for query string config</summary>
      <description>We have seen some queries in production where some of the literals passed in the query have control characters, which result in exception when query string is set in the job xml.Proposing a solution to encode the query string in configuration and provide getters decoded string.Here is a commit in a forked repo : https://github.com/InMobi/hive/commit/2faf5761191fa3103a0d779fde584d494ed75bf5Suggestions are welcome on the solution.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestHooks.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11484" opendate="2015-8-6 00:00:00" fixdate="2015-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ObjectInspector for Char and VarChar</summary>
      <description>The creation of HiveChar and Varchar is not happening through ObjectInspector.Here is fix we pushed internally : https://github.com/InMobi/hive/commit/fe95c7850e7130448209141155f28b25d3504216</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  <bug id="11499" opendate="2015-8-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datanucleus leaks classloaders when used using embedded metastore with HiveServer2 with UDFs</summary>
      <description>When UDFs are used, we create a new classloader to add the UDF jar. Similar to what hadoop's reflection utils does(HIVE-11408), datanucleus caches the classloaders (https://github.com/datanucleus/datanucleus-core/blob/3.2/src/java/org/datanucleus/NucleusContext.java#L161). JDOPersistanceManager factory (1 per JVM) holds on to a NucleusContext reference (https://github.com/datanucleus/datanucleus-api-jdo/blob/3.2/src/java/org/datanucleus/api/jdo/JDOPersistenceManagerFactory.java#L115). Until we call NucleusContext#close, the classloader cache is not cleared. In case of UDFs this can lead to permgen leak, as shown in the attached screenshot, where NucleusContext holds on to several URLClassloader objects.</description>
      <version>0.14.0,1.0.0,1.1.0,1.1.1,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="1150" opendate="2010-2-10 00:00:00" fixdate="2010-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add comment to explain why we check for dir first in add_partitions().</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11583" opendate="2015-8-17 00:00:00" fixdate="2015-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When PTF is used over a large partitions result could be corrupted</summary>
      <description>Dataset: Window has 50001 record (2 blocks on disk and 1 block in memory) Size of the second block is &gt;32Mb (2 splits)Result:When the last block is read from the disk only first split is actually loaded. The second split gets missed. The total count of the result dataset is correct, but some records are missing and another are duplicated.Example:CREATE TABLE ptf_big_src ( id INT, key STRING, grp STRING, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';LOAD DATA LOCAL INPATH '../../data/files/ptf_3blocks.txt.gz' OVERWRITE INTO TABLE ptf_big_src;SELECT grp, COUNT(1) cnt FROM ptf_big_trg GROUP BY grp ORDER BY cnt desc;----- A 25000-- B 20000-- C 5001---CREATE TABLE ptf_big_trg AS SELECT *, row_number() OVER (PARTITION BY key ORDER BY grp) grp_num FROM ptf_big_src;SELECT grp, COUNT(1) cnt FROM ptf_big_trg GROUP BY grp ORDER BY cnt desc;-- -- A 34296-- B 15704-- C 1---Counts by 'grp' are incorrect!</description>
      <version>0.13.1,0.14.0,0.14.1,1.0.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11587" opendate="2015-8-17 00:00:00" fixdate="2015-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix memory estimates for mapjoin hashtable</summary>
      <description>Due to the legacy in in-memory mapjoin and conservative planning, the memory estimation code for mapjoin hashtable is currently not very good. It allocates the probe erring on the side of more memory, not taking data into account because unlike the probe, it's free to resize, so it's better for perf to allocate big probe and hope for the best with regard to future data size. It is not true for hybrid case.There's code to cap the initial allocation based on memory available (memUsage argument), but due to some code rot, the memory estimates from planning are not even passed to hashtable anymore (there used to be two config settings, hashjoin size fraction by itself, or hashjoin size fraction for group by case), so it never caps the memory anymore below 1 Gb. Initial capacity is estimated from input key count, and in hybrid join cache can exceed Java memory due to number of segments.There needs to be a review and fix of all this code.Suggested improvements:1) Make sure "initialCapacity" argument from Hybrid case is correct given the number of segments. See how it's calculated from keys for regular case; it needs to be adjusted accordingly for hybrid case if not done already.1.5) Note that, knowing the number of rows, the maximum capacity one will ever need for probe size (in longs) is row count (assuming key per row, i.e. maximum possible number of keys) divided by load factor, plus some very small number to round up. That is for flat case. For hybrid case it may be more complex due to skew, but that is still a good upper bound for the total probe capacity of all segments.2) Rename memUsage to maxProbeSize, or something, make sure it's passed correctly based on estimates that take into account both probe and data size, esp. in hybrid case.3) Make sure that memory estimation for hybrid case also doesn't come up with numbers that are too small, like 1-byte hashtable. I am not very familiar with that code but it has happened in the past.Other issues we have seen:4) Cap single write buffer size to 8-16Mb. The whole point of WBs is that you should not allocate large array in advance. Even if some estimate passes 500Mb or 40Mb or whatever, it doesn't make sense to allocate that.5) For hybrid, don't pre-allocate WBs - only allocate on write.6) Change everywhere rounding up to power of two is used to rounding down, at least for hybrid case I wanted to put all of these items in single JIRA so we could keep track of fixing all of them.I think there are JIRAs for some of these already, feel free to link them to this one.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11609" opendate="2015-8-20 00:00:00" fixdate="2015-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Capability to add a filter to hbase scan via composite key doesn&amp;#39;t work</summary>
      <description>It seems like the capability to add filter to an hbase scan which was added as part of HIVE-6411 doesn't work. This is primarily because in the HiveHBaseInputFormat, the filter is added in the getsplits instead of getrecordreader. This works fine for start and stop keys but not for filter because a filter is respected only when an actual scan is performed. This is also related to the initial refactoring that was done as part of HIVE-3420.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key3.q.out</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory3.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseScanRange.java</file>
    </fixedFiles>
  </bug>
  <bug id="11613" opendate="2015-8-20 00:00:00" fixdate="2015-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool should return non zero exit status for info command, if state is inconsistent</summary>
      <description>schematool -info just prints the version information, but it is not easy to consume the validity of the state from a tool as the exit code is 0 even if the schema version has mismatch.</description>
      <version>1.0.0,1.1.1,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="11614" opendate="2015-8-21 00:00:00" fixdate="2015-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): ctas after order by has problem</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForReturnPath.java</file>
    </fixedFiles>
  </bug>
  <bug id="1164" opendate="2010-2-12 00:00:00" fixdate="2010-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop_partition_by_name() should use drop_partition_common()</summary>
      <description>drop_partition was refactored to separate common parts to a function in HIVE-1152. drop_partition_by_name should make use of the common function.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11640" opendate="2015-8-25 00:00:00" fixdate="2015-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell command doesn&amp;#39;t work for new CLI[Beeline-cli branch]</summary>
      <description>The shell command doesn't work for the new CLI and "Error: Method not supported (state=,code=0)" was thrown during the execution for option f and e.</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="11641" opendate="2015-8-25 00:00:00" fixdate="2015-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: switch the branch to Tez master from branch</summary>
      <description>Current Tez master is pretty close to what 0.8 release is going to be.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11645" opendate="2015-8-25 00:00:00" fixdate="2015-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add in-place updates for dynamic partitions loading</summary>
      <description>Currently, updates go to log file and on console there is no visible progress.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11654" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>After HIVE-10289, HBase metastore tests failing</summary>
      <description>After the latest merge from trunk a number of the HBase unit tests are failing.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.java</file>
    </fixedFiles>
  </bug>
  <bug id="11655" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean build on the branch appears to be broken</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
    </fixedFiles>
  </bug>
  <bug id="11664" opendate="2015-8-27 00:00:00" fixdate="2015-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make tez container logs work with new log4j2 changes</summary>
      <description>MiniTezCliDriver should log container logs to syslog file. With new log4j2 changes this file is not created anymore.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11695" opendate="2015-8-31 00:00:00" fixdate="2015-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If user have no permission to create LOCAL DIRECTORY ，the Hql does not throw any exception and fail silently.</summary>
      <description>If user have no permission to create LOCAL DIRECTORY such as "/data/wangmeng/hiveserver2" ,the query does not throw any exception and fail silently.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11718" opendate="2015-9-2 00:00:00" fixdate="2015-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC ResultSet.setFetchSize(0) returns no results</summary>
      <description>Hi,According to JDBC document, the driver setFetchSize(0) should ignore, but Hive JDBC driver returns no result.Our product uses setFetchSize to fine tune performance, sometimes we would like to leave setFetchSize(0) up to the driver to make best guess of the fetch size.ThanksSon Nguyen</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11743" opendate="2015-9-4 00:00:00" fixdate="2015-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Port conflict for MiniHBaseCluster</summary>
      <description>HMaster http port conflict. Presumably a bug in HBaseTestingUtility. It suppose not to use default port for everything.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.HBaseIntegrationTests.java</file>
    </fixedFiles>
  </bug>
  <bug id="11766" opendate="2015-9-9 00:00:00" fixdate="2015-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Remove MiniLlapCluster from shim layer after hadoop-1 removal</summary>
      <description>Remove HIVE-11732 changes after HIVE-11378 goes in.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">data.conf.llap.llap-daemon-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11768" opendate="2015-9-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.io.DeleteOnExitHook leaks memory on long running Hive Server2 Instances</summary>
      <description>More than 490,000 paths was added to java.io.DeleteOnExitHook on one of our long running HiveServer2 instances,taken up more than 100MB on heap. Most of the paths contains a suffix of ".pipeout".</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestShutdownHookManager.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11775" opendate="2015-9-10 00:00:00" fixdate="2015-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement limit push down through union all in CBO</summary>
      <description>Enlightened by HIVE-11684 (Kudos to jcamachorodriguez), we can actually push limit down through union all, which reduces the intermediate number of rows in union branches.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.tez.dynpart.hashjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.join.transpose.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11802" opendate="2015-9-11 00:00:00" fixdate="2015-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Float-point numbers are displayed with different precision in Beeline/JDBC</summary>
      <description>When inserting float-point numbers to a table, the values displayed on beeline or jdbc are with different precision.How to reproduce:0: jdbc:hive2://localhost:10000&gt; create table decimals (f float, af array&lt;float&gt;, d double, ad array&lt;double&gt;) stored as parquet;No rows affected (0.294 seconds)0: jdbc:hive2://localhost:10000&gt; insert into table decimals select 1.10058, array(cast(1.10058 as float)), 2.0133, array(2.0133) from dummy limit 1;...No rows affected (20.089 seconds)0: jdbc:hive2://localhost:10000&gt; select f, af, af[0], d, ad[0] from decimals;+---------------------+------------+---------------------+---------+---------+--+| f | af | _c2 | d | _c4 |+---------------------+------------+---------------------+---------+---------+--+| 1.1005799770355225 | [1.10058] | 1.1005799770355225 | 2.0133 | 2.0133 |+---------------------+------------+---------------------+---------+---------+--+When displaying arrays, the values are displayed correctly, but if I print a specific element, it is then displayed with more decimal positions.</description>
      <version>1.2.1</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.Column.java</file>
    </fixedFiles>
  </bug>
  <bug id="11816" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade groovy to 2.4.4</summary>
      <description>Groovy 2.4.4 is the latest release and the first done under ASF.Also there are some issues with old Groovy like CVE-2015-3253, which doesn't seem to affect Hive itself but might affect applications depending on Hive that get leaked classpath artifacts.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11831" opendate="2015-9-15 00:00:00" fixdate="2015-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TXN tables in Oracle should be created with ROWDEPENDENCIES</summary>
      <description>These frequently-updated tables may otherwise suffer from spurious deadlocks.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug id="11846" opendate="2015-9-16 00:00:00" fixdate="2015-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CliDriver shutdown tries to drop index table again which was already dropped when dropping the original table</summary>
      <description>Steps to repro:set hive.stats.dbclass=fs;set hive.stats.autogather=true;set hive.cbo.enable=true;DROP TABLE IF EXISTS aa;CREATE TABLE aa (L_ORDERKEY INT, L_PARTKEY INT, L_SUPPKEY INT, L_LINENUMBER INT, L_QUANTITY DOUBLE, L_EXTENDEDPRICE DOUBLE, L_DISCOUNT DOUBLE, L_TAX DOUBLE, L_RETURNFLAG STRING, L_LINESTATUS STRING, l_shipdate STRING, L_COMMITDATE STRING, L_RECEIPTDATE STRING, L_SHIPINSTRUCT STRING, L_SHIPMODE STRING, L_COMMENT STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '|';LOAD DATA LOCAL INPATH '../../data/files/lineitem.txt' OVERWRITE INTO TABLE aa;CREATE INDEX aa_lshipdate_idx ON TABLE aa(l_shipdate) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(l_shipdate)");ALTER INDEX aa_lshipdate_idx ON aa REBUILD;show tables;explain select l_shipdate, count(l_shipdate)from aagroup by l_shipdate;The problem is that, we create an index table default_aa_lshipdate_idx, (default is the database name) and it comes after the table aa. Then, it first drop aa, which will drop default_aa_lshipdate_idx as well as it is related to aa. It will not find the table default_aa_lshipdate_idx when it tries to drop it again, which will throw an exception.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="11856" opendate="2015-9-17 00:00:00" fixdate="2015-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow split strategies to run on threadpool</summary>
      <description>If a split strategy makes metastore cache calls, it should probably run on the threadpool.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="11881" opendate="2015-9-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supporting HPL/SQL Packages</summary>
      <description>HPL/SQL should support packages similar to Oracle PL/SQL.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.resources.hplsql-site.xml</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Scope.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionOra.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Copy.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Cmp.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="11882" opendate="2015-9-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetch optimizer should stop source files traversal once it exceeds the hive.fetch.task.conversion.threshold</summary>
      <description>Hive 1.0's fetch optimizer tries to optimize queries of the form "select &lt;C&gt; from &lt;T&gt; where &lt;F&gt; limit &lt;L&gt;" to a fetch task (see the hive.fetch.task.conversion property). This optimization gets the lengths of all the files in the specified partition and does some comparison against a threshold value to determine whether it should use a fetch task or not (see the hive.fetch.task.conversion.threshold property). This process of getting the length of all files. One of the main problems in this optimization is the fetch optimizer doesn't seem to stop once it exceeds the hive.fetch.task.conversion.threshold. It works fine on HDFS, but could cause a significant performance degradation on other supported file systems.</description>
      <version>1.0.0,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SplitSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11903" opendate="2015-9-21 00:00:00" fixdate="2015-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add lock metrics to HS2</summary>
      <description>Potential metrics are active zookeeper locks taken by type. Can refine as we go along.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="11915" opendate="2015-9-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BoneCP returns closed connections from the pool</summary>
      <description>It's a very old bug in BoneCP and it will never be fixed... There are multiple workarounds on the internet but according to responses they are all unreliable. We should upgrade to HikariCP (which in turn is only supported by DN 4), meanwhile try some shamanic rituals. In this JIRA we will try a relatively weak drum.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="11925" opendate="2015-9-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive file format checking breaks load from named pipes</summary>
      <description>Opening the file and mucking with it when hive.fileformat.check is true (the default) breaks the LOAD command from a named pipe. Right now, it's done for all the text files blindly to see if they might be in some other format. Files.getAttribute can be used to figure out if the input is a named pipe (or a socket) and skip the format check.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.InputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11927" opendate="2015-9-23 00:00:00" fixdate="2015-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement/Enable constant related optimization rules in Calcite: enable HiveReduceExpressionsRule to fold constants</summary>
      <description></description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1194" opendate="2010-2-24 00:00:00" fixdate="2010-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sorted merge join</summary>
      <description>If the input tables are sorted on the join key, and a mapjoin is being performed, it is useful to exploit the sorted properties of the table.This can lead to substantial cpu savings - this needs to work across bucketed map joins also.Since, sorted properties of a table are not enforced currently, a new parameter can be added to specify to use the sort-merge join.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11950" opendate="2015-9-24 00:00:00" fixdate="2015-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat status file doesn&amp;#39;t show UTF8 character</summary>
      <description>If we do a select on a UTF8 table and store the console output into the status file (enablelog=true), the UTF8 character is garbled. The reason is we don't specify encoding when opening stdout/stderr in statusdir. This will cause problem especially on Windows, when the default OS encoding is not UTF8.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="11978" opendate="2015-9-28 00:00:00" fixdate="2015-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: NPE in Expr toString</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="11983" opendate="2015-9-28 00:00:00" fixdate="2015-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive streaming API uses incorrect logic to assign buckets to incoming records</summary>
      <description>The Streaming API tries to distribute records evenly into buckets. All records in every Transaction that is part of TransactionBatch goes to the same bucket and a new bucket number is chose for each TransactionBatch.Fix: API needs to hash each record to determine which bucket it belongs to.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11984" opendate="2015-9-28 00:00:00" fixdate="2015-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HS2 open operation metrics</summary>
      <description>Some metrics for open operations should be helpful to track operations not closed/cancelled.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="11985" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t store type names in metastore when metastore type names are not used</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11988" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hive] security issue with hive &amp; ranger for import table command</summary>
      <description>if a user does not have permission to create table in hive , then if the same user import data for a table using following command then , it will have to create table also and that is working successfully , ideally it should not workSTR:====1. put some raw data in hdfs path /user/user1/tempdata2. in ranger check policy , user1 should not have any permission on any table3. login through user1 into beeline ( obviously it will fail since user doesnt have permission to create table)create table tt1(id INT,ff String);FAILED: HiveAccessControlException Permission denied: user user1 does not have CREATE privilege on default/tt1 (state=42000,code=40000)4. now try following command to import data into a table ( table should not exist already)import table tt1 from '/user/user1/tempdata';ER:since user1 doesnt have permission to create table so this operation should failAR:table is created successfully and data is also imported !!</description>
      <version>0.14.0,1.2.1</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.import.exported.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.2.exim.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.import.exported.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.24.import.nonexist.authsuccess.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.13.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.12.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.11.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.10.external.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.08.nonpart.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.01.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.00.nonpart.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.uri.import.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="11990" opendate="2015-9-29 00:00:00" fixdate="2015-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loading data inpath from a temporary table dir fails on Windows</summary>
      <description>The query runs:load data inpath 'wasb:///tmp/testtemptable/temptablemisc_5/data' overwrite into table temp2;It fails with:FAILED: SemanticException [Error 10028]: Line 2:37 Path is not legal ''wasb:///tmp/testtemptable/temptablemisc_5/data'': Move from: wasb://humb23-hive1@humboldttesting3.blob.core.windows.net/tmp/testtemptable/temptablemisc_5/data to: hdfs://headnode0.humb23-hive1-ssh.h2.internal.cloudapp.net:8020/tmp/hive/hrt_qa/0d5f8b31-5908-44bf-ae4c-9eee956da066/_tmp_space.db/75b44252-42a7-4d28-baf8-4977daa5d49c is not valid. Please check that values for params "default.fs.name" and "hive.metastore.warehouse.dir" do not conflict.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12002" opendate="2015-9-30 00:00:00" fixdate="2015-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct implementation typo</summary>
      <description>The term "implemenation" is seen in HiveMetaScore INFO logs. Correcting.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.AlternateFailurePreListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">hcatalog.src.packages.templates.conf.hive-site.xml.template</file>
      <file type="M">hcatalog.conf.proto-hive-site.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="12003" opendate="2015-9-30 00:00:00" fixdate="2015-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Streaming API : Add check to ensure table is transactional</summary>
      <description>Check if TBLPROPERTIES ('transactional'='true') is set when opening connection</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="12013" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable most llap tests before merge</summary>
      <description>Tests cannot be parallelized before we merge, and tests cannot run fast enough (they did once, so I guess cannot always run fast enough) when they are not parallelized, and we need tests to pass before merging.LLAP is off by default, we did see tests pass recently (with the exception of a few out file diffs), and some tests will still be run, so it should be ok to proceed as follows.We will disable most of the LLAP q tests for now, merge, enable paralllelism and re-enable.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12032" opendate="2015-10-5 00:00:00" fixdate="2015-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test for HIVE-9855</summary>
      <description></description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12034" opendate="2015-10-5 00:00:00" fixdate="2015-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-4243 broke things for llap branch</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="12042" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update some out files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12043" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UGI instances being used in IO elevator threads are incorrect</summary>
      <description>... which leads to FileSystem closed exceptions.I'm not sure yet if this is a result of the threadpool being used, and UGI not working well with threadpools, or something else.The UGI instance which was setup - at what looks to be thread creation time - ends up being used for several different reads, ignoring the actual UGI passed in. At some point this changes to a new incorrect UGI.A simple fix is to propagate the correct UGI all the way to the reader, and that fixes the FileSystem Closed exception. Figuring out the precise reason would be good though.Related to HIVE-9898.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="12073" opendate="2015-10-8 00:00:00" fixdate="2015-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable session reuse for MiniTez cluster</summary>
      <description>See HIVE-12072</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12156" opendate="2015-10-13 00:00:00" fixdate="2015-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>expanding view doesn&amp;#39;t quote reserved keyword</summary>
      <description>hive&gt; create table testreserved (data struct&lt;`end`:string, id: string&gt;);OKTime taken: 0.274 secondshive&gt; create view testreservedview as select data.`end` as data_end, data.id as data_id from testreserved;OKTime taken: 0.769 secondshive&gt; select data.`end` from testreserved;OKTime taken: 1.852 secondshive&gt; select data_id from testreservedview;NoViableAltException(98@[]) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.identifier(HiveParser_IdentifiersParser.java:10858) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6438) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:6768) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:6828) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7012) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7172) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7332) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7483) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:7634) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8164) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9177) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9296) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9455) at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6105) at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:45840) at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectItem(HiveParser_SelectClauseParser.java:2907) at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectList(HiveParser_SelectClauseParser.java:1373) at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:1128) at org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:45827) at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41495) at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402) at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413) at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283) at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590) at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109) at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)...FAILED: SemanticException line 1:29 cannot recognize input near 'end' 'as' 'data_end' in expression specification in definition of VIEW testreservedview [select `testreserved`.`data`.end as `data_end`, `testreserved`.`data`.id as `data_id` from `test`.`testreserved`] used as testreservedview at Line 1:20When view is expanded, field should be quote with backquote.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12201" opendate="2015-10-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez settings need to be shown in set -v output when execution engine is tez.</summary>
      <description>The set -v output currently shows configurations for yarn, hdfs etc. but does not show tez settings when tez is set as the execution engine.</description>
      <version>1.0.1,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12204" opendate="2015-10-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez queries stopped running with ApplicationNotRunningException</summary>
      <description>In some error cases, if hive can no longer submit DAGs to tez, there is no use retrying to submit. We need to exit by throwing exception in this case.</description>
      <version>1.0.1,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12232" opendate="2015-10-22 00:00:00" fixdate="2015-10-22 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Create external table failed when enabled StorageBasedAuthorization</summary>
      <description>Please look at the stacktrace, when enabled StorageBasedAuthorization, creating external table will failed with write permission about the default warehouse path "/user/hive/warehouse": &gt; CREATE EXTERNAL TABLE test(id int) LOCATION '/tmp/wangmeng/test' ;Error: Error while compiling statement: FAILED: HiveException java.security.AccessControlException: Permission denied: user=wangmeng, access=WRITE, inode="/user/hive/warehouse":hive:hive:drwxr-x--t.</description>
      <version>1.2.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insertoverwrite.bucket.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.insertoverwrite.bucket.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12235" opendate="2015-10-22 00:00:00" fixdate="2015-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve beeline logging for dynamic service discovery</summary>
      <description>It maybe nice to see which host it tried to, and ended up, connecting to.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">beeline.src.main.resources.beeline-log4j2.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12244" opendate="2015-10-23 00:00:00" fixdate="2015-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring code for avoiding of comparison of Strings and do comparison on Path</summary>
      <description>In Hive often String is used for representation path and it causes new issues.We need to compare it with equals() but comparing Strings often is not right in terms comparing paths .I think if we use Path from org.apache.hadoop.fs we will avoid new problems in future.</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestConditionalResolverCommonJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOPrepareCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymbolicInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkSkewJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestCombineHiveInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12245" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support column comments for an HBase backed table</summary>
      <description>Currently the column comments of an HBase backed table are always returned as "from deserializer". For example,CREATE TABLE hbasetbl (key string comment 'It is key', state string comment 'It is state', country string comment 'It is country', country_id int comment 'It is country_id')STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = "info:state,info:country,info:country_id");hive&gt; describe hbasetbl;key string from deserializer state string from deserializer country string from deserializer country_id int from deserializer</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.format.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.queries.q</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12249" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging with tez</summary>
      <description>We need to improve logging across the board. TEZ-2851 added a caller context so that one can correlate logs with the application. This jira adds a new configuration for users that can be used to correlate the logs.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12254" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging with yarn/hdfs</summary>
      <description>In extension to HIVE-12249, adding info for Yarn/HDFS as well. Both HIVE-12249 and HDFS-9184 are required (and upgraded in hive for the HDFS issue) before this can be resolved.</description>
      <version>1.2.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.IDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12256" opendate="2015-10-24 00:00:00" fixdate="2015-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move LLAP registry into llap-client module</summary>
      <description>The registry may need to be accessed by the client to figure out the available nodes. (ql module needs access)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.tez.dag.app.rm.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.ServiceRegistry.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.ServiceInstance.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapYarnRegistryImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12271" opendate="2015-10-27 00:00:00" fixdate="2015-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics around HS2 query execution and job submission for Hive</summary>
      <description>We should add more metrics around query execution. Specifically: Number of in-use worker threads Number of in-use async threads Number of queries waiting for compilation Stats for query planning / compilation time Stats for total job submission time Others?</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.TestLegacyMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.LegacyMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.Metrics.java</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12280" opendate="2015-10-28 00:00:00" fixdate="2015-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveConnection does not try other HS2 after failure for service discovery</summary>
      <description>Found this while mocking some bad connection data in znode.. will try to add a test for this.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12311" opendate="2015-10-31 00:00:00" fixdate="2015-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>explain CTAS fails if the table already exists</summary>
      <description>Explain of a CTAS will fail if the table already exists.This is an annoyance when you're seeing if a large body of SQL queries will function by putting explain in front of every query. hive&gt; create table temp (x int);OKTime taken: 0.252 secondshive&gt; create table temp2 (x int);OKTime taken: 0.407 secondshive&gt; explain create table temp as select * from temp2;FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Table already exists: mydb.tempIf we compare to Postgres "The Zinc Standard of SQL Compliance":carter=# create table temp (x int);CREATE TABLEcarter=# create table temp2 (x int);CREATE TABLEcarter=# explain create table temp as select * from temp2; QUERY PLAN--------------------------------------------------------- Seq Scan on temp2 (cost=0.00..34.00 rows=2400 width=4)(1 row)If the CTAS is something complex it would be nice to see the query plan in advance.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12315" opendate="2015-11-2 00:00:00" fixdate="2015-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>vectorization_short_regress.q has a wrong result issue for a double calculation</summary>
      <description>I suspect it is related to the fancy optimizations in vectorized double divide that try to quickly process the batch without checking each row for null. public static void setNullAndDivBy0DataEntriesDouble( DoubleColumnVector v, boolean selectedInUse, int[] sel, int n, DoubleColumnVector denoms) { assert v.isRepeating || !denoms.isRepeating; v.noNulls = false; double[] vector = denoms.vector; if (v.isRepeating &amp;&amp; (v.isNull[0] = (v.isNull[0] || vector[0] == 0))) { v.vector[0] = DoubleColumnVector.NULL_VALUE;</description>
      <version>0.14.0,1.0.1,1.1.1,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12329" opendate="2015-11-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on limit pushdown optimization by default</summary>
      <description>Whenever applicable, this will always help, so this should be on by default.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.cast.constant.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12345" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup for HIVE-9013 : Hidden conf vars still visible through beeline</summary>
      <description>HIVE-9013 introduced the ability to hide certain conf variables when output through the "set" command. However, there still exists one further bug in it that causes these variables to still be visible through beeline connecting to HS2, wherein HS2 exposes hidden variables such as the HS2's metastore password when "set" is run.</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12365" opendate="2015-11-7 00:00:00" fixdate="2015-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added resource path is sent to cluster as an empty string when externally removed</summary>
      <description>Sometimes the resources (e.g. jar) added via command like "add jars &lt;filepath&gt;" are removed externally from their filepath for some reasons. Their paths are sent to cluster as empty strings which causes the failures to the query that even do not need these jars in execution. The error look like as following:15/11/06 21:56:44 INFO mapreduce.JobSubmitter: Cleaning up the staging area file:/tmp/hadoop-ctang/mapred/staging/ctang734817191/.staging/job_local734817191_0003java.lang.IllegalArgumentException: Can not create a Path from an empty string at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127) at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:135) at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:215) at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:390) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:483) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="12367" opendate="2015-11-9 00:00:00" fixdate="2015-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lock/unlock database should add current database to inputs and outputs of authz hook</summary>
      <description>Click to add description</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.lock.db.in.use.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.drop.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.db.lock.conflict.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.query.tbl.in.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dbtxnmgr.nodbunlock.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dbtxnmgr.nodblock.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12388" opendate="2015-11-12 00:00:00" fixdate="2015-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetTables cannot get external tables when TABLE type argument is given</summary>
      <description>By regression of HIVE-7575, external tables are not shown when "TABLE" type is specified as argument. I'm working on this. Sorry.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="12417" opendate="2015-11-16 00:00:00" fixdate="2015-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for exclamation mark missing in regexp</summary>
      <description>with HIVE-6013 gets support for regular expressions. However, die ! character is valid, too. It is needed for expressions like set hive.support.quoted.identifiers = none;select `^(?!donotuseme).*$` from table;which is the idiom to select all but column donotuseme .See http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html for a reference of supported chars in Java regexp.The patch simply fixes the lexer to support '!' as REGEX char. And does simply work.Please review. If you like to have an iTest for it, I beg you to help me. I tried several days on a different issue to figure out how it is supposed to work and failed miserably.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="12431" opendate="2015-11-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support timeout for compile lock</summary>
      <description>To help with HiveServer2 scalability, it would be useful to allow users to configure a timeout value for queries waiting to be compiled. If the timeout value is reached then the query would abort. One option to achieve this would be to update the compile lock to use a try-lock with the timeout value.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12469" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Commons-Collections dependency from 3.2.1 to 3.2.2. to address vulnerability</summary>
      <description>Currently the commons-collections (3.2.1) library allows for invocation of arbitrary code through InvokerTransformer, need to bump the version of commons-collections from 3.2.1 to 3.2.2 to resolve this issue.Results of mvn dependency:tree:[INFO] ------------------------------------------------------------------------[INFO] Building Hive HPL/SQL 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-hplsql ---[INFO] org.apache.hive:hive-hplsql:jar:2.0.0-SNAPSHOT[INFO] +- com.google.guava:guava:jar:14.0.1:compile[INFO] +- commons-collections:commons-collections:jar:3.2.1:compile[INFO] ------------------------------------------------------------------------[INFO] Building Hive Packaging 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] +- org.apache.hive:hive-hbase-handler:jar:2.0.0-SNAPSHOT:compile[INFO] | +- org.apache.hbase:hbase-server:jar:1.1.1:compile[INFO] | | +- commons-collections:commons-collections:jar:3.2.1:compile[INFO] ------------------------------------------------------------------------[INFO] Building Hive Common 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-common ---[INFO] +- org.apache.hadoop:hadoop-common:jar:2.6.0:compile[INFO] | +- commons-collections:commons-collections:jar:3.2.1:compileHadoop-Common dependency also found in: LLAP, Serde, Storage, Shims, Shims Common, Shims Scheduler)[INFO] ------------------------------------------------------------------------[INFO] Building Hive Ant Utilities 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-ant ---[INFO] | +- commons-collections:commons-collections:jar:3.1:compile[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building Hive Accumulo Handler 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] +- org.apache.accumulo:accumulo-core:jar:1.6.0:compile[INFO] | +- commons-collections:commons-collections:jar:3.2.1:compile</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">ant.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1247" opendate="2010-3-16 00:00:00" fixdate="2010-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hints cannot be passed to transform statements</summary>
      <description>Statements like:select /*+ MAPJOIN(a) */ transform(c1, c2) using '..' as ..does not work</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12470" opendate="2015-11-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow splits to provide custom consistent locations, instead of being tied to data locality</summary>
      <description>LLAP instances may not run on the same nodes as HDFS, or may run on a subset of the cluster.Using split locations based on FileSystem locality is not very useful in such cases - since that guarantees not getting any locality.Allow a split to map to a specific location - so that there's a chance of getting cache locality across different queries.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12471" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HS2 web UI with SSL</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12473" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DPP: UDFs on the partition column side does not evaluate correctly</summary>
      <description>Related to HIVE-12462select count(1) from accounts a, transactions t where year(a.dt) = year(t.dt) and account_id = 22;$hdt$_0:$hdt$_1:a TableScan (TS_2) alias: a filterExpr: (((account_id = 22) and year(dt) is not null) and (year(dt)) IN (RS[6])) (type: boolean)Ends up being evaluated as year(cast(dt as int)) because the pruner only checks for final type, not the column type. ObjectInspector oi = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(TypeInfoFactory .getPrimitiveTypeInfo(si.fieldInspector.getTypeName())); Converter converter = ObjectInspectorConverters.getConverter( PrimitiveObjectInspectorFactory.javaStringObjectInspector, oi);</description>
      <version>1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
    </fixedFiles>
  </bug>
  <bug id="12476" opendate="2015-11-20 00:00:00" fixdate="2015-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore NPE on Oracle with Direct SQL</summary>
      <description>Stack trace looks very similar to HIVE-8485. I believe the metastore's Direct SQL mode requires additional fixes similar to HIVE-8485, around the Partition/StorageDescriptorSerDe parameters.2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.java.lang.NullPointerException at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200) at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579) at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501) at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439) at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490) at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288) at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154) at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072) at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929) at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681) at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="12485" opendate="2015-11-20 00:00:00" fixdate="2015-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HS2 web UI with kerberos</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12492" opendate="2015-11-22 00:00:00" fixdate="2015-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapJoin: 4 million unique integers seems to be a probe plateau</summary>
      <description>After 4 million keys, the map-join implementation seems to suffer from a performance degradation. The hashtable build &amp; probe time makes this very inefficient, even if the data is very compact (i.e 2 ints).Falling back onto the shuffle join or bucket map-join is useful after 2^22 items.(Note: this fixes a statsutil issue - due to the extra clone() in the column stats path)</description>
      <version>1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12531" opendate="2015-11-26 00:00:00" fixdate="2015-1-26 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Implement fast-path for Year/Month UDFs for dates between 1999 and 2038</summary>
      <description>Current codepath goes into the JDK Calendar implementation, which is very slow for the simple cases in the current decade.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
    </fixedFiles>
  </bug>
  <bug id="12537" opendate="2015-11-28 00:00:00" fixdate="2015-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RLEv2 doesn&amp;#39;t seem to work</summary>
      <description>Perhaps I'm doing something wrong or is actually working as expected.Putting 1 million constant int32 values produces an ORC file of 1MB. Surprisingly, 1 million consecutive ints produces a much smaller file.Code and FileDump attached.ObjectInspector inspector = ObjectInspectorFactory.getReflectionObjectInspector( Integer.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);Writer w = OrcFile.createWriter(new Path("/tmp/my.orc"), OrcFile.writerOptions(new Configuration()) .compress(CompressionKind.NONE) .inspector(inspector) .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION) .version(OrcFile.Version.V_0_12) );for (int i = 0; i &lt; 1000000; ++i) { w.addRow(123);}w.close();</description>
      <version>0.14.0,1.0.1,1.1.1,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java.orig</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.java</file>
    </fixedFiles>
  </bug>
  <bug id="12591" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cache counters displays -ve value for CacheCapacityUsed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12605" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement JDBC Connection.isValid</summary>
      <description>http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#isValid(int) implementation in Hive JDBC driver throws "SQLException("Method not supported")".That is a method often used by connection pooling libraries.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="12610" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the rest</summary>
      <description>During processing the spilled partitions, if there's any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="12611" opendate="2015-12-8 00:00:00" fixdate="2015-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure spark.yarn.queue is effective and takes the value from mapreduce.job.queuename if given [Spark Branch]</summary>
      <description>Hive users sometimes specifies a job queue name for the submitted MR jobs. For spark, the property name is spark.yarn.queue. We need to make sure that user is able to submit spark jobs to the given queue. If user specifies the MR property, then Hive on Spark should take that as well to make it backward compatible.</description>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12650" opendate="2015-12-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error messages for Hive on Spark in case the cluster has no resources available</summary>
      <description>I think hive.spark.client.server.connect.timeout should be set greater than spark.yarn.am.waitTime. The default value for spark.yarn.am.waitTime is 100s, and the default value for hive.spark.client.server.connect.timeout is 90s, which is not good. We can increase it to a larger value such as 120s.</description>
      <version>1.1.1,1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="12664" opendate="2015-12-14 00:00:00" fixdate="2015-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in reduce deduplication optimization causing ArrayOutOfBoundException</summary>
      <description>The optimisation check for reduce deduplication only checks the first child node for join and the check itself also contains a major bug causing ArrayOutOfBoundException no matter what.Sample data table form:timeuserhostpathreferercodeagentsizemethodintstringstringstringstringbigintstringbigintstringSample querySELECT t1.host, COUNT(DISTINCT t1.`date`) AS login_count, MAX(t2.code) AS code, unix_timestamp() AS timeFROM ( SELECT HOST, MIN(time) AS DATE FROM www_access WHERE HOST IS NOT NULL GROUP BY HOST ) t1JOIN ( SELECT HOST, MIN(time) AS code FROM www_access WHERE HOST IS NOT NULL GROUP BY HOST ) t2 ON t1.host = t2.hostGROUP BY t1.host</description>
      <version>1.1.1,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  <bug id="12706" opendate="2015-12-17 00:00:00" fixdate="2015-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect output from from_utc_timestamp()/to_utc_timestamp when local timezone has DST</summary>
      <description>Getting wrong output with the local timezone set to PST (which has DST). I don't think this happens when the local timezone does not observe DST.select from_utc_timestamp('2015-03-28 17:00:00', 'Europe/London')2015-03-28 17:00:00select from_utc_timestamp('2015-03-28 18:00:00', 'Europe/London')2015-03-28 19:00:00 &lt;= Wrong, should be 2015-03-28 18:00:00select from_utc_timestamp('2015-03-28 19:00:00', 'Europe/London')2015-03-28 20:00:00 &lt;= Wrong, should be 2015-03-28 19:00:00Also to_utc_timestamp():select to_utc_timestamp('2015-03-28 17:00:00', 'Europe/London')2015-03-28 17:00:00select to_utc_timestamp('2015-03-28 18:00:00', 'Europe/London')2015-03-28 17:00:00 &lt;= Wrongselect to_utc_timestamp('2015-03-28 19:00:00', 'Europe/London')2015-03-28 18:00:00 &lt;= Wrongselect to_utc_timestamp('2015-03-28 20:00:00', 'Europe/London')2015-03-28 19:00:00 &lt;= Wrong</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
    </fixedFiles>
  </bug>
  <bug id="12719" opendate="2015-12-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>As a hive user, I am facing issues using permanent UDAF&amp;#39;s.</summary>
      <description>Currently function name for the permanent function is getting formed as dbName.WINDOW_FUNC_PREFIX+functionName where as function registry has function name as WINDOW_FUNC_PREFIX+dbName.functionName. This is leading to invalid function error when we use permanent function with window function. Fix has been done such that we form the permanent function name rightly as WINDOW_FUNC_PREFIX+dbName+.+functionName so that it matches with the name in function registry. The functionality for built-in/temporary function would remain the same.</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
    </fixedFiles>
  </bug>
  <bug id="1272" opendate="2010-3-23 00:00:00" fixdate="2010-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SymlinkTextInputFormat to Hive</summary>
      <description>We'd like to add a symlink text input format so that we can specify the list of files for a table/partition based on the content of a text file.For example, the location of the table is "/user/hive/mytable".There is a file called "/user/hive/mytable/myfile.txt".Inside the file, there are 2 lines, "/user/myname/textfile1.txt" and "/user/myname/textfile2.txt"We can do:CREATE TABLE mytable (...) STORED AS INPUTFORMAT 'org.apache.hadoop.hive.io.SymlinkTextInputFormat' LOCATION '/user/hive/mytable';SELECT * FROM mytable;which will return the content of the 2 files: "/user/myname/textfile1.txt" and "/user/myname/textfile2.txt"</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12734" opendate="2015-12-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundancy in HiveConfs serialized to UDFContext</summary>
      <description>HCatLoader lands up serializing one HiveConf instance per table-alias, to Pig's UDFContext. This lands up bloating the UDFContext.To reduce the footprint, it makes sense to serialize a default-constructed HiveConf once, and one "diff" per HCatLoader. This should reduce the time taken to kick off jobs from pig -useHCatalog scripts.(Note_to_self: YHIVE-540).</description>
      <version>1.2.1,2.0.0,2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12742" opendate="2015-12-24 00:00:00" fixdate="2015-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL table comparison within CASE does not work as previous hive versions</summary>
      <description>drop table test_1; create table test_1 (id int, id2 int); insert into table test_1 values (123, NULL);SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1; --NULLBut the output should be true (confirmed with postgres.)</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12744" opendate="2015-12-24 00:00:00" fixdate="2015-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GROUPING__ID failed to be recognized in multiple insert</summary>
      <description>When using multiple insert with multiple group by, grouping__id will failed to be parse.hive&gt; create temporary table testtable3 (id string, name string);OKTime taken: 1.019 secondshive&gt; create temporary table testtable2 (id string, name string);OKTime taken: 0.069 secondshive&gt; create temporary table testtable1 (id string, name string);OKTime taken: 0.066 secondshive&gt; insert into table testtable1 values ("id", "2333");...OKTime taken: 32.515 secondshive&gt; from testtable1 &gt; insert into table testtable2 select &gt; id, GROUPING__ID &gt; group by id, name with cube;...OKTime taken: 42.032 secondshive&gt; from testtable1 &gt; insert into table testtable2 select &gt; id, GROUPING__ID &gt; group by id, name with cube &gt; insert into table testtable3 select &gt; id, name &gt; group by id, name grouping sets ((id), (id, name));FAILED: SemanticException &amp;#91;Error 10025&amp;#93;: Line 3:8 Expression not in GROUP BY key 'GROUPING__ID'</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="12751" opendate="2015-12-27 00:00:00" fixdate="2015-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NVL explain syntax</summary>
      <description>predicate: (not if (key = '238') is null returnsfalse) (type: boolean)At best is confusing to a user who used an NVL udf. The best representation is NVL(a,b) as is.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.nvl.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12761" opendate="2015-12-30 00:00:00" fixdate="2015-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add stack trace servlet to HS2 web ui</summary>
      <description>To confirm the state of HS2, I add the servlet which prints stack trace.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12763" opendate="2015-12-30 00:00:00" fixdate="2015-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use bit vector to track NDV</summary>
      <description>This will improve merging of per partitions stats. It will also help merge NDV for auto-gather column stats.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.varchar.udf1.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.long.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.empty.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.udf1.q.java1.7.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.src.protobuf.org.apache.hadoop.hive.metastore.hbase.hbase.metastore.proto.proto</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BooleanColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BinaryColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.StatsCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StringColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DateColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.protobuf.gen-java.org.apache.hadoop.hive.metastore.hbase.HbaseMetastoreProto.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveStatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12772" opendate="2016-1-4 00:00:00" fixdate="2016-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline/JDBC output of decimal values is not 0-padded, does not match with CLI output</summary>
      <description>HIVE-12063 changed the output of decimal values to pad zeros to the column's full scale for Hive CLI.It looks like Beeline and JDBC still have the old behavior that strips trailing 0s.Beeline:+---------------+---------------+--+| c1 | c2 |+---------------+---------------+--+| 1.9999999999 | 1.9999999999 || 9.9999999999 | 9.9999999999 |+---------------+---------------+--+HiveCli:1.99999999990 1.99999999999.99999999990 9.9999999999</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TableSchema.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.RowBasedSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnBasedSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Column.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">data.files.datatypes.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12793" opendate="2016-1-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address TestSparkCliDriver.testCliDriver_order2 failure due to HIVE-12782</summary>
      <description></description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.order2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12794" opendate="2016-1-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cannot run queries against HBase due to missing HBase jars</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12796" opendate="2016-1-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to 32-bits containers for HMS upgrade tests</summary>
      <description>The Hive metastore upgrade tests create LXC containers for each of the databases server supported by HMS. These containers are default to Ubuntu 64-bits. The Oracle database libraries are correctly executed on 32-bits only. We should switch to 32-bits containers for all the database servers to allow tests being executed for Oracle as well.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12811" opendate="2016-1-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Name yarn application name more meaning than just "Hive on Spark"</summary>
      <description>MR uses the query as the application name. Hopefully this can be set via spark.app.name.</description>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12863" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix test failure for TestMiniTezCliDriver.testCliDriver_tez_union</summary>
      <description></description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="12867" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semantic Exception Error Msg should be with in the range of "10000 to 19999"</summary>
      <description>At many places errors encountered during semantic exception is translated as generic error(GENERIC_ERROR, 40000) msg as opposed to semantic error msg.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="12868" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix empty operation-pool metrics</summary>
      <description>The newly-added operation pool metrics (thread-pool size, queue size) are empty because metrics system is initialized too late.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="12878" opendate="2016-1-15 00:00:00" fixdate="2016-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Vectorization for TEXTFILE and other formats</summary>
      <description>Support vectorizing when the input format is TEXTFILE and other formats for better Map Vertex performance.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.table.q</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.VerifyFast.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.fast.DeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.schema.evolution.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.fetchwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.table.q</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRowDynBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRowSameBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRowDynBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRowSameBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSparkPartitionPruningSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NullRowsInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionConversion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorRowObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.queries.clientpositive.avro.schema.evolution.native.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.groupby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.part.q</file>
    </fixedFiles>
  </bug>
  <bug id="12888" opendate="2016-1-19 00:00:00" fixdate="2016-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSparkNegativeCliDriver does not run in Spark mode[Spark Branch]</summary>
      <description>During test, i found TestSparkNegativeCliDriver run in MR mode actually, it should be fixed.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug id="12905" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Issue with mapjoin in tez under certain conditions</summary>
      <description>In a specific case where we have an outer join followed by another join on the same key and the non-outer side of the outer join is empty, hive-on-tez produces incorrect results.</description>
      <version>1.0.1,1.2.1,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13020" opendate="2016-2-8 00:00:00" fixdate="2016-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metastore and HiveServer2 to Zookeeper fails with IBM JDK</summary>
      <description>HiveServer2 and Hive Metastore Zookeeper component is hardcoded to only support the Oracle/Open JDK. I was performing testing of Hadoop running on the IBM JDK and discovered this issue and have since drawn up the attached patch. This looks to resolve the issue in a similar manner as how the Hadoop core folks handle the IBM JDK.</description>
      <version>1.2.0,1.2.1,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13039" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BETWEEN predicate is not functioning correctly with predicate pushdown on Parquet table</summary>
      <description>BETWEEN becomes exclusive in parquet table when predicate pushdown is on (as it is by default in newer Hive versions). To reproduce(in a cluster, not local setup):CREATE TABLE parquet_tbl( key int, ldate string) PARTITIONED BY ( lyear string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';insert overwrite table parquet_tbl partition (lyear='2016') select 1, '2016-02-03' from src limit 1;set hive.optimize.ppd.storage = true;set hive.optimize.ppd = true;select * from parquet_tbl where ldate between '2016-02-03' and '2016-02-03';No row will be returned in a cluster.But if you turn off hive.optimize.ppd, one row will be returned.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.FilterPredicateLeafBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="13056" opendate="2016-2-13 00:00:00" fixdate="2016-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>delegation tokens do not work with HS2 when used with http transport and kerberos</summary>
      <description>We're getting a HiveSQLException on secure windows clusters.2016-02-08 13:48:09,535|beaver.machine|INFO|6114|140264674350912|MainThread|Job ID : 0000000-160208134528402-oozie-oozi-W2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Workflow Name : hive2-wf2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|App Path : wasb://oozie1-hbs24@humbtestings5jp.blob.core.windows.net/user/hrt_qa/test_hiveserver22016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Status : KILLED2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Run : 02016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|User : hrt_qa2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Group : -2016-02-08 13:48:09,547|beaver.machine|INFO|6114|140264674350912|MainThread|Created : 2016-02-08 13:47 GMT2016-02-08 13:48:09,548|beaver.machine|INFO|6114|140264674350912|MainThread|Started : 2016-02-08 13:47 GMT2016-02-08 13:48:09,552|beaver.machine|INFO|6114|140264674350912|MainThread|Last Modified : 2016-02-08 13:48 GMT2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|Ended : 2016-02-08 13:48 GMT2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|CoordAction ID: -2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|Actions2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|ID Status Ext ID Ext Status Err Code2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,571|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@:start: OK - OK -2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@hive-node ERROR - ERROR HiveSQLException2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@fail OK - OK E07292016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13093" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive metastore does not exit on start failure</summary>
      <description>If metastore startup fails for some reason, such as not being able to access the database, it fails to exit. Instead the process continues to be up in a bad state.This is happening because of a non daemon thread.</description>
      <version>0.13.1,1.0.0,1.1.1,1.2.1</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="13108" opendate="2016-2-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operators: SORT BY randomness is not safe with network partitions</summary>
      <description>SORT BY relies on a transient Random object, which is initialized once per deserialize operation.This results in complications during a network partition and when Tez/Spark reuses a cached plan.</description>
      <version>1.2.1,1.3.0,2.0.0,2.0.1</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13144" opendate="2016-2-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node</summary>
      <description>When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13169" opendate="2016-2-26 00:00:00" fixdate="2016-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2: Support delegation token based connection when using http transport</summary>
      <description>HIVE-5155 introduced support for delegation token based connection. However, it was intended for tcp transport mode. We need to have similar mechanisms for http transport.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
    </fixedFiles>
  </bug>
  <bug id="1322" opendate="2010-4-23 00:00:00" fixdate="2010-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cached FileSystem can lead to persistant IOExceptions</summary>
      <description>In the metastore, the FileSystem object is created through Path.getFileSytem(), which caches the created instance for performance. For DFS, the cached FileSystem seems to retain the original IP (resolved from the supplied hostname). If the hostname-&gt;IP mapping changes, then FS operations will throw IOExceptions. Because the FileSystem is cached, re-creating the object has no effect and will continue to result in IOExceptions.One solution is to call close on an IOException. That will remove the entry in the cache.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13264" opendate="2016-3-11 00:00:00" fixdate="2016-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC driver makes 2 Open Session Calls for every open session</summary>
      <description>When HTTP is used as the transport mode by the Hive JDBC driver, we noticed that there is an additional open/close session just to validate the connection. TCLIService.Iface client = new TCLIService.Client(new TBinaryProtocol(transport)); TOpenSessionResp openResp = client.OpenSession(new TOpenSessionReq()); if (openResp != null) { client.CloseSession(new TCloseSessionReq(openResp.getSessionHandle())); }The open session call is a costly one and should not be used to test transport.</description>
      <version>1.2.1,2.0.1</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="13283" opendate="2016-3-14 00:00:00" fixdate="2016-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: make sure IO elevator is enabled by default in the daemons</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13313" opendate="2016-3-18 00:00:00" fixdate="2016-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TABLESAMPLE ROWS feature broken for Vectorization</summary>
      <description>For vectorization, the ROWS clause is ignored causing many rows to be returned.SELECT * FROM source TABLESAMPLE(10 ROWS);</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13320" opendate="2016-3-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
      <description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
      <version>1.2.1,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
    </fixedFiles>
  </bug>
  <bug id="13372" opendate="2016-3-28 00:00:00" fixdate="2016-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Macro overwritten when multiple macros are used in one column</summary>
      <description>When multiple macros are used in one column, results of the later ones are over written by that of the first.For example:Suppose we have created a table called macro_test with single column x in STRING type, and with data as:"a""bb""ccc"We also create three macros:CREATE TEMPORARY MACRO STRING_LEN(x string) length(x);CREATE TEMPORARY MACRO STRING_LEN_PLUS_ONE(x string) length(x)+1;CREATE TEMPORARY MACRO STRING_LEN_PLUS_TWO(x string) length(x)+2;When we ran the following query, SELECT CONCAT(STRING_LEN(x), ":", STRING_LEN_PLUS_ONE(x), ":", STRING_LEN_PLUS_TWO(x)) aFROM macro_testSORT BY a DESC;We get result:3:3:32:2:21:1:1instead of expected:3:4:52:3:41:2:3Currently we are using Hive 1.2.1, and have applied both HIVE-11432 and HIVE-12277 patches.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="13385" opendate="2016-3-30 00:00:00" fixdate="2016-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Cleanup] Streamline Beeline instantiation</summary>
      <description>Janitorial. Remove circular dependencies in BeelineCommandLineCompleter. Stream line code readability.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCommandCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="13391" opendate="2016-3-30 00:00:00" fixdate="2016-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to LLAP to use keytab to authenticate to read data</summary>
      <description>This can be used for non-doAs case to allow access to clients who don't propagate HDFS tokens.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="13393" opendate="2016-3-30 00:00:00" fixdate="2016-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: Print help message for the --incremental option</summary>
      <description>beeline --help doesn't print the usage tips for the --incremental option.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13433" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes for additional incompatible changes in tez-0.8.3</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13439" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: provide a way to retrieve GUID to query Yarn ATS</summary>
      <description>HIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13442" opendate="2016-4-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: refactor submit API to be amenable to signing</summary>
      <description>This is going to be a wire compat breaking change.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13443" opendate="2016-4-6 00:00:00" fixdate="2016-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: signing for the second state of submit (the event)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.tez.dag.api.TaskSpecBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="13444" opendate="2016-4-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add HMAC signatures to LLAP; verify them on LLAP side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapTokenChecker.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenClientFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13445" opendate="2016-4-7 00:00:00" fixdate="2016-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: token should encode application and cluster ids</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenProvider.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13446" opendate="2016-4-7 00:00:00" fixdate="2016-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: set default management protocol acls to deny all</summary>
      <description>The user needs to set the acls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13447" opendate="2016-4-7 00:00:00" fixdate="2016-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: check ZK acls for registry and fail if they are too permissive</summary>
      <description>Only the current ("hive") user can have write access.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13448" opendate="2016-4-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: check ZK acls for ZKSM and fail if they are too permissive</summary>
      <description>Only the current user should have any access.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13457" opendate="2016-4-7 00:00:00" fixdate="2016-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HS2 REST API endpoints for monitoring information</summary>
      <description>Similar to what is exposed in HS2 webui in HIVE-12338, it would be nice if other UI's like admin tools or Hue can access and display this information as well. Hence, we will create some REST endpoints to expose this information.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1351" opendate="2010-5-19 00:00:00" fixdate="2010-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to cat rcfiles</summary>
      <description>It will be useful for debugging</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13520" opendate="2016-4-14 00:00:00" fixdate="2016-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow any test to run for longer than 60minutes in the ptest setup</summary>
      <description>Current timeout for batches is 2hours. This needs to be lowered. 1hour may be too much as well. We can start with this, and reduce timeouts further.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug id="13522" opendate="2016-4-14 00:00:00" fixdate="2016-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>regexp_extract.q hangs on master</summary>
      <description>Disable to unblock Hive QA runs.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13561" opendate="2016-4-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used</summary>
      <description>I can repo this on branch-1.2 and branch-2.0.It looks to be the same issues as: HIVE-11408The patch from HIVE-11408 looks to fix the issue as well.I've updated the patch from HIVE-11408 to be aligned with branch-1.2 and master</description>
      <version>1.2.0,1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13562" opendate="2016-4-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vector bridge for all non-vectorized udfs</summary>
      <description>Mechanism already exists for this via VectorUDFAdaptor but we have arbitrarily hand picked few udfs to go through it. I think we should enable this by default for all udfs.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.between.columns.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13652" opendate="2016-4-29 00:00:00" fixdate="2016-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import table change order of dynamic partitions</summary>
      <description>Table with multiple dynamic partitions like year,month, day exported using "export table" command is imported (using "import table") such a way that order of partitions is changed to day, month, year.Export DB: Hive 0.14Import DB: Hive 1.2.1000.2.4.0.0-169Tables created as:create table T1( ... ) PARTITIONED BY (period_year string, period_month string, period_day string) STORED AS ORC TBLPROPERTIES ("orc.compress"="SNAPPY");export command:export table t1 to 'path'import command:import table t1 from 'path'HDFS file structure on both original table location and export path keeps the original partition order ../year/month/dayHDFS file structure after import is .../day/month/year</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="13653" opendate="2016-4-29 00:00:00" fixdate="2016-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve config error messages for LLAP cache size/etc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13659" opendate="2016-4-30 00:00:00" fixdate="2016-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>An empty where condition leads to vectorization exceptions instead of throwing a compile time error</summary>
      <description>A partial queryselect count (distinct field) from table where field;Note the missing 'field=value'resulted in the following error in task logs, instead of failing early during compileorg.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:326) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83) ... 17 moreCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector at org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.evaluate(SelectColumnIsTrue.java:46) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:106) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:838) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:97) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:164) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45) ... 18 moreComment by Gunther:I think this works by implicitly converting the expr to boolean (if a cast is necessary). This query looks legal to me and probably needs to be handle in VectorizationComment by Ashutosh:Oracle, postgres &amp; sql server throws error for this if type of field is not boolean. However, MySQL &amp; Hive (with vectorization off) executes the query by implicitly adding a cast to boolean. Hive shall be consistent in its behavior regardless whether vectorization is on or off.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="1366" opendate="2010-5-25 00:00:00" fixdate="2010-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>inputFileFormat error if the merge job takes a different input file format than the default output file format</summary>
      <description>If the input file format is say SequenceFileFormat and the default fileformat is RCFile. the merge job after the MR job assumes the input format is SequenceFile format rather than RCFile. This is probably introduced in HIVE-1357.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="13660" opendate="2016-4-30 00:00:00" fixdate="2016-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorizing IN expression with list of columns throws java.lang.ClassCastException ExprNodeColumnDesc cannot be cast to ExprNodeConstantDesc</summary>
      <description>Example:SELECT * FROM alltypesorc WHERE cint in (ctinyint, cbigint);</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13725" opendate="2016-5-10 00:00:00" fixdate="2016-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Streaming API should synchronize calls when multiple threads use the same endpoint</summary>
      <description>Currently, the streaming endpoint creates a metastore client which gets used for RPC. The client itself is not internally thread safe. Therefore, the API methods should provide the relevant synchronization so that the methods can be called from different threads. A sample use case is as follows:1. Thread 1 creates a streaming endpoint and opens a txn batch.2. Thread 2 heartbeats the txn batch.With the current impl, this can result in an "out of sequence response", since the response of the calls in thread1 might end up going to thread2 and vice-versa.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="13756" opendate="2016-5-13 00:00:00" fixdate="2016-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map failure attempts to delete reducer _temporary directory on multi-query pig query</summary>
      <description>A pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.If one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1378" opendate="2010-6-1 00:00:00" fixdate="2010-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return value for map, array, and struct needs to return a string</summary>
      <description>In order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">data.scripts.input20.script</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13853" opendate="2016-5-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add X-XSRF-Header filter to HS2 HTTP mode and WebHCat</summary>
      <description>There is a possibility that there may be a CSRF-based attack on various hadoop components, and thus, there is an effort to add a block for all incoming http requests if they do not contain a X-XSRF-Header header. (See HADOOP-12691 for motivation)This has potential to affect HS2 when running on thrift-over-http mode(if cookie-based-auth is used), and webhcat.We introduce new flags to determine whether or not we're using the filter, and if we are, we will automatically reject any http requests which do not contain this header.To allow this to work, we also need to make changes to our JDBC driver to automatically inject this header into any requests it makes. Also, any client-side programs/api not using the JDBC driver directly will need to make changes to add a X-XSRF-Header header to the request to make calls to HS2/WebHCat if this filter is enabled.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.Utils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13866" opendate="2016-5-26 00:00:00" fixdate="2016-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flatten callstack for directSQL errors</summary>
      <description>These errors look like final errors and confuse people. The callstack may be useful if it's some datanucleus/db issue, but it needs to be flattened and logged with a warning that this is not a final query error and that there's a fallback</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="13884" opendate="2016-5-28 00:00:00" fixdate="2016-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow queries in HMS fetching more than a configured number of partitions</summary>
      <description>Currently the PartitionPruner requests either all partitions or partitions based on filter expression. In either scenarios, if the number of partitions accessed is large there can be significant memory pressure at the HMS server end.We already have a config hive.limit.query.max.table.partition that enforces limits on number of partitions that may be scanned per operator. But this check happens after the PartitionPruner has already fetched all partitions.We should add an option at PartitionPruner level to disallow queries that attempt to access number of partitions beyond a configurable limit.Note that hive.mapred.mode=strict disallow queries without a partition filter in PartitionPruner, but this check accepts any query with a pruning condition, even if partitions fetched are large. In multi-tenant environments, admins could use more control w.r.t. number of partitions allowed based on HMS memory capacity.One option is to have PartitionPruner first fetch the partition names (instead of partition specs) and throw an exception if number of partitions exceeds the configured value. Otherwise, fetch the partition specs.Looks like the existing listPartitionNames call could be used if extended to take partition filter expressions like getPartitionsByExpr call does.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13885" opendate="2016-5-28 00:00:00" fixdate="2016-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive session close is not resetting thread name</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13909" opendate="2016-6-1 00:00:00" fixdate="2016-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade ACLs in LLAP registry when the cluster is upgraded to secure</summary>
      <description>ZK model has authentication and authorization mixed together, so it's impossible to set up acls that would carry over between unsecure and secure clusters in the normal case (i.e. work for a specific users no matter the authentication method).To support cluster updates from unsecure to secure, we'd need to change the ACLs ourselves.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="1394" opendate="2010-6-8 00:00:00" fixdate="2010-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>do not update transient_lastDdlTime if the partition is modified by a housekeeping operation</summary>
      <description>Currently. purging looks at the hdfs time to see the last time the files got modified.It should look at the metastore property instead - these are facebook specific utilities, which do not require any changes to hive.However, in some cases, the operation might be performed by some housekeeping job, which should not modify the timestamp.Since, hive has no way of knowing the origin of the query, it might be a good idea to add a new hint which specifies that the operation is a cleanup operation, and the timestamp in the metastore need not be touched for that scenario.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13941" opendate="2016-6-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve errors returned from SchemaTool</summary>
      <description>We've had feedback from Ambari folks on Schematool usage being opaque on errors.While, yes, the underlying error is present hidden in the stacktrace if you do a --verbose, that is often unwieldy and unusable. And without a --verbose, there is no indication of what actually went wrong.Thus, we need to fix this.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="13947" opendate="2016-6-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS print wrong number for hash table size in map join scenario</summary>
      <description>In sparkHashTableSinkOperator, when flushToFile, before close output stream, it try to get the file length, and will get 0 for it, take hashTableSinkOperator for ref, it should get length after output stream closed</description>
      <version>1.2.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1402" opendate="2010-6-12 00:00:00" fixdate="2010-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add parallel ORDER BY to Hive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.IndexWhereResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14027" opendate="2016-6-15 00:00:00" fixdate="2016-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL values produced by left outer join do not behave as NULL</summary>
      <description>Consider the following setup:create table tbl (n bigint, t string); insert into tbl values (1, 'one'); insert into tbl values(2, 'two');select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a left outer join (select * from tbl where 1 = 2) b on a.n = b.n;1 one false trueThe query should return true for isnull(b.n).I've tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case.</description>
      <version>1.2.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14052" opendate="2016-6-17 00:00:00" fixdate="2016-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup structures when external clients use LLAP</summary>
      <description>Per sseth: There's no cleanup at the moment, and structures used in LLAP to track a query will keep building up slowly over time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="14153" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: beeline history doesn&amp;#39;t work on Hive2</summary>
      <description>The up arrow on console is supposed to display history, which is broken currently. Changes in HIVE-6758 broke it.</description>
      <version>1.2.1,2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  <bug id="14158" opendate="2016-7-4 00:00:00" fixdate="2016-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deal with derived column names</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MaskAndFilterInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.masking.8.q</file>
    </fixedFiles>
  </bug>
  <bug id="14175" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix creating buckets without scheme information</summary>
      <description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="14186" opendate="2016-7-7 00:00:00" fixdate="2016-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display the UDF exception message in MapReduce in beeline console</summary>
      <description>Currently when Mapper or Reducer fails, the beeline console will print the following error.Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask (state=08S01,code=2) It would be helpful if we can print the exceptions from the mapreduce to the beeline console directly so you don't need to dig into the MR log to find it.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="14187" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDOPersistenceManager objects remain cached if MetaStoreClient#close is not called</summary>
      <description>JDOPersistenceManager objects are cached in JDOPersistenceManagerFactory by DataNuclues.A new JDOPersistenceManager object gets created for every HMS thread since ObjectStore is a thread local.In non-embedded metastore mode, JDOPersistenceManager associated with a thread only gets cleaned up if IMetaStoreClient#close is called by the client (which calls ObjectStore#shutdown which calls JDOPersistenceManager#close which in turn removes the object from cache in JDOPersistenceManagerFactory#releasePersistenceManagerhttps://github.com/datanucleus/datanucleus-api-jdo/blob/master/src/main/java/org/datanucleus/api/jdo/JDOPersistenceManagerFactory.java#L1271), i.e. the object will remain cached if client does not call close.For example: If one interrupts out of hive CLI shell (instead of using 'exit;' command), SessionState#close does not get called, and hence IMetaStoreClient#close does not get called.Instead of relying the client to call close, it's cleaner to automatically perform RawStore related cleanup at the server end via deleteContext() which gets called when the server detects a lost/closed connection.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="14188" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAPIF: wrong user field is used from the token</summary>
      <description>realUser is not usually set in all cases for delegation tokens; we should use the owner.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="14204" opendate="2016-7-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize loading dynamic partitions</summary>
      <description>Lots of time is spent in sequential fashion to load dynamic partitioned dataset in driver side. E.g simple dynamic partitioned load as follows takes 300+ secondsINSERT INTO web_sales_test partition(ws_sold_date_sk) select * from tpcds_bin_partitioned_orc_200.web_sales;Time taken to load dynamic partitions: 309.22 seconds</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14205" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive doesn&amp;#39;t support union type with AVRO file format</summary>
      <description>Reproduce steps:hive&gt; CREATE TABLE avro_union_test &gt; PARTITIONED BY (p int) &gt; ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' &gt; STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' &gt; OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' &gt; TBLPROPERTIES ('avro.schema.literal'='{ &gt; "type":"record", &gt; "name":"nullUnionTest", &gt; "fields":[ &gt; { &gt; "name":"value", &gt; "type":[ &gt; "null", &gt; "int", &gt; "long" &gt; ], &gt; "default":null &gt; } &gt; ] &gt; }');OKTime taken: 0.105 secondshive&gt; alter table avro_union_test add partition (p=1);OKTime taken: 0.093 secondshive&gt; select * from avro_union_test;FAILED: RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException: Failed with exception Hive internal error inside isAssignableFromSettablePrimitiveOI void not supported yet.java.lang.RuntimeException: Hive internal error inside isAssignableFromSettablePrimitiveOI void not supported yet. at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettablePrimitiveOI(ObjectInspectorUtils.java:1140) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1149) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1187) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1220) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1200) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219) at org.apache.hadoop.hive.ql.exec.FetchOperator.setupOutputObjectInspector(FetchOperator.java:581) at org.apache.hadoop.hive.ql.exec.FetchOperator.initialize(FetchOperator.java:172) at org.apache.hadoop.hive.ql.exec.FetchOperator.&lt;init&gt;(FetchOperator.java:140) at org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:79) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:482) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:311) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1194) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1289) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1120) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1108) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:218) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:170) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:381) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:773) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:691) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Another test case to show this problem is:hive&gt; create table avro_union_test2 (value uniontype&lt;int,bigint&gt;) stored as avro;OKTime taken: 0.053 secondshive&gt; show create table avro_union_test2;OKCREATE TABLE `avro_union_test2`( `value` uniontype&lt;void,int,bigint&gt; COMMENT '')ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'LOCATION 'hdfs://localhost/user/hive/warehouse/avro_union_test2'TBLPROPERTIES ( 'transient_lastDdlTime'='1468173589')Time taken: 0.051 seconds, Fetched: 12 row(s)Although column value is defined as uniontype&lt;int,bigint&gt; in create table command, its type becomes uniontype&lt;void,int,bigint&gt; after table is defined. Hive accidentally make the nullable definition in avro schema (["null", "int", "long"]) into union definition.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14207" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Strip HiveConf hidden params in webui conf</summary>
      <description>HIVE-12338 introduced a new web ui, which has a page that displays the current HiveConf being used by HS2. However, before it displays that config, it does not strip entries from it which are considered "hidden" conf parameters, thus exposing those values from a web-ui for HS2. We need to add stripping to this.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14348" opendate="2016-7-26 00:00:00" fixdate="2016-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for alter table exchange partition</summary>
      <description></description>
      <version>1.2.1,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exchgpartition2lel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exchange.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14349" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: LIKE should anchor the regexes</summary>
      <description>RLIKE works like contains() and LIKE works like matches().The UDFLike LIKE -&gt; Regex conversion returns unanchored regexes making the vectorized LIKE behave like RLIKE.create temporary table x (a string) stored as orc;insert into x values('XYZa'), ('badXYZa');select * from x where a LIKE 'XYZ%a%' order by 1;OKXYZabadXYZaTime taken: 4.029 seconds, Fetched: 2 row(s)</description>
      <version>1.2.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.udf2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
    </fixedFiles>
  </bug>
  <bug id="1435" opendate="2010-6-25 00:00:00" fixdate="2010-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgraded naming scheme causes JDO exceptions</summary>
      <description>We recently upgraded from Datanucleus 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. Because of this change, connecting to an existing database would throw exceptions such as:2010-06-24 17:59:09,854 ERROR exec.DDLTask (SessionState.java:printError(277)) - FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list'org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list' at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:325) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:2012) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:144) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:633) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:506) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:384) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:302) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14391" opendate="2016-7-30 00:00:00" fixdate="2016-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestAccumuloCliDriver is not executed during precommit tests</summary>
      <description>according to for example this build result:https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/685/testReport/org.apache.hadoop.hive.cli/there is no 'TestAccumuloCliDriver' being run during precommit testing...but i see no reason why and how it was excluded inside the project;my maven executes it when i start it with -Dtest=TestAccumuloCliDriver - so i think the properties/profiles aren't preventing it.maybe i miss something obvious (note: my TestAccumuloCliDriver executions are failed with errors.)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.resources.test-configuration2.properties</file>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreAccumuloCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloTestSetup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="14392" opendate="2016-7-30 00:00:00" fixdate="2016-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>llap daemons should try using YARN local dirs, if available</summary>
      <description>LLAP required hive.llap.daemon.work.dirs to be specified. When running as a YARN app - this can use the local dirs for the container - removing the requirement to setup this parameter (for secure and non-secure clusters).</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14426" opendate="2016-8-4 00:00:00" fixdate="2016-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extensive logging on info level in WebHCat</summary>
      <description>There is an extensive logging in WebHCat at info level, and even some sensitive information could be logged</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14428" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopMetrics2Reporter leaks memory if the metrics sink is not configured correctly</summary>
      <description>About 80MB held after 7 hours of running. Metrics2Collector aggregates only when it's invoked by the Hadoop sink.Options - the first one is better IMO.1. Fix Metrics2Collector to aggregate more often, and fix the dependency in Hive accordingly2. Don't setup the metrics sub-system if a sink is not configured.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1443" opendate="2010-6-29 00:00:00" fixdate="2010-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support to turn off bucketing with ALTER TABLE</summary>
      <description>Currently, there is an alter table command that can change the bucketing / sort columns, as well as the number of buckets (ALTER TABLE table_name CLUSTERED BY (col_name, col_name, ...) &amp;#91;SORTED BY (col_name, ...)&amp;#93; INTO num_buckets BUCKETS). However, this command does not provide a means to disable bucketing. This proposes to introduce a syntax likeALTER TABLE src NOT CLUSTERED;that would turn off bucketing.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14436" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 1.2.1/Hitting "ql.Driver: FAILED: IllegalArgumentException Error: , expected at the end of &amp;#39;decimal(9&amp;#39;" after enabling hive.optimize.skewjoin and with MR engine</summary>
      <description>PROBLEM:The following Query run with MapReduce engine with "hive.optimize.skewjoin = true" fails with error:"FAILED: IllegalArgumentException Error: , expected at the end of 'decimal(9'" &gt; SELECT a.col1 FROM db.tableA a INNER JOIN db.tableB b ON b.key=a.key limit 5;FAILED: IllegalArgumentException Error: , expected at the end of 'decimal(9'16/08/04 12:47:50 &amp;#91;main&amp;#93;: ERROR ql.Driver: FAILED: IllegalArgumentException Error: , expected at the end of 'decimal(9'java.lang.IllegalArgumentException: Error: , expected at the end of 'decimal(9' at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:336) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseParams(TypeInfoUtils.java:378) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parsePrimitiveParts(TypeInfoUtils.java:518) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.parsePrimitiveParts(TypeInfoUtils.java:533) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.createPrimitiveTypeInfo(TypeInfoFactory.java:136) at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo(TypeInfoFactory.java:109) at org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.processSkewJoin(GenMRSkewJoinProcessor.java:214) at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory$SkewJoinJoinProcessor.process(SkewJoinProcFactory.java:60) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110) at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver$SkewJoinTaskDispatcher.dispatch(SkewJoinResolver.java:100) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110) at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.resolve(SkewJoinResolver.java:55) at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:107) at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(MapReduceCompiler.java:270) at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10219) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:459) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:316) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1189) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1237) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1126) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)--------------2) However same query works fine when we set "hive.optimize.skewjoin = false" . And we dont find this issue using Tez execution engine.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="14591" opendate="2016-8-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 is shut down unexpectedly during the startup time</summary>
      <description>If there is issue with Zookeeper (e.g. connection issues), then it takes HS2 some time to connect. During this time, Ambari could issue health checks against HS2 and the CloseSession call will trigger the shutdown of HS2, which is not expected. That triggering should happen only when the HS2 has been deregistered with Zookeeper, not during the startup time when HS2 is not registered with ZK yet.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14651" opendate="2016-8-26 00:00:00" fixdate="2016-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a local cluster for Tez and LLAP</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.llap.tez-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14663" opendate="2016-8-29 00:00:00" fixdate="2016-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change ptest java language version to 1.7, other version changes and fixes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14786" opendate="2016-9-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline displays binary column data as string instead of byte array</summary>
      <description>In Beeline, doing a SELECT binaryColName FROM tableName; results in displays data as string type (which looks corrupted due to unprintable chars). Instead Beeline should display binary columns as byte array.</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestTableOutputFormat.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestIncrementalRowsWithNormalization.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBufferedRows.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="14790" opendate="2016-9-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jenkins is not displaying test results because &amp;#39;set -e&amp;#39; is aborting the script too soon</summary>
      <description>NO PRECOMMIT TESTSJenkins is not displaying test results because 'set -e' is aborting the script too soon</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14793" opendate="2016-9-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ptest branch to be specified, PROFILE override</summary>
      <description>Post HIVE-14734 - the profile is automatically determined. Add an option to override this via Jenkins. Also add an option to specify the branch from which ptest is built (This is hardcoded to github.com/apache/hive)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14807" opendate="2016-9-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>analyze table compute statistics fails due to presence of Infinity value in double column</summary>
      <description>2016-09-21 20:27:45,074 ERROR &amp;#91;pool-3-thread-12&amp;#93;: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(173)) - Retrying HMSHandler after 2000 ms (attempt 5 of 10) with error: javax.jdo.JDODataStoreException: Cannot set Object parameter: value = Infinity for column "`TAB_COL_STATS`.`DOUBLE_HIGH_VALUE`" : 'Infinity' is not a valid numeric or approximate numeric value at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.writeMTableColumnStatistics(ObjectStore.java:5937) at org.apache.hadoop.hive.metastore.ObjectStore.updateTableColumnStatistics(ObjectStore.java:5994) at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114) at com.sun.proxy.$Proxy2.updateTableColumnStatistics(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.update_table_column_statistics(HiveMetaStore.java:4358) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) at com.sun.proxy.$Proxy4.update_table_column_statistics(Unknown Source) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$update_table_column_statistics.getResult(ThriftHiveMetastore.java:10701) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$update_table_column_statistics.getResult(ThriftHiveMetastore.java:10685) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)NestedThrowablesStackTrace:java.sql.SQLException: 'Infinity' is not a valid numeric or approximate numeric value at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1078) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:989) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:975) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:920) at com.mysql.jdbc.PreparedStatement.setDouble(PreparedStatement.java:3743) at com.jolbox.bonecp.PreparedStatementHandle.setDouble(PreparedStatementHandle.java:788) at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.setDouble(ParamLoggingPreparedStatement.java:726) at org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping.setObject(DoubleRDBMSMapping.java:264) at org.datanucleus.store.rdbms.mapping.java.SingleFieldMapping.setObject(SingleFieldMapping.java:211) at org.datanucleus.store.rdbms.fieldmanager.ParameterSetter.storeObjectField(ParameterSetter.java:197) at org.datanucleus.state.JDOStateManager.providedObjectField(JDOStateManager.java:1269) at org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.jdoProvideField(MTableColumnStatistics.java) at org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.jdoProvideFields(MTableColumnStatistics.java) at org.datanucleus.state.JDOStateManager.provideFields(JDOStateManager.java:1346) at org.datanucleus.store.rdbms.request.InsertRequest.execute(InsertRequest.java:289) at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertTable(RDBMSPersistenceHandler.java:167)</description>
      <version>1.2.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14818" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of retries while starting HiveServer for tests</summary>
      <description>Current is 30 retries, with a 1minute sleep between each one.The settings are likely bad for a production cluster as well. For tests, this should be a lot lower.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14852" opendate="2016-9-28 00:00:00" fixdate="2016-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change qtest logging to not redirect all logs to console</summary>
      <description>A change was made recently to redirect all logs to console, to make IDE debugging of regular tests easier. That unfortunately makes qtest debugging tougher - since there's a lot of noise along with the diffs in the output file.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14858" opendate="2016-9-29 00:00:00" fixdate="2016-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze command should support custom input formats</summary>
      <description>Currently analyze command with partialscan or noscan only applies to OrcInputFormat and MapredParquetInputFormat. However, if custom input formats extend these two they should also be able to use the same command to collect stats.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
    </fixedFiles>
  </bug>
  <bug id="14966" opendate="2016-10-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Make cookie-auth work in HTTP mode</summary>
      <description>HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.The repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.The HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.This due to the fact that the cookies are not sent out with matching flags for SSL usage.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="151" opendate="2008-12-10 00:00:00" fixdate="2008-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveQL Query execution bug: java.lang.NullPointerException</summary>
      <description>Executing a query ------------------------------------- query start ----------------------------------------------------SELECT t11.subject, t22.object , t33.subject , t55.object, t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66ON (t66.subject=t55.object)------------------------------------- query end ----------------------------------------------------on table ------------------------------------- table start ----------------------------------------------------CREATE TABLE triples (foo string,subject string, predicate string, object string, foo2 string)------------------------------------- table end -----------------------------------------------------gives the foolowing output ------------------------------------ console output ---------------------------------------------- INFO &amp;#91;main&amp;#93; (Driver.java:156) - Starting command: SELECT t11.subject, t22.object , t33.subject , t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33 ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44 ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object as obh FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55 ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66 ON (t66.subject=t55.obh) INFO &amp;#91;main&amp;#93; (ParseDriver.java:249) - Parsing command: SELECT t11.subject, t22.object , t33.subject , t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33 ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44 ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object as obh FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55 ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66 ON (t66.subject=t55.obh) INFO &amp;#91;main&amp;#93; (ParseDriver.java:263) - Parse Completed INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:126) - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore INFO &amp;#91;main&amp;#93; (ObjectStore.java:124) - ObjectStore, initialize called INFO &amp;#91;main&amp;#93; (ObjectStore.java:146) - found resource jpox.properties at file:/home/vseledkin/workspace/HiveDrv/bin/jpox.properties WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - Bundle "org.jpox" has an optional dependency to "org.eclipse.equinox.registry" but it cannot be resolved WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - Bundle "org.jpox" has an optional dependency to "org.eclipse.core.runtime" but it cannot be resolved INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - ================= Persistence Configuration =============== INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - JPOX Persistence Factory - Vendor: "JPOX" Version: "1.2.2" INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - JPOX Persistence Factory initialised for datastore URL="jdbc:derby:;databaseName=metastore_db;create=true" driver="org.apache.derby.jdbc.EmbeddedDriver" userName="APP" INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - =========================================================== INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Initialising Catalog "", Schema "APP" using "SchemaTable" auto-start option INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MDatabase since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - No manager for annotations was found in the CLASSPATH so all annotations are ignored. WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - MetaData Parser encountered an error in file "jar:file:/home/vseledkin/workspace/hive/build/hive_metastore.jar!/package.jdo" at line 282, column 13 : The content of element type "class" must match "(extension*,implements*,datastore-identity?,primary-key?,inheritance?,version?,join*,foreign-key*,index*,unique*,column*,field*,property*,query*,fetch-group*,extension*)". - Please check your specification of DTD and the validity of the MetaData XML that you have specified. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MStorageDescriptor since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MSerDeInfo since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MTable since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MPartition since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MDatabase &amp;#91;Table : DBS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MSerDeInfo &amp;#91;Table : SERDES, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStorageDescriptor &amp;#91;Table : SDS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTable &amp;#91;Table : TBLS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartition &amp;#91;Table : PARTITIONS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters &amp;#91;Table : SERDE_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.parameters &amp;#91;Table : PARTITION_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.values &amp;#91;Table : PARTITION_KEY_VALS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.parameters &amp;#91;Table : TABLE_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.partitionKeys &amp;#91;Table : PARTITION_KEYS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols &amp;#91;Table : BUCKETING_COLS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cols &amp;#91;Table : COLUMNS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters &amp;#91;Table : SD_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols &amp;#91;Table : SORT_COLS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 0 foreign key(s) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 index(es) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 foreign key(s) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 4 index(es) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 foreign key(s) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 4 index(es) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 0 foreign key(s) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Catalog "", Schema "APP" initialised - managing 14 classes INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - &gt;&gt; Found StoreManager org.jpox.store.rdbms.RDBMSManager INFO &amp;#91;main&amp;#93; (ObjectStore.java:110) - Initialized ObjectStore INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3086) - Starting Semantic Analysis INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3088) - Completed phase 1 of Semantic Analysis INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3091) - Completed getting MetaData in Semantic Analysis INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4, string reducesinkvalue5, string reducesinkvalue6, string reducesinkvalue7} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4, string reducesinkvalue5, string reducesinkvalue6, string reducesinkvalue7} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3107) - Completed partition pruning INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3111) - Completed sample pruning INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string temporarycol0, string temporarycol1, string temporarycol2, string temporarycol3, string temporarycol4} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3120) - Completed plan generation INFO &amp;#91;main&amp;#93; (Driver.java:173) - Semantic Analysis CompletedTotal MapReduce jobs = 3 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Total MapReduce jobs = 3Number of reducers = 1 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Number of reducers = 1In order to change numer of reducers use: INFO &amp;#91;main&amp;#93; (SessionState.java:254) - In order to change numer of reducers use: set mapred.reduce.tasks = &lt;number&gt; INFO &amp;#91;main&amp;#93; (SessionState.java:254) - set mapred.reduce.tasks = &lt;number&gt; WARN &amp;#91;main&amp;#93; (ExecDriver.java:109) - Number of reduce tasks not specified. Defaulting to jobconf value of: 1 INFO &amp;#91;main&amp;#93; (ExecDriver.java:238) - Adding input file /user/hive/warehouse/triples WARN &amp;#91;main&amp;#93; (JobClient.java:547) - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. INFO &amp;#91;main&amp;#93; (FileInputFormat.java:181) - Total input paths to process : 1Starting Job = job_200812091129_0144, Tracking URL = http://ubunder.avicomp.com:50030/jobdetails.jsp?jobid=job_200812091129_0144 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Starting Job = job_200812091129_0144, Tracking URL = http://ubunder.avicomp.com:50030/jobdetails.jsp?jobid=job_200812091129_0144Kill Command = /home/vseledkin/workspace/HiveDrv/programs/hadoop-0.19.0 job -Dmapred.job.tracker=ubunder.avicomp.com:9001 -kill job_200812091129_0144 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Kill Command = /home/vseledkin/workspace/HiveDrv/programs/hadoop-0.19.0 job -Dmapred.job.tracker=ubunder.avicomp.com:9001 -kill job_200812091129_0144 map = 0%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 0%, reduce =0% map = 50%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 50%, reduce =0% map = 100%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 100%, reduce =0% map = 100%, reduce =100% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 100%, reduce =100%ERROR &amp;#91;main&amp;#93; (SessionState.java:263) - Ended Job = job_200812091129_0144 with errorsEnded Job = job_200812091129_0144 with errorsFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.ExecDriverERROR &amp;#91;main&amp;#93; (SessionState.java:263) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.ExecDriver------------------------------------ console output end ----------------------------------------and the stack trace in hadoop logs ------------------------------------ stack trace ---------------------------------------------------java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:81) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:83) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:337) at org.apache.hadoop.mapred.Child.main(Child.java:155)------------------------------------ stack trace end ---------------------------------------------attached file contains table data to test problematic query</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15104" opendate="2016-11-1 00:00:00" fixdate="2016-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark generate more shuffle data than hive on mr</summary>
      <description>the same sql, running on spark and mr engine, will generate different size of shuffle data.i think it is because of hive on mr just serialize part of HiveKey, but hive on spark which using kryo will serialize full of Hivekey object. what is your opionion?</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15181" opendate="2016-11-10 00:00:00" fixdate="2016-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>buildQueryWithINClause didn&amp;#39;t properly handle multiples of ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_IN_CLAUSE</summary>
      <description>As there is a bug, we can still work around the issue by using the settings below (making sure the second setting is always at least 1000 times of the first setting):set hive.direct.sql.max.query.length=1;set hive.direct.sql.max.elements.in.clause=1000;</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15247" opendate="2016-11-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass the purge option for drop table to storage handlers</summary>
      <description>This gives storage handler more control on how to handle drop table.</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15390" opendate="2016-12-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc reader unnecessarily reading stripe footers with hive.optimize.index.filter set to true</summary>
      <description>In a split given to a task, the task's orc reader is unnecessarily reading stripe footers for stripes that are not its responsibility to read. This is happening with hive.optimize.index.filter set to true.Assuming one split per task(no tez grouping considered), a task should not need to read beyond the split's end offset. Even in some split computation strategies where a split's end offset can be in the middle of a stripe, it should not need to read more than one stripe beyond the split's end offset(to fully read a stripe that started in it). However I see that some tasks make unnecessary filesystem calls to read all the stripe footers in a file from the split start offset till the end of the file.</description>
      <version>1.2.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="15517" opendate="2016-12-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NOT (x &lt;=&gt; y) returns NULL if x or y is NULL</summary>
      <description>I created a table as following:create table test(x string, y string);insert into test values ('q', 'q'), ('q', 'w'), (NULL, 'q'), ('q', NULL), (NULL, NULL);Then I try to compare values taking NULLs into account:select *, x&lt;=&gt;y, not (x&lt;=&gt; y), (x &lt;=&gt; y) = false from test;OKq q true false falseq w false true trueq NULL false NULL trueNULL q false NULL trueNULL NULL true NULL falseI expected that 4th column will be the same as 5th one but actually got NULL as result of "not false" and "not true" expressions.Hive 1.2.1000.2.5.0.0-1245Subversion git://c66-slave-20176e25-3/grid/0/jenkins/workspace/HDP-parallel-centos6/SOURCES/hive -r da6c690d384d1666f5a5f450be5cbc54e2fe4bd6Compiled by jenkins on Fri Aug 26 01:39:52 UTC 2016From source with checksum c30648316a632f7a753f4359e5c8f4d6</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
    </fixedFiles>
  </bug>
  <bug id="15518" opendate="2016-12-27 00:00:00" fixdate="2016-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring rows and range related classes to put the window type on Window</summary>
      <description>/* * - A Window Frame that has only the /start/boundary, then it is interpreted as: BETWEEN &lt;start boundary&gt; AND CURRENT ROW * - A Window Specification with an Order Specification and no Window * Frame is interpreted as: ROW BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW * - A Window Specification with no Order and no Window Frame is interpreted as: ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING */The comments in WindowSpec above doesn't really match what it's claimed to do. Correct the comment to reduce the confusion.Also currently the window type is specified on each BoundarySpec but makes sense to put the type (rows or range) for each window.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15522" opendate="2016-12-28 00:00:00" fixdate="2016-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental ALTER_TABLE/ALTER_PTN including renames</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15626" opendate="2017-1-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline should not exit after canceling the query on ctrl-c</summary>
      <description>I am seeing this in 1.2</description>
      <version>1.2.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SunSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="15627" opendate="2017-1-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make hive.vectorized.adaptor.usage.mode=all vectorize all UDFs not just those in supportedGenericUDFs</summary>
      <description>Missed this when doing HIVE-14336.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.number.compare.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.udf1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15698" opendate="2017-1-23 00:00:00" fixdate="2017-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization support for min/max/bloomfilter runtime filtering</summary>
      <description>Enable vectorized execution for HIVE-15269.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomFilter.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.3.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterDecimalColumnBetween.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="15704" opendate="2017-1-24 00:00:00" fixdate="2017-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the default logger for llap to query-routing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15705" opendate="2017-1-24 00:00:00" fixdate="2017-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Event replication for constraints</summary>
      <description>Make event replication for primary key and foreign key work.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageDeserializer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.EventMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15765" opendate="2017-1-31 00:00:00" fixdate="2017-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bracketed comments</summary>
      <description>C-style comments are in the SQL spec as well as supported by all major DBs. The are useful for inline annotation of the SQL. We should have them too.Example:select/*+ MAPJOIN(a) */ /* mapjoin hint */a /* column */from foo join bar;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SelectClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15815" opendate="2017-2-4 00:00:00" fixdate="2017-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to pass some Oozie properties to Spark in HoS</summary>
      <description>Oozie passes some of its properties (e.g. oozie.job.id) to Beeline/HS2 when it invokes Hive2 action. If we allow these properties to be passed to Spark in HoS, we can easily associate an Ooize workflow ID to an HoS client and Spark job in Spark history. It will be very helpful in diagnosing some issues involving Oozie Hive2/HoS/Spark.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15817" opendate="2017-2-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix LlapDump classpath in llapdum.sh</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.llapdump.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15834" opendate="2017-2-7 00:00:00" fixdate="2017-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit tests for org.json usage on master</summary>
      <description>Before switching implementation, we should add some tests that capture the current behavior.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
    </fixedFiles>
  </bug>
  <bug id="15874" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid position alias in Group By when CBO failed</summary>
      <description>for example：create table alias_test_01(a INT, b STRING) ;create table alias_test_02(a INT, b STRING) ;create table alias_test_03(a INT, b STRING) ;set hive.groupby.position.alias = true;set hive.cbo.enable=true;explain select * from alias_test_01 alias01 left join (select 2017 as a, b from alias_test_02 group by 1, 2) alias02 on alias01.a = alias02.a left join alias_test_03 alias03on alias01.a = alias03.a;error info:FAILED: SemanticException &amp;#91;Error 10220&amp;#93;: Invalid position alias in Group ByPosition alias: 2017 does not existThe Select List is indexed from 1 to 2the first process Position Alias result:when CBO optimize failed and reAnalyzeAST is true, position alias will be processed twice.1. 'group by 1, 2' convert to 'group by 2017, b'2. 'group by 2017, b' 2017 column does not exist</description>
      <version>1.2.1,2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15889" opendate="2017-2-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Some tasks still run after hive cli is shutdown</summary>
      <description>E.g: In cross product case, the tight loop in merge join operator ignores any interrupt or abort flag checks, causing the tasks to be in running state even when the client cli is quit.intensionally written cross product query to simulate this.hive&gt; select count(1) from lineitem, orders;;Even when the cli is quit, LLAP would continue executing the task for quite sometime. E.g stack trace"TezTaskRunner" #1945 daemon prio=5 os_prio=0 tid=0x00007fe9e43a5000 nid=0x4c8 runnable [0x00007fc8d881b000] java.lang.Thread.State: RUNNABLE at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:453) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.mergeJoinComputeKeys(CommonMergeJoinOperator.java:603) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:207) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:351) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:282) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchOneRow(CommonMergeJoinOperator.java:410) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchNextGroup(CommonMergeJoinOperator.java:381) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.doFirstFetchIfNeeded(CommonMergeJoinOperator.java:491) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:209) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:351) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:282) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:319) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422)</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15995" opendate="2017-2-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Syncing metastore table with serde schema</summary>
      <description>Hive enables table schema evolution via properties. For avro e.g. we can alter the 'avro.schema.url' property to update table schema to the next version. Updating properties however doesn't affect column list stored in metastore DB so the table is not in the newest version when returned from metastore API. This is problem for tools working with metastore (e.g. Presto).To solve this issue I suggest to introduce new DDL statement syncing metastore columns with those from serde:ALTER TABLE user_test1 UPDATE COLUMNSNote that this is format independent solution. To reproduce, follow the instructions below: Create table based on avro schema version 1 (cxv1.avsc)CREATE EXTERNAL TABLE user_test1 PARTITIONED BY (dt string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' LOCATION '/tmp/schema-evolution/user_test1' TBLPROPERTIES ('avro.schema.url'='/tmp/schema-evolution/cx1.avsc'); Update schema to version 2 (cx2.avsc)ALTER TABLE user_test1 SET TBLPROPERTIES ('avro.schema.url' = '/tmp/schema-evolution/cx2.avsc'); Print serde columns (top info) and metastore columns (Detailed Table Information):DESCRIBE EXTENDED user_test1</description>
      <version>1.2.1,2.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16225" opendate="2017-3-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in webhcat service (FileSystem CACHE entries)</summary>
      <description>This is a known beast. here are detailsThe problem seems to be similar to the one discussed in HIVE-13749. If we submit very large number of jobs like 1000 to 2000 then we can see increase in Configuration objects count.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16315" opendate="2017-3-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table doesn&amp;#39;t show num of partitions</summary>
      <description>This doesn't comply with our wiki: https://cwiki.apache.org/confluence/display/Hive/StatsDev#StatsDev-Examples</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16519" opendate="2017-4-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception thrown by checkOutputSpecs</summary>
      <description>do not throw exception by checkOutputSpecs</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16520" opendate="2017-4-24 00:00:00" fixdate="2017-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache hive metadata in metastore</summary>
      <description>During Hive 2 benchmark, we find Hive metastore operation take a lot of time and thus slow down Hive compilation. In some extreme case, it takes much longer than the actual query run time. Especially, we find the latency of cloud db is very high and 90% of total query runtime is waiting for metastore SQL database operations. Based on this observation, the metastore operation performance will be greatly enhanced if we have a memory structure which cache the database query result.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16576" opendate="2017-5-3 00:00:00" fixdate="2017-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix encoding of intervals when fetching select query candidates from druid</summary>
      <description>Debug logs on HIVE side - 2017-05-03T23:49:00,672 DEBUG [HttpClient-Netty-Worker-0] client.NettyHttpClient: [GET http://localhost:8082/druid/v2/datasources/cmv_basetable_druid/candidates?intervals=1900-01-01T00:00:00.000+05:53:20/3000-01-01T00:00:00.000+05:30] Got response: 500 Server ErrorDruid exception stack trace - 2017-05-03T18:56:58,928 WARN [qtp1651318806-158] org.eclipse.jetty.servlet.ServletHandler - /druid/v2/datasources/cmv_basetable_druid/candidatesjava.lang.IllegalArgumentException: Invalid format: ""1900-01-01T00:00:00.000 05:53:20" at org.joda.time.format.DateTimeFormatter.parseDateTime(DateTimeFormatter.java:899) ~[joda-time-2.8.2.jar:2.8.2] at org.joda.time.convert.StringConverter.setInto(StringConverter.java:212) ~[joda-time-2.8.2.jar:2.8.2] at org.joda.time.base.BaseInterval.&lt;init&gt;(BaseInterval.java:200) ~[joda-time-2.8.2.jar:2.8.2] at org.joda.time.Interval.&lt;init&gt;(Interval.java:193) ~[joda-time-2.8.2.jar:2.8.2] at org.joda.time.Interval.parse(Interval.java:69) ~[joda-time-2.8.2.jar:2.8.2] at io.druid.server.ClientInfoResource.getQueryTargets(ClientInfoResource.java:320) ~[classes/:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_92]Note that intervals being sent as part of the HTTP request URL are not encoded properly when not using UTC timezone.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="16588" opendate="2017-5-4 00:00:00" fixdate="2017-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resource leak by druid http client</summary>
      <description>Current implementation of druid storage handler does leak some resources if the creation of the http client fails due to too many files exception.The reason this is leaking is the fact the cleaning hook is registered after the client starts.In order to fix this will extract the creation of the HTTP client to become static and reusable instead of per query creation.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16613" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SaslClientHandler.sendHello is eating exceptions</summary>
      <description>If we hit exceptions when sending hello to server, the method fails silently. Finally the remote driver fails with SASL timeout instead of the real cause, making it difficult for debugging.To reproduce: throw some exception in KryoMessageCodec.encode.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.SaslHandler.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.Rpc.java</file>
    </fixedFiles>
  </bug>
  <bug id="16617" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from errors in module hive-shims</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
    </fixedFiles>
  </bug>
  <bug id="16618" opendate="2017-5-9 00:00:00" fixdate="2017-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from errors in module hive-common</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.JMXJsonServlet.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidCompactorTxnList.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsVariable.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CompressionUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.CommonCliOptions.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.classification.RetrySemantics.java</file>
      <file type="M">common.src.java.org.apache.hive.http.Log4j2ConfiguratorServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="16619" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from errors in module hive-serde</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyDate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUnion.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.fast.DeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.ParseException.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeStructBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSchemaRetriever.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.AbstractSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="17107" opendate="2017-7-17 00:00:00" fixdate="2017-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Yetus to 0.5.0</summary>
      <description>Yetus 0.5.0 is released, and it contains our fixes.We should upgrade and remove our extra patched files.CC: kgyrtkirk</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.yetus-wrapper.sh</file>
      <file type="M">dev-support.maven.YETUS-506.sh</file>
      <file type="M">dev-support.findbugs.YETUS-471.sh</file>
      <file type="M">dev-support.checkstyle.YETUS-484.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17116" opendate="2017-7-18 00:00:00" fixdate="2017-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Add infrastructure for vectorization of ROW__ID struct</summary>
      <description>Supports new ACID work.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.row..id.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.row..id.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17415" opendate="2017-8-30 00:00:00" fixdate="2017-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hit error "SemanticException View xxx is corresponding to LIMIT, rather than a SelectOperator." in Hive queries</summary>
      <description>Hit error "SemanticException View xxx is corresponding to LIMIT, rather than a SelectOperator." in Hive queries when a user creates a view with limitsset hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;create table my_passwd (username string,uid int);insert into my_passwd values("Dev1", 1),("Dev2", 2),("Dev3", 3),("Dev4", 4),("Dev5", 5),("Dev6", 6);create view my_passwd_vw as select * from my_passwd limit 3;set hive.security.authorization.enabled=true;grant select on table my_passwd to user hive_test_user;grant select on table my_passwd_vw to user hive_test_user;select * from my_passwd_vw;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17845" opendate="2017-10-19 00:00:00" fixdate="2017-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert fails if target table columns are not lowercase</summary>
      <description>eg., INSERT INTO TABLE EMP(ID,NAME) select * FROM SRC;FAILED: SemanticException 1:27 '&amp;#91;ID,NAME&amp;#93;' in insert schema specification are not found among regular columns of default.EMP nor dynamic partition columns.. Error encountered near token 'NAME'Whereas below insert is successful:INSERT INTO TABLE EMP(id,name) select * FROM SRC;</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17847" opendate="2017-10-19 00:00:00" fixdate="2017-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude net.hydromatic:aggdesigner-algorithm jar as compile and runtime dependency</summary>
      <description>Hive doesn't use this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1787" opendate="2010-11-12 00:00:00" fixdate="2010-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize the code path when there are no outer joins</summary>
      <description>Currently, outer joins and joins are handled in the same manner - a special case for no outer joins would be useful</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17978" opendate="2017-11-3 00:00:00" fixdate="2017-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared work optimizer may leave useless operator branches in the plan</summary>
      <description>Failed with the following Exception ERROR [6c707c4e-2849-4ff2-809d-946581e6b83a HiveServer2-Handler-Pool: Thread-78] ql.Driver: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: The column KEY._col0 is not in the vectorization context column map {_col0=0}.org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.HiveException: The column KEY._col0 is not in the vectorization context column map {_col0=0}. at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationNodeProcessor.doVectorize(Vectorizer.java:1665) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkVectorizationNodeProcessor.process(Vectorizer.java:1725) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:43) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.vectorizeMapWork(Vectorizer.java:1245) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:671) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:616) at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111) at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180) at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:1902) at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:674) at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:271) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11621) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:296) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:268) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:169) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:268) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:599) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1463) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1420) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:201) at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:288) at org.apache.hive.service.cli.operation.Operation.run(Operation.java:249) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:532) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:518) at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:311) at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:533) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1497) at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1482) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: The column KEY._col0 is not in the vectorization context column map {_col0=0}. at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getInputColumnIndex(VectorizationContext.java:445) at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getColumnVectorExpression(VectorizationContext.java:543) at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:613) at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(VectorizationContext.java:593) at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(VectorizationContext.java:581) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.vectorizeGroupByOperator(Vectorizer.java:3763) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.vectorizeOperator(Vectorizer.java:4275) at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationNodeProcessor.doVectorize(Vectorizer.java:1657) ... 45 more</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18207" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the test failure for TestCliDriver#vector_complex_join</summary>
      <description>The test result miss the info about bigTableKeyExpressions &amp; bigTableValueExpressions</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.join.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18208" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB Join : Fix the unit tests to run SMB Joins.</summary>
      <description>Most of the SMB Join tests are actually not creating SMB Joins. Need them to test the intended join.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.quotedid.smb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.cache.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="18210" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create resource plan allows duplicates</summary>
      <description>Create resource plan allows duplicates. This was seen in a cluster:+----------+-----------+--------------------+| rp_name | status | query_parallelism |+----------+-----------+--------------------+| plan_2 | ACTIVE | 10 || plan_2 | DISABLED | NULL |+----------+-----------+--------------------+</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18212" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure Yetus check always has a full log</summary>
      <description>Some yetus log files are left incomplete, and in these same runs tee subprocesses are left running and dangling on the ptest server.This is because of a bug in the yetus runner velocity template script where we make a redirection of stdout:./dev-support/test-patch.sh ${patchFile} ..... 2&gt;&amp;1 | tee ${logFile}If the yetus output is big enough (&gt;62K) tee will stop writing the log file and is left running even after test-patch.sh finished successfully. This because we don't make anything consume the stdout and most probably some buffers get full on Linux side.We should also make sure that yetus runs(since they are executed parallel to ptest test phase) are not interfering with each other in case they run very long and overlap.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.YetusPhase.java</file>
    </fixedFiles>
  </bug>
  <bug id="18254" opendate="2017-12-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use proper AVG Calcite primitive instead of Other_FUNCTION</summary>
      <description>Currently Hive-Calcite operator tree treats AVG function as an unknown function that has a Calcite Sql Kind of Other_FUNCTION. This is an issue that can get into the way of rules like org.apache.calcite.rel.rules.AggregateReduceFunctionsRule.This patch adds the avg function to the list of known aggregate function.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="18255" opendate="2017-12-8 00:00:00" fixdate="2017-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark-client jar should be prefixed with hive-</summary>
      <description>Other Hive jars are prefixed with "hive-" except for the spark-client jar. Fixing this to make sure the jar name is consistent across all Hive jars.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18257" opendate="2017-12-9 00:00:00" fixdate="2017-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement scheduling policy configuration instead of hardcoding fair scheduling</summary>
      <description>Not sure it makes sense to actually make it pluggable. At least the standard ones will be an enum; we don't expect people to implement custom classes - phase 2 if someone wants to</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="18258" opendate="2017-12-9 00:00:00" fixdate="2017-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken</summary>
      <description>See Q file. Duplicate columns in key are not handled correctly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18273" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LLAP-level counters for WM</summary>
      <description>On query fragment level (like IO counters)time queued as guaranteed;time running as guaranteed;time running as speculative.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18274" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add AM level metrics for WM</summary>
      <description>Unused guaranteed tasks (1 metric); guaranteed/speculative tasks x updated/update in progress (4 metrics).It should be possible to view those over time as the query is (was) running, to detect any anomalies. This jira is just to save the correct metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18275" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HS2-level WM metrics</summary>
      <description>E.g. time spent in pool queue. Some existing UIs use perflogger output, so we should also include that.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.QueryExecutionBreakdownSummary.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="18347" opendate="2017-12-29 00:00:00" fixdate="2017-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow pluggable dynamic lookup of Hive Metastores from HiveServer2</summary>
      <description>In our organization, we have deployed HiveMetastore and HiveServer2 on Mesos as dynamic services for scalability and flexibility.In this architecture, we would like to allow HiveServer2 to dynamically load balance between Metastores (which may be scaled up and down or to different nodes) for different requests.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="184" opendate="2008-12-17 00:00:00" fixdate="2008-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests fail due to including old hive jar files</summary>
      <description>Running "ant test" fails due to the old hive jar files from hadoopcore (which is inside the build directory by default) being included first in the classpath.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18400" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part2</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files srcbucket0 etc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.scripts.q.test.init.sql</file>
    </fixedFiles>
  </bug>
  <bug id="18406" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part8</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files empty1 etc</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join32.q</file>
    </fixedFiles>
  </bug>
  <bug id="18408" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part10</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files in1 etc and sortdp.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.bucketing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.join.nulls.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.filters.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.opt.bucketing.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join.filters.q</file>
    </fixedFiles>
  </bug>
  <bug id="18409" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part11</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files which are of binary format. (dat, rc etc)This patch will fail the test run as the binary input files cant be patched, similar to HIVE-18403</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.exim.11.nonpart.noncompat.sorting.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.exim.11.nonpart.noncompat.sorting.q</file>
    </fixedFiles>
  </bug>
  <bug id="18414" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18926" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Imporve operator-tree matching</summary>
      <description>currently joins are not matched</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.stat.changes.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestReOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestOperatorCmp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestCounterMapping.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReOptimizePlugin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.SimpleRuntimeStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.RuntimeStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.PlanMapperProcess.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.PlanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.GroupTransformer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.EmptyStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinCondDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CommonMergeJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.Signature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.OpSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.IDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18933" opendate="2018-3-13 00:00:00" fixdate="2018-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable ORC codec pool for now; remove clone</summary>
      <description>See ORC-310.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19259" opendate="2018-4-20 00:00:00" fixdate="2018-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create view on tables having union all fail with "Table not found"</summary>
      <description>create view on table with union work well while "union all" failed with table not found, here are the reproduce steps._hive&gt; create table foo(id int);__OK__Time taken: 0.401 seconds__hive&gt; create table bar(id int);__OK_ _// view on table union__hive&gt; create view unionview as with tmp_1 as ( select * from foo ), tmp_2 as (select * from bar ) select * from tmp_1 union  select * from tmp_2;_ _OK__Time taken: 0.517 seconds__hive&gt; select * from unionview;__OK__Time taken: 5.805 seconds_  _// view on union all_ _hive&gt; create view unionallview as with tmp_1 as ( select * from foo ), tmp_2 as (select * from bar ) select * from tmp_1 union all  select * from tmp_2;_ _OK__Time taken: 1.535 seconds__hive&gt; select * from unionallview;__FAILED: SemanticException Line 1:134 Table not found 'tmp_1' in definition of VIEW unionallview [__with tmp_1 as ( select `foo`.`id` from `default`.`foo` ), tmp_2 as (select `bar`.`id` from `default`.`bar` ) select `tmp_1`.`id` from tmp_1 union all  select `tmp_2`.`id` from tmp_2__] used as unionallview at Line 1:14___</description>
      <version>1.2.1</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.union.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19831" opendate="2018-6-8 00:00:00" fixdate="2018-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiveserver2 should skip doAuth checks for CREATE DATABASE/TABLE if database/table already exists</summary>
      <description>with sqlstdauth on, Create database if exists take TOO LONG if there are too many objects inside the database directory. Hive should not run the doAuth checks for all the objects within database if the database already exists.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19997" opendate="2018-6-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batches for TestMiniDruidCliDriver</summary>
      <description>I have observed TestMiniDruidCliDriver takes a long time to execute. I verified that execution is not batched. We could batch tests as we do with TestHBaseCliDriver, i.e., 5 q files per batch, as there is only a small number of tests.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19999" opendate="2018-6-26 00:00:00" fixdate="2018-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move precommit jobs to jdk 8</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.PTestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="2000" opendate="2011-2-22 00:00:00" fixdate="2011-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>adding comments to Hive Stats JDBC queries</summary>
      <description>Stats gathering could issue a large number of JDBC queries to stats publisher and aggregator. In a shared DB environment, it's hard to tell which SQL statements are contributed by Hive stats gathering tasks. It would be easier to identify these workload by adding SQL comments that identifies the source of the query.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="202" opendate="2008-12-31 00:00:00" fixdate="2008-12-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LINEAGE is not working for join quries</summary>
      <description>lineage is not giving input tables in case of join quires.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.tool.TestLineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.tools.LineageInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2020" opendate="2011-3-2 00:00:00" fixdate="2011-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a separate namespace for Hive variables</summary>
      <description>Support for variable substitution was added in HIVE-1096. However, variable substitution was implemented by reusing the HiveConf namespace, so there is no separation between Hive configuration properties and Hive variables.This ticket encompasses the following enhancements: Create a separate namespace for managing Hive variables. Add support for setting variables on the command line via '-hivevar x=y' Add support for setting variables through the CLI via 'set hivevar:x=y' Add support for referencing variables in statements using either '${hivevar:var_name}' or '${var_name}' Provide a means for differentiating between hiveconf, hivevar, system, and environment properties in the output of 'set -v'</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.VariableSubstitution.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20418" opendate="2018-8-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO may not handle ORC files that have row index disabled correctly for queries with no columns selected</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20527" opendate="2018-9-10 00:00:00" fixdate="2018-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intern table descriptors from spark task</summary>
      <description>Table descriptors from MR tasks and Tez tasks are interned. This fix is to intern table desc from spark tasks as well.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20659" opendate="2018-9-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20914" opendate="2018-11-14 00:00:00" fixdate="2018-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MRScratchDir permission denied when "hive.server2.enable.doAs", "hive.exec.submitviachild" are set to "true" and impersonated/proxy user is used</summary>
      <description>The above issue could be reproduced in none Kerberos cluster using the below steps:1. Set "hive.exec.submitviachild" value to "true".2. Run a count query not using "hive" user.beeline -u 'jdbc:hive2://localhost:10000' -n hdfsThere is no issue when we try to execute the same query using the "hive" user.Exception in thread "main" java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=hive, access=EXECUTE, inode="/tmp/hive/hdfs":hdfs:supergroup:drwx------ at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:279) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:260) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkTraverse(DefaultAuthorizationProvider.java:201) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:154) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:3877) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:3860) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:3847) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(FSNamesystem.java:6822) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4551) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4529) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4502) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:884) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:328) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:641) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2281) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2277) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2275) at org.apache.hadoop.hive.ql.Context.getScratchDir(Context.java:285) at org.apache.hadoop.hive.ql.Context.getMRScratchDir(Context.java:328) at org.apache.hadoop.hive.ql.Context.getMRTmpPath(Context.java:444) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:243) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:771) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.mr.TestMapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20924" opendate="2018-11-15 00:00:00" fixdate="2018-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Property &amp;#39;hive.driver.parallel.compilation.global.limit&amp;#39; should be immutable at runtime</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestRestrictedList.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21018" opendate="2018-12-7 00:00:00" fixdate="2018-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Grouping/distinct on more than 64 columns should be possible</summary>
      <description>Earlier when more than 64 columns were in a group by or distinct an error was given</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21182" opendate="2019-1-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip setting up hive scratch dir during planning</summary>
      <description>During metadata gathering phase hive creates staging/scratch dir which is further used by FS op (FS op sets up staging dir within this dir for tasks to write to).Since FS op do mkdirs to setup staging dir we can skip creating scratch dir during metadata gathering phase. FS op will take care of setting up all the dirs.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="21184" opendate="2019-1-30 00:00:00" fixdate="2019-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explain and explain formatted CBO plan with cost information</summary>
      <description>Plan is more readable than full DAG. Explain formatted/extended will print the plan.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainConfiguration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QOutProcessor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2161" opendate="2011-5-12 00:00:00" fixdate="2011-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remaining patch for HIVE-2148</summary>
      <description>Follow-up jira for HIVE-2148.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="21612" opendate="2019-4-12 00:00:00" fixdate="2019-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade druid to 0.14.0-incubating</summary>
      <description>Druid 0.14.0-incubating is released. This task is to upgrade hive to use 0.14.0-incubating version of druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="21613" opendate="2019-4-13 00:00:00" fixdate="2019-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries with join condition having timestamp or timestamp with local time zone literal throw SemanticException</summary>
      <description>Similar to HIVE-21540.</description>
      <version>None</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21956" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the list of table selected by dump in the dump folder.</summary>
      <description>The list of tables selected by a dump should be kept in the dump folder as a _tables file. This will help user to find out the tables replicated and the list can be used by user for ranger and atlas policy replication.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="22416" opendate="2019-10-28 00:00:00" fixdate="2019-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR-related operation logs missing when parallel execution is enabled</summary>
      <description>Repro steps: 1. Happy path, parallel execution disabled0: jdbc:hive2://localhost:10000&gt; set hive.exec.parallel=false;No rows affected (0.023 seconds)0: jdbc:hive2://localhost:10000&gt; select count (*) from t1;INFO : Compiling command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d): select count (*) from t1INFO : Semantic Analysis CompletedINFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)INFO : Completed compiling command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d); Time taken: 0.309 secondsINFO : Executing command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d): select count (*) from t1WARN : INFO : Query ID = karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7dINFO : Total jobs = 1INFO : Launching Job 1 out of 1INFO : Starting task [Stage-1:MAPRED] in serial modeINFO : Number of reduce tasks determined at compile time: 1INFO : In order to change the average load for a reducer (in bytes):INFO : set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;INFO : In order to limit the maximum number of reducers:INFO : set hive.exec.reducers.max=&lt;number&gt;INFO : In order to set a constant number of reducers:INFO : set mapreduce.job.reduces=&lt;number&gt;DEBUG : Configuring job job_local495362389_0008 with file:/tmp/hadoop/mapred/staging/karencoppage495362389/.staging/job_local495362389_0008 as the submit dirDEBUG : adding the following namenodes' delegation tokens:[file:///]DEBUG : Creating splits at file:/tmp/hadoop/mapred/staging/karencoppage495362389/.staging/job_local495362389_0008INFO : number of splits:0INFO : Submitting tokens for job: job_local495362389_0008INFO : Executing with tokens: []INFO : The url to track the job: http://localhost:8080/INFO : Job running in-process (local Hadoop)INFO : 2019-10-28 15:26:22,537 Stage-1 map = 0%, reduce = 100%INFO : Ended Job = job_local495362389_0008INFO : MapReduce Jobs Launched: INFO : Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 SUCCESSINFO : Total MapReduce CPU Time Spent: 0 msecINFO : Completed executing command(queryId=karencoppage_20191028152610_a26c25e1-9834-446a-9a56-c676cb693e7d); Time taken: 6.497 secondsINFO : OKDEBUG : Shutting down query select count (*) from t1+-----+| c0 |+-----+| 0 |+-----+1 row selected (11.874 seconds)2. Faulty path, parallel execution enabled0: jdbc:hive2://localhost:10000&gt; set hive.server2.logging.operation.level=EXECUTION;No rows affected (0.236 seconds)0: jdbc:hive2://localhost:10000&gt; set hive.exec.parallel=true;No rows affected (0.01 seconds)0: jdbc:hive2://localhost:10000&gt; select count  (*) from t1;INFO  : Compiling command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77): select count  (*) from t1INFO  : Semantic Analysis CompletedINFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)INFO  : Completed compiling command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77); Time taken: 4.707 secondsINFO  : Executing command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77): select count  (*) from t1WARN  : INFO  : Query ID = karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77INFO  : Total jobs = 1INFO  : Launching Job 1 out of 1INFO  : Starting task [Stage-1:MAPRED] in parallelINFO  : MapReduce Jobs Launched: INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESSINFO  : Total MapReduce CPU Time Spent: 0 msecINFO  : Completed executing command(queryId=karencoppage_20191028155346_4e7b793b-654e-4d69-b588-f3f0d3ae0c77); Time taken: 44.577 secondsINFO  : OKDEBUG : Shutting down query select count  (*) from t1+-----+| c0  |+-----+| 0   |+-----+1 row selected (54.665 seconds)The issue is that Log4J stores the session ID and query ID in some atomic thread metadata (org.apache.logging.log4j.ThreadContext.getImmutableContext()). If the queryId is missing from this metadata, then the RoutingAppender (which is defined programmatically in LogDivertAppender) will route the log to a NullAppender, which logs nothing. If the queryId is present, then the RoutingAppender routes the event to the "query-appender", which will log the line in the operation log/Beeline. This is not happening in a multi-threaded context since new threads created for parallel query execution do not have the queryId/sessionId metadata.The solution is to add the queryId/sessionId metadata to any new threads created for MR work.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22683" opendate="2019-12-30 00:00:00" fixdate="2019-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run Eclipse Cleanup Against beeline Module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestShutdownHook.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestClientCommandHookFactory.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBufferedRows.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineOpts.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineExceptionHandling.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.ProxyAuthTest.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.VerticalOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableNameCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ReflectiveCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.IncrementalRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.HS2ConnectionFileUtils.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ColorBuffer.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="22684" opendate="2019-12-30 00:00:00" fixdate="2019-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run Eclipse Cleanup Against hbase-handler Module</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.avro.OfficePhone.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.avro.HomePhone.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.avro.Employee.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.avro.ContactInfo.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.avro.Address.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseScanRange.java</file>
    </fixedFiles>
  </bug>
  <bug id="22772" opendate="2020-1-24 00:00:00" fixdate="2020-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log opType and session level information for each operation</summary>
      <description>if would be useful to have this info available for issue troubleshooting.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
    </fixedFiles>
  </bug>
  <bug id="22774" opendate="2020-1-24 00:00:00" fixdate="2020-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Usability improvements of scheduled queries</summary>
      <description>add feature toggle switch to HMS side rpc call sites - make it possible to disable it rename query state ERRORED state to FAILED ProgressReporter thread should not enter a busy wait loop in case the thread is interrupted</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestMetastoreScheduledQueries.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ScheduledQueryExecutionsMaintTask.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.QueryState.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.schq.TestScheduledQueryService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ScheduledQueryAnalyzer.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3828" opendate="2012-12-21 00:00:00" fixdate="2012-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert overwrite fails with stored-as-dir in cluster</summary>
      <description>The following query works fine in hive TestCliDriver test suite but not minimr because different Hadoop file system is used.The error isCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: .../_task_tmp.-ext-10002/key=103/_tmp.000000_0 to: .../_tmp.-ext-10002/key=103/000000_0</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6091" opendate="2013-12-20 00:00:00" fixdate="2013-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty pipeout files are created for connection create/close</summary>
      <description>Pipeout files are created when a connection is established and removed only when data was produced. Instead we should create them only when data has to be fetched or remove them whether data is fetched or not.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="6535" opendate="2014-3-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: provide an async API to execute query and fetch results</summary>
      <description>The hive jdbc client waits query completion during execute() call. It would be better to block in the jdbc for completion when the results are being fetched.This way the application using hive jdbc driver can do other tasks while asynchronous query execution is happening, until it needs to fetch the result set.</description>
      <version>0.14.0,1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6991" opendate="2014-4-30 00:00:00" fixdate="2014-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>History not able to disable/enable after session started</summary>
      <description>By default history is disabled, after session started if enable history through this command set hive.session.history.enabled=true. It is not working.I think it will help to this user queryhttp://mail-archives.apache.org/mod_mbox/hive-user/201404.mbox/%3CCAJqy7aFAPa_pjS6bUon0o8zYT2qwfN2WT-mtZnwfmuRav_8ZjA@mail.gmail.com%3E</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9601" opendate="2015-2-6 00:00:00" fixdate="2015-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Beeline queries will hang If Beeline terminates in-properly [Spark Branch]</summary>
      <description>User session's Spark application seems to stay around if beeline is not quit properly (!quit) because the user is not disconnected.If Beeline is started, it will create a new Spark application which will hang waiting for the first one.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="9736" opendate="2015-2-20 00:00:00" fixdate="2015-1-20 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>StorageBasedAuthProvider should batch namenode-calls where possible.</summary>
      <description>Consider a table partitioned by 2 keys (dt, region). Say a dt partition could have 10000 associated regions. Consider that the user does:ALTER TABLE my_table DROP PARTITION (dt='20150101');As things stand now, StorageBasedAuthProvider will make individual DistributedFileSystem.listStatus() calls for each partition-directory, and authorize each one separately. It'd be faster to batch the calls, and examine multiple FileStatus objects at once.</description>
      <version>1.2.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.DefaultFileAccess.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
