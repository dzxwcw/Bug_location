<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10364" opendate="2015-4-16 00:00:00" fixdate="2015-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The HMS upgrade script test does not publish results when prepare.sh fails.</summary>
      <description>The HMS upgrade script must publish succeed or failure results to JIRA. This bug is not publishing any results on JIRA is the prepare.sh script fails.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15190" opendate="2016-11-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Field names are not preserved in ORC files written with ACID</summary>
      <description>To repro:drop table if exists orc_nonacid;drop table if exists orc_acid;create table orc_nonacid (a int) clustered by (a) into 2 buckets stored as orc;create table orc_acid (a int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');insert into table orc_nonacid values(1), (2);insert into table orc_acid values(1), (2);Running hive --service orcfiledump &lt;file&gt; on the files created by the insert statements above, you'll see that for orc_nonacid, the files have schema struct&lt;a:int&gt; whereas for orc_acid, the files have schema struct&lt;operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct&lt;_col0:int&gt;&gt;. The last field row should have schema struct&lt;a:int&gt;.</description>
      <version>2.1.0,2.2.0,3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15406" opendate="2016-12-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consider vectorizing the new &amp;#39;trunc&amp;#39; function</summary>
      <description>Rounding function 'trunc' added by HIVE-14582.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrunc.java</file>
    </fixedFiles>
  </bug>
  <bug id="17300" opendate="2017-8-11 00:00:00" fixdate="2017-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI query plan graphs</summary>
      <description>Hi all,I’m working on a feature of the Hive WebUI Query Plan tab that would provide the option to display the query plan as a nice graph (scroll down for screenshots). If you click on one of the graph’s stages, the plan for that stage appears as text below. Stages are color-coded if they have a status (Success, Error, Running), and the rest are grayed out. Coloring is based on status already available in the WebUI, under the Stages tab.There is an additional option to display stats for MapReduce tasks. This includes the job’s ID, tracking URL (where the logs are found), and mapper and reducer numbers/progress, among other info. The library I’m using for the graph is called vis.js (http://visjs.org/). It has an Apache license, and the only necessary file to be included from this library is about 700 KB.I tried to keep server-side changes minimal, and graph generation is taken care of by the client. Plans with more than a given number of stages (default: 25) won't be displayed in order to preserve resources.I’d love to hear any and all input from the community about this feature: do you think it’s useful, and is there anything important I’m missing?Thanks,Karen CoppageReview request: https://reviews.apache.org/r/61663/Any input is welcome!</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.jamon.org.apache.hive.tmpl.QueryProfileTmpl.jamon</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.MapRedStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17303" opendate="2017-8-11 00:00:00" fixdate="2017-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missmatch between roaring bitmap library used by druid and the one coming from tez</summary>
      <description>Caused by: java.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: org.roaringbitmap.buffer.MutableRoaringBitmap.runOptimize()Z  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:165)  ... 25 moreCaused by: java.lang.NoSuchMethodError: org.roaringbitmap.buffer.MutableRoaringBitmap.runOptimize()Z  at org.apache.hive.druid.com.metamx.collections.bitmap.WrappedRoaringBitmap.toImmutableBitmap(WrappedRoaringBitmap.java:65)  at org.apache.hive.druid.com.metamx.collections.bitmap.RoaringBitmapFactory.makeImmutableBitmap(RoaringBitmapFactory.java:88)  at org.apache.hive.druid.io.druid.segment.StringDimensionMergerV9.writeIndexes(StringDimensionMergerV9.java:348)  at org.apache.hive.druid.io.druid.segment.IndexMergerV9.makeIndexFiles(IndexMergerV9.java:218)  at org.apache.hive.druid.io.druid.segment.IndexMerger.merge(IndexMerger.java:438)  at org.apache.hive.druid.io.druid.segment.IndexMerger.persist(IndexMerger.java:186)  at org.apache.hive.druid.io.druid.segment.IndexMerger.persist(IndexMerger.java:152)  at org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl.persistHydrant(AppenderatorImpl.java:996)  at org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl.access$200(AppenderatorImpl.java:93)  at org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl$2.doCall(AppenderatorImpl.java:385)  at org.apache.hive.druid.io.druid.common.guava.ThreadRenamingCallable.call(ThreadRenamingCallable.java:44)  ... 4 more]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:89, Vertex vertex_1502470020457_0005_12_05 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0 (state=08S01,code=2) OptionsAttachments</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18545" opendate="2018-1-25 00:00:00" fixdate="2018-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDF to parse complex types from json</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.JsonSerDe.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.JsonSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="18767" opendate="2018-2-22 00:00:00" fixdate="2018-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some alterPartitions invocations throw &amp;#39;NumberFormatException: null&amp;#39;</summary>
      <description>Error messages:[info] Cause: java.lang.NumberFormatException: null[info] at java.lang.Long.parseLong(Long.java:552)[info] at java.lang.Long.parseLong(Long.java:631)[info] at org.apache.hadoop.hive.metastore.MetaStoreUtils.isFastStatsSame(MetaStoreUtils.java:315)[info] at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:605)[info] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3837)[info] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[info] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[info] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[info] at java.lang.reflect.Method.invoke(Method.java:498)[info] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)[info] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)[info] at com.sun.proxy.$Proxy23.alter_partitions_with_environment_context(Unknown Source)[info] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(HiveMetaStoreClient.java:1527)</description>
      <version>2.3.3,3.1.0,3.2.0,4.0.0</version>
      <fixedVersion>2.3.4,2.4.0,3.1.1,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19254" opendate="2018-4-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NumberFormatException in MetaStoreUtils.isFastStatsSame</summary>
      <description>I see the following exception under some cases in the logs. This possibly happens when you try to add empty partitions.2018-04-19T19:32:19,260 ERROR [pool-7-thread-7] metastore.RetryingHMSHandler: MetaException(message:java.lang.NumberFormatException: null) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6824) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:4864) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions(HiveMetaStore.java:4801) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy24.alter_partitions(Unknown Source) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_partitions.getResult(ThriftHiveMetastore.java:16046) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_partitions.getResult(ThriftHiveMetastore.java:16030) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NumberFormatException: null at java.lang.Long.parseLong(Long.java:552) at java.lang.Long.parseLong(Long.java:631) at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.isFastStatsSame(MetaStoreUtils.java:632) at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:743) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:4827) ... 21 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19367" opendate="2018-5-1 00:00:00" fixdate="2018-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load Data should fail for empty Parquet files.</summary>
      <description>Load data does not validate the input for Parquet tables. This results in query failures.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19725" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to dump non-native tables in replication metadata dump</summary>
      <description>if hive.repl.dump.metadata.only is set to true, allow dumping non native tables also. Data dump for non-native tables should never be allowed.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
    </fixedFiles>
  </bug>
  <bug id="19727" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Signature matching of table aliases</summary>
      <description>there is a probable problem with alias matching: "t1 as a" is matched to "t2 as a"</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="19772" opendate="2018-6-1 00:00:00" fixdate="2018-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming ingest V2 API can generate invalid orc file if interrupted</summary>
      <description>Hive streaming ingest generated 0 length and 3 byte files which are invalid orc files. This will throw the following exception during compactionError: org.apache.orc.FileFormatException: Not a valid ORC file hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/culvert/year=2018/month=7/delta_0000025_0000025/bucket_00005 (maxFileLength= 3) at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:546) at org.apache.orc.impl.ReaderImpl.&lt;init&gt;(ReaderImpl.java:370) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&lt;init&gt;(ReaderImpl.java:60) at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:90) at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.&lt;init&gt;(OrcRawRecordMerger.java:1124) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:2373) at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:1000) at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:977) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:460) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:344) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)</description>
      <version>3.1.0,3.0.1,4.0.0</version>
      <fixedVersion>3.1.0,3.0.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
    </fixedFiles>
  </bug>
  <bug id="19773" opendate="2018-6-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO exception while running queries with tables that are not present in materialized views</summary>
      <description>When we obtain the valid list of write ids, some tables in the materialized views may not be present in the list because they are not present in the query, which leads to exceptions (hidden in logs) when we try to load the materialized views in the planner, as we need to verify whether they are outdated or not.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19775" opendate="2018-6-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should use HS2 embedded mode in privileged auth mode</summary>
      <description>Follow up of HIVE-19389.Authorization checks don't make sense for embedded mode and since it is not used in that mode it leads to issues if authorization is enabled (eg, username not set).</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19776" opendate="2018-6-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2.startHiveServer2 retries of start has concurrency issues</summary>
      <description>HS2 starts the thrift binary/http servers in background, while it proceeds to do other setup (eg create zookeeper entries). If there is a ZK error and it attempts to stop and start in the retry loop within HiveServer2.startHiveServer2, the retry fails because the thrift server doesn't get stopped if it was still getting initialized.The thrift server initialization and stopping needs to be synchronized.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="19778" opendate="2018-6-3 00:00:00" fixdate="2018-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable a flaky test: TestCliDriver#input31</summary>
      <description>Noticed this one has been failing occasionally on precommit test runs.Running: diff -a /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/itests/qtest/target/qfile-results/clientpositive/input31.q.out /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/ql/src/test/results/clientpositive/input31.q.out128c128&lt; 496---&gt; 242</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19787" opendate="2018-6-4 00:00:00" fixdate="2018-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log message when spark-submit has completed</summary>
      <description>If spark-submit runs successfully the "Driver" thread should log a message. Otherwise there is no way to know if spark-submit exited successfully. We should also rename the thread to some more informative than "Driver".Without this, debugging timeout exceptions of the RemoteDriver -&gt; HS2 connection is difficult, because there is no way to know if spark-submit finished or not.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkSubmitSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="19793" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable LLAP IO batch-to-row wrapper for ACID deletes/updates</summary>
      <description>1) Batch to row converter doesn't propagate columns correctly because they are not in the schema.2) Then, even if it did, the current VrbCtx model of ACID column propagation only works with VectorMapOperator. Regular MapOperator has no such context; the reader ends up storing the vector in some fake temporary ctx. I left a TODO that combined with a fix to (1) could fix this instead of disabling it.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19796" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push Down TRUNC Fn to Druid Storage Handler</summary>
      <description>Push down Queries with TRUNC date function such as SELECT SUM((`ssb_druid_100`.`discounted_price` * `ssb_druid_100`.`net_revenue`)) AS `sum_calculation_4998925219892510720_ok`, CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`FROM `druid_ssb`.`ssb_druid_100` `ssb_druid_100`GROUP BY CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE)</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DruidSqlOperatorConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExtractDate.java</file>
    </fixedFiles>
  </bug>
  <bug id="19799" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove jasper dependency</summary>
      <description>jasper dependency version looks old and unwanted. There is a comment which says it is required by thrift but I don't see jasper as thrift dependency. Try removing it to see if its safe (after precommit test run). </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1980" opendate="2011-2-9 00:00:00" fixdate="2011-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging using mapreduce rather than map-only job failed in case of dynamic partition inserts</summary>
      <description>In dynamic partition insert and if merge is set to true and hive.mergejob.maponly=false, the merge MapReduce job will fail.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
    </fixedFiles>
  </bug>
  <bug id="19810" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StorageHandler fail to ship jars in Tez intermittently</summary>
      <description>Hive relies on StorageHandler to ship jars to backend automatically in several cases: JdbcStorageHandler, HBaseStorageHandler, AccumuloStorageHandler. This does not work reliably, in particular, the first dag in the session will have those jars, the second will not unless container is reused. In the later case, the containers allocated to first dag will be reused in the second dag so the container will have additional resources.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="19812" opendate="2018-6-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable external table replication by default via a configuration property</summary>
      <description>use a hive config property to allow external table replication. set this property by default to prevent external table replication.for metadata only hive repl always export metadata for external tables. REPL_DUMP_EXTERNAL_TABLES("hive.repl.dump.include.external.tables", false,"Indicates if repl dump should include information about external tables. It should be \n"+ "used in conjunction with 'hive.repl.dump.metadata.only' set to false. if 'hive.repl.dump.metadata.only' \n"+ " is set to true then this config parameter has no effect as external table meta data is flushed \n"+ " always by default.")This should be done for only replication dump and not for export</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.exim.21.part.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.19.external.over.existing.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.16.part.noncompat.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.15.part.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.14.nonpart.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.13.nonnative.import.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.12.nonnative.export.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.11.nonpart.noncompat.sorting.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.10.nonpart.noncompat.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.09.nonpart.noncompat.serdeparam.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.08.nonpart.noncompat.serde.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.07.nonpart.noncompat.ifof.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.06.nonpart.noncompat.storage.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.05.nonpart.noncompat.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.04.nonpart.noncompat.colnumber.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.03.nonpart.noncompat.colschema.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.repl.2.exim.basic.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19817" opendate="2018-6-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive streaming API + dynamic partitioning + json/regex writer does not work</summary>
      <description>New streaming API for dynamic partitioning only works with delimited record writer. Json and Regex writers does not work.</description>
      <version>3.1.0,3.0.1,4.0.0</version>
      <fixedVersion>3.1.0,3.0.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreamingDynamicPartitioning.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.JsonSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="19829" opendate="2018-6-8 00:00:00" fixdate="2018-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental replication load should create tasks in execution phase rather than semantic phase</summary>
      <description>Split the incremental load into multiple iterations. In each iteration create number of tasks equal to the configured value.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TestTaskTracker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeavesTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.PartitionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TaskTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.TableContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeaves.java</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="1983" opendate="2011-2-10 00:00:00" fixdate="2011-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle Log4j configuration files in Hive JARs</summary>
      <description>Splitting this off as a subtask so that it can be resolved independently of the hive-default.xml issue.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">conf.hive-log4j.properties</file>
      <file type="M">conf.hive-exec-log4j.properties</file>
      <file type="M">common.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19851" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade jQuery version</summary>
      <description>jQuery version seems to be very old. Update to latest stable version. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.static.js.jquery.min.js</file>
    </fixedFiles>
  </bug>
  <bug id="19852" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update jackson to latest</summary>
      <description>Update jackson version to latest 2.9.5</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19853" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow serializer needs to create a TimeStampMicroTZVector instead of TimeStampMicroVector</summary>
      <description>HIVE-19723 changed nanosecond to microsecond in Arrow serialization. However, it needs to be microsecond with time zone.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Deserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19866" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve LLAP cache purge</summary>
      <description>1) Memory needs to be accounted for.2) LRFU eviction doesn't need to maintain state between individual removals.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="19867" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle concurrent INSERTS</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.TxnIdUtils.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.1.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.1.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.1.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.1.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.1.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartition.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterPartitionsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="19868" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for float aggregator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19873" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup operation log on query cancellation after some delay</summary>
      <description>When a query is executed using beeline and the query is cancelled due to query timeout or kill query or triggers and when there is cursor on operation log row set, the cursor can thrown an exception as cancel will cleanup the operation log in the background. This can return a non-zero exit code in beeline. Query cancellation on success should return exit code 0.Adding a delay to the cleanup of operation logging in operation cancel can avoid the close during read. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19875" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase LLAP IO queue size for perf</summary>
      <description>According to gopalv queue limit has perf impact, esp. during hashtable load for mapjoin where in the past IO used to queue up more data for processing to process.1) Overall the default limit could be adjusted higher.2) Depending on Decimal64 availability, the weight for decimal columns could be reduced.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19879" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused calcite sql operator.</summary>
      <description>HIVE-19796 introduced by mistake an unused sql operator.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19880" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl Load to return recoverable vs non-recoverable error codes</summary>
      <description>To enable bootstrap of large databases, application has to have the ability to keep retrying the bootstrap load till it encounters a fatal error. The ability to identify if an error is fatal or not will be decided by hive and communication of the same will happen to application via error codes.So there should be different error codes for recoverable vs non-recoverable failures which should be propagated to application as part of running the repl load command.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.EventUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19881" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow metadata-only dump for database which are not source of replication</summary>
      <description>If the dump is meta data only then allow dump even if the db is not source of replication</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19902" opendate="2018-6-14 00:00:00" fixdate="2018-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Metastore micro-benchmarks</summary>
      <description>It would be very useful to have metastore benchmarks to be able to track perf issues.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19903" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable temporary insert-only transactional table</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19904" opendate="2018-6-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data rewrite into Tez job fails for ACID</summary>
      <description>Load data rewrite into IAS fails for ACID as there is some code which does not take into account the table name could be in upper case, specifically ValidTxnWriteIdList</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.load.data.using.job.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19924" opendate="2018-6-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tag distcp jobs run by Repl Load</summary>
      <description>Add tags in jobconf for distcp related jobs started by replication. This will allow hive to kill these jobs in case beacon retries, or hs2 dies and beacon issues a kill command. one of the tags should definitely be the query_id that starts the job : With this flow beacon before retrying the bootstrap load, will issue a kill command to hs2 with the query id of the previous issued command. hs2 will then kill an running jobs on yarn tagged with the Query_id. To get around the additional failure point as mentioned above. The jobs can be tagged with an additional unique tag_id provided by Beacon in the WITH clause in repl load command to be used to tag distcp jobs ). Enhance the kill api to take the tag as input and kill jobs associated with that tag. Problem here is how do we validate the association of the tag with a hive query id to make sure this api is not used to kill jobs run by other components, however we can provide this capability to only admins and should be ok in that case.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.NullKillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.KillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KillTriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug id="19926" opendate="2018-6-17 00:00:00" fixdate="2018-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated hcatalog streaming</summary>
      <description>hcatalog streaming is deprecated in 3.0.0. We should remove it in 4.0.0.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.package.html</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorTestUtil.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">hcatalog.streaming.src.test.sit</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestDelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestWarehousePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestSequenceValidator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestRecordInspectorImpl.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorImpl.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMetaStorePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestGroupingValidator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestBucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.TestMutations.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingTestUtils.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingAssert.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.ReflectiveMutatorFactory.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.MutableRecord.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.ExampleUseCase.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestTransaction.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestMutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestAcidTableSerializer.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestLock.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestHeartbeatTimerTask.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionError.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatchUnAvailable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingIOFailure.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.SerializationError.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.QueryFailedException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.PartitionCreationFailed.java</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.ConnectionError.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HeartBeatFailure.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.ImpersonationFailed.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidColumn.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidPartition.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidTrasactionState.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTableSerializer.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.ClientException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.ConnectionException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.HeartbeatFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.HeartbeatTimerTask.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.LockException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.LockFailureListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClientBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.TableType.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.Transaction.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.TransactionException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.doc-files.system-overview.dot</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.HiveConfFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.package.html</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.UgiMetaStoreClientFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolver.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.GroupingValidator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.GroupRevisitedException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MetaStorePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.Mutator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinatorBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.OperationType.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.PartitionCreationException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.PartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspector.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspectorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordSequenceException.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.SequenceValidator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.WarehousePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.WorkerException.java</file>
    </fixedFiles>
  </bug>
  <bug id="19956" opendate="2018-6-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include yarn registry classes to jdbc standalone jar</summary>
      <description>HS2 Active/Passive HA requires some yarn registry classes. Include it in JDBC standalone jar. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19963" opendate="2018-6-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metadata_only_queries.q fails</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19970" opendate="2018-6-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication dump has a NPE when table is empty</summary>
      <description>if table directory or partition directory is missing ..dump is throwing NPE instead of file missing exception.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19972" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup to HIVE-19928 : Fix the check for managed table</summary>
      <description>The check for managed table should use ENUM comparison rather than string comparison.The check in the patch will always return false, thus maintaining existing behavior.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="19973" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable materialized view rewriting by default</summary>
      <description>Change property value for hive.materializedview.rewriting to true. For tests, it is already true by default.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19975" opendate="2018-6-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checking writeIdList per table may not check the commit level of a partition on a partitioned table</summary>
      <description>writeIdList is per table entity but stats for a partitioned table are per partition. I.e., each record in PARTITIONS has an independent stats. So if we check the validity of a partition's stats, we need to check in the context of a partiton.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.TxnIdUtils.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.1.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.1.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.1.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.1.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.1.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ColStatsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats.q.out</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AggrStats.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.IsolationLevelCompliance.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartition.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19981" opendate="2018-6-25 00:00:00" fixdate="2018-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Managed tables converted to external tables by the HiveStrictManagedMigration utility should be set to delete data when the table is dropped</summary>
      <description>Using the HiveStrictManagedMigration utility, tables can be converted to conform to the Hive strict managed tables mode.For managed tables that are converted to external tables by the utility, these tables should keep the "drop data on delete" semantics they had when they were managed tables.One way to do this is to introduce a table property "external.table.purge", which if true (and if the table is an external table), will let Hive know to delete the table data when the table is dropped. This property will be set by the HiveStrictManagedMigration utility when managed tables are converted to external tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20016" opendate="2018-6-27 00:00:00" fixdate="2018-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate TestJdbcWithMiniHS2.testParallelCompilation3 random failure</summary>
      <description>org.apache.hive.jdbc.TestJdbcWithMiniHS2.testParallelCompilation3 failed with:java.lang.AssertionError: Concurrent Statement failed: org.apache.hive.service.cli.HiveSQLException: java.lang.AssertionError: Authorization plugins not initialized! at org.junit.Assert.fail(Assert.java:88) at org.apache.hive.jdbc.TestJdbcWithMiniHS2.finishTasks(TestJdbcWithMiniHS2.java:374) at org.apache.hive.jdbc.TestJdbcWithMiniHS2.testParallelCompilation3(TestJdbcWithMiniHS2.java:304)</description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="20018" opendate="2018-6-27 00:00:00" fixdate="2018-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix TestReplicationScenarios on the branch</summary>
      <description>org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testBootstrapWithConcurrentDropPartition (batchId=238)org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testBootstrapWithConcurrentDropTable (batchId=238)org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testBootstrapWithDropPartitionedTable (batchId=238)org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConcatenatePartitionedTable (batchId=238)org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testIncrementalLoadFailAndRetry (batchId=238)org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testStatus (batchId=238)Most tests have errors, not being able to find a table/partition in getPartition. Might be related to catalog handling, or something else.Some have NPEs, some no obvious errors.Update: the only two real failures are [ERROR] TestReplicationScenarios.testBootstrapWithConcurrentDropTable:462 expected:&lt;true&gt; but was:&lt;false&gt;[ERROR] TestReplicationScenarios.testBootstrapWithDropPartitionedTable:687 expected:&lt;true&gt; but was:&lt;false&gt;caused by injection failures due to signature changes. Will fix after HIVE-20062 and HIVE-19975</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20019" opendate="2018-6-27 00:00:00" fixdate="2018-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ban commons-logging and log4j</summary>
      <description>Still seeing several references to commons-logging. We should move all classes to slf4j instead. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.http.JdbcJarDownloadServlet.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.FileMetadataManager.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.FileMetadataHandler.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.valcoersion.JavaIOTmpdirVariableCoercion.java</file>
      <file type="M">common.src.java.org.apache.hive.http.JMXJsonServlet.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapIoMemoryServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapServerSecurityInfo.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginSecurityInfo.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSingleImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkEmptyKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TablePropertyEnrichmentOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.BaseMaskUDF.java</file>
      <file type="M">ql.src.test.org.apache.hive.testutils.MiniZooKeeperCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="20028" opendate="2018-6-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore client cache config is used incorrectly</summary>
      <description>Metastore client cache config is not used correctly. Enabling the cache actually disables it and vice versa. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20029" opendate="2018-6-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add parallel insert, analyze, iow tests</summary>
      <description>1) We need a few tests, esp. for parallel case, where we verify that stats are NOT used.Right now many code paths don't fail but return -1, null or whatever when something else is not present, so positive tests might pass because they skip the check, not because the check passes.2) Analyze table needs a test, esp analyze table after parallel insert, and also analyze table after an invalid transaction.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
    </fixedFiles>
  </bug>
  <bug id="20032" opendate="2018-6-29 00:00:00" fixdate="2018-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t serialize hashCode for repartitionAndSortWithinPartitions</summary>
      <description>Follow up on HIVE-15104, if we don't enable RDD cacheing or groupByShuffles, then we don't need to serialize the hashCode when shuffling data in HoS.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
      <file type="M">kryo-registrator.src.main.java.org.apache.hive.spark.HiveKryoRegistrator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2ErasureCoding.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.exec.spark.TestSparkStatistics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20061" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a config flag to turn off txn stats</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20066" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.load.data.owner is compared to full principal</summary>
      <description>HIVE-19928 compares the user running HS2 to the configured owner (hive.load.data.owner) to check if we're able to move the file with LOAD DATA or need to copy.This check compares the full username (that may contain the full kerberos principal) to hive.load.data.owner. We should compare to the short username (UGI.getShortUserName()) instead. That's used in similar context here.cc djaiswal</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="20103" opendate="2018-7-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM: Only Aggregate DAG counters if at least one is used</summary>
      <description>status = dagClient.getDAGStatus(EnumSet.of(StatusGetOpts.GET_COUNTERS), checkInterval); TezCounters dagCounters = status.getDAGCounters();... if (dagCounters != null &amp;&amp; wmContext != null) { Set&lt;String&gt; desiredCounters = wmContext.getSubscribedCounters(); if (desiredCounters != null &amp;&amp; !desiredCounters.isEmpty()) { Map&lt;String, Long&gt; currentCounters = getCounterValues(dagCounters, vertexNames, vertexProgressMap, desiredCounters, done);Skip collecting DAG counters unless there at least one desired counter in wmContext.The AM has a hard-lock around the counters, so the current jstacks are full of java.lang.Thread.State: RUNNABLE at java.lang.String.intern(Native Method) at org.apache.hadoop.util.StringInterner.weakIntern(StringInterner.java:71) at org.apache.tez.common.counters.GenericCounter.&lt;init&gt;(GenericCounter.java:50) at org.apache.tez.common.counters.TezCounters$GenericGroup.newCounter(TezCounters.java:65) at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:92) at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104) - locked &lt;0x00007efb3ac7af38&gt; (a org.apache.tez.common.counters.TezCounters$GenericGroup) at org.apache.tez.common.counters.AbstractCounterGroup.aggrAllCounters(AbstractCounterGroup.java:204) at org.apache.tez.common.counters.AbstractCounters.aggrAllCounters(AbstractCounters.java:372) - eliminated &lt;0x00007efb3ac64ee8&gt; (a org.apache.tez.common.counters.TezCounters) at org.apache.tez.common.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:357) - locked &lt;0x00007efb3ac64ee8&gt; (a org.apache.tez.common.counters.TezCounters) at org.apache.tez.dag.app.dag.impl.TaskImpl.getCounters(TaskImpl.java:462) at org.apache.tez.dag.app.dag.impl.VertexImpl.aggrTaskCounters(VertexImpl.java:1342) at org.apache.tez.dag.app.dag.impl.VertexImpl.getAllCounters(VertexImpl.java:1202) at org.apache.tez.dag.app.dag.impl.DAGImpl.aggrTaskCounters(DAGImpl.java:755) at org.apache.tez.dag.app.dag.impl.DAGImpl.getAllCounters(DAGImpl.java:704) at org.apache.tez.dag.app.dag.impl.DAGImpl.getDAGStatus(DAGImpl.java:901) at org.apache.tez.dag.app.dag.impl.DAGImpl.getDAGStatus(DAGImpl.java:940) at org.apache.tez.dag.api.client.DAGClientHandler.getDAGStatus(DAGClientHandler.java:73)</description>
      <version>3.0.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="20120" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental repl load DAG generation is causing OOM error.</summary>
      <description>Split the incremental load into multiple iterations. In each iteration create number of tasks equal to the configured value.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="20123" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix masking tests after HIVE-19617</summary>
      <description>Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed.</description>
      <version>3.1.0,3.0.0,3.2.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20127" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix some issues with LLAP Parquet cache</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
    </fixedFiles>
  </bug>
  <bug id="20129" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert to position based schema evolution for orc tables</summary>
      <description>Hive has been doing positional based schema evolution. ORC-54 changed it to column name based schema evolution causing unexpected results. Queries returned results earlier are now returning no results. Change the default in hive to positional schema evolution. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20130" opendate="2018-7-10 00:00:00" fixdate="2018-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better logging for information schema synchronizer</summary>
      <description>The logging of information schema synchronizer should be more useful.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeSynchonizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20147" opendate="2018-7-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive streaming ingest is contented on synchronized logging</summary>
      <description>In one of the observed profile, &gt;30% time spent on synchronized logging. See attachment. We should use async logging for hive streaming ingest by default. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20150" opendate="2018-7-12 00:00:00" fixdate="2018-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey pushdown</summary>
      <description>TopNKey operator is implemented in HIVE-17896, but it needs more work in pushdown implementation. So this issue covers TopNKey pushdown implementation with proper tests.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.llap.text.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.identity.reuse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.conversion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">kudu-handler.src.test.results.positive.kudu.complex.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TopNKeyProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.test.queries.clientpositive.topnkey.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.topnkey.q</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.q93.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20164" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Murmur Hash : Make sure CTAS and IAS use correct bucketing version</summary>
      <description>With the migration to Murmur hash, CTAS and IAS from old table version to new table version does not work as intended and data is hashed using old hash logic.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20165" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable ZLIB for streaming ingest</summary>
      <description>Per gopalv's recommendation tried running streaming ingest with and without zlib. Following are the numbers  Compression: NONE Total rows committed: 93800000 Throughput: 1563333 rows/second$ hdfs dfs -du -s -h /apps/hive/warehouse/prasanth.db/culvert 14.1 G  /apps/hive/warehouse/prasanth.db/culvert   Compression: ZLIB Total rows committed: 92100000 Throughput: 1535000 rows/second$ hdfs dfs -du -s -h /apps/hive/warehouse/prasanth.db/culvert 7.4 G  /apps/hive/warehouse/prasanth.db/culvert   ZLIB is getting us 2x compression and only 2% lesser throughput. We should enable ZLIB by default for streaming ingest. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
    </fixedFiles>
  </bug>
  <bug id="20177" opendate="2018-7-15 00:00:00" fixdate="2018-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Reduce KeyWrapper allocation in GroupBy Streaming mode</summary>
      <description>The streaming mode for VectorGroupBy allocates a large number of arrays due to VectorKeyHashWrapper::duplicateTo()Since the vectors can't be mutated in-place while a single batch is being processed, this operation can be cut by 1000x by allocating a streaming key at the end of the loop, instead of reallocating within the loop. for(int i = 0; i &lt; batch.size; ++i) { if (!batchKeys[i].equals(streamingKey)) { // We've encountered a new key, must save current one // We can't forward yet, the aggregators have not been evaluated rowsToFlush[flushMark] = currentStreamingAggregators; if (keysToFlush[flushMark] == null) { keysToFlush[flushMark] = (VectorHashKeyWrapper) streamingKey.copyKey(); } else { streamingKey.duplicateTo(keysToFlush[flushMark]); } currentStreamingAggregators = streamAggregationBufferRowPool.getFromPool(); batchKeys[i].duplicateTo(streamingKey); ++flushMark; }The duplicateTo can be pushed out of the loop since there only one to truly keep a copy of is the last unique key in the VRB.The actual byte[] values within the keys are safely copied out by - VectorHashKeyWrapperBatch.assignRowColumn() which calls setVal() and not setRef().</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2018" opendate="2011-3-1 00:00:00" fixdate="2011-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid loading Hive aux jars in CLI remote mode</summary>
      <description>CLI load a number of jars (aux jars) including serde, antlr, metastore etc. These jars could be large and takes time to load when they are deployed to heavy loaded NFS mount points. In CLI remote mode, all these jars are not needed by the client side.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20191" opendate="2018-7-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit patch application doesn&amp;#39;t fail if patch is empty</summary>
      <description>I've created some backport tickets to branch-3 (e.g. HIVE-20181) and made the mistake of uploading the patch files with wrong filename (. instead of - between version and branch).These get applied on master, where they're already present, since git apply with -3 won't fail if patch is already there. Tests are run on master instead of failing.I think the patch application should fail if the patch is empty and branch selection logic should probably fail too if the patch name is malformed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20194" opendate="2018-7-17 00:00:00" fixdate="2018-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastoreClient should use reflection to instantiate embedded HMS instance</summary>
      <description>When HiveMetastoreClient is used in embedded mode, it instantiates metastore server. Since we want to separate client and server code we can no longer instantiate the class directly but need to use reflection for that.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20195" opendate="2018-7-17 00:00:00" fixdate="2018-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split MetastoreUtils into common and server-specific parts</summary>
      <description>Parts of MetastoreUtils are used by clients and the server, parts are used by server only. We need to separate server-only parts in a separate class.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.HiveStrictManagedUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.ColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.BooleanColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.BinaryColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="20202" opendate="2018-7-18 00:00:00" fixdate="2018-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add profiler endpoint to HS2 and LLAP</summary>
      <description>Add a web endpoint for profiling based on async-profiler. This servlet should be added to httpserver so that HS2 and LLAP daemons can output flamegraphs when their /prof endpoint is hit. Since this will be based on https://github.com/jvm-profiling-tools/async-profiler heap allocation, lock contentions, HW counters etc. will also be supported in addition to cpu profiling. In most cases the profiling overhead is pretty low and is safe to run on production. More analysis on CPU and memory overhead here https://github.com/jvm-profiling-tools/async-profiler/issues/14 and https://github.com/jvm-profiling-tools/async-profiler/issues/131  For the impatient, here is the usage doc and the sample output https://github.com/prasanthj/nightswatch/blob/master/README.md </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="20203" opendate="2018-7-18 00:00:00" fixdate="2018-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow SerDe leaks a DirectByteBuffer</summary>
      <description>ArrowColumnarBatchSerDe allocates an arrow NullableMapVector for each task that uses the serde.The vector is a DirectByteBuffer allocated from Arrow's off-heap buffer pool.This buffer is never closed and leaks about 1K of physical memory for each task.This patch does three things: Ensure the buffer is closed when the RecordWriter for the task is closed.  Adds per-task memory accounting by assigning a ChildAllocator to each task from the RootAllocator. Enforces that the ChildAllocator for a task has released all memory assigned to it, when the task is completed. The patch assumes that close() is always called on the RecordWriter when a task is finished (even if there is a failure during task execution). </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowWrapperWritable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.WritableByteChannelAdapter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapArrowRecordWriter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2025" opendate="2011-3-3 00:00:00" fixdate="2011-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestEmbeddedHiveMetaStore and TestRemoteHiveMetaStore broken by HIVE-2022</summary>
      <description>The patch for HIVE-2022 broke TestEmbeddedHiveMetaStore and TestRemoteHiveMetaStorehttps://hudson.apache.org/hudson/job/Hive-trunk-h0.20/590/@Paul: Assigning this to you.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20252" opendate="2018-7-27 00:00:00" fixdate="2018-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin Reduction : Cycles due to semi join branch may remain undetected if small table side has a map join upstream.</summary>
      <description>For eg,  # 2018-07-26T17:22:14,664 DEBUG [51377701-dc98-424f-82e0-bbb5d6c84316 main] optimizer.SharedWorkOptimizer: Before SharedWorkOptimizer: # TS[0]-FIL[96]-SEL[2]-MAPJOIN[156]-MAPJOIN[157]-MAPJOIN[161]-MAPJOIN[162]-FIL[47]-SEL[48]-MAPJOIN[163]-FIL[66]-SEL[67]-TNK[105]-GBY[68]-RS[69]-GBY[70]-SEL[71]-RS[72]-SEL[73]-LIM[74]-FS[75] #                                                           -SEL[142]-GBY[143]-RS[144]-GBY[145]-RS[155] # TS[3]-FIL[97]-SEL[5]-RS[34]-MAPJOIN[156] # TS[6]-FIL[98]-SEL[8]-RS[37]-MAPJOIN[157] # TS[9]-FIL[99]-SEL[11]-MAPJOIN[158]-GBY[40]-RS[42]-MAPJOIN[161] # TS[12]-FIL[100]-SEL[14]-RS[16]-MAPJOIN[158] #                       -SEL[131]-GBY[132]-EVENT[133] # TS[19]-FIL[101]-SEL[21]-MAPJOIN[159]-GBY[29]-RS[30]-GBY[31]-SEL[32]-RS[45]-MAPJOIN[162] # TS[22]-FIL[102]-SEL[24]-RS[26]-MAPJOIN[159] #                       -SEL[139]-GBY[140]-EVENT[141] # TS[49]-FIL[103]-SEL[51]-MAPJOIN[160]-GBY[59]-RS[60]-GBY[61]-SEL[62]-RS[64]-MAPJOIN[163] # TS[52]-FIL[104]-SEL[54]-RS[56]-MAPJOIN[160] #                       -SEL[147]-GBY[148]-EVENT[149] # # # DPP information stored in the cache: \{TS[19]=[EVENT[141]], TS[9]=[EVENT[133]], TS[49]=[RS[155], EVENT[149]]} The semi join branch in line 3 feeds into TS&amp;#91;49&amp;#93; in line 12 which feeds to MAPJOIN&amp;#91;163&amp;#93; going back to parent of the semi join branch at line 2.The logic to detect cycle may fail as there is a MAPJOIN&amp;#91;160&amp;#93; at line 12 which could cause the logic to look for wrong TS. The logic to find TS operator upstream must use findOperatorsUpstream() and examine each TS Op for complete coverage. Simplified image of task-cycle, without operator cycles - http://people.apache.org/~gopalv/HIVE_20252_cycle1.svgAnd the artificial edge introduced to trigger cycle detection (in red) - http://people.apache.org/~gopalv/HIVE_20252_cycle_fix.svgcc jcamachorodriguez</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20262" opendate="2018-7-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement stats annotation rule for the UDTFOperator</summary>
      <description>User Defined Table Functions (UDTFs) change the number of rows of the output. A common UDTF is the explode() method that creates a row for each element for each array in the input column. Right now, the number of output rows is equal to the number of input rows. But if the average number of output rows is bigger than 1, the resulting number of rows is underestimated in the execution plan. Implement a rule that can have a factor X as a parameter and for each UDTF function predict that: number of output rows = X * number of input rows   </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20264" opendate="2018-7-29 00:00:00" fixdate="2018-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap repl dump with concurrent write and drop of ACID table makes target inconsistent.</summary>
      <description>During bootstrap dump of ACID tables, let's consider the below sequence. Get lastReplId = last event ID logged. Current session (Thread-1), REPL DUMP -&gt; Open txn (Txn1) - Event-10 Another session (Thread-2), Open txn (Txn2) - Event-11 Thread-2 -&gt; Insert data (T1.D1) to ACID table. - Event-12 Thread-2 -&gt; Commit Txn (Txn2) - Event-13 Thread-2 -&gt; Drop table (T1) - Event-14 Thread-1 -&gt; Dump ACID tables based on current list of tables. So, T1 will be missing. Thread-1 -&gt; Commit Txn (Txn1) REPL LOAD from bootstrap dump will skip T1. Incremental REPL DUMP will start from Event-10 and hence allocate write id for table T1 and drop table(T1) is idempotent. So, at target, exist entries in TXN_TO_WRITE_ID and NEXT_WRITE_ID metastore tables. Now, when we create another table at source with same name T1 and replicate, then it may lead to incorrect data for readers at target on T1.Couple of proposals: 1. Make allocate write ID idempotent which is not possible as table doesn't exist and MM table import may lead to allocate write id before creating table. So, cannot differentiate these 2 cases. 2. Make Drop table event to drop entries from TXN_TO_WRITE_ID and NEXT_WRITE_ID tables irrespective of table exist or not at target.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="20267" opendate="2018-7-29 00:00:00" fixdate="2018-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expanding WebUI to include form to dynamically config log levels</summary>
      <description>Expanding the possibility to change the log levels during runtime, the webUI can be extended to interact with the Log4j2ConfiguratorServlet, this way it can be directly used and users/admins don't need to execute curl commands from commandline.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.llap.html</file>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
    </fixedFiles>
  </bug>
  <bug id="20279" opendate="2018-7-31 00:00:00" fixdate="2018-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveContextAwareRecordReader slows down Druid Scan queries.</summary>
      <description>HiveContextAwareRecordReader add lots of overhead for Druid Scan Queries. See attached flame graph. Looks like the operations for checking for existence of footer/header buffer takes most of time For druid and other storage handlers that do not have footer buffer we should skip the logic for checking the existence for storage handlers atleast.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="2028" opendate="2011-3-4 00:00:00" fixdate="2011-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance instruments for client side execution</summary>
      <description>Hive client side execution could sometimes takes a long time. This task is to instrument the client side code to measure the time spent in the most likely expensive components.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20293" opendate="2018-8-2 00:00:00" fixdate="2018-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Replication of ACID table truncate operation</summary>
      <description>Support truncate acid table replication.1. Write id allocation needs to be removed</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AlterTableEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.exim.TestEximReplicationTasks.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="20294" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix NULL / Wrong Results issues in COALESCE / ELT</summary>
      <description>Write new UT tests that use random data and intentional isRepeating batches to checks for NULL and Wrong Results for vectorized COALESCE and ELT.Also, add tests for ARRAY and MAP indexing, IS &amp;#91;NOT&amp;#93; NULL and NOT</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexStringCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexLongCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexDoubleCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="20299" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>potential race in LLAP signer unit test</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="2030" opendate="2011-3-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>isEmptyPath() to use ContentSummary cache</summary>
      <description>addInputPaths() calls isEmptyPath() for every input path. Now every call is a DFS namenode call. Making isEmptyPath() to use cached ContentSummary, we should be able to avoid some namenode calls and reduce latency in the case of multiple partitions.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20300" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>VectorFileSinkArrowOperator</summary>
      <description>Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapRow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20301" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization for materialized view rewriting tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.empty.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rebuild.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.time.window.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.rebuild.dummy.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.mv.q</file>
    </fixedFiles>
  </bug>
  <bug id="20302" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: non-vectorized execution in IO ignores virtual columns, including ROW__ID</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.nonvector.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.llap.nonvector.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOiBatchToRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BatchToRowReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="20329" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Long running repl load (incr/bootstrap) causing OOM error</summary>
      <description>The task created in the previous iterations of the load are not delinked and thus causing heap memory usage issue. need to delink the tasks to avoid OOM error.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20347" opendate="2018-8-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.optimize.sort.dynamic.partition should work with partitioned CTAS and MV</summary>
      <description></description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2035" opendate="2011-3-8 00:00:00" fixdate="2011-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use block-level merge for RCFile if merging intermediate results are needed</summary>
      <description>Currently if hive.merge.mapredfiles and/or hive.merge.mapfile is set to true the intermediate data could be merged using an additional MapReduce job. This could be quite expensive if the data size is large. With HIVE-1950, merging can be done in the RCFile block level so that it bypasses the (de-)compression, (de-)serialization phases. This could improve the merge process significantly. This JIRA should handle the case where the input table is not stored in RCFile, but the destination table is (which requires the intermediate data should be stored in the same format as the destination table).</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20352" opendate="2018-8-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Support grouping function</summary>
      <description>Support native vectorization for grouping function (part of Grouping Sets) so we don't need to use VectorUDFAdaptor.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="20353" opendate="2018-8-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Follow redirects when hive connects to a passive druid overlord/coordinator</summary>
      <description>When we have multiple druid coordinators/overlords and hive tries to connect to a passive one, it will get a redirect. Currently the http client in druid storage handler does not follow redirects. We need to check if there is a redirect and follow that for druid overlord/coordinator</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20364" opendate="2018-8-10 00:00:00" fixdate="2018-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default for hive.map.aggr.hash.min.reduction</summary>
      <description>Default value is 0.5 Lets update it to 0.99In average case its a trade-off between cpu vs network. Erring on side of CPU is better since perf loss caused by network is usually larger.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20367" opendate="2018-8-11 00:00:00" fixdate="2018-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Support streaming for PTF AVG, MAX, MIN, SUM</summary>
      <description>Add support for vectorizing PTF AVG, MAX, MIN, SUM when:ROWS PRECEDING(MAX)~CURRENT</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2037" opendate="2011-3-9 00:00:00" fixdate="2011-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge result file size should honor hive.merge.size.per.task</summary>
      <description>The merge job set mapred.min.split.size to the value of hive.merge.size.per.task, which roughly equals to the output file size. However the input split size is also determined by mapred.min.split.size.per.node, mapred.min.split.size.per.rack, and mapred.max.split.size. They should be set the same as hive.merge.size.per.task as well.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20377" opendate="2018-8-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Kafka Storage Handler</summary>
      <description>Goal Read streaming data form Kafka queue as an external table. Allow streaming navigation by pushing down filters on Kafka record partition id, offset and timestamp. Insert streaming data form Kafka to an actual Hive internal table, using CTAS statement.ExampleCreate the external table CREATE EXTERNAL TABLE kafka_table (`timestamp` timestamp, page string, `user` string, language string, added int, deleted int, flags string,comment string, namespace string)STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'TBLPROPERTIES ("kafka.topic" = "wikipedia", "kafka.bootstrap.servers"="brokeraddress:9092","kafka.serde.class"="org.apache.hadoop.hive.serde2.JsonSerDe");Kafka MetadataIn order to keep track of Kafka records the storage handler will add automatically the Kafka row metadata eg partition id, record offset and record timestamp. DESCRIBE EXTENDED kafka_tabletimestamp timestamp from deserializer page string from deserializer user string from deserializer language string from deserializer country string from deserializer continent string from deserializer namespace string from deserializer newpage boolean from deserializer unpatrolled boolean from deserializer anonymous boolean from deserializer robot boolean from deserializer added int from deserializer deleted int from deserializer delta bigint from deserializer __partition int from deserializer __offset bigint from deserializer __timestamp bigint from deserializer Filter push down.Newer Kafka consumers 0.11.0 and higher allow seeking on the stream based on a given offset. The proposed storage handler will be able to leverage such API by pushing down filters over metadata columns, namely __partition (int), __offset(long) and __timestamp(long)For instance Query like select `__offset` from kafka_table where (`__offset` &lt; 10 and `__offset`&gt;3 and `__partition` = 0) or (`__partition` = 0 and `__offset` &lt; 105 and `__offset` &gt; 99) or (`__offset` = 109);Will result on a scan of partition 0 only then read only records between offset 4 and 109. With timestamp seeks The seeking based on the internal timestamps allows the handler to run on recently arrived data, by doingselect count(*) from kafka_table where `__timestamp` &gt; 1000 * to_unix_timestamp(CURRENT_TIMESTAMP - interval '20' hours) ;This allows for implicit relationships between event timestamps and kafka timestamps to be expressed in queries (i.e event_timestamp is always &lt; than kafka __timestamp and kafka __timestamp is never &gt; 15 minutes from event etc).More examples with Avro CREATE EXTERNAL TABLE wiki_kafka_avro_tableSTORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'TBLPROPERTIES("kafka.topic" = "wiki_kafka_avro_table","kafka.bootstrap.servers"="localhost:9092","kafka.serde.class"="org.apache.hadoop.hive.serde2.avro.AvroSerDe",'avro.schema.literal'='{ "type" : "record", "name" : "Wikipedia", "namespace" : "org.apache.hive.kafka", "version": "1", "fields" : [ { "name" : "isrobot", "type" : "boolean" }, { "name" : "channel", "type" : "string" }, { "name" : "timestamp", "type" : "string" }, { "name" : "flags", "type" : "string" }, { "name" : "isunpatrolled", "type" : "boolean" }, { "name" : "page", "type" : "string" }, { "name" : "diffurl", "type" : "string" }, { "name" : "added", "type" : "long" }, { "name" : "comment", "type" : "string" }, { "name" : "commentlength", "type" : "long" }, { "name" : "isnew", "type" : "boolean" }, { "name" : "isminor", "type" : "boolean" }, { "name" : "delta", "type" : "long" }, { "name" : "isanonymous", "type" : "boolean" }, { "name" : "user", "type" : "string" }, { "name" : "deltabucket", "type" : "double" }, { "name" : "deleted", "type" : "long" }, { "name" : "namespace", "type" : "string" } ]}');</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.README.md</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniDruidKafkaCliDriver.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.SingleNodeKafkaCluster.java</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20378" opendate="2018-8-13 00:00:00" fixdate="2018-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t update stats during alter for txn table conversion</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20379" opendate="2018-8-13 00:00:00" fixdate="2018-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewriting with partitioned materialized views may reference wrong column</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.part.2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="2038" opendate="2011-3-9 00:00:00" fixdate="2011-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore listener</summary>
      <description>Provide to way to observe changes happening on Metastore</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20382" opendate="2018-8-14 00:00:00" fixdate="2018-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Materialized views: Introduce heuristic to favour incremental rebuild</summary>
      <description>Currently, we do not expose stats over ROW&amp;#95;&amp;#95;ID.writeId to the optimizer (this should be fixed by HIVE-20313). Even if we did, we always assume uniform distribution of the column values, which can easily lead to overestimations on the number of rows read when we filter on ROW&amp;#95;&amp;#95;ID.writeId for materialized views (think about a large transaction for MV creation and then small ones for incremental maintenance). This overestimation can lead to incremental view maintenance not being triggered as cost of the incremental plan is overestimated (we think we will read more rows than we actually do). This could be fixed by introducing histograms that reflect better the column values distribution.Till both fixes are implemented, we will use a config variable that will multiply the estimated cost of the rebuild plan and hence will be able to favour incremental rebuild over full rebuild.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rebuild.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewRule.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20388" opendate="2018-8-14 00:00:00" fixdate="2018-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move common classes out of metastore-server</summary>
      <description>There are many classes in metastore-server module that should be moved to metastore-common.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.SecurityUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.MetastoreDelegationTokenManager.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsRebuildLockCleanerTask.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2039" opendate="2011-3-9 00:00:00" fixdate="2011-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove hadoop version check from hive cli shell script</summary>
      <description>looking at cli startup times - one thing i noticed is that the version check in execHiveCmd.sh consumes 0.5-1s of wall-clock time (depending on where hive is installed).AFAIK - hive doesn't support versions less than 20 right now - and this check is only to check if version is less than 20. So we should be able to safely take it out. please comment if that is not the case.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="20393" opendate="2018-8-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin Reduction : markSemiJoinForDPP behaves inconsistently</summary>
      <description>markSemiJoinForDPP has multiple issues,  Uses map tsOps which is wrong as it disallows going thru same TS which may have filters from more than 1 semijoin edges. This results in inconsistent plans for same query as semijoin edges may be processed in different order each time. Uses getColumnExpr() which is not as robust as extractColumn() thus resulting in NPEs. The logic to mark an edge useful when NPE is hit may end up having bad edge.cc gopalv</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20394" opendate="2018-8-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimized and cleaned up HBaseQTest runner</summary>
      <description>Set proper cluster destroy order Propagated proper HBaseTestContext Ported downstream fixes (CDH-63695) General clean up</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.q.out</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.snapshot.q</file>
      <file type="M">hbase-handler.src.test.queries.negative.cascade.dbdrop.q</file>
    </fixedFiles>
  </bug>
  <bug id="20472" opendate="2018-8-28 00:00:00" fixdate="2018-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn test failing for metastore-tool module</summary>
      <description>Fails because there are no applicable tests. [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.20.1:test (default-test) on project hive-metastore-benchmarks: No tests were executed! (Set -DfailIfNoTests=false to ignore this error.) -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException[ERROR][ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :hive-metastore-benchmarks</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20483" opendate="2018-8-29 00:00:00" fixdate="2018-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Really move metastore common classes into metastore-common</summary>
      <description>HIVE-20388 patch was supposed to move a bunch of files from metastore-server to metastore-common but for some reason it didn't happen, so now these files should be moved.</description>
      <version>3.0.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.SecurityUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.HdfsUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.FileUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.TableIterable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge23.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenSelector.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenSecretManager.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecWithSharedSDProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.partition.spec.PartitionListComposingSpecProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.partition.spec.CompositePartitionSpecProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionExpressionProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreTaskThread.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreFS.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetadataStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.FileMetadataHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.conf.TimeValidator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ColumnType.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20486" opendate="2018-8-30 00:00:00" fixdate="2018-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kafka: Use Row SerDe + vectorization</summary>
      <description>KafkaHandler returns unvectorized rows which causes the operators downstream to be slower and sub-optimal.Hive has a vectorization shim which allows Kafka streams without complex projections to be wrapped into a vectorized reader via hive.vectorized.use.row.serde.deserialize.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.SimpleKafkaWriterTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.SimpleKafkaWriter.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaRecordIterator.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaInputFormat.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.results.clientpositive.druid.kafka.storage.handler.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20489" opendate="2018-8-30 00:00:00" fixdate="2018-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain plan of query hangs</summary>
      <description>Explain on a query that joins 47 views, in effect around 94 joins after view expansion seems to take forever. The case here tries to generate a plan using map join with conditional tasks.When the task graph is huge with many paths, there can be a performance issue during compilation. This is caused by recursive traversal of task graph in internTableDesc and deriveFinalExplainAttributes. The use of recursion is inefficient in a couple of ways. For large graphs the recursion was filling up the stack Instead of finding the map works, the traversal was walking all possible paths from root causing a huge performance problem.The fix is to replace the traversal from recursive to an iterative one, keeping track of the nodes already visited. The fix uses getMRTasks, getSparkTasks and getTezTasks to do iterative traversal. These calls were changed to using iterative calls through HIVE-17195. When pushing this patch to an older release, please make sure HIVE-17195 is also pushed to that release.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.util.DAGTraversal.java</file>
    </fixedFiles>
  </bug>
  <bug id="20491" opendate="2018-8-31 00:00:00" fixdate="2018-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix mapjoin size estimations for Fast implementation</summary>
      <description>HIVE-19824 have fixed the estimations; but it calculated for the "optimized" impl; the "fast" one has a little bit bigger footprint.It also seems like fast is a bit overestimated at runtime...that should be also taken care of. numkeys implementation compiler estimation runtime estimation runtime measurement ce / rm re / rm 25M FAST 1168435456 2189433712 1513584984 .77 1.44 25M OPTIMIZED 1168435456 1191203764 1168439664 100% 1.01</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="20493" opendate="2018-8-31 00:00:00" fixdate="2018-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test getGroupNames for SessionStateUserAuthenticator</summary>
      <description>HIVE-20118 changed the behavior of SessionStateUserAuthenticator which had an NPE bug. This was fixed in HIVE-20389. We should have had unit-tests for these!Add unit tests for getGroupNames..</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.java</file>
    </fixedFiles>
  </bug>
  <bug id="20498" opendate="2018-9-4 00:00:00" fixdate="2018-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support date type for column stats autogather</summary>
      <description>set hive.stats.column.autogather=true;create table dx2(a int,b int,d date);explain insert into dx2 values(1,1,'2011-11-11');-- no compute_stats callsinsert into dx2 values(1,1,'2011-11-11');insert into dx2 values(1,1,'2001-11-11');explain analyze table dx2 compute statistics for columns;-- as expected; has compute_stats callsanalyze table dx2 compute statistics for columns;-- runs okdesc formatted dx2 d;-- looks good</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMergerTest.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">ql.src.test.results.clientpositive.test.teradatabinaryfile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.table.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsAutoGatherContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="20505" opendate="2018-9-5 00:00:00" fixdate="2018-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade org.openjdk.jmh:jmh-core to 1.21</summary>
      <description>This ticket tracks the following CVE(s) that were found in the direct dependency org.openjdk.jmh:jmh-core:1.19: CVE-2009-1896, CVE-2009-2689, CVE-2009-3879, CVE-2009-0733, CVE-2009-2475, CVE-2009-3883, CVE-2009-2476, CVE-2009-3884, CVE-2013-0169, CVE-2012-5373, CVE-2009-3880, CVE-2009-3881, CVE-2009-3882, CVE-2009-0581, CVE-2009-2690, CVE-2012-2739, CVE-2009-0723, CVE-2009-3728 </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20506" opendate="2018-9-5 00:00:00" fixdate="2018-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HOS times out when cluster is full while Hive-on-MR waits</summary>
      <description>My understanding is as follows:Hive-on-MR when the cluster is full will wait for resources to be available before submitting a job. This is because the hadoop jar command is the primary mechanism Hive uses to know if a job is complete or failed. Hive-on-Spark will timeout after SPARK_RPC_CLIENT_CONNECT_TIMEOUT because the RPC client in the AppMaster doesn't connect back to the RPC Server in HS2. This is a behavior difference it'd be great to close.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestRpc.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkSubmitSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientFactory.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="20513" opendate="2018-9-6 00:00:00" fixdate="2018-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Improve Fast Vector MapJoin Bytes Hash Tables</summary>
      <description>Based on HIVE-20491 / HIVE-20503 discussions, improve Fast Vector MapJoin Bytes Hash Tables by only storing a one word slot entry.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="20517" opendate="2018-9-7 00:00:00" fixdate="2018-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Creation of staging directory and Move operation is taking time in S3</summary>
      <description>Operations like insert and add partition creates a staging directory to generate the files and then move the files created to actual location. In replication flow, the files are first copied to the staging directory and then moved (rename) to the actual table location. In case of S3, move is not an atomic operation. It internally does a copy and delete. So it can not guarantee the consistency required. So it is better to copy the files directly to the actual location. This will help in avoiding the staging directory creation (which takes 1-2 seconds in s3) and move (which takes time proportional to file size).</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20531" opendate="2018-9-11 00:00:00" fixdate="2018-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl load on cloud storage file system can skip redundant move or add partition tasks.</summary>
      <description>In replication load, both add partition and insert operations are handled through import. Import creates 3 major tasks. Copy, add partition and move. Copy does the copy of data from source location to staging directory. Then add partition (which runs in parallel to copy) creates the partition in meta store. Its a no op in case of insert and by the time this ddl task is executed for insert partition would be already present. The third operation is move. Which actually moves the file from staging directory to actual location. And then in case of insert it adds the insert event to notification table. It does this for add partition operation which is redundant as the event for add partition would have been written already by ddl task. With the optimization to copy directly to actual table location in S3, move task can be avoided for add partition operation replay and replay of insert need not create the add partition (ddl) task.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20535" opendate="2018-9-11 00:00:00" fixdate="2018-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new configuration to set the size of the global compile lock</summary>
      <description>When removing the compile lock, it is quite risky to remove it entirely.It would be good to provide a pool size for the concurrent compilation, so the administrator can limit the load</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20537" opendate="2018-9-11 00:00:00" fixdate="2018-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-column joins estimates with uncorrelated columns different in CBO and Hive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.correlated.join.keys.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20538" opendate="2018-9-11 00:00:00" fixdate="2018-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to store a key value together with a transaction.</summary>
      <description>This can be useful for example to know if a transaction has already happened.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CommitTxnRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20539" opendate="2018-9-11 00:00:00" fixdate="2018-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on com.metamx.java-util</summary>
      <description>java-util was moved from com.metamx to druid code repository. Currently we are packing both com.metamx.java-jtil and io.druid.java-util, This task is to remove the dependency on com.metamx.java-util</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.RetryIfUnauthorizedResponseHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.ResponseCookieHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.KerberosHttpClient.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20541" opendate="2018-9-12 00:00:00" fixdate="2018-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP on external table with add partition event throws NoSuchElementException.</summary>
      <description>REPL dump on an external table with add partition event throws NoSuchElementException. Need to check if file iterator list hasNext before accessing it.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="20542" opendate="2018-9-12 00:00:00" fixdate="2018-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL DUMP progress information log message is incorrect.</summary>
      <description>Incremental REPL DUMP have the progress information logged as "eventsDumpProgress":"49/0".It should actually log the estimated number of events are denominator but it is coming as 0 always.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventsCountRequest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.events.EventUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="20545" opendate="2018-9-12 00:00:00" fixdate="2018-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to exclude potentially large parameters in HMS Notifications</summary>
      <description>Clients can add large-sized parameters in Table/Partition objects. So we need to enable adding regex patterns through HiveConf to match parameters to be filtered from table and partition objects before serialization in HMS notifications.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20546" opendate="2018-9-12 00:00:00" fixdate="2018-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Apache Druid 0.13.0-incubating</summary>
      <description>This task is to upgrade to druid 0.13.0 when it is released. Note that it will hopefully be first apache release for Druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.ts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.ForkingDruidNode.java</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe2.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DerbyConnectorTestUtility.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.RetryIfUnauthorizedResponseHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.ResponseCookieHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.KerberosHttpClient.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorReport.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorIOConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroStreamInputRowParser.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroParseSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidKafkaUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20553" opendate="2018-9-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>more acid stats tests</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.acid.stats2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.stats2.q</file>
    </fixedFiles>
  </bug>
  <bug id="20558" opendate="2018-9-14 00:00:00" fixdate="2018-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default of hive.hashtable.key.count.adjustment to 0.99</summary>
      <description>Current default is 2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20561" opendate="2018-9-14 00:00:00" fixdate="2018-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the position of the Kafka Consumer to track progress instead of Consumer Records offsets</summary>
      <description>Kafka Partitions with transactional messages (post 0.11) will include commit or abort markers which indicate the result of a transaction. The markers are not returned to applications, yet have an offset in the log. Therefore the end of Stream position can be the offset of a control message. This Patch change the way how we keep track of the consumer position by using consumer.position(topicP) as oppose to using the offset of the consumed messages.Also I have done some refactoring to help code readability hopefully.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaRecordIteratorTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaRecordIterator.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaPullerRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="20572" opendate="2018-9-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of hive.tez.llap.min.reducer.per.executor</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.input.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skiphf.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.count.distinct.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.decimal64.reader.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.except.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.non.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20582" opendate="2018-9-17 00:00:00" fixdate="2018-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make hflush in hive proto logging configurable</summary>
      <description>Hive proto logging does hflush to avoid small files issue in hdfs. This may not be ideal for blobstorage where hflush gets applied only on closing of the file. Make hflush configurable so that blobstorage can do close instead of hflush. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20612" opendate="2018-9-20 00:00:00" fixdate="2018-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create new join multi-key correlation flag for CBO</summary>
      <description>Currently we reuse the flag in Hive side. It would be good to have the flag separated for debugging purposes.</description>
      <version>4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20621" opendate="2018-9-22 00:00:00" fixdate="2018-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetOperationStatus called in resultset.next causing incremental slowness</summary>
      <description>Fetching result set for a result cache hit query gets slower as more rows are fetched. For fetching 10 row result set it took about 900ms but fetching 200 row result set took 8 seconds. Reason for this slowness is GetOperationStatus is invoked inside resultset.next() and it happens for every row even after operation has completed. This is one RPC call per row fetched (there is also connection overhead without keepalive). </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="20623" opendate="2018-9-22 00:00:00" fixdate="2018-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared work: Extend sharing of map-join cache entries in LLAP</summary>
      <description>For a query like thiswith all_sales as (select ss_customer_sk as customer_sk, ss_ext_list_price-ss_ext_discount_amt as ext_price from store_salesUNION ALLselect ws_bill_customer_sk as customer_sk, ws_ext_list_price-ws_ext_discount_amt as ext_price from web_salesUNION ALLselect cs_bill_customer_sk as customer_sk, cs_ext_sales_price - cs_ext_discount_amt as ext_price from catalog_sales)select sum(ext_price) total_price, c_customer_id from all_sales, customer where customer_sk = c_customer_skgroup by c_customer_idorder by total_price desc limit 100;The hashtable used for all 3 joins are identical, which is loaded 3x times in the same LLAP instance because they are named. cacheKey = "HASH_MAP_" + this.getOperatorId() + "_container";in the cache.If those are identical in nature (i.e vectorization, hashtable type etc), then the duplication is just wasted CPU, memory and network - using the cache name for hashtables which will be identical in layout would be extremely useful.In cases where the join is pushed through a UNION, those are identical.This optimization can only be done without concern for accidental delays when the same upstream task is generating all of these hashtables, which is what is achieved by the shared scan optimizer already.In case the shared work is not present, this has potential downsides - in case two customer broadcasts were sourced from "Map 1" and "Map 2", the Map 1 builder will block the other task from reading from Map 2, even though Map 2 might have started after, but finished ahead of Map 1.So this specific optimization can always be considered for cases where the shared work unifies the operator tree and the parents of all the RS entries involved are same (&amp; the RS layout is the same).</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20625" opendate="2018-9-23 00:00:00" fixdate="2018-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regex patterns not working in SHOW MATERIALIZED VIEWS &amp;#39;&lt;pattern&gt;&amp;#39;</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.materialized.views.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.show.materialized.views.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20626" opendate="2018-9-24 00:00:00" fixdate="2018-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log more details when druid metastore transaction fails in callback</summary>
      <description>Below exception does not give much details on what is the actual cause of the error. We also need to log the callback exception when we get it. Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Transaction failed do to exception being thrown from within the callback. See cause for the original exception.) at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:932) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634] at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:937) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634] at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4954) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634] at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:428) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634] at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2668) ~[hive-exec-3.1.0.3.0.0.0-1634.jar:3.1.0.3.0.0.0-1634]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20629" opendate="2018-9-25 00:00:00" fixdate="2018-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive incremental replication fails with events missing error if database is kept idle for more than an hour</summary>
      <description>Start a source cluster with 2 database. Replicate the databases to target after doing some operations. Keep taking incremental dump for both database and keep replicating them to target cluster. Keep one the database idle for more than 24 hrs. After 24 hrs, the incremental dump of idle database fails with event missing error.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadEventsIterator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="20631" opendate="2018-9-25 00:00:00" fixdate="2018-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive returns 20011 error code for re-triable error</summary>
      <description>In case of network issue .repl load is returning non retry-able error code. The scenario is 1. While copying the file, repl load found that source is not reachable and went for copy retry.2. While retying, getting file checksum failed due to network issue and thus its assumed that the source file is not present. So in the next retry copy is tried from cm path.3. In the next retry, network is recovered and it in cm path no file was found. This will cause return of non retry-able error.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20636" opendate="2018-9-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve number of null values estimation after outer join</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20640" opendate="2018-9-26 00:00:00" fixdate="2018-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to use ORC 1.5.3</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20646" opendate="2018-9-27 00:00:00" fixdate="2018-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partition filter condition is not pushed down to metastore query if it has IS NOT NULL.</summary>
      <description>If the partition filter condition has "is not null" then the filter query isn't getting pushed to the SQL query in RDMBS. This slows down metastore api calls for getting list of partitions with filter condition.This condition gets added by optimizer in many cases so this is affecting many queries.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.MockPartitionExpressionForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartFilterExprUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.PartitionExpressionProxy.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20648" opendate="2018-9-27 00:00:00" fixdate="2018-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Vector group by operator should use memory per executor</summary>
      <description>HIVE-15503 treatment has to be applied for vector group by operator as well. Vector group by currently uses MemoryMX bean to get heap usage and heap max memory which will not work for LLAP. Instead it should use memory per executor as upper bound to make flush decision.  </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20649" opendate="2018-9-27 00:00:00" fixdate="2018-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP aware memory manager for Orc writers</summary>
      <description>ORC writer has its own memory manager that assumes memory usage or memory available based on JVM heap (MemoryMX bean). This works on tez container mode execution model but not in LLAP where container sizes (and Xmx) are typically high and there are multiple executors per LLAP daemon. This custom memory manager should be aware of memory bounds per executor. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20656" opendate="2018-9-28 00:00:00" fixdate="2018-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sensible defaults: Map aggregation memory configs are too aggressive</summary>
      <description>The defaults for the following configs seems to be too aggressive. In java this can easily lead to several full GC pauses whose memory cannot be reclaimed.HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.99, "Portion of total memory to be used by map-side group aggregation hash table"),HIVEMAPAGGRMEMORYTHRESHOLD("hive.map.aggr.hash.force.flush.memory.threshold", (float) 0.9, "The max memory to be used by map-side group aggregation hash table.\n" + "If the memory usage is higher than this number, force to flush data"), We can be little bit conservative for these configs to avoid getting into GC pause. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20657" opendate="2018-9-28 00:00:00" fixdate="2018-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-allocate LLAP cache at init time</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocatorForceEvict.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20660" opendate="2018-9-30 00:00:00" fixdate="2018-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by statistics estimation could be improved by bounding the total number of rows to source table</summary>
      <description>Currently the stats for group by is estimated by taking product of NDVs of all the keys and bounding it by the number of rows of its input. This bound could be improved by using the source table instead of immediate input, the insight in this case is that cardinality/ndvs of a table can not go beyond the original (outer joins will only add NULLs thereby increasing the cardinality by 1).Note that the assumption here is that group by keys all belong to the same source table/input.This will improve the estimation in situations where group by is executed after joins wherein Hive could end up estimating the number of rows.Reproducerset hive.stats.fetch.column.stats=true;create table t1(i int, j int);alter table t1 update statistics set('numRows'='10000', 'rawDataSize'='18000');alter table t1 update statistics for column i set('numDVs'='2500','numNulls'='50','highValue'='1000','lowValue'='0');alter table t1 update statistics for column j set('numDVs'='500','numNulls'='30','highValue'='100','lowValue'='50');create table t2(i2 int, j2 int);alter table t2 update statistics set('numRows'='100000000', 'rawDataSize'='10000');alter table t2 update statistics for column i2 set('numDVs'='10000000','numNulls'='0','highValue'='8000','lowValue'='0');alter table t2 update statistics for column j2 set('numDVs'='10','numNulls'='0','highValue'='800','lowValue'='-1');explain select count (1) from t1,t2 where t1.j=t2.i2 group by t1.i, t1.j;Reducer 2 Reduce Operator Tree: Merge Join Operator condition map: Inner Join 0 to 1 keys: 0 _col1 (type: int) 1 _col0 (type: int) outputColumnNames: _col0, _col1 Statistics: Num rows: 99700 Data size: 797288 Basic stats: COMPLETE Column stats: COMPLETE Group By Operator aggregations: count() keys: _col0 (type: int), _col1 (type: int) mode: hash outputColumnNames: _col0, _col1, _col2 Statistics: Num rows: 49850 Data size: 797448 Basic stats: COMPLETE Column stats: COMPLETE &lt;========== Reduce Output Operator key expressions: _col0 (type: int), _col1 (type: int) sort order: ++ Map-reduce partition columns: _col0 (type: int), _col1 (type: int) Statistics: Num rows: 49850 Data size: 797448 Basic stats: COMPLETE Column stats: COMPLETE value expressions: _col2 (type: bigint) .....................</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20662" opendate="2018-10-1 00:00:00" fixdate="2018-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part3]</summary>
      <description>TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;load_dyn_part3&amp;#93; is more than flaky.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="20680" opendate="2018-10-3 00:00:00" fixdate="2018-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap is missing partitions in replicated DB when retry after kill query.</summary>
      <description>The issue is1. When bootstrap was going on, kill query was called to kill the repl load command.2. During restart, one table with no partition set the scope to table as the ckpt property was not yet set for that table.3. Due to this, all partitioned table after this didn't get their tasks related to partition appended in the root task if the ckpt property is already updated for those tables.The fix is to reset the table scope to false if for that table there are no tasks added.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20686" opendate="2018-10-3 00:00:00" fixdate="2018-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sync QueryIDs across hive and druid</summary>
      <description>For the queries that hive passes to druid, pass on additional queryID as query context. It will be useful in tracing query level metrics across druid and hive.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="20701" opendate="2018-10-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow HiveStreaming to receive a key value to commit atomically together with the transaction</summary>
      <description>Following up with HIVE-20538 it'd be nice to be able to use this feature with hive streaming</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.UnManagedSingleTransaction.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionBatch.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingTransaction.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractStreamingTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug id="20702" opendate="2018-10-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Account for overhead from datastructure aware estimations during mapjoin selection</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="20705" opendate="2018-10-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Native Vector MapJoin doesn&amp;#39;t support Complex Big Table values</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20707" opendate="2018-10-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatic partition management</summary>
      <description>In current scenario, to add partitions for external tables to metastore, MSCK REPAIR command has to be executed manually. To avoid this manual step, external tables can be specified a table property based on which a background metastore thread can sync partitions periodically. Tables can also be specified with partition retention period. Any partition whose age exceeds the retention period will be dropped automatically.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestCatalogOldClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.NonCatCallsWithCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetTableMeta.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.alter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.2.exim.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.msck.repair.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.whroot.external1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.nonprintable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.exim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.table.purge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.locks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckDropPartitionsInBatches.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckCreatePartitionsInBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.CheckResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20711" opendate="2018-10-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Race Condition when Multi-Threading in SessionState.createRootHDFSDir</summary>
      <description>java.util.concurrent.ExecutionException: java.lang.RuntimeException: The root scratch dir: /home/hiveptest/hive-ptest-cloudera-slaves-17e5-13.gce.cloudera.com-hiveptest-0/cdh-source/itests/hive-unit/target/tmp/scratchdir on HDFS should be writable. Current permissions are: rwxr-xr-x at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:714) at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:637) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:567) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:532) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:512) at</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.exec.spark.TestSparkSessionTimeout.java</file>
    </fixedFiles>
  </bug>
  <bug id="20719" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SELECT statement fails after UPDATE with hive.optimize.sort.dynamic.partition optimization and vectorization on</summary>
      <description>Reproducer set hive.optimize.sort.dynamic.partition=true ;create table acid_uap(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');insert into table acid_uap partition (ds='tomorrow') values (1, 'bah'),(2, 'yah') ;insert into table acid_uap partition (ds='today') values (1, 'bah'),(2, 'yah') ;select a,b,ds from acid_uap order by a,b;update acid_uap set b = 'fred';select a,b,ds from acid_uap order by a,b;ErrorStatus: FailedVertex failed, vertexName=Map 1, vertexId=vertex_1539123809352_0001_5_00, diagnostics=[Task failed, taskId=task_1539123809352_0001_5_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1539123809352_0001_5_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1539123809352_0001_5_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1539123809352_0001_5_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1539123809352_0001_5_00, diagnostics=[Task failed, taskId=task_1539123809352_0001_5_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1539123809352_0001_5_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 16 moreCaused by: java.io.IOException: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203) ... 21 moreCaused by: java.io.IOException: Corrupted records with different bucket ids from the containing bucket file found! Expected bucket id 0, however found DeleteRecordKey(2,536936448(1.1.0),0). (OrcSplit [file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delta_0000002_0000002_0000/bucket_00000, start=3, length=361, isOriginal=false, fileLength=798, hasFooter=false, hasBase=true, deltas=2],file:/Users/vgarg/hive_temp/vgarg/hive/warehouse/dp_sort.db/acid_uap/ds=today/delete_delta_0000003_0000003_0000/bucket_00000) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.checkBucketId(VectorizedOrcAcidRowBatchReader.java:1441) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry$DeleteReaderValue.next(VectorizedOrcAcidRowBatchReader.java:1362) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.readAllDeleteEventsFromDeleteDeltas(VectorizedOrcAcidRowBatchReader.java:1644) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:1567) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:243) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:138) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.&lt;init&gt;(VectorizedOrcAcidRowBatchReader.java:133) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2003) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:416) ... 22 more]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1539123809352_0001_5_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1539123809352_0001_5_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1539123809352_0001_5_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20737" opendate="2018-10-12 00:00:00" fixdate="2018-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local SparkContext is shared between user sessions and should be closed only when there is no active</summary>
      <description>1. Local SparkContext is shared between user sessions and should be closed only when there is no active.  2. Possible race condition in SparkSession.open() in case when user queries run in parallel within the same session.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="20744" opendate="2018-10-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use SQL constraints to improve join reordering algorithm</summary>
      <description>Till now, it was all based on stats stored for the base tables and their columns. Now the optimizer can rely on constraints. Hence, this patch is for the join reordering costing to use constraints, and if it does not find any, rely on old code path.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query17.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinConstraintsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="20746" opendate="2018-10-15 00:00:00" fixdate="2018-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveProtoHookLogger does not close file at end of day.</summary>
      <description>The file rotation happens with an event currently. If there are no queries fired for a long time, then the file rotation does not happen and we do not close the file. This causes the clients to poll for the file for an indeterminate amount of time. If there are multiple hiveservers there is no way to tell which file will get more data. Fix this to close at end of day.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestHiveProtoLoggingHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20748" opendate="2018-10-16 00:00:00" fixdate="2018-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable materialized view rewriting when plan pattern is not allowed</summary>
      <description>For instance, currently rewriting algorithm does not support some operators. Or we cannot have non-deterministic function in the MV definition. In those cases, we should fail either when we try to create the MV with rewriting enabled, or when when we enable the rewriting for a MV already created.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOpMaterializationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20752" opendate="2018-10-16 00:00:00" fixdate="2018-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In case of LLAP start failure add info how to find YARN logs</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2076" opendate="2011-3-24 00:00:00" fixdate="2011-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Metastore upgrade scripts and default schemas for PostgreSQL</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="20761" opendate="2018-10-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select for update on notification_sequence table has retry interval and retries count too small.</summary>
      <description>Hive DDL's are intermittently failing with Error- Couldn't acquire the DB log notification lock because we reached the maximum # of retries: 5 retriesError while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Couldn't acquire the DB log notification lock because we reached the maximum # of retries: 5 retries. If this happens too often, then is recommended to increase the maximum number of retries on the hive.notification.sequence.lock.max.retries configuration :: Error executing SQL query "select "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" for update".)2018-08-28 01:17:56,808|INFO|MainThread|machine.py:183 - run()||GUID=94e6ff4d-5db8-45eb-8654-76f546e7f0b3|java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Couldn't acquire the DB log notification lock because we reached the maximum # of retries: 5 retries. If this happens too often, then is recommended to increase the maximum number of retries on the hive.notification.sequence.lock.max.retries configuration :: Error executing SQL query "select "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" for update".)It seems, metastore operations are slow in this cluster and hence concurrent writes/DDL operations are failing to lock the row for update.Currently, the sleep interval between retries is specified via the config hive.notification.sequence.lock.retry.sleep.interval. The default value is 500 ms which seems to be too small. Can we set higher values for sleep interval and retries count,hive.notification.sequence.lock.retry.sleep.interval=10shive.notification.sequence.lock.max.retries=10</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20762" opendate="2018-10-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NOTIFICATION_LOG cleanup interval is hardcoded as 60s and is too small.</summary>
      <description>NOTIFICATION_LOG cleanup interval is hardcoded as 60s and is too small. It should be set to several hours or else the number of metastore calls would be too high and impact other operations.Make it configurable item and set it as 2Hrs.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="20763" opendate="2018-10-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add google cloud storage (gs) to the exim uri schema whitelist</summary>
      <description>import/export is enabled for s3a by default. Ideally this list should include other cloud storage options. This Jira adds Google Storage to the list.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20765" opendate="2018-10-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fetch partitions for txn stats validation in get_aggr_stats with one call</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionProjectionEvaluator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20767" opendate="2018-10-17 00:00:00" fixdate="2018-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple project between join operators may affect join reordering using constraints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="20768" opendate="2018-10-18 00:00:00" fixdate="2018-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding Tumbling Window UDF</summary>
      <description>Goal is to provide a UDF that truncates a timestamp to a beginning of a tumbling window interval./** * Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. * Tumbling windows are inclusive start exclusive end. * By default the beginning instant of fist window is Epoch 0 Thu Jan 01 00:00:00 1970 UTC. * Optionally users may provide a different origin as a timestamp arg3. * * This an example of series of window with an interval of 5 seconds and origin Epoch 0 Thu Jan 01 00:00:00 1970 UTC: * * * interval 1 interval 2 interval 3 * Jan 01 00:00:00 Jan 01 00:00:05 Jan 01 00:00:10 * 0 -------------- 4 : 5 --------------- 9: 10 --------------- 14 * * This UDF rounds timestamp agr1 to the beginning of window interval where it belongs to. * */</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="20775" opendate="2018-10-18 00:00:00" fixdate="2018-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Factor cost of each SJ reduction when costing a follow-up reduction</summary>
      <description>Currently, while costing the SJ in a plan, the stats of the a TS that is reduced by a SJ are not adjusted after we have decided to keep a SJ in the tree. Ideally, we could adjust the stats to take into account decisions that have already been made.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query66.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20796" opendate="2018-10-24 00:00:00" fixdate="2018-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc URL can contain sensitive information that should not be logged</summary>
      <description>It is possible to put passwords in the jdbc connection url and some jdbc drivers will supposedly use that. (derby, mysql). This information is considered sensitive, and should be masked out, while logging the connection url.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20801" opendate="2018-10-24 00:00:00" fixdate="2018-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Allow DbTxnManager to ignore non-ACID table read locking</summary>
      <description>Enabling ACIDv1 on a cluster produces a central locking bottleneck for all table types, which is not always the intention.The Hive locking for non-acid tables are advisory (i.e a client can write/read without locking), which means that the implementation does not offer strong consistency despite the lock manager consuming resources centrally.Disabling this lock acquisition would improve the performance of non-ACID tables co-existing with a globally configured DbTxnManager implementation.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20806" opendate="2018-10-25 00:00:00" fixdate="2018-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ASF license for files added in HIVE-20679</summary>
      <description>HIVE-20679 added couple of new files Deserialzer/Serialzer that needs the ASF license header.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.gzip.Serializer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.gzip.DeSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20820" opendate="2018-10-26 00:00:00" fixdate="2018-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MV partition on clause position</summary>
      <description>It should obey the following syntax as per https://cwiki.apache.org/confluence/display/Hive/Materialized+views :CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [DISABLE REWRITE] [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)]AS&lt;query&gt;;Currently it is positioned just before TBLPROPERTIES.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="20821" opendate="2018-10-26 00:00:00" fixdate="2018-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite SUM0 into SUM + COALESCE combination</summary>
      <description>Since SUM0 is not vectorized, but SUM + COALESCE are.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateReduceFunctionsRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="20822" opendate="2018-10-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvements to push computation to JDBC from Calcite</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedwork.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.external.jdbc.table2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCSortPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCProjectPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAggregationPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.HiveJdbcImplementor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.jdbc.JdbcHiveTableScan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.jdbc.HiveJdbcConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveBetween.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20835" opendate="2018-10-30 00:00:00" fixdate="2018-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interaction between constraints and MV rewriting may create loop in Calcite planner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query11.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectJoinTransposeRule.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20841" opendate="2018-10-30 00:00:00" fixdate="2018-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make dynamic ports configurable</summary>
      <description>Some ports in llap -&gt; tez interaction code uses dynamic ports, provide an option to make them configurable to facilitate adding them to iptable rules in some environment. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20842" opendate="2018-10-30 00:00:00" fixdate="2018-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix logic introduced in HIVE-20660 to estimate statistics for group by</summary>
      <description>HIVE-20660 introduced better estimation for group by operator. But the logic did not account for Partial and Full group by separately.For partial group by parallelism (i.e. number of tasks) should be taken into account.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20850" opendate="2018-10-31 00:00:00" fixdate="2018-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push case conditional from projections to dimension tables if possible</summary>
      <description>noticed by gopalv: If there is a project which could be only evaluated after the join; but the condition references only a single column from a small dimension table; hive will end up evaluating the same thing over and over again...explainselect s_store_name, s_store_id, sum(case when (d_day_name='Sunday') then ss_sales_price else null end) sun_sales, sum(case when (d_day_name='Monday') then ss_sales_price else null end) mon_sales, sum(case when (d_day_name='Tuesday') then ss_sales_price else null end) tue_sales, sum(case when (d_day_name='Wednesday') then ss_sales_price else null end) wed_sales, sum(case when (d_day_name='Thursday') then ss_sales_price else null end) thu_sales, sum(case when (d_day_name='Friday') then ss_sales_price else null end) fri_sales, sum(case when (d_day_name='Saturday') then ss_sales_price else null end) sat_sales from date_dim, store_sales, store where d_date_sk = ss_sold_date_sk and s_store_sk = ss_store_sk and s_gmt_offset = -6 and d_year = 1998 group by s_store_name, s_store_id order by s_store_name, s_store_id,sun_sales,mon_sales,tue_sales,wed_sales,thu_sales,fri_sales,sat_sales limit 100;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.join46.mr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez-tag.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.use.ts.stats.for.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.position.alias.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.with.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.multi.output.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedworkext.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.q93.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.shared.scan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.prod.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.AbstractJdbcTriggersTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.inclause.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.const.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20862" opendate="2018-11-3 00:00:00" fixdate="2018-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryId no longer shows up in the logs</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnCommonUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20891" opendate="2018-11-8 00:00:00" fixdate="2018-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Call alter_partition in batch when dynamically loading partitions</summary>
      <description>When dynamically loading partitions, the setStatsPropAndAlterPartition() is called for each partition one by one, resulting in unnecessary calls to the metastore client. This whole logic can be changed to just one call. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.metastore.SynchronizedMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="20893" opendate="2018-11-8 00:00:00" fixdate="2018-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BloomK Filter probing method is not thread safe</summary>
      <description>As far i can tell this is not an issue for Hive yet (most of the usage of probing seems to be done by one thread at a time) but it is an issue of other users like Druid as per the following issue.https://github.com/apache/incubator-druid/issues/6546The fix is proposed by the author of https://github.com/apache/incubator-druid/pull/6584 is to make couple of local fields as ThreadLocals.Idea looks good to me and doesn't have any perf drawbacks. </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomKFilter.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20897" opendate="2018-11-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestJdbcDriver2#testSelectExecAsync2 fails with result set not present error</summary>
      <description>if async prepare is enabled, control will be returned to the client before driver could set of the query has a result set or not. But in current code, while generating the response for the query, it is not checked if the result set field is set or not. </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationStatus.java</file>
    </fixedFiles>
  </bug>
  <bug id="20898" opendate="2018-11-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For time related functions arguments may not be casted to a non nullable type</summary>
      <description>create table t (a string); insert into t values (null),('1988-11-11');set hive.cbo.enable=true;select 'expected 1 (second)', count(1) from t where second(a) is null;this may only cause trouble if Calcite is exploiting the datatype nullability; it will be need after CALCITE-2469 (1.18.0)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20903" opendate="2018-11-10 00:00:00" fixdate="2018-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup code inspection issue on the druid adapter.</summary>
      <description>This is a simple cleanup of the code and minor refactor.I did not change any of the behavior. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DerbyConnectorTestUtility.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidWritable.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDeUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.RetryResponseHolder.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.RetryIfUnauthorizedResponseHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.ResponseCookieHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.KerberosHttpClient.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.DruidKerberosUtil.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.TaskReportData.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorReport.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorIOConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.HiveDruidSplit.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerInfo.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="20904" opendate="2018-11-12 00:00:00" fixdate="2018-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yetus fails to resolve module dependencies due to usage of exec plugin in metastore-server</summary>
      <description>metastore-server uses exec-maven-plugin to generate metastore-site.xml.template with ConfTemplatePrinter.It expects some arguments. Because yetus also uses the exec-maven-plugin to determine the order of the modules to be built, but with zero params, the execution fails.https://github.com/apache/yetus/blob/6ebaa1119e611db14f219e289e33ab8ac5c254a7/precommit/src/main/shell/test-patch.d/maven.sh#L658Steps to reproduce the issue:mvn -q exec:exec -Dexec.executable=pwd -Dexec.args=''</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20905" opendate="2018-11-12 00:00:00" fixdate="2018-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>querying streaming table fails with out of memory exception</summary>
      <description>Streaming app was ran for 24hrs post which it went down due authentication issue . The table was accessible for 12hrs into the run, however currently querying the table fails with OOM exception.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="20915" opendate="2018-11-14 00:00:00" fixdate="2018-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make dynamic sort partition optimization available to HoS and MR</summary>
      <description>HIVE-20703 put dynamic sort partition optimization under cost based decision, but it also makes the optimizer only available to tez. hive.optimize.sort.dynamic.partition works with other execution engines for a long time, we should keep the optimizer available to them.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.orc.format.part.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.orc.nonstd.partitions.loc.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.parquet.format.part.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.parquet.nonstd.partitions.loc.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.rcfile.format.part.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.rcfile.nonstd.partitions.loc.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge1.q</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast.during.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.16.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20918" opendate="2018-11-14 00:00:00" fixdate="2018-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flag to enable/disable pushdown of computation from Calcite into JDBC connection</summary>
      <description>Currently, there is no way to disable it. We will add a flag for that. By default, pushdown of computation will be enabled.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.external.jdbc.table2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20926" opendate="2018-11-15 00:00:00" fixdate="2018-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join reduction hint fails when bloom filter entries are high or when there are no stats</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="20948" opendate="2018-11-20 00:00:00" fixdate="2018-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate file rename in compactor</summary>
      <description>Once HIVE-20823 is committed, we should investigate if it's possible to have compactor write directly to base_x_cZ or delta_x_y_cZ.  For query based compaction: can we control location of temp table dir?  We support external temp tables so this may work but we'd need to have non-acid insert create files with bucket_xxxxx names. For MR/Tez/LLAP based (should this be done at all?), need to figure out how retries of tasks will work.  Just like we currently generate an MR job to compact, we should be able to generate a Tez job. </description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="20949" opendate="2018-11-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve PKFK cardinality estimation in physical planning</summary>
      <description>Missing case for cartesian product and full outer joins.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query6.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20951" opendate="2018-11-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Set Xms to 50% always</summary>
      <description>The lack of GC pauses is killing LLAP containers whenever the significant amount of memory is consumed by the off-heap structures which aren't cleaned up automatically until the GC runs.There's a java.nio.DirectByteBuffer.Deallocator which runs when the Direct buffers are garbage collected, which actually does the cleanup of the underlying off-heap buffers.The lack of Garbage collection activity for several hours while responding to queries triggers a build-up of these off-heap structures which end up forcing YARN to kill the process instead.It is better to hit a GC pause occasionally rather than to lose a node every few hours.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20953" opendate="2018-11-21 00:00:00" fixdate="2018-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove a function from function registry when it can not be added to the metastore when creating it.</summary>
      <description>The testcase is intended to test REPL LOAD with retry. The test creates a partitioned table and a function in the source database and loads those to the replica. The first attempt to load a dump is intended to fail while loading one of the partitions. Based on the order in which the objects get loaded, if the function is queued after the table, it will not be available in replica after the load failure. But if it's queued before the table, it will be available in replica even after the load failure. The test assumes the later case, which may not be true always.Hence fix the testcase to order the objects by a fixed ordering. By setting hive.in.repl.test.files.sorted to true, the objects are ordered by the directory names. This ordering is available with minimal changes for testing, hence we use it. With this ordering a function gets loaded before a table. So changed the test to not expect the function to be available after the failed load, but be available after the retry.While writing that testcase, I found that even if a function fails to load, it's visible through show functions and also is available to be called just as if the failure has not happened. Digging further it was found that when creating a function we add it to the registry and also to the metastore. If the later fails, we do not clean it up from the registry and thus it remains visible after failure. Fixed the same.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="20961" opendate="2018-11-22 00:00:00" fixdate="2018-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retire NVL implementation</summary>
      <description>Right now we have coalesce and nvl implemented separetly; it might be better to remove one of them as they are doing the same. Because Coalesce is in the standard - I think NVL have to go...and became an alias to Coalesce.Further benefit is: that optimizations which coalesce already recieves; will be done on NVL calls as well.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropWhen.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nvl.mismatch.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20967" opendate="2018-11-25 00:00:00" fixdate="2018-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle alter events when replicate to cluster with hive.strict.managed.tables enabled.</summary>
      <description>Some of the events from Hive2 may cause conflicts in Hive3 (hive.strict.managed.tables=true) when applied. So, need to handle them properly. 1. Alter table to convert non-acid to acid. Do not allow this conversion on source of replication if strict.managed is false.2. Alter table or partition that changes the location. For managed tables at source, the table location shouldn't be changed for the given non-partitioned table and partition location shouldn't be changed for partitioned table as alter event doesn't capture the new files list. So, it may cause data inconsistsency. So, if database is enabled for replication at source, then alter location on managed tables should be blocked. For external partitioned tables, if location is changed at source, the the location should be changed for the table and any partitions which reside within the table location, but not for the partitions which are not within the table location. (may be we just need the test).</description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.BaseReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="20969" opendate="2018-11-26 00:00:00" fixdate="2018-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS sessionId generation can cause race conditions when uploading files to HDFS</summary>
      <description>The observed exception is:Caused by: java.io.FileNotFoundException: File does not exist: /tmp/hive/_spark_session_dir/0/hive-exec-2.1.1-SNAPSHOT.jar (inode 21140) [Lease. Holder: DFSClient_NONMAPREDUCE_304217459_39, pending creates: 1] at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2781) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2660) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:872) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:550) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:815) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2675)</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20992" opendate="2018-11-30 00:00:00" fixdate="2018-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split the config "hive.metastore.dbaccess.ssl.properties" into more meaningful configs</summary>
      <description>HIVE-13044 brought in the ability to enable TLS encryption from the HMS Service to the HMSDB by configuring the following two properties: javax.jdo.option.ConnectionURL: JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. (E.g. "jdbc:postgresql://myhost/db?ssl=true") hive.metastore.dbaccess.ssl.properties: Comma-separated SSL properties for metastore to access database when JDO connection URL. (E.g. javax.net.ssl.trustStore=/tmp/truststore,javax.net.ssl.trustStorePassword=pwd)However, the latter configuration option is opaque and poses some problems. The most glaring of which is it takes in any java.lang.System system property, whether it is TLS-related or not. This can cause some unintended side-effects for other components of the HMS, especially if it overrides an already-set system property. If the user truly wishes to add an unrelated Java property, setting it statically using the "-D" option of the java command is more appropriate. Secondly, the truststore password is stored in plain text. We should add Hadoop Shims back to the HMS to prevent exposing these passwords, but this effort can be done after this ticket.I propose we deprecate hive.metastore.dbaccess.ssl.properties, and add the following properties: hive.metastore.dbaccess.ssl.use.SSL (metastore.dbaccess.ssl.use.SSL) Set this to true to for using SSL/TLS encryption from the HMS Service to the HMS backend store Default: false hive.metastore.dbaccess.ssl.truststore.path (metastore.dbaccess.ssl.truststore.path) Truststore location Directly maps to javax.net.ssl.trustStore System property Default: None E.g. /tmp/truststore hive.metastore.dbaccess.ssl.truststore.password (metastore.dbaccess.ssl.truststore.password) Truststore password Directly maps to javax.net.ssl.trustStorePassword System property Default: None E.g. password hive.metastore.dbaccess.ssl.truststore.type (metastore.dbaccess.ssl.truststore.type) Truststore type Directly maps to javax.net.ssl.trustStoreType System property Default: JKS E.g. pkcs12 We should guide the user towards an easier TLS configuration experience. This is the minimum configuration necessary to configure TLS to the HMSDB. If we need other options, such as the keystore location/password for dual-authentication, then we can add those on afterwards.Also, document these changes - javax.jdo.option.ConnectionURL does not have up-to-date documentation, and these new parameters will need documentation as well.Note "TLS" refers to both SSL and TLS. TLS is simply the successor of SSL.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21022" opendate="2018-12-10 00:00:00" fixdate="2018-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix remote metastore tests which use ZooKeeper</summary>
      <description>Per vgarg's comment on HIVE-20794 at https://issues.apache.org/jira/browse/HIVE-20794?focusedCommentId=16714093&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16714093, the remote metatstore tests using ZooKeeper are flaky. They are failing with error "Got exception: org.apache.zookeeper.KeeperException$NoNodeException KeeperErrorCode = NoNode for /hs2mszktest".Both of these tests are using the same root namespace and hence the reason for this failure could be that the root namespace becomes unavailable to one test when the other drops it. The drop seems to be happening automatically through TestingServer code.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreZK.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.ZooKeeperHiveHelper.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreZKBindHost.java</file>
    </fixedFiles>
  </bug>
  <bug id="21023" opendate="2018-12-10 00:00:00" fixdate="2018-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test for replication to a target with hive.strict.managed.tables enabled</summary>
      <description>Tests added are timing out in ptest run. Need to skip these test cases from batching and run them separately.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21043" opendate="2018-12-14 00:00:00" fixdate="2018-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable move optimization for cloud replication with strict managed tables.</summary>
      <description>If hive.repl.enable.move.optimization is set to true, then Hive REPL LOAD avoids move operation and copy the data files directly to target location. This is helpful for Cloud replication where the move is non-atomic and also implemented as copy.Currently, if hive.strict.managed.tables is enabled at target and if migration to transactional table is needed during REPL LOAD, then this optimization is disabled.Need to support it.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
    </fixedFiles>
  </bug>
  <bug id="21048" opendate="2018-12-16 00:00:00" fixdate="2018-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove needless org.mortbay.jetty from hadoop exclusions</summary>
      <description>During HIVE-20638 i found that org.mortbay.jetty exclusions from e.g. hadoop don't take effect, as the actual groupId of jetty is org.eclipse.jetty for most of the current projects, please find attachment (example for hive commons project).https://en.wikipedia.org/wiki/Jetty_(web_server)#History</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21071" opendate="2018-12-27 00:00:00" fixdate="2018-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve getInputSummary</summary>
      <description>There is a global lock in the getInptSummary code, so it is important that it be fast. The current implementation has quite a bit of overhead that can be re-engineered.For example, the current implementation keeps a map of File Path to ContentSummary object. This map is populated by several threads concurrently. The method then loops through the map, in a single thread, at the end to add up all of the ContentSummary objects and ignores the paths. The code can be be re-engineered to not use a map, or a collection at all, to store the results and instead just keep a running tally. By keeping a tally, there is no O&amp;#40;n) operation at the end to perform the addition.There are other things can be improved. The method returns an object which is never used anywhere, so change method to void return type.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="21073" opendate="2018-12-27 00:00:00" fixdate="2018-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Extra String Object</summary>
      <description>public static String generatePath(Path baseURI, String filename) { String path = new String(baseURI + Path.SEPARATOR + filename); return path; } public static String generateFileName(Byte tag, String bigBucketFileName) { String fileName = new String("MapJoin-" + tag + "-" + bigBucketFileName + suffix); return fileName; }It's a bit odd to be performing string concatenation and then wrapping the results in a new string. This is creating superfluous String objects.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="21085" opendate="2019-1-4 00:00:00" fixdate="2019-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Materialized views registry starts non-external tez session</summary>
      <description>Materialized views registry is doing SessionState.start() which will start regular tez session. In the presence of external tez sessions, it should get session from external sessions pool. There are also other places where SessionState.start() might be invoked.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="21116" opendate="2019-1-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HADOOP_CREDSTORE_PASSWORD is not populated under yarn.app.mapreduce.am.admin.user.env</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestHiveCredentialProviders.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21132" opendate="2019-1-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join edge is not being removed despite max bloomfilter entries set to 1</summary>
      <description>Reproducer--! qt:dataset:lineitem--! qt:dataset:part--! qt:dataset:srcset hive.support.concurrency=true;set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;--set hive.compute.query.using.stats=false;set hive.mapred.mode=nonstrict;set hive.explain.user=false;set hive.optimize.ppd=true;set hive.ppd.remove.duplicatefilters=true;set hive.tez.dynamic.partition.pruning=true;set hive.tez.dynamic.semijoin.reduction=true;set hive.optimize.metadataonly=false;set hive.optimize.index.filter=true;set hive.stats.autogather=true;set hive.tez.bigtable.minsize.semijoin.reduction=1;set hive.tez.min.bloom.filter.entries=1;set hive.stats.fetch.column.stats=true;set hive.tez.bloom.filter.factor=1.0f;set hive.auto.convert.join=false;set hive.optimize.shared.work=false;create database tpch_test;use tpch_test;CREATE TABLE `customer`( `c_custkey` bigint, `c_name` string, `c_address` string, `c_nationkey` bigint, `c_phone` string, `c_acctbal` double, `c_mktsegment` string, `c_comment` string)ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'TBLPROPERTIES ( 'bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default', 'transient_lastDdlTime'='1543026723');CREATE TABLE `lineitem`( `l_orderkey` bigint, `l_partkey` bigint, `l_suppkey` bigint, `l_linenumber` int, `l_quantity` double, `l_extendedprice` double, `l_discount` double, `l_tax` double, `l_returnflag` string, `l_linestatus` string, `l_shipdate` string, `l_commitdate` string, `l_receiptdate` string, `l_shipinstruct` string, `l_shipmode` string, `l_comment` string)ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'TBLPROPERTIES ( 'bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default', 'transient_lastDdlTime'='1543027179');CREATE TABLE `orders`( `o_orderkey` bigint, `o_custkey` bigint, `o_orderstatus` string, `o_totalprice` double, `o_orderdate` string, `o_orderpriority` string, `o_clerk` string, `o_shippriority` int, `o_comment` string)ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'TBLPROPERTIES ( 'bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default', 'transient_lastDdlTime'='1543026824');alter table customer update statistics set('numRows'='150000000','rawDataSize'='8633707142');alter table lineitem update statistics set('numRows'='5999989709','rawDataSize'='184245066955');alter table orders update statistics set('numRows'='1500000000','rawDataSize'='46741318253');create view q18_tmp_cached asselect l_orderkey, sum(l_quantity) as t_sum_quantityfrom lineitemwhere l_orderkey is not nullgroup by l_orderkey;-- Set bloom filter size to huge number so we get any possible semijoin reductionsset hive.tez.min.bloom.filter.entries=0;set hive.tez.max.bloom.filter.entries=1;create table q18_large_volume_customer_cached stored as orc tblproperties ('transactional'='true', 'transactional_properties'='default') asselect c_name, c_custkey, o_orderkey, o_orderdate, o_totalprice, sum(l_quantity)from customer, orders, q18_tmp_cached t, lineitem lwhere c_custkey = o_custkey and o_orderkey = t.l_orderkey and o_orderkey is not null and t.t_sum_quantity &gt; 300 and o_orderkey = l.l_orderkey and l.l_orderkey is not nullgroup by c_name, c_custkey, o_orderkey, o_orderdate, o_totalpriceorder by o_totalprice desc, o_orderdatelimit 100;drop database tpch_test cascade;To reproduce run the above as TestMiniLlapLocalCliDriver test</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21134" opendate="2019-1-18 00:00:00" fixdate="2019-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Build Version as UDF</summary>
      <description>This Jira is to get the Hive Build Version as UDF.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="21148" opendate="2019-1-22 00:00:00" fixdate="2019-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use StandardCharsets Where Possible</summary>
      <description>Starting in Java 1.7, JDKs must support a set of standard charsets. When using this facility, instead of passing around the name (string) of the character set, one can pass around a Charset object and there is no need to catch a UnsupportedEncodingException.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestampLocalTZ.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestDataWritableWriter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.debug.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21173" opendate="2019-1-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache Thrift to 0.9.3-1</summary>
      <description>The project currently depends on libthrift-0.9.3, however thrift released 0.12.0 on 2019-JAN-04. This release includes a security fix for THRIFT-4506 (CVE-2018-1320). Updating thrift to the latest version will remove that vulnerability.Also note the Apache Thrift project does not publish "libfb303" any longer. fb303 is contributed code (in '/contrib') and it has not been maintained. Ps.: 0.9.3.1 also addresses the CVE, see THRIFT-4506</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21186" opendate="2019-1-30 00:00:00" fixdate="2019-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External tables replication throws NPE if hive.repl.replica.external.table.base.dir is not fully qualified HDFS path.</summary>
      <description>REPL DUMP is fine. Load seems to be throwing exception:2019-01-29 09:25:12,671 ERROR HiveServer2-Background-Pool: Thread-4864: ql.Driver (SessionState.java:printError(1129)) - FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask. java.lang.NullPointerException2019-01-29 09:25:12,671 INFO HiveServer2-Background-Pool: Thread-4864: ql.Driver (Driver.java:execute(1661)) - task failed withorg.apache.hadoop.hive.ql.parse.SemanticException: java.lang.NullPointerExceptionat org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.tasks(LoadTable.java:154)at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.executeBootStrapLoad(ReplLoadTask.java:141)at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.execute(ReplLoadTask.java:82)at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:177)at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:93)at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1777)at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1511)at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1308)at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1175)at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:273)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)at java.util.concurrent.FutureTask.run(FutureTask.java:266)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)at java.util.concurrent.FutureTask.run(FutureTask.java:266)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerExceptionat org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.PathUtils.getExternalTmpPath(PathUtils.java:35)at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.loadTableTask(LoadTable.java:245)at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.newTableTasks(LoadTable.java:189)at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.tasks(LoadTable.java:136)... 23 moreREPL Load statement: REPL LOAD `testdb1_tgt` FROM 'hdfs://ctr-e139-1542663976389-56533-01-000011.hwx.site:8020/apps/hive/repl/c9476207-8179-4db7-b947-ba67c950a340' WITH ('hive.query.id'='testHive1_3dd5e281-89ef-4054-850e-8a34386fc2c8','hive.exec.parallel'='true','hive.repl.replica.external.table.base.dir'='/tmp/someNewloc/','hive.repl.include.external.tables'='true','mapreduce.map.java.opts'='-Xmx640m','hive.distcp.privileged.doAs'='beacon','distcp.options.pugpb'='')This is an issue with Hive unable to handle path without schema/authority input for "hive.repl.replica.external.table.base.dir".Here the input was 'hive.repl.replica.external.table.base.dir'='/tmp/someNewloc/','.If we set a fully qualified HDFS path (such as hdfs://&lt;host&gt;:&lt;port:/tmp/someNewloc/), then it works fine.Need to fix it in Hive to accept path without schema/authority and obtain it from local cluster.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21189" opendate="2019-1-31 00:00:00" fixdate="2019-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.merge.nway.joins should default to false</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join.grp.diff.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.sum.if.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stat.estimate.drill.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.runtime.skewjoin.mapjoin.spark.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.filter.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nullsafe.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.reddedup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.is.not.distinct.from.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.empty.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.prod.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.prod.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.spark.local.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-cluster.hive-site.xml</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.map.join.on.filter.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.map.ppr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.filter.on.outerjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.test.outer.q</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.scalar.multi.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.empty.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.to.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="21197" opendate="2019-2-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive replication can add duplicate data during migration to a target with hive.strict.managed.tables enabled</summary>
      <description>During bootstrap phase it may happen that the files copied to target are created by events which are not part of the bootstrap. This is because of the fact that, bootstrap first gets the last event id and then the file list. During this period if some event are added, then bootstrap will include files created by these events also.The same files will be copied again during the first incremental replication just after the bootstrap. In normal scenario, the duplicate copy does not cause any issue as hive allows the use of target database only after the first incremental. But in case of migration, the file at source and target are copied to different location (based on the write id at target) and thus this may lead to duplicate data at target. This can be avoided by having at check at load time for duplicate file. This check can be done only for the first incremental and the search can be done in the bootstrap directory (with write id 1). if the file is already present then just ignore the copy.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.RemoteCompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MetaStoreCompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.TableContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="21199" opendate="2019-2-1 00:00:00" fixdate="2019-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace all occurences of new Byte with Byte.valueOf</summary>
      <description>Creating Byte objects with new Byte(...) creates a new object, while Byte.valueOf(...) can be cached (and is actually cached in most if not all JVMs) thus reducing GC overhead.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestHCatRecordSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestDefaultHCatRecord.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Reflector.java</file>
    </fixedFiles>
  </bug>
  <bug id="2120" opendate="2011-4-19 00:00:00" fixdate="2011-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>auto convert map join may miss good candidates</summary>
      <description>In case in a join, there is a subquery which does a simple select, the auto convert map join may miss a good candidate at run time. The plan generated is correct, but the selection at runtime has a bug.For example:set hive.smalltable.filesize=1000;create table src_one as select * from src where key=100;select count(1)from (select * from src) subq join src_small on src.key = subq.key;The table src_small can be a small table. This is in the plan, but at runtime it gets filtered out.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="21215" opendate="2019-2-5 00:00:00" fixdate="2019-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Read Parquet INT64 timestamp</summary>
      <description>This patch enables Hive to start reading timestamps from Parquet written with the new semantics:With Parquet version 1.11, a new timestamp LogicalType with base INT64 and the following metadata is introduced: boolean isAdjustedToUtc: marks whether the timestamp is converted to UTC (aka Instant semantics) or not (LocalDateTime semantics). enum TimeUnit (NANOS, MICROS, MILLIS): granularity of timestampUpon reading, the semantics of these new timestamps will be determined by their metadata, while the semantics of INT96 timestamps will continue to be deduced from the writer metadata. This feature will be behind a flag for now.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.convert.TestETypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Timestamp.java</file>
    </fixedFiles>
  </bug>
  <bug id="21216" opendate="2019-2-5 00:00:00" fixdate="2019-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write Parquet INT64 timestamp</summary>
      <description>This patch enables Hive to start writing int64 timestamps in Parquet.With Parquet version 1.11, a new timestamp LogicalType with base INT64 and the following metadata is introduced: boolean isAdjustedToUtc: marks whether the timestamp is converted to UTC (aka Instant semantics) or not (LocalDateTime semantics) enum TimeUnit (NANOS, MICROS, MILLIS): granularity of timestampThe timestamp will have LocalDateTime semantics (not converted to UTC). Timestamps outside of range 1677-09-21 00:12:43.145224192 – 2262-04-11 23:47:16.854775807 cannot be written in nanos and will be recorded as NULL. Time unit (granularity) will be determined by the user. Default is milliseconds.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestDataWritableWriter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.HiveParquetSchemaTestUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.timestamp.ParquetTimestampUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21217" opendate="2019-2-5 00:00:00" fixdate="2019-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize range calculation for PTF</summary>
      <description>During window function execution Hive has to iterate on neighbouring rows of the current row to find the beginning and end of the proper range (on which the aggregation will be executed).When we're using range based windows and have many rows with a certain key value this can take a lot of time. (e.g. partition size of 80M, in which we have 2 ranges of 40M rows according to the orderby column: within these 40M rowsets we're doing 40M x 40M/2 steps.. which is of n^2 time complexity)I propose to introduce a cache that keeps track of already calculated range ends so it can be reused in future scans.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.ValueBoundaryScanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFPartition.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21221" opendate="2019-2-5 00:00:00" fixdate="2019-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make HS2 and LLAP consistent - Bring up LLAP WebUI in test mode if WebUI port is configured</summary>
      <description>When HiveServer2 comes up, it skips the start of the WebUI if1) hive.in.test is set to trueAND2) the WebUI port is not specified or default (hive.server2.webui.port) Right now, on LLAP daemon start, it is only checked if hive is in test (condition 1) above. The LLAP Daemon start up code (to skip WebUI creation) should be consistent with HS2, therefore if a port is specified (other than the default), the WebUI should also be started in test mode.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="21224" opendate="2019-2-6 00:00:00" fixdate="2019-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade tests JUnit3 to JUnit4</summary>
      <description>Old JUnit3 tests should be upgraded to JUnit4</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperatorNames.java</file>
      <file type="M">service.src.test.org.apache.hive.service.TestCookieSigner.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionHooks.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionCleanup.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestPlainSaslHelper.java</file>
      <file type="M">service.src.test.org.apache.hive.http.TestJdbcJarDownloadServlet.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.typeinfo.TestTypeInfoUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestStatsSerde.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.teradata.TestTeradataBinarySerdeGeneral.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.teradata.TestTeradataBinarySerdeForTimeStamp.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.teradata.TestTeradataBinarySerdeForDecimal.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.teradata.TestTeradataBinarySerdeForDate.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestUnionStructObjectInspector.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestThriftObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestReflectionObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestProtocolBuffersObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestFullMapEqualComparer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.fast.TestLazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinaryFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFVersion.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUUID.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUnhex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUnbase64.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFSha1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFMd5.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFHex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFDateFormatGranularity.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFCrc32.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFBuildVersion.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFBase64.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestToInteger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrunc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSubstringIndex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSoundex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFSha2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFReplace.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRegexp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFQuote.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFQuarter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFNextDay.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMonthsBetween.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLevenshtein.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLeast.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLastDay.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFInitCap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFGreatest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFromUtcTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFactorial.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEnforceConstraint.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEncode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDecode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCbrt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAddMonths.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFCorrelation.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.tool.TestLineageInfo.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestEximUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveFileFormatUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestCombineHiveInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestArrayWritableObjectInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorRowObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.TestDebugDisplay.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPartitionKeySampler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloConnectionParameters.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloDefaultIndexScanner.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveAsyncLogging.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConfRestrictList.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.mr.TestGenericMR.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.serde2.TestRegexSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestLazyHBaseObject.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestUseDatabase.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.schema.TestHCatSchema.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestDefaultHCatRecord.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestHCatRecordSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.commands.TestNoopCommand.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.TestReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestDesc.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestServer.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.security.TestDBTokenStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.security.TestZooKeeperTokenStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.tools.metatool.TestHiveMetaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.BaseTestQueries.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.metadata.TestSemanticAnalyzerHookLoading.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestMTQueries.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.serde2.dynamic.type.TestDynamicSerDe.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.serde2.TestSerdeWithFieldComments.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="21225" opendate="2019-2-6 00:00:00" fixdate="2019-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: getAcidState() should cache a recursive dir listing locally</summary>
      <description>Currently getAcidState() makes 3 calls into the FS api which could be answered by making a single recursive listDir call and reusing the same data to check for isRawFormat() and isValidBase().All delta operations for a single partition can go against a single listed directory snapshot instead of interacting with the NameNode or ObjectStore within the inner loop.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingAssert.java</file>
    </fixedFiles>
  </bug>
  <bug id="21227" opendate="2019-2-7 00:00:00" fixdate="2019-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-20776 causes view access regression</summary>
      <description>HIVE-20776 introduces a change that causes regression for view access.Before the change, a user with select access of a view can get all columns of a view with select access of a view that is derived from a partitioned table.With the change, that user cannot access that view.The reason is that When user accesses columns of a view, Hive needs to get the partitions of the table that the view is derived from. The user name is the user who issues the query to access the view. The change in HIVE-20776 checks if user has access to a table before getting its partitions. When user only has access of a view, not the access of a table itself, this change denies the user access of the view.The solution is when getting table partitions, do not filter on table at HMS client</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21228" opendate="2019-2-7 00:00:00" fixdate="2019-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace all occurences of new Integer with Integer.valueOf</summary>
      <description>Creating Integer objects with new Integer(...) creates a new object, while Integer.valueOf(...) can be cached (and is actually cached in most if not all JVMs) thus reducing GC overhead.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.thrift.test.CreateSequenceFile.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.SerdeRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFPosExplode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CryptoProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LimitDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionMisc.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestLazyHCatRecord.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordSerDe.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Reflector.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="21232" opendate="2019-2-8 00:00:00" fixdate="2019-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add a cache-miss friendly split affinity provider</summary>
      <description>If one of the LLAP nodes have data-locality, preferring that over another does have advantages for the first query or a more general cache-miss.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21235" opendate="2019-2-8 00:00:00" fixdate="2019-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: make the name of log4j2 properties file configurable</summary>
      <description>For llap daemon, the name of llap-daemon-log4j2.properties is fixed. If a conf dir and jar contain the same filename, it will mess up log4j2 initialization. </description>
      <version>4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapConstants.java</file>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21240" opendate="2019-2-11 00:00:00" fixdate="2019-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON SerDe Re-Write</summary>
      <description>The JSON SerDe has a few issues, I will link them to this JIRA. Use Jackson Tree parser instead of manually parsing Added support for base-64 encoded data (the expected format when using JSON) Added support to skip blank lines (returns all columns as null values) Current JSON parser accepts, but does not apply, custom timestamp formats in most cases Added some unit tests Added cache for column-name to column-index searches, currently O&amp;#40;n&amp;#41; for each row processed, for each column in the row</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.json.HiveJsonStructReader.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.JsonSerDe.java</file>
      <file type="M">ql.src.test.results.clientpositive.kafka.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFJsonRead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFJsonRead.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.JsonSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="2125" opendate="2011-4-22 00:00:00" fixdate="2011-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table concatenate fails and deletes data</summary>
      <description>the number of reducers is not set by this command (unlike other hive queries). since mapred.reduce.tasks=-1 (to let hive infer this automatically) - jobtracker fails the job (number of reducers cannot be negative)hive&gt; alter table ad_imps_2 partition(ds='2009-06-16') concatenate;alter table ad_imps_2 partition(ds='2009-06-16') concatenate;Starting Job = job_201103101203_453180, Tracking URL = http://curium.data.facebook.com:50030/jobdetails.jsp?jobid=job_201103101203_453180Kill Command = /mnt/vol/hive/sites/curium/hadoop/bin/../bin/hadoop job -Dmapred.job.tracker=curium.data.facebook.com:50029 -kill job_201103101203_453180Hadoop job information for null: number of mappers: 0; number of reducers: 02011-04-22 10:21:24,046 null map = 100%, reduce = 100%Ended Job = job_201103101203_453180 with errorsMoved to trash: /user/facebook/warehouse/ad_imps_2/_backup.ds=2009-06-16after the job fails - the partition is deletedthankfully it's still in trash</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21254" opendate="2019-2-12 00:00:00" fixdate="2019-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pre-upgrade tool should handle exceptions and skip db/tables</summary>
      <description>When exceptions like AccessControlException is thrown, pre-upgrade tool fails. If hive user does not have read access to database or tables (some external tables denies read access to hive), pre-upgrade tool should just assume they are external tables and move on without failing pre-upgrade process. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.src.test.java.org.apache.hadoop.hive.upgrade.acid.TestPreUpgradeTool.java</file>
      <file type="M">upgrade-acid.pre-upgrade.src.main.java.org.apache.hadoop.hive.upgrade.acid.PreUpgradeTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="21261" opendate="2019-2-13 00:00:00" fixdate="2019-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL LOAD adds redundant COPY and MOVE tasks for external table events.</summary>
      <description>For external tables replication, the data gets copied as separate task based on data locations listed in _external_tables_info file in the dump. So, individual events such as ADD_PARTITION or INSERT on the external tables should avoid copying data. So, it is enough to create table/add partition DDL tasks. COPY and MOVE tasks should be skipped.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21269" opendate="2019-2-14 00:00:00" fixdate="2019-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mandate -update and -delete as DistCp options to sync data files for external tables replication.</summary>
      <description>Currently, external tables replication, copies the data in directory level. So, if target directory exist, then DistCp should compare and update or skip data files in the directory instead of creating new directory inside pre-existing target directory.This can be achieved using -update.Also, -delete option is needed to delete the files missing in source directory but present in target.Hive should mandate these DistCp options even if user passes other options.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.test.org.apache.hadoop.hive.shims.TestHadoop23Shims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="2127" opendate="2011-4-23 00:00:00" fixdate="2011-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve stats gathering reliability by retries on failures with hive.stats.retries.max and hive.stats.retries.wait</summary>
      <description>Stats publishing and aggregation only try once and if there is any exception it will fail and return. If many mappers/reducers updating stats at the same time, it is very common to get lock timeout. We should make stats more reliable by retry when there is an SQLException.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21281" opendate="2019-2-16 00:00:00" fixdate="2019-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl checkpointing doesn&amp;#39;t work when retry bootstrap load with partitions of external tables.</summary>
      <description>Repl checkpoint feature optimises the retry logic of bootstrap repl load by skipping the properly loaded tables and partitions. In case of retry of bootstrap load with external tables having partitions, the checkpoint doesn't work and load partitions always.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="21283" opendate="2019-2-17 00:00:00" fixdate="2019-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Synonym mid for substr, position for locate</summary>
      <description>Create new synonym for the existing function Mid for substrpostiion for locate </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.substring.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="21286" opendate="2019-2-19 00:00:00" fixdate="2019-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should support clean-up of previously bootstrapped tables when retry from different dump.</summary>
      <description>If external tables are enabled for replication on an existing repl policy, then bootstrapping of external tables are combined with incremental dump.If incremental bootstrap load fails with non-retryable error for which user will have to manually drop all the external tables before trying with another bootstrap dump. For full bootstrap, to retry with different dump, we suggested user to drop the DB but in this case they need to manually drop all the external tables which is not so user friendly. So, need to handle it in Hive side as follows.REPL LOAD takes additional config (passed by user in WITH clause) that says, drop all the tables which are bootstrapped from previous dump. hive.repl.clean.tables.from.bootstrap=&lt;previous_bootstrap_dump_dir&gt;Hive will use this config only if the current dump is combined bootstrap in incremental dump.Caution to be taken by user that this config should not be passed if previous REPL LOAD (with bootstrap) was successful or any successful incremental dump+load happened after "previous_bootstrap_dump_dir".</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21297" opendate="2019-2-20 00:00:00" fixdate="2019-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace all occurences of new Long, Boolean, Double etc with the corresponding .valueOf</summary>
      <description>Creating new objects with new Long(...), new Boolean etc creates a new object, while Long.valueOf(...), Boolean.valueOf(...) can be cached (and is actually cached in most if not all JVMs) thus reducing GC overhead. I already had two similar tickets (HIVE-21228, HIVE-21199) - this one finishes the job.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.metrics.PerfLogger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.common.ndv.fm.FMSketch.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestUnionStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ColumnBuffer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapPreVectorizationPass.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Udf.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionString.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Cmp.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="21298" opendate="2019-2-20 00:00:00" fixdate="2019-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Hive Schema Tool classes to their own package to have cleaner structure</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.TestMetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.DbInstallBase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskValidate.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskUpgrade.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskMoveTable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskMoveDatabase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskInitOrUpgrade.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskInit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskCreateUser.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskCreateCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskAlterCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTask.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreSchemaInfo.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.schematool.TestSchemaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.tools.TestSchemaToolCatalogOps.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.schematool.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.schematool.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="21306" opendate="2019-2-22 00:00:00" fixdate="2019-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade HttpComponents to the latest versions similar to what Hadoop has done.</summary>
      <description>The use of HTTPClient 4.5.2 breaks the use of SPNEGO over TLS.It mistakenly added HTTPS instead of HTTP to the principal when over SSL and thus breaks the authentication.This was upgraded recently in Hadoop and needs to be done for Hive as well.See: HADOOP-16076Where we upgraded from 4.5.2 and 4.4.4 to 4.5.6 and 4.4.10.&lt;!-- httpcomponents versions --&gt;&lt;httpclient.version&gt;4.5.2&lt;/httpclient.version&gt;&lt;httpcore.version&gt;4.4.4&lt;/httpcore.version&gt;+ &lt;httpclient.version&gt;4.5.6&lt;/httpclient.version&gt;+ &lt;httpcore.version&gt;4.4.10&lt;/httpcore.version&gt;</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21307" opendate="2019-2-22 00:00:00" fixdate="2019-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to set GzipJSONMessageEncoder as default config for EVENT_MESSAGE_FACTORY.</summary>
      <description>Currently, we use JsonMessageEncoder as the default message factory for Notification events. As the size of some of the events are really huge and cause OOM issues in RDBMS. So, it is needed to enable GzipJSONMessageEncoder as default message factory to optimise the memory usage.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplWithJsonMessageFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplTableMigrationWithJsonFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplIncrementalLoadAcidTablesWithJsonMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplAcrossInstancesWithJsonMessageFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplAcidTablesWithJsonMessage.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.api.TestHCatClientNotification.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21308" opendate="2019-2-22 00:00:00" fixdate="2019-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Negative forms of variables are not supported in HPL/SQL</summary>
      <description>In the following HPL/SQL programs:declare num = 1; print -num;The expected result should be '-1'，but it print '1' .</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.declare2.out.txt</file>
      <file type="M">hplsql.src.test.queries.local.declare2.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="21316" opendate="2019-2-25 00:00:00" fixdate="2019-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparision of varchar column and string literal should happen in varchar</summary>
      <description>this is most probably the root cause behind HIVE-21310 as well</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.varchar.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21320" opendate="2019-2-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_fields() and get_tables_by_type() are not protected by HMS server access control</summary>
      <description>User without any privilege can call these functions and get all meta data back as if user has full access privilege.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="21325" opendate="2019-2-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive external table replication failed with Permission denied issue.</summary>
      <description>During external table replication the file copy is done in parallel to the meta data replication. If the file copy task creates the directory with do as set to true, it will create the directory with permission set to the user running the repl command. In that case the meta data task while creating the table may fail as hive user might not have access to the created directory.The fix should be While creating directory, if sql based authentication is enabled, then disable storage based authentication for hive user. Currently the created directory has the login user access, it should retain the source clusters owner, group and permission. For external table replication don't create the directory during create table and add partition. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ExternalTableCopyTaskBuilder.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.java</file>
    </fixedFiles>
  </bug>
  <bug id="21329" opendate="2019-2-27 00:00:00" fixdate="2019-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom Tez runtime unordered output buffer size depending on operator pipeline</summary>
      <description>For instance, if we have a reduce sink operator with no keys followed by a Group By (merge partial), we can decrease the output buffer size since we will only produce a single row.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21332" opendate="2019-2-27 00:00:00" fixdate="2019-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache Purge command does purge the in-use buffer.</summary>
      <description>Cache Purge command, is purging what is not suppose to evict.This can lead to unrecoverable state. TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1545278897356_0093_27_00_000001_3:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 32768; at 0 out of 1 (entire cache is fragmented and locked, or an internal issue) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 32768; at 0 out of 1 (entire cache is fragmented and locked, or an internal issue) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:80) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 15 moreCaused by: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 32768; at 0 out of 1 (entire cache is fragmented and locked, or an internal issue) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:365) at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79) at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33) at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151) at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68) ... 17 moreCaused by: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 32768; at 0 out of 1 (entire cache is fragmented and locked, or an internal issue) at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:513) at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:407) at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:266) at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:263) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:263) at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:110) ... 6 moreCaused by: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 32768; at 0 out of 1 (entire cache is fragmented and locked, or an internal issue) at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:358) at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.allocateMultiple(EncodedReaderImpl.java:1295) at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedStream(EncodedReaderImpl.java:923)To reproduce this, set the LRFU policy to LRU (lambda = 1), then purge the cache after running some queries that fills the cache.The only way to recover is to restart LLAP.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.MetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21333" opendate="2019-2-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>[trivial] Fix argument order in TestDateWritableV2#setupDateStrings</summary>
      <description>Calendar#add (int field, int amount) is given parameters (1, Calendar.DAY_OF_YEAR) which i presume is backwards especially since this method is called 365 times.</description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.MetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21340" opendate="2019-2-28 00:00:00" fixdate="2019-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Prune non-key columns feeding into a SemiJoin</summary>
      <description>explain cbo with ss as (select count(1), ss_item_sk, ss_ticket_number from store_sales group by ss_item_sk, ss_ticket_number having count(1) &gt; 1) select count(1) from item where i_item_sk IN (select ss_item_sk from ss);Notice the HiveProject(ss_item_sk=&amp;#91;$0&amp;#93;, ss_ticket_number=&amp;#91;$1&amp;#93;, $f2=&amp;#91;$2&amp;#93;) Only ss_item_sk is relevant for the HiveSemiJoinCBO PLAN:HiveAggregate(group=[{}], agg#0=[count()]) HiveSemiJoin(condition=[=($0, $1)], joinType=[inner]) HiveProject(i_item_sk=[$0]) HiveFilter(condition=[IS NOT NULL($0)]) HiveTableScan(table=[[tpcds_copy_orc_partitioned_10000, item]], table:alias=[item]) HiveProject(ss_item_sk=[$0], ss_ticket_number=[$1], $f2=[$2]) HiveFilter(condition=[&gt;($2, 1)]) HiveAggregate(group=[{1, 8}], agg#0=[count()]) HiveFilter(condition=[IS NOT NULL($1)]) HiveTableScan(table=[[tpcds_copy_orc_partitioned_10000, store_sales]], table:alias=[store_sales])</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.semijoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSemiJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="21341" opendate="2019-2-28 00:00:00" fixdate="2019-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sensible defaults : hive.server2.idle.operation.timeout and hive.server2.idle.session.timeout are too high</summary>
      <description>Defaults are too high, which results in extra resources being held too long in HS2 memory.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21362" opendate="2019-2-28 00:00:00" fixdate="2019-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an input format and serde to read from protobuf files.</summary>
      <description>Logs are being generated using the HiveProtoLoggingHook and tez ProtoHistoryLoggingService. These are sequence files written using ProtobufMessageWritable.Implement a SerDe and input format to be able to create tables using these files.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.metatool.package-info.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestCachedStoreUpdateUsingEvents.java</file>
      <file type="M">contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21365" opendate="2019-3-1 00:00:00" fixdate="2019-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Hep planner steps in CBO</summary>
      <description>Using subprograms to decrease number of planner instantiations and benefit fully from metadata providers caching, among other benefits.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.ext.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.ext.query1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.EstimateUniqueKeys.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSortPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
    </fixedFiles>
  </bug>
  <bug id="21372" opendate="2019-3-1 00:00:00" fixdate="2019-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Apache Commons IO To Read Stream To String</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21389" opendate="2019-3-5 00:00:00" fixdate="2019-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive distribution miss javax.ws.rs-api.jar after HIVE-21247</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21390" opendate="2019-3-5 00:00:00" fixdate="2019-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BI split strategy does not work for blob stores</summary>
      <description>BI split strategy cuts the split at block boundaries however there are no block boundaries in blob storage so we end up with 1 split for BI split strategy. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21396" opendate="2019-3-6 00:00:00" fixdate="2019-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestCliDriver#vector_groupby_reduce is flaky - rounding error</summary>
      <description>http://104.198.109.242/logs/PreCommit-HIVE-Build-16349/failed/61-TestCliDriver-multi_insert_partitioned.q-parquet_types.q-udf_to_unix_timestamp.q-and-27-more/TEST-61-TestCliDriver-multi_insert_partitioned.q-parquet_types.q-udf_to_unix_timestamp.q-and-27-more-TEST-org.apache.hadoop.hive.cli.TestCliDriver.xmlhttp://104.198.109.242/logs/PreCommit-HIVE-Build-16351/failed/61-TestCliDriver-multi_insert_partitioned.q-parquet_types.q-udf_to_unix_timestamp.q-and-27-more/TEST-61-TestCliDriver-multi_insert_partitioned.q-parquet_types.q-udf_to_unix_timestamp.q-and-27-more-TEST-org.apache.hadoop.hive.cli.TestCliDriver.xml-5080.17 --&gt; -5080.169999999999actual:1 85411 816 58.285714285714285 -5080.169999999999 -362.86928571428564 621.350000000000000000 44.382142857142857143expected:1 85411 816 58.285714285714285 -5080.17 -362.8692857142857 621.350000000000000000 44.382142857142857143https://github.com/apache/hive/blob/268a6e5af11e0fdc3887d570c1680035fd9426c3/ql/src/test/results/clientpositive/vector_groupby_reduce.q.outit's a result of sum (max(ss_net_profit) np)select ss_ticket_number, sum(ss_item_sk), sum(q), avg(q), sum(np), avg(np), sum(decwc), avg(decwc)from (select ss_ticket_number, ss_item_sk, min(ss_quantity) q, max(ss_net_profit) np, max(ss_wholesale_cost_decimal) decwc from store_sales_n3 where ss_ticket_number = 1 group by ss_ticket_number, ss_item_sk) agroup by ss_ticket_numberorder by ss_ticket_number</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="21402" opendate="2019-3-7 00:00:00" fixdate="2019-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction state remains &amp;#39;working&amp;#39; when major compaction fails</summary>
      <description>When calcite is not on the HMS classpath, and query based compaction is enabled then the compaction fails with NoClassDefFound error. Since the catch block only catches Exceptions the following code block is not executed:} catch (Exception e) { LOG.error("Caught exception while trying to compact " + ci + ". Marking failed to avoid repeated failures, " + StringUtils.stringifyException(e)); msc.markFailed(CompactionInfo.compactionInfoToStruct(ci)); msc.abortTxns(Collections.singletonList(compactorTxnId));}So the compaction is not set to failed.Would be better to catch Throwable instead of Exception</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
    </fixedFiles>
  </bug>
  <bug id="21403" opendate="2019-3-7 00:00:00" fixdate="2019-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect error code returned when retry bootstrap with different dump.</summary>
      <description>When retry incremental bootstrap on a table with different bootstrap dump throws 40000 as error code instead of 20017.Error while processing statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask. InvalidOperationException(message:Load path hdfs://ctr-e139-1542663976389-61669-01-000003.hwx.site:8020/apps/hive/repl/3d704b34-bf1a-40c9-b70c-57319e6462f6 not valid as target database is bootstrapped from some other path : hdfs://ctr-e139-1542663976389-61669-01-000003.hwx.site:8020/apps/hive/repl/c3e5ec9e-d951-48aa-b3f4-9aeaf5e010ea.)</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="21445" opendate="2019-3-14 00:00:00" fixdate="2019-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support range check for DECIMAL type in stats annotation</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.decimal64.reader.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="21446" opendate="2019-3-14 00:00:00" fixdate="2019-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Server going OOM during hive external table replications</summary>
      <description>The file system objects opened using proxy users are not closed.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.TestCopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ExternalTableCopyTaskBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21449" opendate="2019-3-14 00:00:00" fixdate="2019-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement &amp;#39;WITHIN GROUP&amp;#39; clause</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.disc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.cont.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.disc.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.cont.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFPercentileDisc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFPercentileCont.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileDisc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileCont.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="21457" opendate="2019-3-15 00:00:00" fixdate="2019-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perf optimizations in ORC split-generation</summary>
      <description>Minor split generation optimizations Reuse vectorization checks Reuse isAcid checks Reuse filesystem objects Improved logging (log at top-level instead of inside the thread pool)</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21460" opendate="2019-3-16 00:00:00" fixdate="2019-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Load data followed by a select * query results in incorrect results</summary>
      <description>This affects current master as well. Created an orc file such that it spans multiple stripes and ran a simple select *, and got incorrect row counts (when comparing with select count. The problem seems to be that after split generation and creating min/max rowId for each row (note that since the loaded file is not written by Hive ACID, it does not have ROW_ID in the file; but the ROWID is applied on read by discovering min/max bounds which are used for calculating ROW_ID.rowId for each row of a split), Hive is only reading the last split.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>3.1.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="21467" opendate="2019-3-18 00:00:00" fixdate="2019-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated junit.framework.Assert imports</summary>
      <description>These imports trigger lots of warnings in ide, which could be annoying, and it can be replaced easily with org.junit.Assert, the signature and behavior are the same, so the tests should pass.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestReportParser.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestLogDirectoryCleaner.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestLocalCommand.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestJIRAService.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.ssh.TestSSHCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.ssh.TestRSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestTestParser.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestTestConfiguration.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestQFileTestBatch.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.AbstractTestPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.api.server.TestTestLogger.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.api.server.TestTestExecutor.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.TestValidReadTxnList.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestHiveSQLException.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionHooks.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMacro.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.OperatorTestUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestErrorMsg.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestCompileProcessor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestTezWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestDropMacroDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestCreateMacroDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBCompact.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.ListSizeMatcher.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.TestMapJoinOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorSubStr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringUnary.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineExceptionHandling.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineOpts.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestClientCommandHookFactory.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestVariableSubstitution.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestDefaultHCatRecord.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestHCatRecordSerDe.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatInputFormatMethods.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestInputJobInfo.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestPigHCatUtil.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestDelimitedInputWriter.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestWebHCatE2e.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestJobIDParser.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestNoSaslAuth.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TestCustomAuthentication.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.ShowMapredStatsHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyIsLocalModeHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsNotSubdirectoryOfTableHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsSubdirectoryOfTableHook.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinEqualityTableContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinKey.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinRowContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.persistence.Utilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.TestVectorAggregation.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
    </fixedFiles>
  </bug>
  <bug id="21471" opendate="2019-3-19 00:00:00" fixdate="2019-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replicating conversion of managed to external table leaks HDFS files at target.</summary>
      <description>While replicating the ALTER event to convert managed table to external table, the data location for the table is changed under input base directory for external tables replication. But, the old location remains there and would be leaked for ever.ALTER TABLE T1 SET TBLPROPERTIES('EXTERNAL'='true');</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.CreateTableOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21473" opendate="2019-3-19 00:00:00" fixdate="2019-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bumping jackson version to 2.9.8</summary>
      <description>Bump jackson version to 2.9.8</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21474" opendate="2019-3-19 00:00:00" fixdate="2019-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preparation for bumping guava version</summary>
      <description>Bump guava to 24.1.1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStats.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="21478" opendate="2019-3-19 00:00:00" fixdate="2019-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore cache update shall capture exception</summary>
      <description>We definitely need to capture any exception during CacheUpdateMasterWork.update(), otherwise, Java would refuse to schedule future update().</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2148" opendate="2011-5-3 00:00:00" fixdate="2011-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add interface classification in Hive.</summary>
      <description>Add mechanism for marking stability and intended audiences in Hive.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="21484" opendate="2019-3-20 00:00:00" fixdate="2019-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore API getVersion() should return real version</summary>
      <description>Currently I see the getVersion implementation in the metastore is returning a hard-coded "3.0". It would be good to return the real version of the metastore server using HiveversionInfo so that clients can take certain actions based on metastore server versions.Possible use-cases are:1. Client A can make use of new features introduced in given Metastore version else stick to the base functionality.2. This version number can be used to do a version handshake between client and server in the future to improve our cross-version compatibity story.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21500" opendate="2019-3-25 00:00:00" fixdate="2019-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable conversion of managed table to external and vice versa at source via alter table.</summary>
      <description>Couple of scenarios for Hive2 to Hive3(strict managed tables enabled) replication where managed table is converted to external at source. Scenario-1: (ACID/MM table converted to external at target)1. Create non-ACID ORC format table.2. Insert some rows3. Replicate this create event which creates ACID table at target (due to migration rule). Each insert event adds transactional metadata in HMS corresponding to the current table.4. Convert table to external table using ALTER command at source.Scenario-2: (External table at target changes table location)1. Create non-ACID avro format table.2. Insert some rows3. Replicate this create event which creates external table at target (due to migration rule). The data path is chosen under default external warehouse directory.4. Convert table to external table using ALTER command at source.It is unable to convert an ACID table to external table at target. Also, it is hard to detect what would be the table type at target when perform this ALTER table operation at source.So, it is decided to disable conversion of managed table at source (Hive2) to EXTERNAL or vice-versa if the DB is enabled for replication and strict managed is disabled.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.HiveStrictManagedUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21529" opendate="2019-3-28 00:00:00" fixdate="2019-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive support bootstrap of ACID/MM tables on an existing policy.</summary>
      <description>If ACID/MM tables to be enabled (hive.repl.dump.include.acid.tables) on an existing repl policy, then need to combine bootstrap dump of these tables along with the ongoing incremental dump. Shall add a one time config "hive.repl.bootstrap.acid.tables" to include bootstrap in the given dump.The support for hive.repl.bootstrap.cleanup.type for ACID tables to clean-up partially bootstrapped tables in case of retry is already in place, thanks to the work done during external tables. Need to test that it actually works.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2154" opendate="2011-5-9 00:00:00" fixdate="2011-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add exception handling to hive&amp;#39;s record reader</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.java</file>
      <file type="M">conf.hive-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21547" opendate="2019-3-30 00:00:00" fixdate="2019-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temp Tables: Use stORC format for temporary tables</summary>
      <description>Using st(reaming)ORC (hive.exec.orc.delta.streaming.optimizations.enabled=true) format has massive performance advantages when creating data-sets which will not be stored for long-term.The format is compatible with ORC for vectorization and other features, while being cheaper to write out to filesystem.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="21564" opendate="2019-4-2 00:00:00" fixdate="2019-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data into a bucketed table is ignoring partitions specs and loads data into default partition.</summary>
      <description>When running below command to load data into bucketed tables it is not loading into specified partition instead loaded into default partition.LOAD DATA INPATH '/tmp/files/000000_0' OVERWRITE INTO TABLE call PARTITION(year_partition=2012, month=12);SELECT * FROM call WHERE year_partition=2012 AND month=12; --&gt; returns 0 rows.CREATE TABLE call( date_time_date date, ssn string, name string, location string) PARTITIONED BY ( year_partition int, month int) CLUSTERED BY ( date_time_date) SORTED BY ( date_time_date ASC) INTO 1 BUCKETS STORED AS ORC;If set hive.exec.dynamic.partition to false, it fails with below error.Error: Error while compiling statement: FAILED: SemanticException 1:18 Dynamic partition is disabled. Either enable it by setting hive.exec.dynamic.partition=true or specify partition column values. Error encountered near token 'month' (state=42000,code=40000)When we "set hive.strict.checks.bucketing=false;", the load works fine.This is a behaviour imposed by HIVE-15148 to avoid incorrectly named data files being loaded to the bucketed tables. In customer use case, if the files are named properly with bucket_id (00000_0, 00000_1 etc), then it is safe to set this flag to false.However, current behaviour of loading into default partitions when hive.strict.checks.bucketing=true and partitions specified, was a bug injected by HIVE-19311 where the given query is re-written into a insert query (to handle incorrect file names and Orc versions) but missed to incorporate the partitions specs to it.</description>
      <version>4.0.0</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21571" opendate="2019-4-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW COMPACTIONS shows column names as its first output row</summary>
      <description>SHOW COMPACTIONS yields a resultset with nice column names, and then the first row of data is a repetition of those column names. This is somewhat confusing and hard to read.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.ShowCompactionsOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21582" opendate="2019-4-4 00:00:00" fixdate="2019-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prefix msck configs with metastore</summary>
      <description>HIVE-20707 moved msck configs to metastore but the configs are not prefixed with "metastore". Will be good to prefix it with "metastore" for consistency with other configs. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.discovery.q</file>
    </fixedFiles>
  </bug>
  <bug id="21584" opendate="2019-4-5 00:00:00" fixdate="2019-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java 11 preparation: system class loader is not URLClassLoader</summary>
      <description>Currently, Hive assumes that the system class loader is instance of URLClassLoader. In Java 11 this is not the case. There are a few (unresolved) JIRAs about specific occurrences of URLClassLoader (e.g. HIVE-21237, HIVE-17909), but no "remove all occurrences". Also I couldn't find umbrella "Java 11 upgrade" JIRA.This ticket is to remove all unconditional casts of any random class loader to URLClassLoader.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.FunctionLocalizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="21591" opendate="2019-4-8 00:00:00" fixdate="2019-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using triggers in non-LLAP mode should not require wm queue</summary>
      <description>Resource plan triggers are supported in non-LLAP (tez container) mode. But fetching of resource plan happens only when hive.server2.tez.interactive.queue is set. For tez container mode, only triggers are applicable, so this queue dependency can be removed. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21598" opendate="2019-4-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS on ACID table during incremental does not replicate data</summary>
      <description>Scenariocreate database dumpdb with dbproperties('repl.source.for'='1,2,3');use dumpdb;create table t1 (id int) clustered by(id) into 3 buckets stored as orc tblproperties ("transactional"="true");insert into t1 values(1);insert into t1 values(2);repl dump dumpdb;repl load loaddb from &lt;bootstrap load directory&gt;;use loaddb;select * from t1;-------- t6.id -------- 1 2 +--------use dumpdb;create table t6 stored as orc tblproperties ("transactional"="true") as select * from t1;select * from t6;-------- t6.id -------- 1 2 --------repl dump dumpdb from &lt;last repl id&gt;repl load loaddb from &lt;inc dump location&gt;;use loaddb;select * from t6;-------- t6.id ----------------t6 gets created but there's no data. On further investigation, I see that the CommitTxnEvent's dump directory has _files but it is empty. Looks like we do not log names of the files created as part of CTAS.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.CreateTableOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplAcidTablesWithJsonMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="216" opendate="2009-1-7 00:00:00" fixdate="2009-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>generate ruby bindings for service</summary>
      <description>From: Josh Ferguson &lt;josh@besquared.net&gt;Reply-To: &lt;hive-user@hadoop.apache.org&gt;Date: Tue, 6 Jan 2009 21:03:50 -0800To: &lt;hive-user@hadoop.apache.org&gt;Subject: rb-gen loveCan we get some ruby loving in the default service/src dir?Josh</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2160" opendate="2011-5-12 00:00:00" fixdate="2011-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Few code improvements in the metastore,hwi and ql packages.</summary>
      <description>Few code improvements in the metastore,hwi and ql packages.1) Little performance Improvements 2) Effective varaible management.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
    </fixedFiles>
  </bug>
  <bug id="21602" opendate="2019-4-11 00:00:00" fixdate="2019-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dropping an external table created by migration case should delete the data directory.</summary>
      <description>For external table, if the table is dropped, the location is not removed. But If the source table is managed and at target the table is converted to external, then the table location should be removed if the table is dropped.Replication flow should set additional parameter "external.table.purge"="true" for migration to external table.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
    </fixedFiles>
  </bug>
  <bug id="21603" opendate="2019-4-11 00:00:00" fixdate="2019-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java 11 preparation: update powermock version</summary>
      <description>PowerMock1 has no support for Java11, therefore we need to bump its version to 2.0.0</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidInputFormat.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestHdfsUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolTaskDrop.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.metatool.TestMetaToolTaskUpdateLocation.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestTableIterable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestMsckCheckPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreLdapAuthenticationProviderImpl.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestUserSearchFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestUserFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestSearchResultHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestLdapSearch.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestGroupFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestCustomQueryFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.ldap.TestChainFilter.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestJobHandle.java</file>
      <file type="M">shims.common.src.main.test.org.apache.hadoop.hive.io.TestHdfsUtils.java</file>
      <file type="M">shims.common.src.main.test.org.apache.hadoop.fs.TestProxyFileSystem.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftHttpServletTest.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestUserSearchFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestUserFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestSearchResultHandler.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestLdapSearch.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestGroupFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestCustomQueryFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestChainFilter.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.ptf.TestBoundaryCache.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFExtractUnion.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFEvaluator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestCompileLock.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHivePrivilegeObjectOwnerNameAndType.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.TestCopyUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.dump.TestHiveWrapper.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestRelSignature.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.rules.TestHiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.rules.TestHivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.schematool.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBufferedRows.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestTableOutputFormat.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestHeartbeatTimerTask.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.lock.TestLock.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestMutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.client.TestTransaction.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMetaStorePartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestMutatorImpl.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.ConcurrentJobRequestsTestBase.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestConcurrentJobRequestsThreads.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestConcurrentJobRequestsThreadsAndTimeout.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerShowFilters.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIServiceFeatures.java</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc-handler.src.test.java.org.apache.hive.storage.jdbc.TestJdbcInputFormat.java</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestHivePreparedStatement.java</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.io.encoded.TestVectorDeserializeOrcWriter.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestBlacklistingLlapMetricsListener.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestLlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeavesTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestGetInputSummary.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckCreatePartitionsInBatches.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckDropPartitionsInBatches.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.monitoring.TestTezProgressMonitor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.util.DAGTraversalTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestQueryHooks.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="21604" opendate="2019-4-12 00:00:00" fixdate="2019-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>preCommit job should not be triggered on non-patch attachments</summary>
      <description>latest example: HIVE-14469https://issues.apache.org/jira/browse/HIVE-14669?focusedCommentId=16815520&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16815520I think one should be able to upload any kind of attachments (e.g. screenshot) without triggering the precommit job2 possible ways:1. strict: enable only .patch (should work)(2. lenient: introduce blacklist, .png ...)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21624" opendate="2019-4-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Cpu metrics at thread level is broken</summary>
      <description>ExecutorThreadCPUTime and ExecutorThreadUserTime relies on thread mx bean cpu metrics when available. At some point, the thread name which the metrics publisher looks for has changed causing no metrics to be published for these counters.  The above counters looks for thread with name starting with "ContainerExecutor" but the llap task executor thread got changed to "Task-Executor"</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="21633" opendate="2019-4-18 00:00:00" fixdate="2019-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Estimate range for value generated by aggregate function in statistics annotation</summary>
      <description>In some cases, we can infer the estimate of the range for a value generated by an aggregate function during statistics annotation. For instance, we can estimate the min of the sum of a column with positive min value as that same min value.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="21634" opendate="2019-4-19 00:00:00" fixdate="2019-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Materialized view rewriting over aggregate operators containing with grouping sets</summary>
      <description>A possible approach to support rewriting queries with an aggregate with grouping sets is implementing a rule that splits the aggregate in the query into an aggregate without grouping sets (bottom) and an aggregate with grouping sets (top). Then the materialized view rewriting rule will trigger on the former.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21654" opendate="2019-4-26 00:00:00" fixdate="2019-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External table location is not preserved at target when base dir is set as /.</summary>
      <description>External table location is not preserved same as source path when base directory is set as "/".Source path: /tmp/ext/src/db1/ext1Target path: /ext/src/db1/ext1 --&gt; It should be /tmp/ext/src/db1/ext1 itself.External table base dir supplied: /If the base dir input is changed to "/abc", then target path is set as "/abc/tmp/ext/src/db1/ext1 which is correct.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21657" opendate="2019-4-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable flaky cbo_rp_limit.q in TestMiniLlapLocalCliDriver</summary>
      <description>Fails intermittently with diff:Client Execution succeeded but contained differences (error code = 1) after executing cbo_rp_limit.q 11c11&lt; 1 4 2---&gt; 1 4 2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="21672" opendate="2019-4-30 00:00:00" fixdate="2019-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 needs to support sidecar&amp;#39;s &amp;#39;ldap.xml&amp;#39; file</summary>
      <description>SDX sidecar will publish freeIPA ldap credentials in a shared location that will be added to classpath.Hive needs to be able to load LDAP settings from that file for LDAP authentication.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21679" opendate="2019-5-2 00:00:00" fixdate="2019-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replicating a CTAS event creating an MM table fails.</summary>
      <description>use dumpdb;create table t1 (a int, b int);insert into t1 values (1, 2), (3, 4);create table t6_mm_part partitioned by (a) stored as orc tblproperties ("transactional"="true", "transactional_properties"="insert_only") as select * from t1create table t6_mm stored as orc tblproperties ("transactional"="true", "transactional_properties"="insert_only") as select * from t1;repl dump dumpdb;create table t6_mm_part_2 partitioned by (a) stored as orc tblproperties ("transactional"="true", "transactional_properties"="insert_only") as select * from t1;create table t6_mm_2 partitioned by (a) stored as orc tblproperties ("transactional"="true", "transactional_properties"="insert_only") as select * from t1;repl dump dumpdb from &lt;last repl id&gt;repl load loaddb from '/tmp/dump/next';ERROR : failed replicationorg.apache.hadoop.hive.ql.parse.SemanticException: Invalid table name loaddb.dumpdb.t6_mm_part_2 at org.apache.hadoop.hive.ql.exec.Utilities.getDbTableName(Utilities.java:2253) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.Utilities.getDbTableName(Utilities.java:2239) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.plan.AlterTableDesc.setOldName(AlterTableDesc.java:419) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.tableUpdateReplStateTask(IncrementalLoadTasksBuilder.java:286) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.addUpdateReplStateTasks(IncrementalLoadTasksBuilder.java:371) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.analyzeEventLoad(IncrementalLoadTasksBuilder.java:244) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.build(IncrementalLoadTasksBuilder.java:139) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.executeIncrementalLoad(ReplLoadTask.java:488) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.execute(ReplLoadTask.java:102) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2709) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2361) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2028) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1788) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1782) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:162) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:233) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation.access$600(SQLOperation.java:88) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:332) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_191] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_191] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) ~[hadoop-common-3.1.0.3.0.0.0-1634.jar:?] at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:350) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_191] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_191] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_191] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_191] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask. Invalid table name loaddb.dumpdb.t6_mm_part_2Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask. Invalid table name loaddb.dumpdb.t6_mm_part_2 (state=42000,code=1)</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.creation.CreateTableDesc.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.ReplicationTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21685" opendate="2019-5-3 00:00:00" fixdate="2019-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong simplification in query with multiple IN clauses</summary>
      <description>Simple test to reproduce:select * from table1 where name IN(‘g’,‘r’) AND name IN(‘a’,‘b’);</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21694" opendate="2019-5-6 00:00:00" fixdate="2019-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive driver wait time is fixed for task getting executed in parallel.</summary>
      <description>During a command execution hive driver executes the task in a separate thread if the task to be executed is set as parallel. After starting the task, driver checks if the task has finished execution or not. If the task execution is not finished it waits for 2 seconds before waking up again to check the task status. In case of task with execution time in milliseconds, this wait time can induce substantial overhead. So the execution thread can call notify to interrupt the wait call. This will make sure that the next thread execution can start immediately.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
    </fixedFiles>
  </bug>
  <bug id="21696" opendate="2019-5-6 00:00:00" fixdate="2019-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include partition columns and column stats in explain cbo formatted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelWriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21700" opendate="2019-5-7 00:00:00" fixdate="2019-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive incremental load going OOM while adding load task to the leaf nodes of the DAG.</summary>
      <description>While listing the child nodes to check for leaf node, we need to filter out tasks which are already added to the children list. If a task is added multiple time to the children list then it may cause the list to grow exponentially. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.util.DAGTraversal.java</file>
    </fixedFiles>
  </bug>
  <bug id="21711" opendate="2019-5-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regression caused by HIVE-21279 for blobstorage fs</summary>
      <description>HIVE-21279 caused a regression wherein CTAS/create materialized views statement for blobstorage is now always renaming files.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="21717" opendate="2019-5-10 00:00:00" fixdate="2019-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename is failing for directory in move task</summary>
      <description>Rename fails with destination directory not empty in case a directory is move directly to the table location from staging directory as rename cannot overwrite non empty destination directory. In replication scenarios, if user does some concurrent write during bootstrap dump, it may happen that some data which are already replicated through bootstrap, will be tried during next incremental load also. This is handled by making the operations reentrant during repl load. But here move task is not able to delete the directory created by bootstrap load even though replace flag is set to true. This is causing the incremental load to fail.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="21722" opendate="2019-5-13 00:00:00" fixdate="2019-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL:: logs are missing in hiveStatement.getQueryLog output during parallel execution mode.</summary>
      <description>getQueryLog only reads logs from Background thread scope. If parallel execution is set to true, a new thread is created for execution and all the logs added by the new thread are not added to the parent  Background thread scope. In replication scope, replStateLogTasks are started in parallel mode causing the logs to be skipped from getQueryLog scope. There is one more issue, with the conf is not passed while creating replStateLogTask during bootstrap load end. The same issue is there with event load during incremental load. The incremental load end log task is created with the proper config. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplStateLogTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="21729" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow serializer sometimes shifts timestamp by one second</summary>
      <description>This happens due to secondInMicros are calculated likefinal long secondInMicros = (secondInMillis - secondInMillis % MILLIS_PER_SECOND) * MICROS_PER_MILLIS;Instead this should be calculated like(by taking nanos from timestampColumnVector itself)final long nanos = timestampColumnVector.getNanos(j);final long secondInMicros = (secondInMillis - nanos / NS_PER_MILLIS) * MICROS_PER_MILLIS;</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21731" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive import fails, post upgrade of source 3.0 cluster, to a target 4.0 cluster with strict managed table set to true.</summary>
      <description>The scenario is  Replication policy is set with hive  3.0 source cluster (strict managed table set to false) and hive 4.0 target cluster with strict managed table set  true. User upgrades the 3.0 source cluster to 4.0 cluster using upgrade tool. The upgrade converts all managed tables to acid tables. In the next repl dump, user sets hive .repl .dump .include .acid .tables and hive .repl .bootstrap. acid. tables set true triggering bootstrap of newly converted ACID tables. As the old tables are non-txn tables, dump is not filtering the events even tough bootstrap acid table is set to true. This is causing the repl load to fail as the write id is not set in the table object. If we ignore the event replay, the bootstrap is failing with dump directory mismatch error.The fix should be  Ignore dumping the alter table event if bootstrap acid table is set true and the alter is converting a non-acid table to acid table. In case of bootstrap during incremental load, ignore the dump directory property set in table object.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21732" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable injection of load for LLAP task execution</summary>
      <description>For evaluating testing, it would be good to have a configurable way to inject latency for LLAP tasks.The configuration should be able to control how much latency is injected into each daemon.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21734" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation: Pending items from code review</summary>
      <description>A sub-task of HIVE-21663. Some items came from the review feedback and some were left out from the initial implementation.1) Enforce limit being passed into get_tables_ext. Currently being ignored.2) Filter out some capabilities being returned to called based on the capabilities possessed by the processor.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDirectSqlUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="21777" opendate="2019-5-22 00:00:00" fixdate="2019-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven jar goal is producing warning due to missing dependency</summary>
      <description>org.apache.directory.client.ldap:ldap-client-directory is a test scope dependecy. Hive is using version 0.1 but 0.1-SNAPSHOT is also there as transitive dependency (omitted for collision with 0.1 which is already there on top level) causing warning in the maven default lifecycle execution:&amp;#91;WARNING&amp;#93; The POM for org.apache.directory.client.ldap:ldap-client-api:jar:0.1-SNAPSHOT is missing, no dependency information availableThe warning appears in the jar goal logs and it can easily be removed by excluding this transitive dependency. </description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21787" opendate="2019-5-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore table cache LRU eviction</summary>
      <description>Metastore currently uses black/white list to specify patterns of tables to load into the cache. Cache is loaded in one shot "prewarm", and updated by a background thread. This is not a very efficient design. In this feature, we try to enhance the cache for Tables with LRU to improve cache utilization.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CacheUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStoreUpdateUsingEvents.java</file>
    </fixedFiles>
  </bug>
  <bug id="21804" opendate="2019-5-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation: External tables with no capabilities returns duplicate entries/</summary>
      <description>2019-05-24T12:50:52,978 WARN &amp;#91;pool-6-thread-4&amp;#93; metastore.HiveMetaStore: Unexpected resultset size:22019-05-24T12:50:52,981 ERROR &amp;#91;pool-6-thread-4&amp;#93; metastore.RetryingHMSHandler: MetaException(message:Unexpected result from metadata transformer:return list size=2) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getTableInternal(HiveMetaStore.java:3154) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_req(HiveMetaStore.java:3118) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy28.get_table_req(Unknown Source) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_req.getResult(ThriftHiveMetastore.java:16497) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_req.getResult(ThriftHiveMetastore.java:16481) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21812" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement get partition related methods on temporary tables</summary>
      <description>IMetastoreClient exposes several APIs related to getting partitions. The definition of the APIs are:Partition getPartition(String dbName, String tblName, List&lt;String&gt; partVals);Partition getPartition(String catName, String dbName, String tblName, List&lt;String&gt; partVals);Partition getPartition(String dbName, String tblName, String name);Partition getPartition(String catName, String dbName, String tblName, String name);Partition getPartitionWithAuthInfo(String dbName, String tableName, List&lt;String&gt; pvals, String userName, List&lt;String&gt; groupNames);Partition getPartitionWithAuthInfo(String catName, String dbName, String tableName, List&lt;String&gt; pvals, String userName, List&lt;String&gt; groupNames);List&lt;Partition&gt; getPartitionsByNames(String db_name, String tbl_name, List&lt;String&gt; part_names);List&lt;Partition&gt; getPartitionsByNames(String catName, String db_name, String tbl_name, List&lt;String&gt; part_names);Partition getPartitionWithAuthInfo(String dbName, String tableName, List&lt;String&gt; pvals, String userName, List&lt;String&gt; groupNames);Partition getPartitionWithAuthInfo(String catName, String dbName, String tableName, List&lt;String&gt; pvals, String userName, List&lt;String&gt; groupNames);In order to support partitions on temporary tables, the majority of these methods must be implemented in SessionHiveMetastoreClient.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.MetaStoreClientTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21814" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement list partitions related methods on temporary tables</summary>
      <description>IMetaStoreClient exposes the following methods related to listing partitions:List&lt;String&gt; listPartitionNames(String db_name, String tbl_name, List&lt;String&gt; part_vals, short max_parts);List&lt;String&gt; listPartitionNames(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, int max_parts);List&lt;Partition&gt; listPartitions(String db_name, String tbl_name, List&lt;String&gt; part_vals, short max_parts);List&lt;Partition&gt; listPartitions(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, int max_parts);List&lt;String&gt; listPartitionNames(String db_name, String tbl_name, short max_parts);List&lt;String&gt; listPartitionNames(String catName, String db_name, String tbl_name, int max_parts);List&lt;String&gt; listPartitionNames(String db_name, String tbl_name, List&lt;String&gt; part_vals, short max_parts);List&lt;String&gt; listPartitionNames(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, int max_parts);PartitionSpecProxy listPartitionSpecs(String dbName, String tableName, int maxParts);PartitionSpecProxy listPartitionSpecs(String catName, String dbName, String tableName,int maxParts);List&lt;Partition&gt; listPartitionsWithAuthInfo(String dbName, String tableName, List&lt;String&gt; partialPvals, short maxParts, String userName, List&lt;String&gt; groupNames);List&lt;Partition&gt; listPartitionsWithAuthInfo(String catName, String dbName, String tableName, List&lt;String&gt; partialPvals, int maxParts, String userName, List&lt;String&gt; groupNames);In order to support partitions on temporary tables, the majority of these methods must be implemented in SessionHiveMetastoreClient.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestListPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.ConditionalIgnoreOnSessionHiveMetastoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21815" opendate="2019-5-31 00:00:00" fixdate="2019-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats in ORC file are parsed twice</summary>
      <description>ORC record reader unnecessarily parses stats twice if (orcTail == null) { Reader orcReader = OrcFile.createReader(file.getPath(), OrcFile.readerOptions(context.conf) .filesystem(fs) .maxLength(AcidUtils.getLogicalLength(fs, file))); orcTail = new OrcTail(orcReader.getFileTail(), orcReader.getSerializedFileFooter(), file.getModificationTime()); if (context.cacheStripeDetails) { context.footerCache.put(new FooterCacheKey(fsFileId, file.getPath()), orcTail); } } stripes = orcTail.getStripes(); stripeStats = orcTail.getStripeStatistics();We go from Reader -&gt; OrcTail -&gt; StripeStatistics.stripeStats is read out of the orcTail and is already read inside orcReader.getStripeStatistics(). tez-am-2x-protobuf.svg</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="21825" opendate="2019-6-3 00:00:00" fixdate="2019-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve client error msg when Active/Passive HA is enabled</summary>
      <description>When Active/Passive HA is enabled and when client tries to connect to Passive HA or when HS2 is still starting up, clients will receive the following the error msg'Cannot open sessions on an inactive HS2 instance; use service discovery to connect'This error msg can be improved to say that HS2 is still starting up (or more user-friendly error msg). </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21841" opendate="2019-6-6 00:00:00" fixdate="2019-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leader election in HMS to run housekeeping tasks.</summary>
      <description>HMS performs housekeeping tasks. When there are multiple HMSes we need to have a leader HMS elected which will carry out those housekeeping tasks. These tasks include execution of compaction tasks, auto-discovering partitions for external tables, generation of compaction tasks, repl thread etc.Note that, though the code for compaction tasks, auto-discovery of partitions etc. is in Hive, the actual tasks are initiated by an HMS configured to do so. So, leader election is required only for HMS and not for HS2.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="21858" opendate="2019-6-11 00:00:00" fixdate="2019-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default to store runtime statistics in the metastore</summary>
      <description>Right now the reuse scope of runtime statistics is limited to re-running the actual query</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.tez.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2186" opendate="2011-5-26 00:00:00" fixdate="2011-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic Partitioning Failing because of characters not supported globStatus</summary>
      <description>Some dynamic queries failed on the stage of loading partitions if dynamic partition columns contain special characters. We need to escape all of them.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21868" opendate="2019-6-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorize CAST...FORMAT</summary>
      <description>Vectorize UDFs for CAST (&lt;TIMESTAMP/DATE&gt; AS STRING/CHAR/VARCHAR FORMAT &lt;STRING&gt;) and CAST (&lt;STRING/CHAR/VARCHAR&gt; AS TIMESTAMP/DATE FORMAT &lt;STRING&gt;).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cast.datetime.with.sql.2016.format.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cast.datetime.with.sql.2016.format.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCastFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="21869" opendate="2019-6-13 00:00:00" fixdate="2019-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up the Kafka storage handler readme and examples</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="21892" opendate="2019-6-18 00:00:00" fixdate="2019-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trusted domain authentication should look at X-Forwarded-For header as well</summary>
      <description>HIVE-21783 added trusted domain authentication. However, it looks only at request.getRemoteAddr() which works in most cases where there are no intermediate forward/reverse proxies. In trusted domain scenarios, if there intermediate proxies, the proxies typically append its own ip address "X-Forwarded-For" header. The X-Forwarded-For will look like clientIp -&gt; proxyIp1 -&gt; proxyIp2. The left most ip address in the X-Forwarded-For represents the real client ip address. For such scenarios, add a config to optionally look at X-Forwarded-For header when available to determine the real client ip.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21894" opendate="2019-6-19 00:00:00" fixdate="2019-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop credential password storage for the Kafka Storage handler when security is SSL</summary>
      <description>The Kafka storage handler assumes that if the Hive service is configured with Kerberos then the destination Kafka cluster is also secured with the same Kerberos realm or trust of realms.  The security configuration of the Kafka client can be overwritten due to the additive operations of the Kafka client configs, but, the only way to specify SSL and the keystore/truststore user/pass is via plain text table properties. This ticket proposes adding Hadoop credential security to the Kafka storage handler in support of SSL secured Kafka clusters.  </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.kafka.kafka.storage.handler.q.out</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaUtilsTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaUtils.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaTableProperties.java</file>
      <file type="M">kafka-handler.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="21952" opendate="2019-7-3 00:00:00" fixdate="2019-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should allow to delete serde properties too, not just add them</summary>
      <description>Hive should allow to delete serde properties not just add/change themWe have a use case when a presence of certain serde properties causes issues and we want to delete just that one serde property. It's not currently possible.Thanks. </description>
      <version>2.3.5,3.0.0,4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.table.storage.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.table.storage.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.storage.serde.AlterTableSetSerdePropsAnalyzer.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21957" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create temporary table like should omit transactional properties</summary>
      <description>In case of create temporary table like queries, where the source table is transactional, the transactional properties should not be copied over to the new table. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21958" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The list of table expression in the inclusion and exclusion list should be separated by &amp;#39;|&amp;#39; instead of comma.</summary>
      <description>Java regex expression does not support comma. If user wants multiple expression to be present in the include or exclude list, then the expressions can be provided separated by pipe ('|') character. The policy will look something like db_name.'(t1*)|(t3)'.'t100'</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.repl.ReplScope.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
    </fixedFiles>
  </bug>
  <bug id="21960" opendate="2019-7-5 00:00:00" fixdate="2019-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable HMS tasks on replica databases.</summary>
      <description>An HMS performs a number of housekeeping tasks. Assess whether They are required to be performed in the replicated data Performing those on replicated data causes any issues and how to fix those.</description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestPartitionManagement.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionManagementTask.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.repl.ReplConst.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.creation.CreateTableOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.BaseReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="21970" opendate="2019-7-8 00:00:00" fixdate="2019-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using RegistryUtils.currentUser()</summary>
      <description>RegistryUtils.currentUser() does replacement of '_' with '-' for DNS reasons. This is used inconsistently in some places causing issues wrt. ZK (deletion token secret manager, llap cluster membership for external clients). Replace RegistryUtils.currentUser() with UserGroupInformation.getCurrentUser().getShortUserName() for consistency. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.TezAmRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
    </fixedFiles>
  </bug>
  <bug id="21991" opendate="2019-7-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ORC version to 1.5.6</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.multi.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22007" opendate="2019-7-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not push unsupported types to specific JDBC sources from Calcite</summary>
      <description>We should not push a project expression if it uses a type that a specific dialect does not support, e.g., boolean in Oracle.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCProjectPushDownRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="22009" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTLV with user specified location is not honoured</summary>
      <description>Steps to repro : CREATE TABLE emp_table (id int, name string, salary int);insert into emp_table values(1,'aaaaa',20000);CREATE VIEW emp_view AS SELECT * FROM emp_table WHERE salary&gt;10000;CREATE EXTERNAL TABLE emp_ext_table like emp_view LOCATION '/tmp/emp_ext_table';show create table emp_ext_table; +----------------------------------------------------+| createtab_stmt |+----------------------------------------------------+| CREATE EXTERNAL TABLE `emp_ext_table`( || `id` int, || `name` string, || `salary` int) || ROW FORMAT SERDE || 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' || STORED AS INPUTFORMAT || 'org.apache.hadoop.mapred.TextInputFormat' || OUTPUTFORMAT || 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' || LOCATION || 'hdfs://nn:8020/warehouse/tablespace/external/hive/emp_ext_table' || TBLPROPERTIES ( || 'bucketing_version'='2', || 'transient_lastDdlTime'='1563467962') |+----------------------------------------------------+Table Location is not '/tmp/emp_ext_table', instead location is set to default warehouse path.  </description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.creation.CreateTableLikeOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="22045" opendate="2019-7-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-21711 introduced regression in data load</summary>
      <description>Better fix for HIVE-21711 is to specialize the handling for CTAS/Create MV statements to avoid intermittent rename operation but keep INSERT etc statements do intermittent rename since otherwise final move by file operation is significantly slow for such statements.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="22059" opendate="2019-7-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar doesn&amp;#39;t contain (fasterxml) jackson library</summary>
      <description>While deploying master branch into a container I've noticed that the jackson libraries are not 100% sure that are available at runtime - this is probably due to the fact that we are still using the "old" codehaus jackson and also the "new" fasterxml one.]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1INFO : Completed executing command(queryId=vagrant_20190729141949_8d8c7f0d-0ac4-4d76-ba12-6ec01561b040); Time taken: 5.127 secondsINFO : Concurrency mode is disabled, not creating a lock managerError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1564408646590_0005_1_00, diagnostics=[Vertex vertex_1564408646590_0005_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: _dummy_table initializer failed, vertex=vertex_1564408646590_0005_1_00 [Map 1], java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapperat org.apache.hadoop.hive.ql.exec.Utilities.&lt;clinit&gt;(Utilities.java:226)at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:428)at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:508)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:488)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:337)at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:122)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.ObjectMapperat java.net.URLClassLoader.findClass(URLClassLoader.java:382)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 19 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 (state=08S01,code=2)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22063" opendate="2019-7-30 00:00:00" fixdate="2019-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Authorization in Hive based on object ownership - HMS code path</summary>
      <description>This takes care of adding the owner and ownertype in the HMS code path</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.CreateTableEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="22066" opendate="2019-7-31 00:00:00" fixdate="2019-5-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache parent POM to version 21</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.css.bootstrap-theme.min.css</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.images.hive.logo.jpeg</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22075" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the max-reducers=1 regression from HIVE-14200</summary>
      <description>The condition does not kick in when minPartition=1, maxPartition=1, nReducers=1, maxReducers=1</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="22079" opendate="2019-8-2 00:00:00" fixdate="2019-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Post order walker for iterating over expression tree</summary>
      <description>Current DefaultGraphWalker is used to iterate over an expression tree. This walker uses hash map to keep track of visited/processed nodes. If an expression tree is large this adds significant overhead due to map lookup.For an expression trees we can instead use post order traversal and avoid using map.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.ExpressionWalker.java</file>
    </fixedFiles>
  </bug>
  <bug id="22110" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initialize ReplChangeManager before starting actual dump</summary>
      <description>REPL DUMP calls ReplChageManager.encodeFileUri() to add cmroot and checksum to the url. This requires ReplChangeManager to be initialized. So, initialize Repl change manager when taking a dump.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FunctionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="22120" opendate="2019-8-16 00:00:00" fixdate="2019-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix wrong results/ArrayOutOfBound exception in left outer map joins on specific boundary conditions</summary>
      <description>Vectorized version of left outer map join produces wrong results or encounters ArrayOutOfBound exception.The boundary conditions are: The complete batch of the big table should have the join key repeated for all the join columns. The complete batch of the big table should have not have a matched key value in the small table The repeated value should not be a null value Some rows should be filtered out as part of the on clause filter.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22125" opendate="2019-8-16 00:00:00" fixdate="2019-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move to Kafka 2.3 Clients</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.TransactionalKafkaWriterTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaBrokerResource.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.HiveKafkaProducerTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.TransactionalKafkaWriter.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaStorageHandler.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.HiveKafkaProducer.java</file>
      <file type="M">kafka-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22136" opendate="2019-8-22 00:00:00" fixdate="2019-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on tez.bucket.pruning</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22145" opendate="2019-8-26 00:00:00" fixdate="2019-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid optimizations for analyze compute statistics</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22151" opendate="2019-8-27 00:00:00" fixdate="2019-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn off hybrid grace hash join by default</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.join.noncbo.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22158" opendate="2019-8-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation layer - Disallow non-ACID MANAGED tables.</summary>
      <description>In the recent commits, we have allowed non-ACID MANAGED tables to be created by clients that have some form of ACID WRITE capabilities. I think it would make sense to disallow this entirely. MANAGED tables should be ACID tables only.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOnHDFSEncryptedZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22164" opendate="2019-8-30 00:00:00" fixdate="2019-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Limit operator returns wrong number of results with offset</summary>
      <description>Vectorized Limit operator returns wrong number of results with offset</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22170" opendate="2019-9-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="22189" opendate="2019-9-11 00:00:00" fixdate="2019-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation: Enforce strict locations for managed vs external tables.</summary>
      <description>Currently, HMS allows flexibility with location of a table. External tables can be located within Hive managed warehouse space and managed tables can be located within the external warehouse directory if the user chooses to do so.There are certain advantages to restrict such flexibility. We could have different encryption policies for different warehouses, different replication policies etc.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreMetadataTransformer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsProjectionSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsFilterSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterTableRequest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2219" opendate="2011-6-14 00:00:00" fixdate="2011-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make "alter table drop partition" more efficient</summary>
      <description>The current function dropTable() that handles dropping multiple partitions is somewhat inefficient. For each partition you want to drop, it loops through each partition in the table to see if the partition exists. This is an O(mn) operation, where m is the number of partitions to drop, and n is the number of partitions in the table. The running time of this function can be improved, which is useful for tables with many partitions.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
    </fixedFiles>
  </bug>
  <bug id="22205" opendate="2019-9-14 00:00:00" fixdate="2019-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper and curator versions</summary>
      <description>Other components like hadoop have switched to using new ZK versions. So these jars end up in classpath for hive services and could cause issues due to in-compatible curator versions that hive uses.So it makes sense for hive to upgrade the ZK and curator versions to try to keep up.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hive.testutils.MiniZooKeeperCluster.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestUtils.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.llap.registry.impl.TestLlapZookeeperRegistryImpl.java</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestMiniClusters.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.llap.LlapItUtils.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloTestSetup.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.server.TestInformationSchemaWithPrivilege.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.security.TestZooKeeperTokenStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.ManyMiniCluster.java</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22209" opendate="2019-9-16 00:00:00" fixdate="2019-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Creating a materialized view with no tables should be handled more gracefully</summary>
      <description>Currently, materialized views without a table reference are not supported. However, instead of printing a clear message about it, when a materialized view is created without a table reference, we fail with an unclear message.&gt; create materialized view mv_test1 as select 5;(...)ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Add request failed :INSERT INTO MV_TABLES_USED (MV_CREATION_METADATA_ID,TBL_ID) VALUES (?,?) )INFO : Completed executing command(queryId=hive_20190916203511_b609cccf-f5e3-45dd-abfd-6e869d94e39a); Time taken: 10.469 secondsError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Add request failed : INSERT INTO MV_TABLES_USED (MV_CREATION_METADATA_ID,TBL_ID) VALUES (?,?) ) (state=08S01,code=1)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
    </fixedFiles>
  </bug>
  <bug id="22210" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization may reuse computation output columns involved in filtering</summary>
      <description>running the following test with TestMiniLlapLocalCliDriver leads to an unexpected results; the coalesce calculated inside the subquery has a value of 1 instead of the correct(922) value.drop table if exists u_table_4;create table u_table_4(smallint_col_22 smallint, int_col_5 int);insert into u_table_4 values(238,922);drop table u_table_7;create table u_table_7 ( bigint_col_3 bigint, int_col_10 int);insert into u_table_7 values (571,198);drop table u_table_19;create table u_table_19 (bigint_col_18 bigint ,int_col_19 int, STRING_COL_7 string);insert into u_table_19 values (922,5,'500');set hive.mapjoin.full.outer=true;set hive.auto.convert.join=true;set hive.query.results.cache.enabled=false;set hive.merge.nway.joins=true;set hive.vectorized.execution.enabled=true;--explain analyze SELECT a5.int_col, 922 as expected, COALESCE(a5.int_col, a5.aa) as expected2, a5.int_col_3 as realityFROM u_table_19 a1 FULL OUTER JOIN ( SELECT a2.int_col_5 AS int_col, a2.smallint_col_22 as aa, COALESCE(a2.int_col_5, a2.smallint_col_22) AS int_col_3 FROM u_table_4 a2 ) a5 ON ( a1.bigint_col_18) = (a5.int_col_3) INNER JOIN ( SELECT a3.bigint_col_3 AS int_col, Cast (COALESCE(a3.bigint_col_3, a3.bigint_col_3, a3.int_col_10) AS BIGINT) * Cast (a3.bigint_col_3 AS BIGINT) AS int_col_3 FROM u_table_7 a3 WHERE bigint_col_3=571 ) a4ON (a1.int_col_19=5) OR ((a5.int_col_3) IN (a4.int_col, 10)) where a1.STRING_COL_7='500'ORDER BY int_col DESC nulls last limit 100;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22211" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change maven phase to generate test sources</summary>
      <description>Some protobuf files are generated in the wrong phase; so I get compile errors because they are not there for eclipse...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22212" opendate="2019-9-17 00:00:00" fixdate="2019-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement append partition related methods on temporary tables</summary>
      <description>The following methods must be implemented in SessionHiveMetastoreClient, in order to support partition append on temporary tables: Partition appendPartition(String dbName, String tableName, List&lt;String&gt; partVals) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String catName, String dbName, String tableName, List&lt;String&gt; partVals) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String dbName, String tableName, String name) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String catName, String dbName, String tableName, String name) throws InvalidObjectException, AlreadyExistsException, MetaException, TException;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22213" opendate="2019-9-18 00:00:00" fixdate="2019-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TxnHander cleanupRecords should only clean records belonging to default catalog</summary>
      <description>Currently it removes record for given database and given table without checking for the catalog, as a result it can end up removing records when it shouldn't.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22214" opendate="2019-9-18 00:00:00" fixdate="2019-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain vectorization should disable user level explain</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22227" opendate="2019-9-20 00:00:00" fixdate="2019-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez bucket pruning produces wrong result with shared work optimization</summary>
      <description>Reproducerset hive.tez.bucket.pruning=true;set hive.optimize.shared.work=true;CREATE TABLE srcbucket_mapjoin_n16(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;CREATE TABLE tab_part_n10 (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS ORCFILE;CREATE TABLE srcbucket_mapjoin_part_n17 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;load data local inpath '$HIVE_SRC/data/files/bmj/000000_0' INTO TABLE srcbucket_mapjoin_n16 partition(ds='2008-04-08');load data local inpath '.$HIVE_SRC/data/files/bmj1/000001_0' INTO TABLE srcbucket_mapjoin_n16 partition(ds='2008-04-08');load data local inpath '$HIVE_SRC/data/files/bmj/000000_0' INTO TABLE srcbucket_mapjoin_part_n17 partition(ds='2008-04-08');load data local inpath '$HIVE_SRC/data/files/bmj/000001_0' INTO TABLE srcbucket_mapjoin_part_n17 partition(ds='2008-04-08');load data local inpath '$HIVE_SRC/data/files/bmj/000002_0' INTO TABLE srcbucket_mapjoin_part_n17 partition(ds='2008-04-08');set hive.optimize.bucketingsorting=false;insert overwrite table tab_part_n10 partition (ds='2008-04-08')select key,value from srcbucket_mapjoin_part_n17;CREATE TABLE tab_n9(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORCFILE;insert overwrite table tab_n9 partition (ds='2008-04-08')select key,value from srcbucket_mapjoin_n16;select * from(select * from tab_n9 where tab_n9.key = 0)ajoin(select * from tab_part_n10 where tab_part_n10.key = 98)b full outer join tab_part_n10 c on a.key = b.key and b.key = c.keyorder by 1,2,3,4,5,6,7,8,9;</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.mergejoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22267" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support password based authentication in HMS</summary>
      <description>Similar to HS2, support password based authentication in HMS.Right now we provide LDAP and CONFIG based options. The later allows to set user and password in config and is used only for testing.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22291" opendate="2019-10-4 00:00:00" fixdate="2019-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation: Limit translation to hive default catalog only</summary>
      <description>HMS Translation should only be limited to a single catalog.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22292" opendate="2019-10-4 00:00:00" fixdate="2019-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Hypothetical-Set Aggregate Functions</summary>
      <description>&lt;hypothetical set function&gt; ::= &lt;rank function type&gt; &lt;left paren&gt; &lt;hypothetical set function value expression list&gt; &lt;right paren&gt; &lt;within group specification&gt;&lt;rank function type&gt; ::= RANK | DENSE_RANK | PERCENT_RANK | CUME_DISTExample:CREATE TABLE table1 (column1 int);INSERT INTO table1 VALUES (NULL), (3), (8), (13), (7), (6), (20), (NULL), (NULL), (10), (7), (15), (16), (8), (7), (8), (NULL);SELECT rank(6) WITHIN GROUP (ORDER BY column1) FROM table1;2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.disc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.cont.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestParseWithinGroupClause.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileDisc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileCont.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="22313" opendate="2019-10-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some of the HMS auth LDAP hive config names do not start with "hive."</summary>
      <description>Click to add description</description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22315" opendate="2019-10-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Decimal64 column division with decimal64 scalar</summary>
      <description>Currently division operation is not supported for Decimal64 column. This Jira will take care of supporting decimal64 column division with a decimal64 scalar.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.col.scalar.division.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22317" opendate="2019-10-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Beeline-site parser does not handle the variable substitution correctly</summary>
      <description>beeline-site.xml&lt;configuration xmlns:xi="http://www.w3.org/2001/XInclude"&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.jdbc.url.container&lt;/name&gt; &lt;value&gt;jdbc:hive2://c3220-node2.host.com:2181,c3220-node3.host.com:2181,c3220-node4.host.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.jdbc.url.default&lt;/name&gt; &lt;value&gt;test&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;beeline.hs2.jdbc.url.test&lt;/name&gt;&lt;value&gt;${beeline.hs2.jdbc.url.container}?tez.queue.name=myqueue&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;beeline.hs2.jdbc.url.llap&lt;/name&gt; &lt;value&gt;jdbc:hive2://c3220-node2.host.com:2181,c3220-node3.host.com:2181,c3220-node4.host.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;beeline fail to connect because it does not parse the substituted value correctlybeelineError in parsing jdbc url: ${beeline.hs2.jdbc.url.container}?tez.queue.name=myqueue from beeline-site.xmlbeeline&gt;</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.hs2connection.TestBeelineSiteParser.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.BeelineSiteParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="22331" opendate="2019-10-13 00:00:00" fixdate="2019-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unix_timestamp without argument returns timestamp in millisecond instead of second.</summary>
      <description>After HIVE-22170, select unix_timestamp(); is returning milliseconds, but expected output is in seconds.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
    </fixedFiles>
  </bug>
  <bug id="22332" opendate="2019-10-14 00:00:00" fixdate="2019-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should ensure valid schema evolution settings since ORC-540</summary>
      <description>For details please see: https://issues.apache.org/jira/browse/ORC-558</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22339" opendate="2019-10-14 00:00:00" fixdate="2019-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default time for MVs refresh in registry</summary>
      <description>Default was set to 60secs in HIVE-21344. It seems it may be too aggressive; suggestion is to change default to 1500secs.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22342" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation: HIVE-22189 too strict with location for EXTERNAL tables</summary>
      <description>HIVE-22189 restricts EXTERNAL tables being created to be restricted to the EXTERNAL_WAREHOUSE_DIR. This might be too strict as any other location should be allowed as long as the location is outside the MANAGED warehouse directory.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22382" opendate="2019-10-22 00:00:00" fixdate="2019-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Decimal64 column division with decimal64 Column</summary>
      <description>Support Decimal64 column division with decimal64 Column</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal64.div.decimal64scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal64.div.decimal64scalar.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.decimal64.div.decimal64scalar.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnDivideDecimal64Scalar.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22444" opendate="2019-11-1 00:00:00" fixdate="2019-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Project POM Files</summary>
      <description>Address warnings in the build process Use DependencyManagement in Root POM for ITest (see HIVE-22426) General POM cleanup</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-kudu.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22498" opendate="2019-11-14 00:00:00" fixdate="2019-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema tool enhancements to merge catalogs</summary>
      <description>Schema tool currently supports relocation of database from one catalog to another, one at a time. While having to do this one at a time is painful, it also lacks support for converting them to external tables during migration, in lieu of the changes to the translation layer where a MANAGED table is strictly ACID-only table.Hence we also need to convert them to external tables during relocation.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskMergeCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="22512" opendate="2019-11-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use direct SQL to fetch column privileges in refreshPrivileges</summary>
      <description>refreshPrivileges() calls listTableAllColumnGrants() to fetch the column level privileges. The later function retrieves the individual column objects by firing one query per column privilege object, thus causing the backend db to be swamped by these queries when PrivilegeSynchronizer is run. PrivilegeSynchronizer synchronizes privileges of all the databases, tables and columns and thus the backend db can get swamped really bad when there are thousands of tables with hundreds of columns.The output of listTableAllColumnGrants() is not used completely so all the columns the PM has tried to retrieves anyway goes waste.Fix this by using direct SQL to fetch column privileges.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="22513" opendate="2019-11-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation of casted column in filter ops can cause incorrect results</summary>
      <description>This issue happens if CBO is disabled.We should not be propagating constants if the corresponding ExprNodeColumnDesc instance is wrapped inside a CAST operator as casting might truncate information from the original column.This can happen if we're using CAST in a WHERE clause, which will cause the projected columns to be replaced in a SELECT operator. Their new value will be the result of casting which could be a different value compared to that in the original column:set hive.cbo.enable=false;set hive.fetch.task.conversion=more; --just for testing conveniencecreate table testtb (id string);insert into testtb values('2019-11-05 01:01:11');select id, CAST(id AS VARCHAR(10)) from testtb where CAST(id AS VARCHAR(9)) = '2019-11-0';+------------+------------+|     id     |    _c1     |+------------+------------+| 2019-11-0  | 2019-11-0  |+------------+------------+1 row selected (0.168 seconds)-- VS expected: 2019-11-05 01:01:11 | 2019-11-05 As to what types of casting (from and where types) cause information loss it's hard to properly keep track of, and I don't think it should be taken into consideration when deciding whether or not to propagate a constant. Rather than adding a big and potentially convoluted and fragile check for this, I propose to prevent constant mappings to be spawned out of CASTed columns.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22515" opendate="2019-11-20 00:00:00" fixdate="2019-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support cast to decimal64 in Vectorization</summary>
      <description>Support cast to decimal64 in Vectorization</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22518" opendate="2019-11-20 00:00:00" fixdate="2019-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLStdHiveAuthorizerFactoryForTest doesn&amp;#39;t work correctly for llap tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.schq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sysdb.schq.q</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2252" opendate="2011-7-1 00:00:00" fixdate="2011-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display a sample of partitions created when Fatal Error occurred due to too many partitioned created</summary>
      <description>In dynamic partition inserts, if a mapper created too many partitions, a fatal error is raised and the job got killed. Sometimes the error is caused by data error and it will be helpful for users to debug if we display a sample of dynamic partitions generated.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22537" opendate="2019-11-25 00:00:00" fixdate="2019-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getAcidState() not saving directory snapshot causes multiple calls to S3 api</summary>
      <description>Fix for HIVE-21225 is not enabled in query coordinator codepath. The last argument (generateDirSnapshots) for getAcidState() is set to false when invoked by callInternal(). Also, snapshot is not used for file exists calls.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22539" opendate="2019-11-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 SPNEGO authentication should skip if authorization header is empty</summary>
      <description>Currently HiveServer2 SPNEGO authentication waits until setting up Kerberos before checking header. This can be checked up front to avoid doing any Kerberos related work if the header is empty. This is helpful in a lot of cases since typically the first request is empty with the client waiting for a 401 before returning the Authorization header.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="22553" opendate="2019-11-27 00:00:00" fixdate="2019-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose locks and transactions in sys db</summary>
      <description>Create new sysdb tables/views to access lock and transaction data.This allows to provide admins with live data about ongoing locks and transacions. Due to this being in the sys db access to this information can be restricted to select privileged users.Information about locks and compactions can be joined and accessed at the same time.Compaction related transactions would also be visible.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug id="22555" opendate="2019-11-27 00:00:00" fixdate="2019-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ORC version to 1.5.8</summary>
      <description>Hive currently depends on ORC 1.5.6. We need 1.5.8 upgrade for https://issues.apache.org/jira/browse/HIVE-22499ORC-1.5.7 includes https://issues.apache.org/jira/browse/ORC-361 . It causes some tests overriding MemoryManager to fail. These need to be addressed while upgrading. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22558" opendate="2019-11-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore: Passwords jceks should be read lazily, in case of connection pools</summary>
      <description>The jceks file is parsed for every instance of the metastore conf to populate the password in plain-text, which is irrelevant for the scenario where the DB connection pool is already active. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PersistenceManagerProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="22596" opendate="2019-12-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RawStore used by Initiator is not thread-safe</summary>
      <description>RawStore used by Initiator is not thread-safe. To avoid synchronization, we can replace it with ThreadLocal variable.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MetaStoreCompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22599" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query results cache: 733 permissions check is not necessary</summary>
      <description>The query results cache initialization makes a call to Utilties.ensurePathIsWritable(), which checks the results cache directory for 733 permissions (default cache dir is/tmp/hive/resultscache).The 733 permissions (at least the 033 part) are not actually necessary - we actually don't really want the results cache directory to be world-writable, and the subdirectories we create within this one are actually done with 700 perms. So I think the call to Utilties.ensurePathIsWritable() can be removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="22629" opendate="2019-12-11 00:00:00" fixdate="2019-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AST Node Children can be quite expensive to build due to List resizing</summary>
      <description>As per the attached profile, The AST Node can be a major source of CPU and memory churn, due to the ArrayList resizing and copy.In my Opinion this can be amortized by providing the actual size.jcamachorodriguez / vgarg</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="22635" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable scheduled query executor for unittests</summary>
      <description>HIVE-21884 missed to set the default to off; so it may sometime interfere with unit tests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22637" opendate="2019-12-12 00:00:00" fixdate="2019-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid cost based rules during generating expressions from AST</summary>
      <description>genExprNode uses default dispatcher which fire rules based on cost, computation of cost is expensive and is likely un-necessary.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22638" opendate="2019-12-12 00:00:00" fixdate="2019-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix insert statement issue with return path</summary>
      <description>Insert statements were not handled properly with return path. It was revealed during examining why TestUpgradeTool is not working with return path.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.opconventer.HiveTableScanVisitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.opconventer.HiveTableFunctionScanVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22708" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test fix for http transport</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>2.3.8,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22745" opendate="2020-1-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config option to turn off read locks</summary>
      <description>Although its not recommended but in perf critical scenario this option may be exercised. We have observed lock acquisition to take long time in heavily loaded system.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2275" opendate="2011-7-8 00:00:00" fixdate="2011-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2219 and apply correct patch to improve the efficiency of dropping multiple partitions</summary>
      <description>HIVE-2219 applied an incorrect patch that fails unit tests. This patch reverts those changes and adds the intended changes to improve the efficiency of dropping multiple partitions.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
    </fixedFiles>
  </bug>
  <bug id="22754" opendate="2020-1-21 00:00:00" fixdate="2020-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trim some extra HDFS find file name calls that can be deduced using current TXN watermark</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22767" opendate="2020-1-23 00:00:00" fixdate="2020-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesn&amp;#39;t parse semicolons in comments properly</summary>
      <description>HIVE-12646 fixed the handling of semicolons in quoted strings, but leaves the problem of semicolons in comments. E.g. with beeline connected to any database...this works: select 1; select /* */ 2; select /* */ 3;this doesn't work: select 1; select /* ; */ 2; select /* ; */ 3;This has been fixed and reintroduced before (possibly multiple times). Ideally, there should be a single utility method somewhere to separate comments, strings and commands &amp;#8211; with the proper testing in place (q files).However, I'm trying to make this fix back-portable, so a light touch is needed. I'm focusing on beeline for now, and only writing (very thorough) unit tests, as I cannot exclude any new q files from TestCliDriver (which would break, since it's using a different parsing method).P.S. excerpt of the error message:0: jdbc:hive2://...&gt; select 1; select /* ; */ 2; select /* ; */ 3;INFO : Compiling command(queryId=...): select 1INFO : Semantic Analysis Completed (retrial = false)INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)INFO : Completed compiling command(queryId=...); Time taken: 0.38 secondsINFO : Executing command(queryId=...): select 1INFO : Completed executing command(queryId=...); Time taken: 0.004 secondsINFO : OK+------+| _c0 |+------+| 1 |+------+1 row selected (2.007 seconds)INFO : Compiling command(queryId=...): select /*ERROR : FAILED: ParseException line 1:9 cannot recognize input near '&lt;EOF&gt;' '&lt;EOF&gt;' '&lt;EOF&gt;' in select clauseorg.apache.hadoop.hive.ql.parse.ParseException: line 1:9 cannot recognize input near '&lt;EOF&gt;' '&lt;EOF&gt;' '&lt;EOF&gt;' in select clause at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:233) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:79) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:72) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:598) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1505) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1452) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1447) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) at ...Similarly, the following query also fails:select /* ' */ 1; select /* ' */ 2;I suspect line comments are also not handled properly but I cannot reproduce this in interactive beeline...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestCommands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="22783" opendate="2020-1-27 00:00:00" fixdate="2020-1-27 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add test for HIVE-22366</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestNumMetastoreCallsObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestNumMetastoreCalls.java</file>
    </fixedFiles>
  </bug>
  <bug id="22786" opendate="2020-1-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Agg with distinct can be optimised in HASH mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.nocolumnalign.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.distinct.gby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22788" opendate="2020-1-29 00:00:00" fixdate="2020-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query cause NPE due to implicit cast on ROW__ID</summary>
      <description>ReproCREATE TABLE table_16 (timestamp_col_19 timestamp,timestamp_col_29 timestamp,int_col_27 int,int_col_39 int,boolean_col_18 boolean,varchar0045_col_23 varchar(45));CREATE TABLE table_7 (int_col_10 int,bigint_col_3 bigint);CREATE TABLE table_10 (boolean_col_8 boolean,boolean_col_16 boolean,timestamp_col_5 timestamp,timestamp_col_15 timestamp,timestamp_col_30 timestamp,decimal3825_col_26 decimal(38, 25),smallint_col_9 smallint,int_col_18 int);explain cbo SELECT DISTINCT COALESCE(a4.timestamp_col_15, IF(a4.boolean_col_16, a4.timestamp_col_30, a4.timestamp_col_5)) AS timestamp_colFROM table_7 a3RIGHT JOIN table_10 a4 WHERE (a3.bigint_col_3) &gt;= (a4.int_col_18)INTERSECT ALLSELECT COALESCE(LEAST( COALESCE(a1.timestamp_col_19, CAST('2010-03-29 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2014-08-16 00:00:00' AS TIMESTAMP)) ), GREATEST(COALESCE(a1.timestamp_col_19, CAST('2013-07-01 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2028-06-18 00:00:00' AS TIMESTAMP))) ) AS timestamp_colFROM table_16 a1 GROUP BY COALESCE(LEAST( COALESCE(a1.timestamp_col_19, CAST('2010-03-29 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2014-08-16 00:00:00' AS TIMESTAMP)) ), GREATEST( COALESCE(a1.timestamp_col_19, CAST('2013-07-01 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2028-06-18 00:00:00' AS TIMESTAMP))) );</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2279" opendate="2011-7-12 00:00:00" fixdate="2011-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement sort_array UDF</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="22793" opendate="2020-1-30 00:00:00" fixdate="2020-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default settings in HMS Benchmarking tool</summary>
      <description>HMS Benchmarking tool has invalid default setting values. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.tools-common.src.main.java.org.apache.hadoop.hive.metastore.tools.Constants.java</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="22824" opendate="2020-2-4 00:00:00" fixdate="2020-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinProjectTranspose rule should skip Projects containing windowing expression</summary>
      <description>Otherwise this rule could end up creating plan with windowing expression within join condition which hive doesn't know how to process.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.results.cache.with.auth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join46.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.results.cache.with.auth.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join46.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22825" opendate="2020-2-4 00:00:00" fixdate="2020-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce directory lookup cost for acid tables</summary>
      <description>With objectstores, directory lookup costs are expensive. For acid tables, it would be good to have a directory cache to reduce number of lookup calls.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22827" opendate="2020-2-4 00:00:00" fixdate="2020-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flatbuffer version</summary>
      <description>Hive currently uses Flatbuffer 1.2.0. Other Apache projects use a more up-to-date version, e.g. 1.6.0.1. Upgrade to that version.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22853" opendate="2020-2-7 00:00:00" fixdate="2020-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow JDBC FetchSize to Be Set in Beeline</summary>
      <description>Currently beeline uses a hard coded default of 1000 rows for fetchSize. This default value is different from what the server has set. While the beeline user can reset the value via set command, its cumbersome to change the workloads.Rather it should default to the server-side value and set should be used to override within the session.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="22860" opendate="2020-2-10 00:00:00" fixdate="2020-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support metadata only replication for external tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22863" opendate="2020-2-10 00:00:00" fixdate="2020-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Commit compaction txn if it is opened but compaction is skipped</summary>
      <description>Currently if a table does not have enough directories to compact, compaction is skipped and the compaction is either (a) marked ready for cleaning or (b) marked compacted. However, the txn the compaction runs in is never committed, it remains open, so TXNS and TXN_COMPONENTS will never be cleared of information about the attempted compaction.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
    </fixedFiles>
  </bug>
  <bug id="22870" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DML execution on TEZ always outputs the message &amp;#39;No rows affected&amp;#39;</summary>
      <description>Executing an update or insert statement in beeline doesn't show the actual rows inserted/updated.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.input.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="22873" opendate="2020-2-11 00:00:00" fixdate="2020-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to identify which hs2 instance executed a scheduled query</summary>
      <description>right now only the query_id is shown; in case of multiple hs2 instances the question...users have to resort to grepping the logs for the given query id....</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.schq.TestScheduledQueryService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="22877" opendate="2020-2-12 00:00:00" fixdate="2020-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix decimal boundary check for casting to Decimal64</summary>
      <description>During vectorization, decimal fields that are obtained via generic udfs are cast to Decimal64 in some circumstances. For decimal to decimal64 cast, hive compares the source column's `scale + precision` to 18(maximum number of digits that can be represented by a long). A decimal can fit in a long as long as its `precision` is smaller than or equal to 18. Scale is irrelevant.Since vectorized generic udf expression takes scale into account, it computes wrong output column vector: Decimal instead of Decimal64. This in turn causes ClassCastException down the operator chain.Below query fails with class cast exception: create table mini_store( s_store_sk int, s_store_id string)row format delimited fields terminated by '\t'STORED AS ORC;create table mini_sales( ss_store_sk int, ss_quantity int, ss_sales_price decimal(7,2))row format delimited fields terminated by '\t'STORED AS ORC;insert into mini_store values (1, 'store');insert into mini_sales values (1, 2, 1.2);select s_store_id, coalesce(ss_sales_price*ss_quantity,0) sumsalesfrom mini_sales, mini_store where ss_store_sk = s_store_sk  </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22880" opendate="2020-2-12 00:00:00" fixdate="2020-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: All delete event readers should ignore ORC SARGs</summary>
      <description>Delete delta readers should not apply any SARGs other than the ones related to the transaction id ranges within the inserts.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="22881" opendate="2020-2-12 00:00:00" fixdate="2020-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revise non-recommended Calcite api calls</summary>
      <description>RexUtil.simplify* methods</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectOverIntersectRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinConstraintsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveSubQRemoveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
    </fixedFiles>
  </bug>
  <bug id="22888" opendate="2020-2-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite checkLock inner select with JOIN operator</summary>
      <description>Replaced inner select under checkLocks using multiple IN statements with JOIN operator;generated query looks like :SELECT LS.* FROM ( SELECT HL_LOCK_EXT_ID, HL_DB, HL_TABLE, HL_PARTITION, HL_LOCK_STATE, HL_LOCK_TYPE FROM HIVE_LOCKS WHERE HL_LOCK_EXT_ID &lt; 333) LSINNER JOIN ( SELECT HL_DB, HL_TABLE, HL_PARTITION, HL_LOCK_TYPE FROM HIVE_LOCKS WHERE HL_LOCK_EXT_ID = 333) LBCON LS.HL_DB = LBC.HL_DB AND (LS.HL_TABLE IS NULL OR LBC.HL_TABLE IS NULL OR LS.HL_TABLE = LBC.HL_TABLE AND (LS.HL_PARTITION IS NULL OR LBC.HL_PARTITION IS NULL OR LS.HL_PARTITION = LBC.HL_PARTITION))WHERE (LBC.HL_TXNID = 0 OR LS.HL_TXNID != LBC.HL_TXNID) AND (LBC.HL_LOCK_TYPE='e' AND !(LS.HL_TABLE IS NULL AND LS.HL_LOCK_TYPE='r' AND LBC.HL_TABLE IS NOT NULL ) OR LBC.HL_LOCK_TYPE='w' AND LS.HL_LOCK_TYPE IN ('w','e') OR LBC.HL_LOCK_TYPE='r' AND LS.HL_LOCK_TYPE='e' AND !(LS.HL_TABLE IS NOT NULL AND LBC.HL_TABLE IS NULL))LIMIT 1;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="229" opendate="2009-1-13 00:00:00" fixdate="2009-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug output enabled in HIVE-206</summary>
      <description>The log4j was changed in HIVE-206 to enable debug for the root logger. This makes the shell hard to use with a lot of logging output flying by.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22914" opendate="2020-2-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive Connection ZK Interactions Easier to Troubleshoot</summary>
      <description>Add better logging and make errors more consistent and meaningful.Recently was trying to troubleshoot an issue where the ZK namespace of the client and the HS2 were different and it was way too difficult to diagnose.</description>
      <version>3.1.2,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2292" opendate="2011-7-19 00:00:00" fixdate="2011-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comment clause should immediately follow identifier field in CREATE DATABASE statement</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.database.location.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
    </fixedFiles>
  </bug>
  <bug id="22920" opendate="2020-2-21 00:00:00" fixdate="2020-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add row format OpenCSVSerde to the metastore column managed list</summary>
      <description>Add row format OpenCSVSerde to the metastore column managed list</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22922" opendate="2020-2-21 00:00:00" fixdate="2020-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: ShuffleHandler may not find shuffle data if pod restarts in k8s</summary>
      <description>Executor logs shows "Invalid map id: TTP/1.1 500 Internal Server Error". This happens when executor pod restarts with same hostname and port, but missing shuffle data.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22926" opendate="2020-2-25 00:00:00" fixdate="2020-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schedule Repl Dump Task using Hive scheduler</summary>
      <description>https://github.com/apache/hive/pull/927</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.repl.load.requires.admin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.repl.dump.requires.admin.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestStatsReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTablesMetaDataOnly.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTablesBootstrap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOnHDFSEncryptedZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestMetaStoreEventListenerInRepl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestCopyUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.ReplicationTestUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.BaseReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="22927" opendate="2020-2-25 00:00:00" fixdate="2020-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP should filter tasks in HB, instead of killing all tasks on error attempts</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22948" opendate="2020-2-29 00:00:00" fixdate="2020-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryCache: Treat query cache locations as temporary storage</summary>
      <description>The WriteEntity with a query cache query is considered for user authorization without having direct access for users.https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/command/CommandAuthorizerV2.java#L111 if (privObject instanceof WriteEntity &amp;&amp; ((WriteEntity)privObject).isTempURI()) { // do not authorize temporary uris continue; }is not satisfied by the queries qualifying for the query cache.</description>
      <version>3.1.2,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="22954" opendate="2020-3-2 00:00:00" fixdate="2020-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schedule Repl Load using Hive Scheduler</summary>
      <description>https://github.com/apache/hive/pull/932</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.repl.load.requires.admin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.repl.load.old.version.q</file>
      <file type="M">ql.src.test.queries.clientnegative.repl.load.requires.admin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.repl.dump.requires.admin.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestStatsReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTablesMetaDataOnly.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTablesBootstrap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOnHDFSEncryptedZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestMetaStoreEventListenerInRepl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestCopyUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.ReplicationTestUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.BaseReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.BaseReplicationAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="22971" opendate="2020-3-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate file rename in insert-only compactor</summary>
      <description>File rename is expensive for object stores, so MM (insert-only) compaction should skip that step when committing and write directly to base_x_cZ or delta_x_y_cZ.This also fixes the issue that for MM QB compaction the temp tables were stored under the table directory, and these temp dirs were never cleaned up.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22972" opendate="2020-3-4 00:00:00" fixdate="2020-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow table id to be set for table creation requests</summary>
      <description>Hive Metastore should accept requests for table creation where the id is set, ignoring it.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22974" opendate="2020-3-4 00:00:00" fixdate="2020-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore&amp;#39;s table location check should be applied when location changed</summary>
      <description>In HIVE-22189 a check was introduced to make sure managed and external tables are located at the proper space. This condition cannot be satisfied during an upgrade.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22982" opendate="2020-3-5 00:00:00" fixdate="2020-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopN Key efficiency check might disable filter too soon</summary>
      <description>The check is triggered after every n batches but there can be multiple filters, one for each partition. Some filters might have less data then the others.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestTopNKeyFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TopNKeyDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperGeneralComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNKeyOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22987" opendate="2020-3-6 00:00:00" fixdate="2020-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException in VectorCoalesce when DataTypePhysicalVariation is null</summary>
      <description>ClassCastException in VectorCoalesce when DataTypePhysicalVariation is null</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="2299" opendate="2011-7-21 00:00:00" fixdate="2011-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize Hive query startup time for multiple partitions</summary>
      <description>Added an optimization to the way input splits are computed.Reduced an O(n^2) operation to O n operation.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="22990" opendate="2020-3-6 00:00:00" fixdate="2020-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build acknowledgement mechanism for repl dump and load</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTablesBootstrap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="23039" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing for repl dump bootstrap phase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.DirCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.TableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="23053" opendate="2020-3-19 00:00:00" fixdate="2020-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean Up Stats Mergers</summary>
      <description>DEBUG log each invocation of merge Use the inherited logger properly Use parameterized logging Simplify code</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.TimestampColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.StringColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.ColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.BooleanColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.BinaryColumnStatsMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="23171" opendate="2020-4-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Tool To Visualize Hive Parser Tree</summary>
      <description>For some of the work I would like to do on HIVE-23149, it would be nice to visualize the output of the statement parser.I have created a tool that spits out the parser tree in DOT file format. This allows it to be visualized using a plethora of tools.To use it, compile the hive-parser test JAR and run it.  The application takes a single command line argument of a String.  The String is the SQL statement to parse:HqlParser "SELECT 1"I have attached an example of the output that I generated for a SELECT 1 statement:  </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">parser.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23173" opendate="2020-4-9 00:00:00" fixdate="2020-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>User login success/failed attempts should be logged</summary>
      <description>User login success &amp; failure attempts should be logged in server logs</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpRequestInterceptorBase.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="23192" opendate="2020-4-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"default" database locationUri should be external warehouse root.</summary>
      <description>When creating the default database, the database locationUri should be set to external warehouse.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="23195" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>set hive.cluster.delegation.token.gc-interval to 15 minutes instead of an hour</summary>
      <description>the config "hive.cluster.delegation.token.gc-interval" is set as long duration, 1 hour. This created some issues in a heavy loaded cluster in which the tokens may not be cleaned up fast enough and the cleaner thread may fail to clean the tokens. This may cause issues like eating too much space or LLAP startup failures, or slow system startup.If this hive.cluster.delegation.token.gc-interval” is reduced from 1 hour to a relatively shorter period such as 15 mins, then the zookeeper tokens will be cleaned more timely and mitigate these issues.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23216" opendate="2020-4-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new api as replacement of get_partitions_by_expr to return PartitionSpec instead of Partitions</summary>
      <description></description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventRequest.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WriteNotificationLogRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableValidWriteIds.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SchemaVersion.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplTblWriteIdStateRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplLastIdInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RenamePartitionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRow.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestSessionHiveMetastoreClientListPartitionsTempTable.java</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.wise.fileformat15.q</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CommitTxnRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CreateTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ExtendedTableInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FindSchemasByColsResp.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequestData.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetDatabaseRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsByNamesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsByNamesResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsFilterSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsProjectionSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesExtRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="2322" opendate="2011-7-28 00:00:00" fixdate="2011-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ColumnarSerDe to the list of native SerDes</summary>
      <description>We store metadata about ColumnarSerDes in the metastore, so it should be considered a native SerDe. Then, column information can be retrieved from the metastore instead of from deserialization.Currently, for non-native SerDes, column comments are only shown as "from deserializer". Adding ColumnarSerDe to the list of native SerDes will persist column comments. See HIVE-2171 for persisting the column comments of custom SerDes.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample.islocalmode.hook.q</file>
    </fixedFiles>
  </bug>
  <bug id="23260" opendate="2020-4-21 00:00:00" fixdate="2020-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for unmodified_metadata capability</summary>
      <description>Currently, the translator removes bucketing info for tables for clients that do not possess the HIVEBUCKET2 capability. While this is desirable, some clients that have write access to these tables can turn around overwrite the metadata thus corrupting original bucketing info.So adding support for a capability for client that are capable of interpreting the original metadata would prevent such corruption.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23262" opendate="2020-4-21 00:00:00" fixdate="2020-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on activemq</summary>
      <description>Activemq is a test dependency introduced to test message bus feature in hcatalog. Even when it was introduced very first, there were concerns of taking activemq as a dependency. https://issues.apache.org/jira/browse/HCATALOG-3?focusedCommentId=13035113&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-13035113AFAIK no one uses message bus feature of HCatalog. We can remove it altogether. As a first step removing tests for it.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestMsgBusConnection.java</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.conf.jndi.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23269" opendate="2020-4-22 00:00:00" fixdate="2020-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unsafe comparing bigints and strings</summary>
      <description>Comparing bigints and varchars or chars may result to wrong result, for example:CREATE TABLE test_a (appid1 varchar(256), appid2 char(20));INSERT INTO test_a VALUES ('2882303761517473127', '2882303761517473127'), ('2882303761517473276','2882303761517473276');SET hive.strict.checks.type.safety=false;SELECT appid1 FROM test_a WHERE appid1 = 2882303761517473127;SELECT appid2 FROM test_a WHERE appid2 = 2882303761517473127;​Both queries will output the row: ('2882303761517473276','2882303761517473276')</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2327" opendate="2011-7-31 00:00:00" fixdate="2011-5-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize REGEX UDFs with constant parameter information</summary>
      <description>There are a lot of UDFs which would show major performance differences if one assumes that some of its arguments are constant.Consider, for example, any UDF that takes a regular expression as input: This can be complied once (fast) if it's a constant, or once per row (wicked slow) if it's not a constant.Or, consider any UDF that reads from a file and/or takes a filename as input; it would have to re-read the whole file if the filename changes.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRegExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="23345" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>INT64 Parquet timestamps cannot be read into bigint Hive type</summary>
      <description>How to reproduce: create external table ts_pq (ts timestamp) stored as parquet; insert into ts_pq values ('1998-10-03 09:58:31.231'); create external table ts_pq_2 (ts bigint) stored as parquet location '&lt;location of ts_pq&gt;'; select * from ts_pq_2;The following exception occurs during the select:Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.TimestampWritableV2 cannot be cast to org.apache.hadoop.io.LongWritable</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.convert.TestETypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23353" opendate="2020-5-1 00:00:00" fixdate="2020-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Atlas metadata replication scheduling</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23365" opendate="2020-5-4 00:00:00" fixdate="2020-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Put RS deduplication optimization under cost based decision</summary>
      <description>Currently, RS deduplication is always executed whenever it is semantically correct. However, it could be beneficial to leave both RS operators in the plan, e.g., if the NDV of the second RS is very low. Thus, we would like this decision to be cost-based. We could use a simple heuristic that would work fine for most of the cases without introducing regressions for existing cases, e.g., if NDV for partition column is less than estimated parallelism in the second RS, do not execute deduplication.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplicationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  <bug id="23368" opendate="2020-5-5 00:00:00" fixdate="2020-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MV rebuild should produce the same view as the one configured at creation time</summary>
      <description>There might be some configrations which might affect the rel-tree of the materialized views.In case rewrites to use datasketches for count(distinct) is enabled; the view should store sketches instead of distinct values</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23375" opendate="2020-5-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track MJ HashTable Load time</summary>
      <description>Introduce TezCounter to track MJ HashTable Load time</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="2338" opendate="2011-8-3 00:00:00" fixdate="2011-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table always throws an unhelpful error on failure</summary>
      <description>Every failure in an alter table function always return a MetaException. When altering tables and catching exceptions, we throw a MetaException in the "finally" part of a try-catch-finally block, which overrides any other exceptions thrown.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23389" opendate="2020-5-6 00:00:00" fixdate="2020-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FilterMergeRule can lead to AssertionError</summary>
      <description>I have not been able to reproduce the issue in latest master. However, this could potentially happen since Filter creation has a check on whether the expression is flat (here) and Filter merge does not flatten an expression when it is created.java.lang.AssertionError: AND(=($3, 100), OR(OR(null, IS NOT NULL(CAST(100):INTEGER)), =(CAST(100):INTEGER, CAST(200):INTEGER))) at org.apache.calcite.rel.core.Filter.&lt;init&gt;(Filter.java:74) at org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.&lt;init&gt;(HiveFilter.java:39) at org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories$HiveFilterFactoryImpl.createFilter(HiveRelFactories.java:126) at org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.filter(HiveRelBuilder.java:99) at org.apache.calcite.tools.RelBuilder.filter(RelBuilder.java:1055) at org.apache.calcite.rel.rules.FilterMergeRule.onMatch(FilterMergeRule.java:81)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23423" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check of disabling hash aggregation ignores grouping set</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-23356 fixed the issue with disabling hash aggregation on grouping set queries. Need a fix for VectorGroupbyOperator operator.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23424" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Dependency on Log4J from hive-shims-common</summary>
      <description>The project uses SLF4J but not the log4j specific libraries.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2343" opendate="2011-8-3 00:00:00" fixdate="2011-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats not updated for non "load table desc" operations</summary>
      <description>Bug introduced in HIVE-306 so that stats are updated only for LoadTableDesc operations. For other operations (analyze table), null ptr is thrown and stats are not updated.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="23432" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Ranger Replication Metrics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.ReplState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.ReplLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.IncrementalLoadLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.BootstrapLoadLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.IncrementalDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.BootstrapDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.NoOpRangerRestClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpTask.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="23433" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Deny Policy on Target Database After Ranger Replication to avoid writes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.RangerRestClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.RangerRestClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.NoOpRangerRestClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23442" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID major compaction doesn&amp;#39;t read base directory correctly if it was written by insert overwrite</summary>
      <description>Steps to reproduce:SET hive.acid.direct.insert.enabled=true;CREATE EXTERNAL TABLE test_comp_txt(a int, b int, c int) STORED AS TEXTFILE;INSERT INTO test_comp_txt values (1, 1, 1), (2, 2, 2), (3, 3, 3), (4, 4, 4);CREATE TABLE test_comp(a int, b int, c int) STORED AS ORC TBLPROPERTIES('transactional'='true');INSERT OVERWRITE TABLE test_comp SELECT * FROM test_comp_txt;UPDATE test_comp SET b=55, c=66 WHERE a=2;DELETE FROM test_comp WHERE a=4;UPDATE test_comp SET b=77 WHERE a=1;SELECT * FROM test_comp;3 3 32 55 661 77 1ALTER TABLE test_comp COMPACT 'MAJOR';SELECT * FROM test_comp;2 55 661 77 1This issue only occurs if the base directory was created with an insert overwrite command and the hive.acid.direct.insert.enabled parameter was true. This issue doesn't affect the query based compaction.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23444" opendate="2020-5-11 00:00:00" fixdate="2020-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrent ACID direct inserts may fail with FileNotFoundException</summary>
      <description>The following exception may occur when concurrently inserting into an ACID table with static partitions and the 'hive.acid.direct.insert.enabled' parameter is true. This issue occurs intermittently.Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: File hdfs://ns1/warehouse/tablespace/managed/hive/tpch_unbucketed.db/concurrent_insert_partitioned/l_tax=0.0/_tmp.delta_0000001_0000001_0000 does not exist. at org.apache.hadoop.hive.ql.metadata.Hive.loadPartitionInternal(Hive.java:2465) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:2228) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.MoveTask.handleStaticParts(MoveTask.java:522) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:442) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:359) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:721) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:488) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:482) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:225) ~[hive-service-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] ... 13 moreCaused by: java.io.IOException: java.io.FileNotFoundException: File hdfs://ns1/warehouse/tablespace/managed/hive/tpch_unbucketed.db/concurrent_insert_partitioned/l_tax=0.0/_tmp.delta_0000001_0000001_0000 does not exist. at org.apache.hadoop.hive.ql.io.AcidUtils.getHdfsDirSnapshots(AcidUtils.java:1472) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.io.AcidUtils.getAcidState(AcidUtils.java:1297) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.io.AcidUtils.getAcidFilesForStats(AcidUtils.java:2695) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.metadata.Hive.loadPartitionInternal(Hive.java:2448) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:2228) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.MoveTask.handleStaticParts(MoveTask.java:522) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:442) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:359) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:721) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:488) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:482) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) ~[hive-exec-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:225) ~[hive-service-3.1.3000.7.1.1.0-493.jar:3.1.3000.7.1.1.0-493]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23445" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove mapreduce.workflow.* configs</summary>
      <description>These configs were introduced in HIVE-3708 in the hope to develop tools to visualize and monitor multiple MR jobs from Hive back in a day when MR was used. Even that time in spite of these config additions, no such tools were developed AFAIK. And now MR is hardly ever used. We can get rid of these configs. That will help to reduce the size of HiveConf by a bit.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Executor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Compiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23447" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid sending configs to tasks and AM which are only relevant for HS2</summary>
      <description>There are many configs which are only relevant for HS2. Longer term fix for this is to split HiveConf in multiple config classes relevant only for HS2, HMS, AM and tasks. And use only the objects in process where its relevant. In the interim, we can avoid configs with large value strings to send across.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23449" opendate="2020-5-12 00:00:00" fixdate="2020-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Reduce mkdir and config creations in submitWork hotpath</summary>
      <description> For short jobs, submitWork gets into hotpath. This can lazy load conf and can get rid of dir creations (which needs to be enabled only when DirWatcher is enabled)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="23467" opendate="2020-5-13 00:00:00" fixdate="2020-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a skip.trash config for HMS to skip trash when deleting external table data</summary>
      <description>We have an auto.purge flag, which means skip trash. It can be confusing as we have 'external.table.purge'='true' to indicate delete table data when this tblproperties is set. We should make the meaning clearer by introducing a skip trash alias/option. Additionally, we shall add an alias for external.table.purge, and name it external.table.autodelete, and document it more prominently, so as to maintain backward compatibility, and make the meaning of auto deletion of data more obvious. The net effect of these 2 changes will be. If the user sets 'external.table.autodelete'='true'the table data will be removed when table is dropped. and if 'skip.trash'='true' is set, HMS will not move the table data to trash folder when removing the files. This will result in faster removal, especially when underlying FS is S3. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="23470" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move TestCliDriver tests to TestMiniLlapCliDriver if they are failing with TestMiniLlapLocalCliDriver</summary>
      <description>Some tests are failing with TestMiniLlapLocalCliDriver, but running fine with TestMiniLlapCliDriver, let's move them there.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.temp.table.partcols1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.custom.udf.configure.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.script.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.printf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.sum.list.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform1.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.queries.clientpositive.input5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.regexp.extract.q</file>
      <file type="M">ql.src.test.queries.clientpositive.select.transform.hint.q</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.binary.data.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.macro.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.macro.duplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.test.dummy.operator.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.newline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partcols1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.with.semi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="23563" opendate="2020-5-28 00:00:00" fixdate="2020-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Early abort the build in case new commits are added to the PR</summary>
      <description>if the job is waiting for a long to acquire the lock for a long time; it would be favourable to do a quick check after lock acqusition wether there are new triggershttps://github.com/apache/hive/pull/948#discussion_r432093713</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="23580" opendate="2020-5-29 00:00:00" fixdate="2020-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleteOnExit set is not cleaned up, causing memory pressure</summary>
      <description>removeScratchDir doesn't always calls cancelDeleteOnExit() on context::clear</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="23582" opendate="2020-5-29 00:00:00" fixdate="2020-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make SplitLocationProvider impl pluggable</summary>
      <description>LLAP uses HostAffinitySplitLocationProvider implementation by default. For non zookeeper based environments, a different split location provider may be used. To facilitate that make the SplitLocationProvider implementation class a pluggable. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23590" opendate="2020-6-2 00:00:00" fixdate="2020-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close stale PRs automatically</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.asf.yaml</file>
    </fixedFiles>
  </bug>
  <bug id="23619" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new plugin to rerun queries when Tez AM is down due to lost node</summary>
      <description>If the TezAM was running a query and it gets killed because of external factors like node going node, HS2 should retry the query in different TezAM.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23620" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explore moving to SpotBugs</summary>
      <description>We may want to eventually switch to SpotBugs&amp;#8211; the spiritual successor of FindBugs, carrying on from the point where it left off with support of its communitySpotBugs is in a reality a fork of FindBugs: https://mailman.cs.umd.edu/pipermail/findbugs-discuss/2016-November/004321.htmlmvn -P spotbugs test-compile com.github.spotbugs:spotbugs-maven-plugin:4.0.0:spotbugs</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.findbugs.findbugs-exclude.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.findbugs.findbugs-exclude.xml</file>
      <file type="M">standalone-metastore.findbugs.findbugs-exclude.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">Jenkinsfile</file>
      <file type="M">findbugs.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23621" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce ASF headers on source files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">Jenkinsfile</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.splitsupport.SplitSupportDummy.java</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.splitsupport.split125.SplitSupportDummy.java</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.splitsupport.split0.SplitSupportDummy.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryServiceProvider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryCleaner.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.qoption.QTestReplaceHandler.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.SplitSupport.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.MiniDruidLlapLocalCliDriver.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.Wikipedia.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.SingleNodeKafkaCluster.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestTransactionalValidationListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStoreUpdateUsingEvents.java</file>
      <file type="M">common.src.java.org.apache.hive.http.JMXJsonServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="23624" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metastore metrics to show the compaction status</summary>
      <description>Add metrics for showing the number of compactions grouped by status: Successful Working Failed Attempted Initiated Ready for Clean</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.metrics.MetricsConstants.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="23625" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 Web UI displays query drill-down results in plain text, not html</summary>
      <description>Opening a drilldown link on the HS2 Web UI, you are directed to the following URL: /query_page?operationId=&lt;ID&gt;Since the path /query_page contains no file extensions, Jetty cannot determine the mimetype and therefore the Hive HttpServer returns response header Content-Type: text/plain;charset=utf-8, and the information does not render as html in the browser. This should be corrected to return text/html;charset=utf-8.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="23626" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build failure is incorrectly reported as tests passed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="23629" opendate="2020-6-6 00:00:00" fixdate="2020-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce clean findbugs in PRs</summary>
      <description>We should start enforcing clean findbugs reports as soon as we fix a module. Otherwise, it will continue collecting findbugs errors. We can add a stage to Jenkins pipeline to enforce findbugs and possibly other checks. It will selectively run findbugs for specified sub modules. Eventually we can get rid of the list and enable findbugs for the whole project.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.CombineHiveKey.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="23631" opendate="2020-6-7 00:00:00" fixdate="2020-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the test target instead of install</summary>
      <description>after a full install; there are some issues with:time mvn test -pl ql,itests/hive-unit -Dtest=TestJdbcGenericUDTFGetSplits[INFO] Running org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 15.317 s &lt;&lt;&lt; FAILURE! - in org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits[ERROR] org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits Time elapsed: 15.316 s &lt;&lt;&lt; ERROR!org.apache.hive.service.ServiceException: org.apache.hive.service.ServiceException: Unable to setup tez session pool at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:733) at org.apache.hive.jdbc.miniHS2.MiniHS2.start(MiniHS2.java:382) at org.apache.hive.jdbc.AbstractTestJdbcGenericUDTFGetSplits.beforeTest(AbstractTestJdbcGenericUDTFGetSplits.java:88) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)Caused by: org.apache.hive.service.ServiceException: Unable to setup tez session pool at org.apache.hive.service.server.HiveServer2.initAndStartTezSessionPoolManager(HiveServer2.java:832) at org.apache.hive.service.server.HiveServer2.startOrReconnectTezSessions(HiveServer2.java:807) at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:730) ... 20 moreCaused by: java.io.FileNotFoundException: /home/dev/hive/ql/target/classes (Is a directory) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.&lt;init&gt;(RawLocalFileSystem.java:110) at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:212) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.&lt;init&gt;(ChecksumFileSystem.java:147) at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:950) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getSha(TezSessionState.java:863) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:809) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:288) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.openInternal(TezSessionPoolSession.java:124) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:240) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:231) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.startInitialSession(TezSessionPool.java:297) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.start(TezSessionPool.java:117) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.startPool(TezSessionPoolManager.java:112) at org.apache.hive.service.server.HiveServer2.initAndStartTezSessionPoolManager(HiveServer2.java:829) ... 22 more[INFO] [INFO] Results:[INFO] [ERROR] Errors: [ERROR] TestJdbcGenericUDTFGetSplits&gt;AbstractTestJdbcGenericUDTFGetSplits.beforeTest:88 » Service[INFO] [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="23696" opendate="2020-6-15 00:00:00" fixdate="2020-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DB Metadata and Progress column not taking the defined length</summary>
      <description>Caused by: org.datanucleus.exceptions.NucleusUserException: Attempt to store value "{"dbName":"testAcidTablesReplLoadBootstrapIncr_1592205875387","replicationType":"BOOTSTRAP","stagingDir":"hdfs://localhost:65158/tmp/org_apache_hadoop_hive_ql_parse_TestReplicationScenarios_245261428230295/hrepl0/dGVzdGFjaWR0YWJsZXNyZXBsbG9hZGJvb3RzdHJhcGluY3JfMTU5MjIwNTg3NTM4Nw==/0/hive","lastReplId":25}" in column "RM_METADATA" that has maximum length of 255. Please correct your data!</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.metric.TestReplicationMetricSink.java</file>
    </fixedFiles>
  </bug>
  <bug id="2384" opendate="2011-8-17 00:00:00" fixdate="2011-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>import of multiple partitions from a partitioned table with external location overwrites files</summary>
      <description>when we import multiple partitions from an exported partitioned table, if we import it with a specified external location, then the partitions end up overwriting.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2393" opendate="2011-8-18 00:00:00" fixdate="2011-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix whitespace test diff accidentally introduced in HIVE-1360</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.named.struct.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="2417" opendate="2011-8-29 00:00:00" fixdate="2011-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging of compressed rcfiles fails to write the valuebuffer part correctly</summary>
      <description>The blockmerge task does not create proper rc files when merging compressed rc files as the valuebuffer writing is incorrect.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="242" opendate="2009-1-22 00:00:00" fixdate="2009-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>line breaks are not treated as spaces if there are more than 1 line break in the whole query.</summary>
      <description>describeextendedtab_name;is translated as 'describeextended tab_name'</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2426" opendate="2011-9-2 00:00:00" fixdate="2011-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test that views with joins work properly</summary>
      <description>With the testcasedrop table invites;drop table invites2;create table invites (foo int, bar string) partitioned by (ds string);create table invites2 (foo int, bar string) partitioned by (ds string);set hive.mapred.mode=strict;-- test join views: see HIVE-1989create view v as select invites.bar, invites2.foo, invites2.ds from invites join invites2 on invites.ds=invites2.ds;explain select * from v where ds='2011-09-01';drop view v;drop table invites;drop table invites2;We should not have the partition pruner complain about invites.ds not having a predicate because the predicate invites2.ds='2011-09-01' will be inferred with the ppd transitivity optimization</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2429" opendate="2011-9-7 00:00:00" fixdate="2011-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>skip corruption bug that cause data not decompressed</summary>
      <description>This is a regression of https://issues.apache.org/jira/browse/HIVE-2404</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="2602" opendate="2011-11-22 00:00:00" fixdate="2011-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add support for insert partition overwrite(...) if not exists</summary>
      <description>INSERT OVERWRITE TABLE X PARTITION (a=b, c=d) IF NOT EXISTS ...The partition should be created and written if and only if it's not there already.The support can be added for dynamic partitions in the future, but this jira is for adding this support for static partitions.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="2607" opendate="2011-11-24 00:00:00" fixdate="2011-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add caching to json_tuple</summary>
      <description>get_json_object uses a variety of caches to improve its performance. json_tuple could benefit from having a similar cache from JSON string to JSONObject.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
    </fixedFiles>
  </bug>
  <bug id="262" opendate="2009-1-30 00:00:00" fixdate="2009-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>outer join gets some duplicate rows in some scenarios</summary>
      <description>SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key &lt; 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key &lt; 20);returns duplicate rows for outer join</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2622" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive POMs reference the wrong Hadoop artifacts</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2642" opendate="2011-12-9 00:00:00" fixdate="2011-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix Hive-2566 and make union optimization more aggressive</summary>
      <description>Hive-2566 did some optimizations to union, but cause some problems. And then got reverted. This is to get it back and fix the problems we saw, and also make union optimization more aggressive.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2648" opendate="2011-12-12 00:00:00" fixdate="2011-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel tests fail if master directory is not present</summary>
      <description>Parallel tests should create directories as needed.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="2651" opendate="2011-12-13 00:00:00" fixdate="2011-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The variable hive.exec.mode.local.auto.tasks.max should be changed</summary>
      <description>It should be called hive.exec.mode.local.auto.input.files.max instead.The number of input files are checked currently.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.sample.islocalmode.hook.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2681" opendate="2011-12-26 00:00:00" fixdate="2011-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUCESS is misspelled</summary>
      <description>C'mon!</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.MapRedStats.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
