<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="15190" opendate="2016-11-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Field names are not preserved in ORC files written with ACID</summary>
      <description>To repro:drop table if exists orc_nonacid;drop table if exists orc_acid;create table orc_nonacid (a int) clustered by (a) into 2 buckets stored as orc;create table orc_acid (a int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');insert into table orc_nonacid values(1), (2);insert into table orc_acid values(1), (2);Running hive --service orcfiledump &lt;file&gt; on the files created by the insert statements above, you'll see that for orc_nonacid, the files have schema struct&lt;a:int&gt; whereas for orc_acid, the files have schema struct&lt;operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct&lt;_col0:int&gt;&gt;. The last field row should have schema struct&lt;a:int&gt;.</description>
      <version>2.1.0,2.2.0,3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="18767" opendate="2018-2-22 00:00:00" fixdate="2018-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some alterPartitions invocations throw &amp;#39;NumberFormatException: null&amp;#39;</summary>
      <description>Error messages:[info] Cause: java.lang.NumberFormatException: null[info] at java.lang.Long.parseLong(Long.java:552)[info] at java.lang.Long.parseLong(Long.java:631)[info] at org.apache.hadoop.hive.metastore.MetaStoreUtils.isFastStatsSame(MetaStoreUtils.java:315)[info] at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:605)[info] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3837)[info] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[info] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[info] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[info] at java.lang.reflect.Method.invoke(Method.java:498)[info] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)[info] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)[info] at com.sun.proxy.$Proxy23.alter_partitions_with_environment_context(Unknown Source)[info] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(HiveMetaStoreClient.java:1527)</description>
      <version>2.3.3,3.1.0,3.2.0,4.0.0</version>
      <fixedVersion>2.3.4,2.4.0,3.1.1,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18874" opendate="2018-3-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HiveConnection shades log4j interfaces</summary>
      <description>This prevents Hive JDBC from being instantiated into a regular SLF4J logger env.java.lang.IncompatibleClassChangeError: Class org.apache.logging.slf4j.Log4jLoggerFactory does not implement the requested interface org.apache.hive.org.slf4j.ILoggerFactory at org.apache.hive.org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:285)</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18875" opendate="2018-3-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable SMB Join by default in Tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.notin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18910" opendate="2018-3-8 00:00:00" fixdate="2018-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate to Murmur hash for shuffle and bucketing</summary>
      <description>Hive uses JAVA hash which is not as good as murmur for better distribution and efficiency in bucketing a table.Migrate to murmur hash but still keep backward compatibility for existing users so that they dont have to reload the existing tables.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.bucket.many.q</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.truncate.column.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.shared.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.nested.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.union.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.delimited.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.db.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.alter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.3.exim.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.2.exim.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.array.null.element.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.named.column.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.insert.into.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skiphf.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.capacity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.count.distinct.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.column.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.multi.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.except.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.with.different.encryption.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.nonascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.defaultformats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.infinity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.num.reducers.acid2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.num.reducers2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.num.reducers.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.basicstat.partval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.excludeHadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.col.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.file.format.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.dynpart.hashjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.update.delete.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.num.reducers.acid2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.num.reducers2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.num.reducers.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestBucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.table.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.directory.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.table.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSerializedImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnAddPartition.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.queries.clientpositive.archive.excludeHadoop20.q</file>
    </fixedFiles>
  </bug>
  <bug id="19168" opendate="2018-4-11 00:00:00" fixdate="2018-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger changes for llap commands</summary>
      <description>New llap commands "llap cluster -info" and "llap cache -purge" require some changes so that Ranger can log the commands for auditing.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapClusterResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapCacheResourceProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
    </fixedFiles>
  </bug>
  <bug id="1917" opendate="2011-1-15 00:00:00" fixdate="2011-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS (create-table-as-select) throws exception when showing results</summary>
      <description>CTAS throws an exception in CliDriver when showing results at the end of a query. CTAS should not show results because it is not a 'select' query or 'desc'/explain etc. It should be the same as create table/view/index and insert overwrite statements.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19175" opendate="2018-4-11 00:00:00" fixdate="2018-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver.testCliDriver update_access_time_non_current_db failing</summary>
      <description>Caused by HIVE-18060. Instead of generating golden file under clientpositive/llap it is under clientpositive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.update.access.time.non.current.db.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19176" opendate="2018-4-11 00:00:00" fixdate="2018-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HoS support to progress bar on Beeline client</summary>
      <description>Make whats was done inHIVE-15473work for HoS.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.ServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.status.TestSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1918" opendate="2011-1-17 00:00:00" fixdate="2011-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add export/import facilities to the hive system</summary>
      <description>This is an enhancement request to add export/import features to hive.With this language extension, the user can export the data of the table - which may be located in different hdfs locations in case of a partitioned table - as well as the metadata of the table into a specified output location. This output location can then be moved over to another different hadoop/hive instance and imported there. This should work independent of the source and target metastore dbms used; for instance, between derby and mysql.For partitioned tables, the ability to export/import a subset of the partition must be supported.Howl will add more features on top of this: The ability to create/use the exported data even in the absence of hive, using MR or Pig. Please see http://wiki.apache.org/pig/Howl/HowlImportExport for these details.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19206" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatic memory management for open streaming writers</summary>
      <description>Problem: When there are 100s of record updaters open, the amount of memory required by orc writers keeps growing because of ORC's internal buffers. This can lead to potential high GC or OOM during streaming ingest.Solution: The high level idea is for the streaming connection to remember all the open record updaters and flush the record updater periodically (at some interval). Records written to each record updater can be used as a metric to determine the candidate record updaters for flushing. If stripe size of orc file is 64MB, the default memory management check happens only after every 5000 rows which may which may be too late when there are too many concurrent writers in a process. Example case would be 100 writers open and each of them have almost full stripe of 64MB buffered data, this would take 100*64MB ~=6GB of memory. When all of the record writers flush, the memory usage drops down to 100*~2MB which is just ~200MB memory usage.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictDelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.ConnectionInfo.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19209" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming ingest record writers should accept input stream</summary>
      <description>Record writers in streaming ingest currently accepts byte[]. Provide an option for clients to pass in input stream directly from which byte[] for record can be constructed.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictDelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.RecordWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19210" opendate="2018-4-13 00:00:00" fixdate="2018-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for streaming ingest</summary>
      <description>This will retain the old hcat streaming API for old clients. The new streaming ingest API will be separate module under hive.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19211" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New streaming ingest API and support for dynamic partitioning</summary>
      <description>New streaming API under new hive sub-module Dynamic partitioning support Auto-rollover transactions Automatic heartbeating</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.QueryFailedException.java</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestDelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionError.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionBatchUnAvailable.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.TransactionBatch.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingIOFailure.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingException.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.SerializationError.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.RecordWriter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.AcidTableSerializer.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.Lock.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.lock.LockFailureListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClient.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.MutatorClientBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.client.Transaction.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.HiveConfFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.package.html</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.UgiMetaStoreClientFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolver.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.BucketIdResolverImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.Mutator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinatorBuilder.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorFactory.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.PartitionHelper.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspector.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.RecordInspectorImpl.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.package-info.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.ConnectionError.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.DelimitedInputWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HeartBeatFailure.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveEndPoint.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.ImpersonationFailed.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidColumn.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidPartition.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidTable.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.InvalidTrasactionState.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.PartitionCreationFailed.java</file>
    </fixedFiles>
  </bug>
  <bug id="19212" opendate="2018-4-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs yetus pre-commit checks</summary>
      <description>Follow up from HIVE-18883, the committed patch isn't working and Findbugs is still not working.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.YetusPhase.java</file>
      <file type="M">dev-support.yetus-wrapper.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19214" opendate="2018-4-13 00:00:00" fixdate="2018-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>High throughput ingest ORC format</summary>
      <description>Create delta files with all ORC overhead disabled (no index, no compression, no dictionary). Compactor will recreate the orc files with index, compression and dictionary encoding.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.acid3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.exception.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.acid3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.acid3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.exception.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19237" opendate="2018-4-18 00:00:00" fixdate="2018-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only use an operatorId once in a plan</summary>
      <description>Column stats autogather plan part is added from a plan compiled by the driver itself; however that driver starts to use operatorIds from 1 ; so it's possible that 2 SEL_1 operators end up in the same plan...</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.NoOperatorReuseCheckerHook.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.1.q.out</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MemoryDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsAutoGatherContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestCounterMapping.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestOperatorCmp.java</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.non.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.input.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19239" opendate="2018-4-18 00:00:00" fixdate="2018-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for possible null timestamp fields during SerDe from Druid events</summary>
      <description>Currently we do not check for possible null timestamp events.This might lead to NPE.This Patch add addition check for such case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="19243" opendate="2018-4-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop.version to 3.1.0</summary>
      <description>Given that Hadoop 3.1.0 has been released, we need to upgrade hadoop.version to 3.1.0. This change is requiredforHIVE-18037sinceit depends on YARN Service which had its first release in 3.1.0 (and is non-existent in 3.0.0).</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19249" opendate="2018-4-19 00:00:00" fixdate="2018-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication: WITH clause is not passing the configuration to Task correctly in all cases</summary>
      <description>When running repl load like following:REPL LOAD `repldb_kms207` FROM 'hdfs://url:8020/apps/hive/repl/f8b057a7-c3f2-43bd-8baa-f7408a9008fc' WITH ('hive.exec.parallel'='true','hive.distcp.privileged.doAs'='beacon','hive.metastore.uris'='thrift://metastore-url:9083','hive.metastore.warehouse.dir'='s3a://s3-warehouse','hive.warehouse.subdir.inherit.perms'='false','hive.repl.replica.functions.root.dir'='s3a://s3-warehouse','fs.s3a.bucket.ss-datasets.endpoint'='s3-bucket-endpoint','fs.s3a.impl.disable.cache'='true','fs.s3a.server-side-encryption-algorithm'='SSE-KMS','fs.s3a.server-side-encryption.key'='encr-key','distcp.options.pp'='','distcp.options.pg'='','distcp.options.pu'='');the task that get created need to use the configs that are passed in the USING clause. However, in some cases the wrong config object gets used.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="19254" opendate="2018-4-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NumberFormatException in MetaStoreUtils.isFastStatsSame</summary>
      <description>I see the following exception under some cases in the logs. This possibly happens when you try to add empty partitions.2018-04-19T19:32:19,260 ERROR [pool-7-thread-7] metastore.RetryingHMSHandler: MetaException(message:java.lang.NumberFormatException: null) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6824) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:4864) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions(HiveMetaStore.java:4801) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy24.alter_partitions(Unknown Source) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_partitions.getResult(ThriftHiveMetastore.java:16046) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_partitions.getResult(ThriftHiveMetastore.java:16030) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NumberFormatException: null at java.lang.Long.parseLong(Long.java:552) at java.lang.Long.parseLong(Long.java:631) at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.isFastStatsSame(MetaStoreUtils.java:632) at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:743) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:4827) ... 21 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19257" opendate="2018-4-20 00:00:00" fixdate="2018-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-19157 commit references wrong jira</summary>
      <description>1eea5a80ded2df33d57b2296b3bed98cb18383fd on master references wrong jira.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19258" opendate="2018-4-20 00:00:00" fixdate="2018-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add originals support to MM tables (and make the conversion a metadata only operation)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.conversions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.conversions.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19277" opendate="2018-4-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Active/Passive HA web endpoints does not allow cross origin requests</summary>
      <description>CORS is not allowed with web endpoints added for active/passive HA. Enable CORS by default for all web endpoints.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2Peers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2LeadershipStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="19323" opendate="2018-4-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore SQL install and upgrade scripts for 3.1</summary>
      <description>Now that we've branched for 3.0 we need to create SQL install and upgrade scripts for 3.1</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade.order.postgres</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade.order.oracle</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade.order.mysql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade.order.mssql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade.order.derby</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19324" opendate="2018-4-26 00:00:00" fixdate="2018-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve YARN queue check error message in Tez pool</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.YarnQueueHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="19344" opendate="2018-4-27 00:00:00" fixdate="2018-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of msck.repair.batch.size</summary>
      <description>msck.repair.batch.size default to 0 which means msck will try to add all the partitions in one API call to HMS. This can potentially add huge memory pressure on HMS. The default value should be changed to a reasonable number so that in case of large number of partitions we can batch the addition of partitions. Same goes for msck.repair.batch.max.retries</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19347" opendate="2018-4-28 00:00:00" fixdate="2018-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestTriggersWorkloadManager tests are failing consistently</summary>
      <description>Caused by the patch which turned on vectorization. Following tests are failing due to the patch: org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver&amp;#91;spark_explainuser_1&amp;#93; org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;explainuser_1&amp;#93; org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighBytesWrite 10 sec 14 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomReadOps 7.7 sec 14 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomCreatedFiles 15 sec 14 org.apache.hive.jdbc.TestTriggersWorkloadManager.testMultipleTriggers2 17 sec 14 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerSlowQueryExecutionTime 1.5 sec 14 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerSlowQueryElapsedTime org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerVertexRawInputSplitsNoKill 20 sec 18 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighShuffleBytes 1.4 sec 18 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomNonExistent 2.6 sec 18 org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighBytesReadError MessageExpected query to succeed expected null, but was:&lt;java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 3, vertexId=vertex_1524884047358_0001_21_01, diagnostics=[Task failed, taskId=task_1524884047358_0001_21_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1524884047358_0001_21_01_000000_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:80) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:419) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267) ... 15 more</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="19353" opendate="2018-4-29 00:00:00" fixdate="2018-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: ConstantVectorExpression --&gt; RuntimeException: Unexpected column vector type LIST</summary>
      <description>Found by enablingvectorization for org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaDataCaused by: java.lang.RuntimeException: Unexpected column vector type LIST at org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:237) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:146) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:955) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:928) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:125) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.closeOp(VectorMapOperator.java:984) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:722) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:193) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19362" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable LLAP cache affinity by default</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19363" opendate="2018-4-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove cryptic metrics from LLAP IO output</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.LLAPioSummary.java</file>
    </fixedFiles>
  </bug>
  <bug id="19367" opendate="2018-5-1 00:00:00" fixdate="2018-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load Data should fail for empty Parquet files.</summary>
      <description>Load data does not validate the input for Parquet tables. This results in query failures.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19369" opendate="2018-5-1 00:00:00" fixdate="2018-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locks: Add new lock implementations for always zero-wait readers</summary>
      <description>Hive Locking with Micro-managed and full-ACID tables needs a better locking implementation which allows for no-wait readers always.EXCL_DROPEXCL_WRITESHARED_WRITESHARED_READShort write-upEXCL_DROP is a "drop partition" or "drop table" and waits for all others to exitEXCL_WRITE excludes all writes and will wait for all existing SHARED_WRITE to exit.SHARED_WRITE allows all SHARED_WRITES to go through, but will wait for an EXCL_WRITE &amp; EXCL_DROP (waiting so that you can do drop + insert in different threads).SHARED_READ does not wait for any lock - it fails fast for a pending EXCL_DROP, because even if there is an EXCL_WRITE or SHARED_WRITE pending, there's no semantic reason to wait for them to succeed before going ahead with a SHARED_WRITE.a select * =&gt; SHARED_READan insert into =&gt; SHARED_WRITEan insert overwrite or MERGE =&gt; EXCL_WRITEa drop table =&gt; EXCL_DROPTODO:The fate of the compactor needs to be added to this before it is a complete description.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.LockTypeUtilTest.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.LockTypeUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explain.locks.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explain.locks.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ValidTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19385" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional hive env variable to redirect bin/hive to use Beeline</summary>
      <description>With beeline-site and beeline-user-site, the user can easily specify default hs2 urls to connect. We can use an optional env variable, which when set, will enable bin/hive to use beeline.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19387" opendate="2018-5-2 00:00:00" fixdate="2018-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate table for Acid tables conflicts with ResultSet cache</summary>
      <description>How should this work? Should it work like Insert Overwrite T select * from T where 1=2?This should create a new empty base_x/ and thus operate w/o violating Snapshot Isolation semantics.This makes sense for specific partition or unpartitioned table. What about "Truncate T" where T is partitioned? Is the expectation to wipe out all partition info or to make each partition empty?</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19389" opendate="2018-5-2 00:00:00" fixdate="2018-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool: For Hive&amp;#39;s Information Schema, use embedded HS2 as default</summary>
      <description>Currently, for initializing/upgrading Hive's information schema, we require a full jdbc url (for HS2). It will be good to have it connect using embedded HS2 by default.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19390" opendate="2018-5-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Useless error messages logged for dummy table stats</summary>
      <description>Queries like INSERT INTO t1 VALUES (20); gets rewritten into insert into t1 select 20 from default_db.default_tblname which later throws as compiler tries to get stats</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="194" opendate="2008-12-22 00:00:00" fixdate="2008-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support specifying decimal places for ROUND function</summary>
      <description>Standard behavior:ROUND( number, [ decimal_places ] )decimal_places can be negative, which rounds digits to the left of the decimal point. NULL is returned if either argument is NULL.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRound.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19415" opendate="2018-5-3 00:00:00" fixdate="2018-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CORS for all HS2 web endpoints</summary>
      <description>HIVE-19277 changes alone are not sufficient to support CORS. CrossOriginFilter has to be added to jetty which will serve appropriate response for OPTIONS pre-flight request.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19440" opendate="2018-5-7 00:00:00" fixdate="2018-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make StorageBasedAuthorizer work with information schema</summary>
      <description>With HIVE-19161, Hive information schema works with external authorizer (such as ranger). However, we also need to make StorageBasedAuthorizer synchronization work as it is also widely use.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade.order.postgres</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade.order.oracle</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade.order.mysql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade.order.mssql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade.order.derby</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTablePrivilege.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestRestrictedList.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.server.TestInformationSchemaWithPrivilege.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.DatabaseAccessorFactory.java</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeSynchonizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRestrictInformationSchema.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.client.builder.HiveObjectPrivilegeBuilder.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MDBPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.java</file>
    </fixedFiles>
  </bug>
  <bug id="19474" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal type should be casted as part of the CTAS or INSERT Clause.</summary>
      <description>HIVE-18569 introduced a runtime config variable to allow the indexing of Decimal as Double, this leads to kind of messy state, Hive metadata think the column is still decimal while it is stored as double. Since the Hive metadata of the column is Decimal the logical optimizer will not push down aggregates.i tried to fix this by adding some logic to the application but it makes the code very clumsy with lot of branches. Instead i propose to revert HIVE-18569 and let the user introduce an explicit cast this will be better since the metada reflects actual storage type and push down aggregates will kick in and there is no config needed without adding any code or bug.cc ashutoshc and nishantbangarwaYou can see the difference with the following DDLcreate table test_base_table(`timecolumn` timestamp, `interval_marker` string, `num_l` DECIMAL(10,2));insert into test_base_table values ('2015-03-08 00:00:00', 'i1-start', 4.5);set hive.druid.approx.result=true;CREATE TABLE druid_test_tableSTORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'TBLPROPERTIES ("druid.segment.granularity" = "DAY")ASselect cast(`timecolumn` as timestamp with local time zone) as `__time`, `interval_marker`, cast(`num_l` as double)FROM test_base_table;describe druid_test_table;explain select sum(num_l), min(num_l) FROM druid_test_table;CREATE TABLE druid_test_table_2STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'TBLPROPERTIES ("druid.segment.granularity" = "DAY")ASselect cast(`timecolumn` as timestamp with local time zone) as `__time`, `interval_marker`, `num_l`FROM test_base_table;describe druid_test_table_2;explain select sum(num_l), min(num_l) FROM druid_test_table_2;</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19485" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>dump directory for non native tables should not be created</summary>
      <description></description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="19488" opendate="2018-5-10 00:00:00" fixdate="2018-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable CM root based on db parameter, identifying a db as source of replication.</summary>
      <description>add a parameter at db level to identify if its a source of replication. user should set this. Enable CM root only for databases that are a source of a replication policy, for other db's skip the CM root functionality. prevent database drop if the parameter indicating its source of a replication, is set. as an upgrade to this version, usershould set the property on all existing database policies, in affect. the parameter should be of the form .  repl.source.for : List &lt; policy ids &gt;</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MDatabase.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">ql.src.test.results.clientnegative.repl.load.requires.admin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.repl.dump.requires.admin.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.repl.load.requires.admin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.repl.dump.requires.admin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MetaDataExportListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOnHDFSEncryptedZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestCopyUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="19490" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locking on Insert into for non native and managed tables.</summary>
      <description>Current state of the art:Managed non native table like Druid Tables, will need to get a Lock on Insert into or insert Over write. The nature of this lock is set to Exclusive by default for any non native table.This implies that Inserts into Druid table will Lock any read query as well during the execution of the insert into. IMO this lock (on insert into) is not needed since the insert statement is appending data and the state of loading it is managed partially by Hive Storage handler hook and part of it by Druid.What i am proposing is to relax the lock level to shared for all non native tables on insert into operations and keep it as Exclusive Write for insert Overwrite for now.Any feedback is welcome.cc ekoifman / ashutoshc / jdere / hagleitnAlso am not sure what is the best way to unit test this currently am using debugger to check if locks are what i except, please let me know if there is a better way to do this.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="19494" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accept shade prefix during reflective instantiation of output format</summary>
      <description>Hive Streaming API jars are sometimes shaded with a different prefix when used in environments where another version of hive already exists (spark for example). In most cases, shading is done with rename of classes with some prefix. If an uber/assembly jar is generated with renamed prefix, Hive Streaming API will not work as Hive Streaming API will reflectively instantiate outputformat class using FQCN string provided by metastore table storage descriptor object. For example: RecordWriter will create instance of OutputFormat using string "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat". When a shaded jar with renamed class references are used, this class will not be found by the classloader. We can optionally accept a shade prefix from user via config which will be tried (as fallback) when ClassNotFoundException is thrown.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19495" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow SerDe itest failure</summary>
      <description>"You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowColumnarBatchSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="19496" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check untar folder</summary>
      <description>We need to check if the file is under untar folder.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.CompressionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="19503" opendate="2018-5-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a test that checks for dropPartitions with directSql</summary>
      <description>As a followup jira, it might be good to check that every RDBMS table is empty after a dropTable happened with DirectSQLAlso checking the JDO &lt;-&gt; DirectSQL cache handling</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="19510" opendate="2018-5-12 00:00:00" fixdate="2018-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add performance metric to find the total time spend in rsync</summary>
      <description>I think we are spending a lot of time copying logs from worker nodes to the server. We should add some logging to print aggregate time spent in rsync to confirm.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.MockRSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncResult.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.Phase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.LocalCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
    </fixedFiles>
  </bug>
  <bug id="19517" opendate="2018-5-13 00:00:00" fixdate="2018-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable/delete TestNegativeCliDriver merge_negative_5 and mm_concatenate</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="1952" opendate="2011-2-3 00:00:00" fixdate="2011-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix some outputs and make some tests deterministic</summary>
      <description>Some of the tests are un-deterministic, and are causing intermediate diffs</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.ppr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.vs.table.metadata.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input.part7.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19529" opendate="2018-5-14 00:00:00" fixdate="2018-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Date/Timestamp NULL issues</summary>
      <description>Wrong results found for: date_add/date_subUT areas: date_add/date_subdatediffto_dateinterval_year_month + interval_year_month interval_day_time + interval_day_time interval_day_time + timestamp timestamp + interval_day_time date + interval_day_time interval_day_time + date interval_year_month + date date + interval_year_month interval_year_month + interval_year_month timestamp + interval_year_monthdate - date interval_year_month - interval_year_month interval_day_time - interval_day_time timestamp - interval_day_time timestamp - timestamp date - timestamp timestamp - date date - interval_day_time date - interval_year_month timestamp - interval_year_month</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomBatchSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareTruncStringScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19530" opendate="2018-5-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix JDBCSerde and re-enable vectorization</summary>
      <description>According tojcamachorodriguezthere is a big switch statement in the code that has might have missing types. This can lead to the string types seen.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19560" opendate="2018-5-15 00:00:00" fixdate="2018-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry test runner and retry rule for flaky tests</summary>
      <description>Implement custom test runner that retries failed tests as a workaround for flakiness. Also a test rule for retrying failed tests (for cases where custom test runner is not possible, e.g ParametrizedTests which already is a customer TestRunner).</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="19562" opendate="2018-5-15 00:00:00" fixdate="2018-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: TestMiniSparkOnYarn FileNotFoundException in spark-submit</summary>
      <description>Seeing sporadic failures during test setup. Specifically, when spark-submit runs this error (or a similar error) gets thrown:2018-05-15T10:55:02,112 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: Exception in thread "main" java.io.FileNotFoundException: File file:/tmp/spark-56e217f7-b8a5-4c63-9a6b-d737a64f2820/__spark_libs__7371510645900072447.zip does not exist2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:316)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:356)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:478)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:565)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:863)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:169)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.run(Client.scala:1146)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1518)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Essentially, Spark is writing some files for container localization to a tmp dir, and that tmp dir is getting deleted. We have seen a lot of issues with writing files to /tmp/ in the past, so its probably best to write these files to a test-specific dir.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-cluster.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.spark.local.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19564" opendate="2018-5-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix NULL / Wrong Results issues in Arithmetic</summary>
      <description>Write new UT tests that use random data and intentional isRepeating batches to checks for NULL and Wrong Results for vectorized arithmetic (+,-,*,/,%)Divide by 0 bugfound in Long Scalar % Column.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumnChecked.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumn.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19565" opendate="2018-5-15 00:00:00" fixdate="2018-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix NULL / Wrong Results issues in STRING Functions</summary>
      <description>Write new UT tests that use random data and intentional isRepeating batches to checks for NULL and Wrong Results for vectorized STRING functions: char_length concat initcap length lower ltrim octet_length rtrim trim upper UDF: hex substr BUGs found in LTRIM, CONCAT, and SUBSTR.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VarCharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupConcatColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CharScalarConcatStringGroupCol.java</file>
    </fixedFiles>
  </bug>
  <bug id="19568" opendate="2018-5-16 00:00:00" fixdate="2018-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Active/Passive HS2 HA: Disallow direct connection to passive HS2 instance</summary>
      <description>The recommended usage for clients when connecting to HS2 with Active/Passive HA configuration is via ZK service discovery URL. But some applications do not support ZK service discovery in which case they use direct URL to connect to HS2 instance. If direct connection is to passive HS2 instance, the connection should be dropped with proper error message.</description>
      <version>3.1.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestCLIServiceRestore.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestCLIServiceConnectionLimits.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestPluggableHiveSessionImpl.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestPlainSaslHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
    </fixedFiles>
  </bug>
  <bug id="19615" opendate="2018-5-18 00:00:00" fixdate="2018-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper handling of is null and not is null predicate when pushed to Druid</summary>
      <description>Recent development in Druid introduced new semantic of null handling hereBased on those changes when need to honer push down of expressions with is null/ is not null predicates.The prosed fix overrides the mapping of Calcite Function to Druid Expression to much the correct semantic.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DruidSqlOperatorConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19620" opendate="2018-5-21 00:00:00" fixdate="2018-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change tmp directory used by PigServer in HCat tests</summary>
      <description></description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="19632" opendate="2018-5-21 00:00:00" fixdate="2018-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove webapps directory from standalone jar</summary>
      <description>JDBC standalone jar contains webapps static files which just adds to the jar size and are not required by the clients.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19633" opendate="2018-5-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove/Migrate Minimr tests</summary>
      <description>MR has been deprecated for a long time. Minimr tests are incredibly slow. We should remove the tests or migrate to faster options for coverage.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.using.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.root.dir.external.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.using.q</file>
      <file type="M">ql.src.test.queries.clientpositive.root.dir.external.table.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMinimrCliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19641" opendate="2018-5-21 00:00:00" fixdate="2018-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sync up hadoop version used by storage-api with hive</summary>
      <description>There is hadoop version mismatch between hive and storage-api and hence different transitive dependency versions gets pulled.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19643" opendate="2018-5-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM table conversion doesn&amp;#39;t need full ACID structure checks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.conversions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.conversions.q</file>
    </fixedFiles>
  </bug>
  <bug id="19653" opendate="2018-5-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect predicate pushdown for groupby with grouping sets</summary>
      <description>Consider the following query:CREATE TABLE T1(a STRING, b STRING, s BIGINT);INSERT OVERWRITE TABLE T1 VALUES ('aaaa', 'bbbb', 123456);SELECT * FROM (SELECT a, b, sum(s)FROM T1GROUP BY a, b GROUPING SETS ((), (a), (b), (a, b))) t WHERE a IS NOT NULL;When hive.optimize.ppd is enabled (and hive.cbo.enable=false), the query will output:NULL NULL 123456NULL bbbb 123456aaaa NULL 123456aaaa bbbb 123456We can see the predicate "a IS NOT NULL" takes no effect, which is incorrect.When performing PPD optimization for a GBY operator, we should make sure all grouping sets contains the processing expr before pushdown. otherwise the expr value after GBY is changed and the result is wrong.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="19654" opendate="2018-5-22 00:00:00" fixdate="2018-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change tmp staging mapred directory for TestBlobstoreCliDriver</summary>
      <description>Similar to HIVE-19626.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-blobstore.src.test.resources.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19669" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ORC to 1.5.1</summary>
      <description></description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19677" opendate="2018-5-23 00:00:00" fixdate="2018-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable sample6.q</summary>
      <description>Flaky test, already found similar behavior with sample2.q and sample4.q (HIVE-19657). More info to reproduce and try to fix the issue in HIVE-19673.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19708" opendate="2018-5-25 00:00:00" fixdate="2018-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl copy retrying with cm path even if the failure is due to network issue</summary>
      <description>During repl load for filesystem based copying of file if the copy fails due to a connection error to source Name Node, we should recreate the filesystem object. the retry logic for local file copy should be triggered using the original source file path ( and not the CM root path ) since failure can be due to network issues between DFSClient and NN. When listing files in tables / partition to include them in _files, we should add retry logic when failure occurs. FileSystem object here also should be recreated since the existing one might be in inconsistent state.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1971" opendate="2011-2-8 00:00:00" fixdate="2011-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verbose/echo mode for the Hive CLI</summary>
      <description>It would be very beneficial to have a mode which allows a user to run a SQL script, and have each command echoed to the console as it's executed. This would be useful in figuring out which SQL statement is causing failures during test runs, especially when running particularly long scripts.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19725" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to dump non-native tables in replication metadata dump</summary>
      <description>ifhive.repl.dump.metadata.only is set to true, allow dumping non native tables also.Data dump for non-native tables should never be allowed.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
    </fixedFiles>
  </bug>
  <bug id="19727" opendate="2018-5-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Signature matching of table aliases</summary>
      <description>there is a probable problem with alias matching: "t1 as a" is matched to "t2 as a"</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="19772" opendate="2018-6-1 00:00:00" fixdate="2018-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming ingest V2 API can generate invalid orc file if interrupted</summary>
      <description>Hive streaming ingest generated 0 length and 3 byte files which are invalid orc files. This will throw the following exception during compactionError: org.apache.orc.FileFormatException: Not a valid ORC file hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/culvert/year=2018/month=7/delta_0000025_0000025/bucket_00005 (maxFileLength= 3) at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:546) at org.apache.orc.impl.ReaderImpl.&lt;init&gt;(ReaderImpl.java:370) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&lt;init&gt;(ReaderImpl.java:60) at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:90) at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.&lt;init&gt;(OrcRawRecordMerger.java:1124) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:2373) at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:1000) at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:977) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:460) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:344) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)</description>
      <version>3.1.0,3.0.1,4.0.0</version>
      <fixedVersion>3.1.0,3.0.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.AbstractRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
    </fixedFiles>
  </bug>
  <bug id="19773" opendate="2018-6-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO exception while running queries with tables that are not present in materialized views</summary>
      <description>When we obtain the valid list of write ids, some tables in the materialized views may not be present in the list because they are not present in the query, which leads to exceptions (hidden in logs) when we try to load the materialized views in the planner, as we need to verify whether they are outdated or not.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19775" opendate="2018-6-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should use HS2 embedded mode in privileged auth mode</summary>
      <description>Follow up of HIVE-19389.Authorization checks don't make sense for embedded mode and since it is not used in that mode it leads to issues if authorization is enabled (eg, username not set).</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="19776" opendate="2018-6-2 00:00:00" fixdate="2018-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2.startHiveServer2 retries of start has concurrency issues</summary>
      <description>HS2 starts the thrift binary/http servers in background, while it proceeds to do other setup (eg create zookeeper entries). If there is a ZK error and it attempts to stop and start in the retry loop within HiveServer2.startHiveServer2, the retry fails because the thrift server doesn't get stopped if it was still getting initialized.The thrift server initialization and stopping needs to be synchronized.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="19778" opendate="2018-6-3 00:00:00" fixdate="2018-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable a flaky test: TestCliDriver#input31</summary>
      <description>Noticed this one has been failing occasionally on precommit test runs.Running: diff -a /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/itests/qtest/target/qfile-results/clientpositive/input31.q.out /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/ql/src/test/results/clientpositive/input31.q.out128c128&lt; 496---&gt; 242</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19796" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push Down TRUNC Fn to Druid Storage Handler</summary>
      <description>Push down Queries with TRUNC date function such as SELECT SUM((`ssb_druid_100`.`discounted_price` * `ssb_druid_100`.`net_revenue`)) AS `sum_calculation_4998925219892510720_ok`, CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`FROM `druid_ssb`.`ssb_druid_100` `ssb_druid_100`GROUP BY CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE)</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DruidSqlOperatorConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExtractDate.java</file>
    </fixedFiles>
  </bug>
  <bug id="19799" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove jasper dependency</summary>
      <description>jasper dependency version looks old and unwanted. There is a comment which says it is required by thrift but I don't see jasper as thrift dependency. Try removing it to see if its safe (after precommit test run).</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1980" opendate="2011-2-9 00:00:00" fixdate="2011-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging using mapreduce rather than map-only job failed in case of dynamic partition inserts</summary>
      <description>In dynamic partition insert and if merge is set to true and hive.mergejob.maponly=false, the merge MapReduce job will fail.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
    </fixedFiles>
  </bug>
  <bug id="19801" opendate="2018-6-5 00:00:00" fixdate="2018-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Add some missing classes to jdbc standalone jar and remove hbase classes</summary>
      <description></description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19812" opendate="2018-6-6 00:00:00" fixdate="2018-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable external table replication by default via a configuration property</summary>
      <description>use a hive config property to allow external table replication. set this property by default to prevent external table replication.for metadata only hive repl always export metadata for external tables.REPL_DUMP_EXTERNAL_TABLES("hive.repl.dump.include.external.tables", false,"Indicates if repl dump should include information about external tables. It should be \n"+ "used in conjunction with 'hive.repl.dump.metadata.only' set to false. if 'hive.repl.dump.metadata.only' \n"+ " is set to true then this config parameter has no effect as external table meta data is flushed \n"+ " always by default.")This should be done for only replication dump and not for export</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.exim.21.part.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.19.external.over.existing.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.16.part.noncompat.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.15.part.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.14.nonpart.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.13.nonnative.import.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.12.nonnative.export.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.11.nonpart.noncompat.sorting.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.10.nonpart.noncompat.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.09.nonpart.noncompat.serdeparam.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.08.nonpart.noncompat.serde.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.07.nonpart.noncompat.ifof.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.06.nonpart.noncompat.storage.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.05.nonpart.noncompat.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.04.nonpart.noncompat.colnumber.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.03.nonpart.noncompat.colschema.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.repl.2.exim.basic.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19815" opendate="2018-6-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl dump should not propagate the checkpoint and repl source properties</summary>
      <description>For replication scenarios of A-&gt; B -&gt; C the repl dump on B should not include the checkpoint property when dumping out table information. Alter tables/partitions during incremental should not propagate this as well.Also should not propagate the the db level parameters set byreplication internally.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.PartitionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="19817" opendate="2018-6-6 00:00:00" fixdate="2018-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive streaming API + dynamic partitioning + json/regex writer does not work</summary>
      <description>New streaming API for dynamic partitioning only works with delimited record writer. Json and Regex writers does not work.</description>
      <version>3.1.0,3.0.1,4.0.0</version>
      <fixedVersion>3.1.0,3.0.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreamingDynamicPartitioning.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictRegexWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.JsonSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="19827" opendate="2018-6-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hiveserver2 startup should provide a way to override TEZ_CONF_DIR</summary>
      <description>HS2 should use /etc/tez/conf, HSI should use /etc/tez_llap/conf</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="19829" opendate="2018-6-8 00:00:00" fixdate="2018-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental replication load should create tasks in execution phase rather than semantic phase</summary>
      <description>Split the incremental load into multiple iterations. In each iteration create number of tasks equal to the configured value.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TestTaskTracker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeavesTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.PartitionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TaskTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.TableContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.AddDependencyToLeaves.java</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="1983" opendate="2011-2-10 00:00:00" fixdate="2011-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle Log4j configuration files in Hive JARs</summary>
      <description>Splitting this off as a subtask so that it can be resolved independently of the hive-default.xml issue.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">conf.hive-log4j.properties</file>
      <file type="M">conf.hive-exec-log4j.properties</file>
      <file type="M">common.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19851" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade jQuery version</summary>
      <description>jQuery version seems to be very old. Update to latest stable version.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.static.js.jquery.min.js</file>
    </fixedFiles>
  </bug>
  <bug id="19852" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update jackson to latest</summary>
      <description>Update jackson version to latest 2.9.5</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19853" opendate="2018-6-11 00:00:00" fixdate="2018-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow serializer needs to create a TimeStampMicroTZVector instead of TimeStampMicroVector</summary>
      <description>HIVE-19723 changednanosecondto microsecondin Arrow serialization. However, it needs to be microsecond with time zone.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Deserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19873" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup operation log on query cancellation after some delay</summary>
      <description>When a query is executed using beeline and the query is cancelled due to query timeout or kill query or triggers and when there is cursor on operation log row set, the cursor can thrown an exception as cancel will cleanup the operation log in the background. This can return a non-zero exit code in beeline. Query cancellation on success should return exit code 0.Adding a delay to the cleanup of operation logging in operation cancel can avoid theclose during read.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19875" opendate="2018-6-12 00:00:00" fixdate="2018-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase LLAP IO queue size for perf</summary>
      <description>According to gopalv queue limit has perf impact, esp. during hashtable load for mapjoin where in the past IO used to queue up more data for processing to process.1) Overall the default limit could be adjusted higher.2) Depending on Decimal64 availability, the weight for decimal columns could be reduced.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19879" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused calcite sql operator.</summary>
      <description>HIVE-19796 introduced by mistake an unused sql operator.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="19880" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl Load to return recoverable vs non-recoverable error codes</summary>
      <description>To enable bootstrap of large databases, applicationhas to have the ability to keep retrying the bootstrap load till it encounters a fatal error. The ability to identify if an error is fatal or not will be decided by hive and communication of the same will happen to applicationvia error codes.So there should be different error codes for recoverable vs non-recoverable failures which should be propagated to applicationas part of running the repl load command.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.messaging.EventUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19881" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow metadata-only dump for database which are not source of replication</summary>
      <description>If the dump is meta data only then allow dump even if the db is not source of replication</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19884" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalidation cache may throw NPE when there is no data in table used by materialized view</summary>
      <description></description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsInvalidationCache.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19885" opendate="2018-6-13 00:00:00" fixdate="2018-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Kafka Ingestion - Allow user to set kafka consumer properties via table properties</summary>
      <description>Allow users to set kafka consumer properties via table properties.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidkafkamini.basic.q</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniDruidKafkaCliDriver.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="19902" opendate="2018-6-14 00:00:00" fixdate="2018-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Metastore micro-benchmarks</summary>
      <description>It would be very useful to have metastore benchmarks to be able to track perf issues.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19924" opendate="2018-6-17 00:00:00" fixdate="2018-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tag distcp jobs run by Repl Load</summary>
      <description>Add tags in jobconf for distcp related jobs started by replication. This will allow hive to kill these jobs in case beacon retries, or hs2 dies and beacon issues a kill command. one of the tags should definitely be the query_id that starts the job : With this flow beacon before retrying the bootstrap load, will issue a kill command to hs2 with the query id of the previous issued command. hs2 will then kill an running jobs on yarn tagged with the Query_id. To get around the additional failure point as mentioned above. The jobs can be tagged with an additional unique tag_id provided by Beacon in the WITH clause in repl load command to be used to tag distcp jobs ). Enhance the kill api to take the tag as input and kill jobs associated with that tag. Problem here is how do we validate the association of the tag with a hive query id to make sure this api is not used to kill jobs run by other components, however we can provide this capability to only admins and should be ok in that case.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.NullKillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.KillQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.KillTriggerActionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug id="19928" opendate="2018-6-18 00:00:00" fixdate="2018-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load Data for managed tables should set the owner of loaded files to a configurable user</summary>
      <description>load data of managed tables should set the owner of the loaded files to a configurable user. the default user should be hive.`If the owner of existing file is not hive, then a rename/move operation should be replaced by copy with the copied file having hive as owner.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveCopyFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19938" opendate="2018-6-18 00:00:00" fixdate="2018-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade scripts for information schema</summary>
      <description>To make schematool -upgradeSchema work for information schema.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade.order.hive</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.1.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug id="19956" opendate="2018-6-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include yarn registry classes to jdbc standalone jar</summary>
      <description>HS2 Active/Passive HA requires some yarn registry classes. Include it in JDBC standalone jar.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19970" opendate="2018-6-22 00:00:00" fixdate="2018-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication dump has a NPE when table is empty</summary>
      <description>if table directory or partition directory is missing ..dump is throwing NPE instead of file missing exception.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="19972" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup to HIVE-19928 : Fix the check for managed table</summary>
      <description>The check for managed table should use ENUM comparison rather than string comparison.The check in the patch will always return false, thus maintaining existing behavior.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="19973" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable materialized view rewriting by default</summary>
      <description>Change property value for hive.materializedview.rewriting to true. For tests, it is already true by default.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19981" opendate="2018-6-25 00:00:00" fixdate="2018-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Managed tables converted to external tables by the HiveStrictManagedMigration utility should be set to delete data when the table is dropped</summary>
      <description>Using the HiveStrictManagedMigration utility, tables can be converted to conform to the Hive strict managed tables mode.For managed tables that are converted to external tables by the utility, these tables should keep the "drop data on delete" semantics they had when they were managed tables.One way to do this is to introduce a table property "external.table.purge", which if true (and if the table is an external table), will let Hive know to delete the table data when the table is dropped. This property will be set by the HiveStrictManagedMigration utility when managed tables are converted to external tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20040" opendate="2018-6-30 00:00:00" fixdate="2018-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HTTP listen queue is 50 and SYNs are lost</summary>
      <description>When testing with 5000 concurrent users, the JDBC HTTP port ends up overflowing on SYNs when the HS2 gc pauses.This is because each getQueryProgress request is an independent HTTP request, so unlike the BINARY mode, there are more connections being established &amp; closed in HTTP mode.LISTEN 0 50 *:10004 *:* This turns into connection errors when enabling net.ipv4.tcp_abort_on_overflow=1, but the better approach is to enqueue the connections until the HS2 is done with its GC pause.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="20061" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a config flag to turn off txn stats</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20066" opendate="2018-7-3 00:00:00" fixdate="2018-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.load.data.owner is compared to full principal</summary>
      <description>HIVE-19928 compares the user running HS2 to the configured owner (hive.load.data.owner) to check if we're able to move the file with LOAD DATA or need to copy.This check compares the full username (that may contain the full kerberos principal) to hive.load.data.owner. We should compare to the short username (UGI.getShortUserName()) instead. That's used in similar context here.cc djaiswal</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="20120" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental repl load DAG generation is causing OOM error.</summary>
      <description>Split the incremental load into multiple iterations. In each iteration create number of tasks equal to the configured value.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="20123" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix masking tests after HIVE-19617</summary>
      <description>Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed.</description>
      <version>3.1.0,3.0.0,3.2.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20293" opendate="2018-8-2 00:00:00" fixdate="2018-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Replication of ACID table truncate operation</summary>
      <description>Support truncate acid table replication.1. Write id allocation needs to be removed</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AlterTableEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.repl.exim.TestEximReplicationTasks.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="20294" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix NULL / Wrong Results issues in COALESCE / ELT</summary>
      <description>Write new UT tests that use random data and intentional isRepeating batches to checks for NULL and Wrong Results for vectorized COALESCE and ELT.Also, add tests for ARRAY and MAP indexing, IS &amp;#91;NOT&amp;#93; NULL and NOT</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexStringCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexLongCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexDoubleCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBaseCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMapIndexBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ListIndexColColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="20299" opendate="2018-8-2 00:00:00" fixdate="2018-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>potential race in LLAP signer unit test</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="2030" opendate="2011-3-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>isEmptyPath() to use ContentSummary cache</summary>
      <description>addInputPaths() calls isEmptyPath() for every input path. Now every call is a DFS namenode call. Making isEmptyPath() to use cached ContentSummary, we should be able to avoid some namenode calls and reduce latency in the case of multiple partitions.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20300" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>VectorFileSinkArrowOperator</summary>
      <description>Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapRow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20301" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization for materialized view rewriting tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.empty.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rebuild.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.time.window.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.rebuild.dummy.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.mv.q</file>
    </fixedFiles>
  </bug>
  <bug id="20302" opendate="2018-8-3 00:00:00" fixdate="2018-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: non-vectorized execution in IO ignores virtual columns, including ROW__ID</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.nonvector.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.llap.nonvector.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOiBatchToRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BatchToRowReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="20316" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip external table file listing for create table event.</summary>
      <description>We are currently skipping external table replication. We shallalso skip listing all the files in create table event generation for external tables.External tables might have very large number of files, so it would take long time to list them.</description>
      <version>3.1.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="20329" opendate="2018-8-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Long running repl load (incr/bootstrap) causing OOM error</summary>
      <description>The task created in the previous iterations of the load are not delinked and thus causing heap memory usage issue. need to delink the tasks to avoid OOM error.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20338" opendate="2018-8-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Force synthetic file-id for filesystems which have HDFS protocol impls with POSIX mutation semantics</summary>
      <description>HDFS client protocol is not a guarantee of the immutability of files - the synthetic file-id includes the mtime of the file as well, which is a fail-safe for filesystems which implement the client wire protocol without offering the same storage side restrictions on immutability (i.e allow NFS read-write-modify on the backend).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20339" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Lift unneeded restriction causing some PTF with RANK not to be vectorized</summary>
      <description>Unnecessary: "PTF operator: More than 1 argument expression of aggregation function rank"</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2034" opendate="2011-3-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport HIVE-1991 after overridden by HIVE-1950</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="20340" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Needs Explicit CASTs from Timestamp to STRING when the output of timestamp function is used as String</summary>
      <description>Druid time expressions return numeric values in form of ms (instead of formatted timestamp). Because of this expressions/function which expects its argument as string type ended up returning different values for time expressions input.e.g.SELECT SUBSTRING(to_date(datetime0),4) FROM tableau_orc.calcs;| 4-07-25 |SELECT SUBSTRING(to_date(datetime0),4) FROM druid_tableau.calcs;| 0022400000 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM tableau_orc.calcs;| 2004-07-17 00:00:00 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM druid_tableau.calcs;| 1090454400000 00:00:00 |Druid needs explicit cast to make this work</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="20343" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 3: CTAS does not respect transactional_properties</summary>
      <description>Steps to reproduce:create table ctasexampleinsertonly stored as orc TBLPROPERTIES ("transactional_properties"="insert_only") as select * from testtable limit 1;look for transactional_properties which is 'default' not the expected "insert_only" describe formatted ctasexampleinsertonly +-------------------------------+----------------------------------------------------+-----------------------+| col_name | data_type | comment |+-------------------------------+----------------------------------------------------+-----------------------+| # col_name | data_type | comment || name | varchar(8) | || time | double | || | NULL | NULL || # Detailed Table Information | NULL | NULL || Database: | default | NULL || OwnerType: | USER | NULL || Owner: | hive | NULL || CreateTime: | Wed Aug 08 21:35:15 UTC 2018 | NULL || LastAccessTime: | UNKNOWN | NULL || Retention: | 0 | NULL || Location: | hdfs://xxxxxxxxxx:8020/warehouse/tablespace/managed/hive/ctasexampleinsertonly | NULL || Table Type: | MANAGED_TABLE | NULL || Table Parameters: | NULL | NULL || | COLUMN_STATS_ACCURATE | {} || | bucketing_version | 2 || | numFiles | 1 || | numRows | 1 || | rawDataSize | 0 || | totalSize | 754 || | transactional | true || | transactional_properties | default || | transient_lastDdlTime | 1533764115 || | NULL | NULL || # Storage Information | NULL | NULL || SerDe Library: | org.apache.hadoop.hive.ql.io.orc.OrcSerde | NULL || InputFormat: | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat | NULL || OutputFormat: | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat | NULL || Compressed: | No | NULL || Num Buckets: | -1 | NULL || Bucket Columns: | [] | NULL || Sort Columns: | [] | NULL || Storage Desc Params: | NULL | NULL || | serialization.format | 1 |+-------------------------------+----------------------------------------------------+-----------------------+not sure whether its a cosmatic issue but it does creates a problem with insert CREATE TABLE TABLE42 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe' STORED AS RCFILE LOCATION '/tmp/test10' as select * from testtable limit 1;it takes the default transactional_properties as insert_only instead of default and failed with the following Exception ERROR : Job Commit failed with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(The following files were committed but not found: [/tmp/test10/delta_0000004_0000004_0000/000000_0])'org.apache.hadoop.hive.ql.metadata.HiveException: The following files were committed but not found: [/tmp/test10/delta_0000004_0000004_0000/000000_0] at org.apache.hadoop.hive.ql.exec.Utilities.handleMmTableFinalPath(Utilities.java:4329) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1393) at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:798) at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803) at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803) at org.apache.hadoop.hive.ql.exec.tez.TezTask.close(TezTask.java:579) at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:316) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2668) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2339) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2015) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1713) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1707) at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224) at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:316) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:329) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20344" opendate="2018-8-8 00:00:00" fixdate="2018-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PrivilegeSynchronizer for SBA might hit AccessControlException</summary>
      <description>__If "hive" user does not have privilege of corresponding hdfs folders, PrivilegeSynchronizer won't be able to get metadata of the table because SBA is preventing it. Here is a sample stack:Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.security.AccessControlException: Permission denied: user=hive, access=EXECUTE, inode="/tmp/sba_is/sba_db":hrt_7:hrt_qa:dr-------- at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:315) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242) at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkDefaultEnforcer(RangerHdfsAuthorizer.java:512) at org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer$RangerAccessControlEnforcer.checkPermission(RangerHdfsAuthorizer.java:305) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1850) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1834) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1784) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7767) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkPermissions(StorageBasedAuthorizationProvider.java:424) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkPermissions(StorageBasedAuthorizationProvider.java:382) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:355) at org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(StorageBasedAuthorizationProvider.java:203) at org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeReadTable(AuthorizationPreEventListener.java:192) ... 23 moreI simply skip the table if that happens. In practice, managed tables are owned by "hive" user, so only external tables will be impacted. User need to grant execute permission of db folder and read permission of the table folders to "hive" user if they want to query the information schema for the tables, whose permission is only granted via SBA.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeSynchonizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20357" opendate="2018-8-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce initOrUpgradeSchema option to schema tool</summary>
      <description>Currently, schematool has two option: initSchema/upgradeSchema. User needs to use different command line for different action. However, from the schema version stored in db, we shall able to figure out if there's a need to init/upgrade, and choose the right action automatically.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool.java</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.0.0-to-3.1.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug id="20360" opendate="2018-8-10 00:00:00" fixdate="2018-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QTest: ignore driver/qtest exclusions if -Dqfile param is set</summary>
      <description>Sometimes I need to run qtestswith another driver for testing purposes. In this case I have to edit testconfiguration.properties which seems a bit hacky, even if it's temporary.In this case, no tests will run (however there's a log message):mvn test -Pitests -pl itests/qtest -pl itests/util -Dtest=TestCliDriver -Dqfile=bucketizedhiveinputformat.q</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.ql.TestQOutProcessor.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QOutProcessor.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCliConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="20382" opendate="2018-8-14 00:00:00" fixdate="2018-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Materialized views: Introduce heuristic to favour incremental rebuild</summary>
      <description>Currently, we do not expose stats over ROW&amp;#95;&amp;#95;ID.writeId to the optimizer (this should be fixed by HIVE-20313). Even if we did, we always assume uniform distribution of the column values, which can easily lead to overestimations on the number of rows read when we filter on ROW&amp;#95;&amp;#95;ID.writeId for materialized views (think about a large transaction for MV creation and then small ones for incremental maintenance). This overestimation can lead to incremental view maintenance not being triggered as cost of the incremental plan is overestimated (we think we will read more rows than we actually do). This could be fixed by introducing histograms that reflect better the column values distribution.Till both fixes are implemented, we will use a config variable that will multiply the estimated cost of the rebuild plan and hence will be able to favour incremental rebuild over full rebuild.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rebuild.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewRule.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20431" opendate="2018-8-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>txn stats write ID check triggers on set location</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.acid.stats4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.stats4.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20433" opendate="2018-8-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implicit String to Timestamp conversion is slow</summary>
      <description>getTimestampFromString() is slow at casting dates. It throws twice before date conversion can happen.cc gopalv ashutoshc</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20499" opendate="2018-9-4 00:00:00" fixdate="2018-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetTablesOperation pull all the tables meta irrespective of auth.</summary>
      <description>GetTablesOperation pull all the tables meta irrespective of auth.dbvisualizer and other ui based jdbc client pull tableemta similar to following operation:ResultSet res = con.getMetaData().getTables("", "", "%", new String[] { "TABLE", "VIEW" });https://github.com/rajkrrsingh/HiveServer2JDBCSample/blob/master/src/main/java/TestConnection.java#L20</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreFilterHook.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="20503" opendate="2018-9-5 00:00:00" fixdate="2018-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use datastructure aware estimations during mapjoin selection</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestVectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="20507" opendate="2018-9-5 00:00:00" fixdate="2018-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: Add a utility command to retrieve all uris from beeline-site.xml</summary>
      <description>It will be useful for some clients to get the url list when beeline-site is present.</description>
      <version>3.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="20508" opendate="2018-9-5 00:00:00" fixdate="2018-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive does not support user names of type "user@realm"</summary>
      <description>Hive does not support user names of type "user@realm". This causes authorization problems with Ranger for user names containing realms in Kerberos environment.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20509" opendate="2018-9-6 00:00:00" fixdate="2018-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Plan: fix wasted memory in plans with large partition counts</summary>
      <description>public void addPathToAlias(Path path, String newAlias){ ArrayList&lt;String&gt; aliases = pathToAliases.get(path); if (aliases == null) { aliases = new ArrayList&lt;&gt;(); StringInternUtils.internUriStringsInPath(path); pathToAliases.put(path, aliases); } aliases.add(newAlias.intern()); }ArrayList::DEFAULT_CAPACITY is 10, so this wastes 500 bytes of memory due to the new ArrayList&lt;&gt;();.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
    </fixedFiles>
  </bug>
  <bug id="2051" opendate="2011-3-11 00:00:00" fixdate="2011-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getInputSummary() to call FileSystem.getContentSummary() in parallel</summary>
      <description>getInputSummary() now call FileSystem.getContentSummary() one by one, which can be extremely slow when the number of input paths are huge. By calling those functions in parallel, we can cut latency in most cases.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="20521" opendate="2018-9-8 00:00:00" fixdate="2018-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 doAs=true has permission issue with hadoop.tmp.dir, with MR and S3A filesystem</summary>
      <description>This is a result of changes in HIVE-18858. As described by puneetj in HIVE-18858 -This seems to have broken working scenarios with Hive MR. We now see hadoop.tmp.dir is always set to /tmp/hadoop-hive (in job.xml). This creates problems on a multi-tenant hadoop cluster since ownership of tmp folder is set to the user who executes the jobs first and other users fails to write to tmp folder.E.g. User1 run job and /tmp/hadoop-hive is created on worker node with ownership to user1 and sibsequently user2 tries to run a job and job fails due to no write permission on /tmp/hadoop-hive/Old behavior allowed multiple tenants to write to their respective tmp folders which was secure and contention free. User1 - /tmp/hadoop-user1, User2 - /tmp/hadoop-user2.The change in HIVE-18858 causes variable expansion to happen in HiveServer2, while it was happening in the tasks (ExecMapper, ExecReducer) before that change. THis causes "/tmp/hadoop-{user.name}" to be expanded as /tmp/hadoop-hive instead of /tmp/hadoop-user1</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.UtilsForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="20545" opendate="2018-9-12 00:00:00" fixdate="2018-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to exclude potentially large parameters in HMS Notifications</summary>
      <description>Clients can add large-sized parameters in Table/Partition objects. So we need to enableadding regex patterns through HiveConf to match parameters to be filtered from table and partition objects before serialization in HMS notifications.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20546" opendate="2018-9-12 00:00:00" fixdate="2018-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Apache Druid 0.13.0-incubating</summary>
      <description>This task is to upgrade to druid 0.13.0 when it is released. Note that it will hopefully be first apache release for Druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.ts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.ForkingDruidNode.java</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe2.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DerbyConnectorTestUtility.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.RetryIfUnauthorizedResponseHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.ResponseCookieHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.KerberosHttpClient.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorReport.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorIOConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroStreamInputRowParser.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroParseSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidKafkaUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20555" opendate="2018-9-13 00:00:00" fixdate="2018-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2: Preauthenticated subject for http transport is not retained for entire duration of http communication in some cases</summary>
      <description>As implemented in HIVE-8705, for http transport, we add the logged in subject's credentials in the http header via a request interceptor. The request interceptor doesn't seem to be getting used for some http traffic (e.g. knox ssl in the same rpc). It would also be better to cache the logged in subject for the duration of the whole session.</description>
      <version>2.3.2,3.1.0</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpKerberosRequestInterceptor.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="20558" opendate="2018-9-14 00:00:00" fixdate="2018-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default of hive.hashtable.key.count.adjustment to 0.99</summary>
      <description>Current default is 2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20618" opendate="2018-9-21 00:00:00" fixdate="2018-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>During join selection BucketMapJoin might be choosen for non bucketed tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="20646" opendate="2018-9-27 00:00:00" fixdate="2018-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partition filter condition is not pushed down to metastore query if it has IS NOT NULL.</summary>
      <description>If the partition filter condition has "is not null" then the filter query isn't getting pushed to the SQL query in RDMBS. This slows down metastore api calls for getting list of partitions with filter condition.This condition gets added by optimizer in many cases so this is affecting many queries.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.MockPartitionExpressionForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartFilterExprUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.PartitionExpressionProxy.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20659" opendate="2018-9-29 00:00:00" fixdate="2018-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20679" opendate="2018-10-3 00:00:00" fixdate="2018-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL operations on hive might create large messages for DBNotification</summary>
      <description>Certain type of ddl operations might create large messages as part of DBNoitification, this might lead to the rdbms throwing an error when storing the message since its size is to large. It will also increase the footprint of the rdbms space usage. We should try store compressed messages to allow handling these situations. Edit: For notification_log table the message column for all supported databases can store messages from 2GB to 4GB</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddUniqueConstraintMessage.java</file>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateFunctionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateDatabaseMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCommitTxnMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterDatabaseMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterCatalogMessage.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DefaultHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.OpenTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.ConstraintsSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbstractMessageHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.dump.events.TestEventHandlerFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.EventMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAcidWriteMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddForeignKeyMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddNotNullConstraintMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPrimaryKeyMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="2068" opendate="2011-3-22 00:00:00" fixdate="2011-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Speed up query "select xx,xx from xxx LIMIT xxx" if no filtering or aggregation</summary>
      <description>Currently, "select xx,xx from xxx where ...(only partition conditions) LIMIT xxx" will start a MapReduce job with input to be the whole table or partition. The latency can be huge if the table or partition is big. We could reduce number of input files to speed up the queries.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LimitDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2070" opendate="2011-3-22 00:00:00" fixdate="2011-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW GRANT grantTime field should be a human-readable timestamp</summary>
      <description>Unix timestamps are not super useful when trying to interpret metadatahive&gt; show grant user foo on table bar;database default table bar principalName foo principalType USER privilege Select grantTime 1300828549 grantor natty</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20700" opendate="2018-10-5 00:00:00" fixdate="2018-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add config to disable rollup for druid</summary>
      <description>Add a table property - 'druid.rollup' to allow disabling rollup for druid tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="20714" opendate="2018-10-8 00:00:00" fixdate="2018-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW tblproperties for a single property returns the value in the name column</summary>
      <description>show tblproperties default.tmpfoo("bar") returns:+------------+-------------+| prpt_name | prpt_value |+------------+-------------+| bar value | NULL |+------------+-------------+It should return+------------+-------------+| prpt_name | prpt_value |+------------+-------------+| bar | bar value |+------------+-------------+</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="20715" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable test: udaf_histogram_numeric</summary>
      <description>this qtest is breaking a lot of testruns latelyI think it should be disabled</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="20716" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set default value for hive.cbo.stats.correlated.multi.key.joins to true</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20718" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add perf cli driver with constraints</summary>
      <description>Now that subtasks in HIVE-17039 will be completed, it will be good to have a perf cli driver with constraints declaration.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.tez.hive-site.xml</file>
      <file type="M">data.scripts.q.perf.test.init.sql</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestTezPerfCliDriver.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinConstraintsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainConfiguration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20735" opendate="2018-10-12 00:00:00" fixdate="2018-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address some of the review comments plus Kerberos support</summary>
      <description>As part of the review comments we agreed to: remove start and end offsets columns remove the best effort mode make the 2pc as default protocol for EOSAlso this patch will include an additional enhancement to add kerberos support.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.SimpleKafkaWriterTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaWritableTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaUtilsTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaRecordIteratorTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.SimpleKafkaWriter.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.MetadataColumn.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaWritable.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaUtils.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaTableProperties.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaStorageHandler.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaOutputFormat.java</file>
      <file type="M">kafka-handler.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="20751" opendate="2018-10-16 00:00:00" fixdate="2018-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade arrow version to 0.10.0</summary>
      <description>Need to upgrade arrow version as spark ismoving to arrow version 0.10.0 in it's upcoming release 2.4.0</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowWrapperWritable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapArrowRecordWriter.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20772" opendate="2018-10-18 00:00:00" fixdate="2018-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>record per-task CPU counters in LLAP</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.counters.LlapIOCounters.java</file>
    </fixedFiles>
  </bug>
  <bug id="20778" opendate="2018-10-19 00:00:00" fixdate="2018-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join reordering may not be triggered if all joins in plan are created by decorrelation logic</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.join.ptp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.select.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.notin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="20785" opendate="2018-10-20 00:00:00" fixdate="2018-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong key name in the JDBC DatabaseMetaData.getPrimaryKeys method</summary>
      <description>According to the documentation (1)the key should beKEY_SEQ, not KEQ_SEQ.Pull request available:https://github.com/apache/hive/pull/440(1)https://docs.oracle.com/javase/8/docs/api/java/sql/DatabaseMetaData.html#getPrimaryKeys-java.lang.String-java.lang.String-java.lang.String-</description>
      <version>3.1.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetPrimaryKeysOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="20792" opendate="2018-10-23 00:00:00" fixdate="2018-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inserting timestamp with zones truncates the data</summary>
      <description>For example with the table:CREATE TABLE myTable(a TIMESTAMP)STORED AS ORCtblproperties("transactional"="true");The following inserts store the wrong data:INSERT INTO myTable VALUES("2018-10-19 10:35:00 UTC"); -&gt; 2018-10-19 00:00:00.0INSERT INTO myTable VALUES("2018-10-19 10:35:00 ZZZ"); -&gt; 2018-10-19 00:00:00.0The second one should fail since ZZZ is not a time zone.Similarly if the column is of type DATE,INSERT INTO myTableDate VALUES("2018-10-19 AAAA"); -&gt; 2018-10-19</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.TimestampUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20793" opendate="2018-10-23 00:00:00" fixdate="2018-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add RP namespacing to workload management</summary>
      <description>The idea is to be able to use the same warehouse for multiple clusters in the cloud use cases. This scenario is not currently supported by WM.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateOrDropTriggerToPoolMappingRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropPoolRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropTriggerRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetActiveResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMMapping.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMNullablePool.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMNullableResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMPool.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMPoolTrigger.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMTrigger.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MWMResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="20839" opendate="2018-10-30 00:00:00" fixdate="2018-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Cannot find field" error during dynamically partitioned hash join</summary>
      <description>Occurs in some cases in the non-CBO optimized queries, either if CBO is disabled or has failed due to error.2018-10-11T04:40:22,724 ERROR [TezTR-85144_8944_1085_28_996_2 (1539092085144_8944_1085_28_000996_2)] tez.ReduceRecordProcessor: Hit error while closing operators - failing tree2018-10-11T04:40:22,724 ERROR [TezTR-85144_8944_1085_28_996_2 (1539092085144_8944_1085_28_000996_2)] tez.TezProcessor: java.lang.RuntimeException: cannot find field _col304 from [0:_col0, 1:_col1, 2:_col2, 3:_col3, 4:_col4, 5:_col5, 6:_col6, 7:_col7, 8:_col8, 9:_col9, 10:_col10, 11:_col11, 12:_col12, 13:_col13, 14:_col15, 15:_col16, 16:_col17, 17:_col18, 18:_col19, 19:_col20, 20:_col21, 21:_col22, 22:_col23, 23:_col24, 24:_col25, 25:_col26, 26:_col27, 27:_col28, 28:_col29, 29:_col30, 30:_col31, 31:_col32, 32:_col33, 33:_col34, 34:_col35, 35:_col36, 36:_col37, 37:_col38, 38:_col39, 39:_col40, 40:_col41, 41:_col42, 42:_col43, 43:_col44, 44:_col45, 45:_col46, 46:_col47, 47:_col48, 48:_col49, 49:_col50, 50:_col51, 51:_col52, 52:_col53, 53:_col54, 54:_col55, 55:_col56, 56:_col57, 57:_col58, 58:_col59, 59:_col60, 60:_col61, 61:_col62, 62:_col63, 63:_col64, 64:_col65, 65:_col66, 66:_col67, 67:_col68, 68:_col70, 69:_col72, 70:_col73, 71:_col74, 72:_col75, 73:_col76, 74:_col77, 75:_col78, 76:_col79, 77:_col80, 78:_col81, 79:_col82, 80:_col83, 81:_col84, 82:_col85, 83:_col86, 84:_col87, 85:_col88, 86:_col89, 87:_col90, 88:_col91, 89:_col92, 90:_col93, 91:_col94, 92:_col95, 93:_col96, 94:_col97, 95:_col98, 96:_col99, 97:_col100, 98:_col101, 99:_col102, 100:_col103, 101:_col104, 102:_col105, 103:_col106, 104:_col107, 105:_col108, 106:_col109, 107:_col110, 108:_col111, 109:_col112, 110:_col113, 111:_col114, 112:_col115, 113:_col116, 114:_col117, 115:_col118, 116:_col119, 117:_col120, 118:_col121, 119:_col122, 120:_col123, 121:_col124, 122:_col125, 123:_col126, 124:_col127, 125:_col128, 126:_col129, 127:_col130, 128:_col131, 129:_col132, 130:_col133, 131:_col134, 132:_col135, 133:_col136, 134:_col137, 135:_col138, 136:_col139, 137:_col140, 138:_col141, 139:_col142, 140:_col143, 141:_col144, 142:_col145, 143:_col146, 144:_col147, 145:_col148, 146:_col149, 147:_col150, 148:_col151, 149:_col152, 150:_col153, 151:_col154, 152:_col155, 153:_col156, 154:_col157, 155:_col158, 156:_col159, 157:_col160, 158:_col161, 159:_col162, 160:_col163, 161:_col164, 162:_col165, 163:_col166, 164:_col167, 165:_col168, 166:_col169, 167:_col170, 168:_col171, 169:_col318]at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:485)at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:153)at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:80)at org.apache.hadoop.hive.ql.exec.JoinUtil.getObjectInspectorsFromEvaluators(JoinUtil.java:91)at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:74)at org.apache.hadoop.hive.ql.exec.MapJoinOperator.initializeOp(MapJoinOperator.java:144)at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:374)at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:195)at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:188)at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:172)at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20850" opendate="2018-10-31 00:00:00" fixdate="2018-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push case conditional from projections to dimension tables if possible</summary>
      <description>noticed by gopalv: If there is a project which could be only evaluated after the join; but the condition references only a single column from a small dimension table; hive will end up evaluating the same thing over and over again...explainselect s_store_name, s_store_id, sum(case when (d_day_name='Sunday') then ss_sales_price else null end) sun_sales, sum(case when (d_day_name='Monday') then ss_sales_price else null end) mon_sales, sum(case when (d_day_name='Tuesday') then ss_sales_price else null end) tue_sales, sum(case when (d_day_name='Wednesday') then ss_sales_price else null end) wed_sales, sum(case when (d_day_name='Thursday') then ss_sales_price else null end) thu_sales, sum(case when (d_day_name='Friday') then ss_sales_price else null end) fri_sales, sum(case when (d_day_name='Saturday') then ss_sales_price else null end) sat_sales from date_dim, store_sales, store where d_date_sk = ss_sold_date_sk and s_store_sk = ss_store_sk and s_gmt_offset = -6 and d_year = 1998 group by s_store_name, s_store_id order by s_store_name, s_store_id,sun_sales,mon_sales,tue_sales,wed_sales,thu_sales,fri_sales,sat_sales limit 100;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.join46.mr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez-tag.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.use.ts.stats.for.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.position.alias.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.with.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.multi.output.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedworkext.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.q93.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.shared.scan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.prod.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.AbstractJdbcTriggersTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.inclause.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.const.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20854" opendate="2018-11-1 00:00:00" fixdate="2018-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sensible Defaults: Hive&amp;#39;s Zookeeper heartbeat interval is 20 minutes, change to 2</summary>
      <description>HIVE_ZOOKEEPER_SESSION_TIMEOUT("hive.zookeeper.session.timeout", "1200000ms", new TimeValidator(TimeUnit.MILLISECONDS), "ZooKeeper client's session timeout (in milliseconds). The client is disconnected, and as a result, all locks released, \n" + "if a heartbeat is not sent in the timeout."),That's 1,200,000ms which is too long for all practical purposes - a 20 minute outage in case a node has a failure is too long.That is too long for the JDBC load-balancing, LLAP failure tolerance and the lock manager expiry.Change to 2 minutes, as a sensible default</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.conf.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20880" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default value for hive.stats.filter.in.min.ratio</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20881" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation oversimplifies projections</summary>
      <description>create table cx2(bool1 boolean);insert into cx2 values (true),(false),(null);set hive.cbo.enable=true;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+--------+| _c0 |+--------+| true || false || NULL |+--------+set hive.cbo.enable=false;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+-------+| _c0 |+-------+| true || NULL || NULL |+-------+from explain it seems the expression was simplified to: (_col0 is true or null)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="2093" opendate="2011-4-5 00:00:00" fixdate="2011-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create/drop database should populate inputs/outputs and check concurrency and user permission</summary>
      <description>concurrency and authorization are needed for create/drop table. Also to make concurrency work, it's better to have LOCK/UNLOCK DATABASE and SHOW LOCKS DATABASE</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.db.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.24.import.nonexist.authsuccess.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.23.import.part.authsuccess.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.22.import.exist.authsuccess.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.17.part.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.16.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.14.managed.location.over.existing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.13.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.12.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.11.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.10.external.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.08.nonpart.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.hadoop20.q.out</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.security.HdfsAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Entity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.database.drop.does.not.exist.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.database.drop.not.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.database.drop.not.empty.restrict.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.22.export.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.23.import.exist.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.24.import.part.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.25.import.nonexist.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join.nonexistent.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.exist.part.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.nonpart.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.authfail.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.database.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.database.removes.partition.dirs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.00.nonpart.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.01.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.03.nonpart.over.compat.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20930" opendate="2018-11-16 00:00:00" fixdate="2018-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>VectorCoalesce in FILTER mode doesn&amp;#39;t take effect</summary>
      <description>HIVE-20277 fixed vectorized case expressions for FILTER, butVectorCoalesce is still not fixed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="20931" opendate="2018-11-16 00:00:00" fixdate="2018-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor code cleaning for Druid Storage Handler and some Hive-exec classes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.java</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.joins.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.floorTime.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.extractTime.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.dynamic.partition.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidkafkamini.basic.q</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe2.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.QTestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.io.TestHiveDruidSplit.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DerbyConnectorTestUtility.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.TaskReportData.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorIOConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.InlineSchemaAvroBytesDecoder.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroStreamInputRowParser.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroParseSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.AvroBytesDecoder.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidKafkaUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.conf.DruidConstants.java</file>
      <file type="M">data.scripts.q.test.cleanup.sql</file>
      <file type="M">data.files.datasets.druid.table.alltypesorc.load.hive.sql</file>
      <file type="M">checkstyle.checkstyle.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20932" opendate="2018-11-16 00:00:00" fixdate="2018-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorize Druid Storage Handler Reader</summary>
      <description>This patch aims at adding support for vectorize read of data from Druid to Hive.t3rmin4t0r suggested that this will improve the performance of the top level operators that supports vectorization.As a first cut am just adding a wrapper around the existing Record Reader to read up to 1024 row at a time.Future work will be to avoid going via old reader and convert straight the Json (smile format) to Vector primitive types.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="21004" opendate="2018-12-4 00:00:00" fixdate="2018-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Less object creation for Hive Kafka reader</summary>
      <description>Reduce the amount of un-needed object allocation by using a row boat as way to carry data around.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.VectorizedKafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="21007" opendate="2018-12-5 00:00:00" fixdate="2018-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join + Union can lead to wrong plans</summary>
      <description>Tez compiler has the ability to push JOIN within UNION (by replicating join on each branch). If this JOIN had a SJ branch outgoing (or incoming) it could mess up the plan and end up generating incorrect or wrong plan.As a safe measure any SJ branch after UNION should be removed (until we improve the logic to better handle SJ branches)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="21029" opendate="2018-12-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External table replication for existing deployments running incremental replication.</summary>
      <description>Existing deployments using hive replication do not get external tables replicated. For such deployments to enable external table replication they will have to provide a specific switch to first bootstrap external tables as part of hive incremental replication, following which the incremental replication will take care of further changes in external tables.The switch will be provided by an additional hive configuration (for ex: hive.repl.bootstrap.external.tables) and is to be used in WITH clause of REPL DUMP command. Additionally the existing hive config hive.repl.include.external.tables will always have to be set to "true" in the above clause. Proposed usage for enabling external tables replication on existing replication policy.1. Consider an ongoing repl policy &lt;db1&gt; in incremental phase.Enable hive.repl.include.external.tables=true and hive.repl.bootstrap.external.tables=true in next incremental REPL DUMP. Dumps all events but skips events related to external tables. Instead, combine bootstrap dump for all external tables under _bootstrap directory. Also, includes the data locations file "_external_tables_info. LIMIT or TO clause shouldnt be there to ensure the latest events are dumped before bootstrap dumping external tables.2. REPL LOAD on this dump applies all the events first, copies external tables data and then bootstrap external tables (metadata). It is possible that the external tables (metadata) are not point-in time consistent with rest of the tables. But, it would be eventually consistent when the next incremental load is applied. This REPL LOAD is fault tolerant and can be retried if failed.3. All future REPL DUMPs on this repl policy should set hive.repl.bootstrap.external.tables=false. If not set to false, then target might end up having inconsistent set of external tables as bootstrap wouldnt clean-up any dropped external tables.</description>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.ConstraintEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21038" opendate="2018-12-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix checkstyle for standalone-metastore</summary>
      <description>SinceHIVE-17506 checkstyle is not working for standalone-metastore and it's sub projects.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21124" opendate="2019-1-15 00:00:00" fixdate="2019-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL does not support the CREATE TABLE LIKE statement</summary>
      <description>Hive supports the CREATE TABLE LIKE statement but HPL/SQL does not support.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="21145" opendate="2019-1-22 00:00:00" fixdate="2019-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable cbo to use runtime statistics during re-optimization</summary>
      <description>This could enable to reorder joins according to runtime rowcounts.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.runtime.stats.merge.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.runtime.stats.merge.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestStatEstimations.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestReOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestOperatorCmp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestRuntimeStatsPersistence.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.OperatorStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSources.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.PlanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.MetastoreStatsConnector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.MapBackedStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.EmptyStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.CachingStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.RuntimeStatsMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignatureFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelWriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21171" opendate="2019-1-25 00:00:00" fixdate="2019-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip creating scratch dirs for tez if RPC is on</summary>
      <description>There are few places e.g. during creating DAG/Vertices where scratch directories are created for each vertex even if plan is being sent using RPC. This adds un-necessary overhead for cloud file system e.g. S3A.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21368" opendate="2019-3-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Unnecessary Decimal64 -&gt; HiveDecimal conversion</summary>
      <description>Joins projecting Decimal64 have a suspicious cast in the inner loopConvertDecimal64ToDecimal(col 14:decimal(7,2)/DECIMAL_64) -&gt; 24:decimal(7,2)'create temporary table foo(x int , y decimal(7,2));create temporary table bar(x int , y decimal(7,2));set hive.explain.user=false;explain vectorization detail select sum(foo.y) from foo, bar where foo.x = bar.x;' Map Join Operator'' condition map:'' Inner Join 0 to 1'' keys:'' 0 _col0 (type: int)'' 1 _col0 (type: int)'' Map Join Vectorization:'' bigTableKeyColumnNums: [0]'' bigTableRetainedColumnNums: [3]'' bigTableValueColumnNums: [3]'' bigTableValueExpressions: ConvertDecimal64ToDecimal(col 1:decimal(7,2)/DECIMAL_64) -&gt; 3:decimal(7,2)'' className: VectorMapJoinInnerBigOnlyLongOperator'' native: true'' nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true, Fast Hash Table and No Hybrid Hash Join IS true'' projectedOutputColumnNums: [3]'</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out.orig</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out.orig</file>
    </fixedFiles>
  </bug>
  <bug id="21383" opendate="2019-3-4 00:00:00" fixdate="2019-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC storage handler: Use catalog and schema to retrieve tables if specified</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="21384" opendate="2019-3-4 00:00:00" fixdate="2019-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to dbcp2 in JDBC storage handler</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcStorageHandler.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21385" opendate="2019-3-5 00:00:00" fixdate="2019-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling pushdown of non-splittable computation to JDBC sources</summary>
      <description>Until pushdown is cost-based decision, we will be able to enable / disable pushdown of operators that prevent reading results from the JDBC connection in parallel.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21444" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional tests for materialized view rewriting</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.1.q</file>
    </fixedFiles>
  </bug>
  <bug id="21462" opendate="2019-3-18 00:00:00" fixdate="2019-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrading SQL server backed metastore when changing data type of a column with constraints</summary>
      <description>SQL server does not allow changing data type of a column which has a constraint or an index on it. The constraint or theindex needs to be dropped beforechanging the data type and needs to be recreated after that. Metastore upgrade scripts aren't doing this and thus upgrade fails.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.1.0-to-2.2.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="21499" opendate="2019-3-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>should not remove the function from registry if create command failed with AlreadyExistsException</summary>
      <description>As a part of HIVE-20953 we are removing the function if creation for same failed with any reason, this will yield into the following situation.1. create function failed since function already exists2. on #1 failure hive will clear the permanent function from the registry3. this function will be of no use until hiveserver2 restarted.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="21540" opendate="2019-3-29 00:00:00" fixdate="2019-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query with join condition having date literal throws SemanticException.</summary>
      <description>This semantic exception is thrown for the following query. SemanticException '2019-03-20' encountered with 0 childrencreate table date_1 (key int, dd date);create table date_2 (key int, dd date);select d1.key, d2.dd from( select key, dd as start_dd, current_date as end_dd from date_1) d1 join date_2 as d2 on d1.key = d2.key where d2.dd between start_dd and end_dd;When the WHERE condition below is commented out, the query completes successfully.where d2.dd between start_dd and end_dd------------------------------------------------</description>
      <version>3.1.0,3.1.1</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21541" opendate="2019-3-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing asf headers from HIVE-15406</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloatNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimalNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromDate.java</file>
    </fixedFiles>
  </bug>
  <bug id="21543" opendate="2019-3-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use FilterHooks for show compactions</summary>
      <description>Use FilterHooks for checking dbs/tables/partitions for showCompactions</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21544" opendate="2019-3-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation corrupts coalesce/case/when expressions during folding</summary>
      <description>set hive.fetch.task.conversion=none;set hive.optimize.ppd=false;create table t (s1 string,s2 string);insert into t values (null,null);explainselect coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;select coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;incorrect result is:null_value null_value NULL noteqexpected result:null_value null_value true eq</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21579" opendate="2019-4-4 00:00:00" fixdate="2019-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce more complex SQL:2016 datetime formats</summary>
      <description>Enable Hive to parse the following datetime formats when any combination/subset of these or previously implemented patterns is provided in one string. Also catch combinationsthat conflict. MONTH MON D DAY DY Q WW Whttps://docs.google.com/document/d/1V7k6-lrPGW7_uhqM-FhKl3QsxwCRy69v2KIxPsGjc1k/edit</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="21587" opendate="2019-4-6 00:00:00" fixdate="2019-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain formatted CBO should write row type in JSON</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelWriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21645" opendate="2019-4-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include CBO json plan in explain formatted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21651" opendate="2019-4-26 00:00:00" fixdate="2019-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move protobuf serde into hive-exec.</summary>
      <description>The serde and input format is not accessible without doing an add jar or modifying hive aux libs. Moving it to hive-exec will let us use the serde.Can't move the serde to hive/serde since it depends on ProtobufMessageWriter which is in hive-exec.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.HiveEvents.proto</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.io.encoded.TestVectorDeserializeOrcWriter.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.serde2.TestProtoMessageSerDe.java</file>
      <file type="M">contrib.src.protobuf-test.SampleProtos.proto</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufMessageSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufBytesWritableSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.input.ProtobufMessageInputFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.input.package-info.java</file>
      <file type="M">contrib.src.gen-test.protobuf.gen-java.org.apache.hadoop.hive.contrib.serde2.SampleProtos.java</file>
      <file type="M">contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21668" opendate="2019-4-30 00:00:00" fixdate="2019-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove tomcat dependencies even from tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21783" opendate="2019-5-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid authentication for connection from the same domain</summary>
      <description>When a connection comes from the same domain do not authenticate the user. This is similar to NONE authentication but only for the connection from the same domain.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TrustDomainAuthenticationTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TestTrustDomainAuthenticationHttp.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TestTrustDomainAuthenticationBinary.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TestImproperTrustDomainAuthenticationHttp.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.auth.TestImproperTrustDomainAuthenticationBinary.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.MiniHiveKdc.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21832" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New metrics to get the average queue/serving/response time</summary>
      <description>SimpleDescriptiveStatisticswith window size would do here. Time is not important in this case.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.WmFragmentCounters.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21833" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Authorization in Hive based on object ownership</summary>
      <description>Background: Currently Hive Authorizer for Ranger does not provide owner information for Hive objects as part of AuthZ calls. This has resulted in gaps with respect to Sentry AuthZ and customers/partners cannot leverage privileges for owners in their authorization model.User Story: As an enterprise security admin, I need to be able to set privileges based on Hive object ownership for setting up access controls in Ranger so that I can provide appropriate protections and permissions for my enterprise users.Acceptance criteria:1) Owner information is available in Hive -Ranger AuthZ calls2) Ranger admin users can use owner information to set policies based on object ownership in Ranger UI and APIs3) OWNER Macro based policies continue to work for Hive objects</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21834" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid unnecessary calls to simplify filter conditions</summary>
      <description>Every time we create a filter, we try to simplify its condition. However, we already have a rule that simplifies the expressions and it is within the same loop as most of the rules that end up creating new filters. Hence, it may seem like we could remove some of the calls to simplify those conditions.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="21836" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache directory server version to 1.5.7</summary>
      <description>I've bumped into some issues when downloading 1.5.6 artifacts...changing it to 1.5.7 worked fineit seems apacheds is only used during testing</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21932" opendate="2019-6-27 00:00:00" fixdate="2019-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IndexOutOfRangeException in FileChksumIterator</summary>
      <description>According to definition of InsertEventRequestData in hive_metastore.thrift the filesAddedChecksum is a optional field. But the FileChksumIterator does not handle it correctly when a client fires a insert event which does not have file checksums. The issue is that InsertEvent class initializes fileChecksums list to a empty arrayList so the following check will never come into playresult = ReplChangeManager.encodeFileUri(files.get(i), chksums != null ? chksums.get(i) : null, subDirs != null ? subDirs.get(i) : null);The chksums check above should include a !chksums.isEmpty() check as well in the above line.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="21934" opendate="2019-6-28 00:00:00" fixdate="2019-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Materialized view on top of Druid not pushing everything</summary>
      <description>The title is not very informative, but examples hopefully are.this is the plan with the viewexplain SELECT MONTH(`dates_n1`.`__time`) AS `mn___time_ok`,CAST((MONTH(`dates_n1`.`__time`) - 1) / 3 + 1 AS BIGINT) AS `qr___time_ok`,SUM(1) AS `sum_number_of_records_ok`,YEAR(`dates_n1`.`__time`) AS `yr___time_ok`FROM `mv_ssb_100_scale`.`lineorder_n0` `lineorder_n0`JOIN `mv_ssb_100_scale`.`dates_n1` `dates_n1` ON (`lineorder_n0`.`lo_orderdate` = `dates_n1`.`d_datekey`)JOIN `mv_ssb_100_scale`.`customer_n1` `customer_n1` ON (`lineorder_n0`.`lo_custkey` = `customer_n1`.`c_custkey`)JOIN `mv_ssb_100_scale`.`supplier_n0` `supplier_n0` ON (`lineorder_n0`.`lo_suppkey` = `supplier_n0`.`s_suppkey`)JOIN `mv_ssb_100_scale`.`ssb_part_n0` `ssb_part_n0` ON (`lineorder_n0`.`lo_partkey` = `ssb_part_n0`.`p_partkey`)GROUP BY MONTH(`dates_n1`.`__time`),CAST((MONTH(`dates_n1`.`__time`) - 1) / 3 + 1 AS BIGINT),YEAR(`dates_n1`.`__time`)INFO : Starting task [Stage-3:EXPLAIN] in serial modeINFO : Completed executing command(queryId=sbouguerra_20190627113101_1493ee87-0288-4e30-b53c-0ee729ce3977); Time taken: 0.005 secondsINFO : OK+----------------------------------------------------+| Explain |+----------------------------------------------------+| Plan optimized by CBO. || || Vertex dependency in root stage || Reducer 2 &lt;- Map 1 (SIMPLE_EDGE) || || Stage-0 || Fetch Operator || limit:-1 || Stage-1 || Reducer 2 vectorized, llap || File Output Operator [FS_13] || Select Operator [SEL_12] (rows=300018951 width=38) || Output:["_col0","_col1","_col2","_col3"] || Group By Operator [GBY_11] (rows=300018951 width=38) || Output:["_col0","_col1","_col2","_col3"],aggregations:["sum(VALUE._col0)"],keys:KEY._col0, KEY._col1, KEY._col2 || &lt;-Map 1 [SIMPLE_EDGE] vectorized, llap || SHUFFLE [RS_10] || PartitionCols:_col0, _col1, _col2 || Group By Operator [GBY_9] (rows=600037902 width=38) || Output:["_col0","_col1","_col2","_col3"],aggregations:["sum(1)"],keys:_col0, _col1, _col2 || Select Operator [SEL_8] (rows=600037902 width=38) || Output:["_col0","_col1","_col2"] || TableScan [TS_0] (rows=600037902 width=38) || mv_ssb_100_scale@ssb_mv_druid_100,ssb_mv_druid_100,Tbl:COMPLETE,Col:NONE,Output:["vc"],properties:\{"druid.fieldNames":"vc","druid.fieldTypes":"timestamp","druid.query.json":"{\"queryType\":\"scan\",\"dataSource\":\"mv_ssb_100_scale.ssb_mv_druid_100\",\"intervals\":[\"1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"],\"virtualColumns\":[{\"type\":\"expression\",\"name\":\"vc\",\"expression\":\"\\\"__time\\\"\",\"outputType\":\"LONG\"}],\"columns\":[\"vc\"],\"resultFormat\":\"compactedList\"}","druid.query.type":"scan"} || |+----------------------------------------------------+if i use a simple druid table without MVexplain SELECT MONTH(`__time`) AS `mn___time_ok`,CAST((MONTH(`__time`) - 1) / 3 + 1 AS BIGINT) AS `qr___time_ok`,SUM(1) AS `sum_number_of_records_ok`,YEAR(`__time`) AS `yr___time_ok`FROM `druid_ssb.ssb_druid_100`GROUP BY MONTH(`__time`),CAST((MONTH(`__time`) - 1) / 3 + 1 AS BIGINT),YEAR(`__time`);+----------------------------------------------------+| Explain |+----------------------------------------------------+| Plan optimized by CBO. || || Stage-0 || Fetch Operator || limit:-1 || Select Operator [SEL_1] || Output:["_col0","_col1","_col2","_col3"] || TableScan [TS_0] || Output:["extract_month","vc","$f3","extract_year"],properties:\{"druid.fieldNames":"extract_month,vc,extract_year,$f3","druid.fieldTypes":"int,bigint,int,bigint","druid.query.json":"{\"queryType\":\"groupBy\",\"dataSource\":\"druid_ssb.ssb_druid_100\",\"granularity\":\"all\",\"dimensions\":[{\"type\":\"extraction\",\"dimension\":\"__time\",\"outputName\":\"extract_month\",\"extractionFn\":{\"type\":\"timeFormat\",\"format\":\"M\",\"timeZone\":\"America/New_York\",\"locale\":\"en-US\"}},\{\"type\":\"default\",\"dimension\":\"vc\",\"outputName\":\"vc\",\"outputType\":\"LONG\"},\{\"type\":\"extraction\",\"dimension\":\"__time\",\"outputName\":\"extract_year\",\"extractionFn\":{\"type\":\"timeFormat\",\"format\":\"yyyy\",\"timeZone\":\"America/New_York\",\"locale\":\"en-US\"}}],\"virtualColumns\":[\{\"type\":\"expression\",\"name\":\"vc\",\"expression\":\"CAST(((CAST((timestamp_extract(\\\"__time\\\",'MONTH','America/New_York') - 1), 'DOUBLE') / CAST(3, 'DOUBLE')) + CAST(1, 'DOUBLE')), 'LONG')\",\"outputType\":\"LONG\"}],\"limitSpec\":\{\"type\":\"default\"},\"aggregations\":[\{\"type\":\"longSum\",\"name\":\"$f3\",\"expression\":\"1\"}],\"intervals\":[\"1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\"]}","druid.query.type":"groupBy"} || |+----------------------------------------------------+</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21939" opendate="2019-7-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protoc:2.5.0 dependence has broken building on aarch64</summary>
      <description>When I try to build master of Hive from source code on "aarch64" server, I met following error:&amp;#91;ERROR&amp;#93; Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.5.1.1:run (default) on project hive-standalone-metastore-common: Error resolving artifact: com.google.protobuf:protoc:2.5.0: Could not find artifact com.google.protobuf:protoc:exe:linux-aarch_64:2.5.0 in central (https://repo.maven.apache.org/maven2)that is because Hive using the "com.google.protobuf:protoc:2.5.0" as requiredartifact, whichdoes not have released package for "aarch64" platform.In order to fix this, I bumped the protobuf used in standalone-metadata to 2.6.1 and added a new profile, this profile will identifythe hardware architecture and if it is Aarch64, it will override the protobufgroup.idand package to com.github.os72 whichincludes ARM support. For X86 platform, Hive will still download the protobuf packages from org.google repo. With this method,we can make Hive able to run on Aarch64 and keep the influence to existing x86 users to the minimum.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21941" opendate="2019-7-2 00:00:00" fixdate="2019-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use checkstyle ruleset in Pre Upgrade Tool project</summary>
      <description>The project upgrade-acid/pre-upgrade does not uses the same checkstyle ruleset as hive root project</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21948" opendate="2019-7-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement parallel processing in Pre Upgrade Tool</summary>
      <description>Pre Upgrade Tool scans for all databases and tables in the warehouse sequentially which can be very slow in case of lots of tables.Example: It took the process 8-10 hours to complete on ~500k tables.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.src.main.java.org.apache.hadoop.hive.upgrade.acid.RunOptions.java</file>
      <file type="M">upgrade-acid.pre-upgrade.src.main.java.org.apache.hadoop.hive.upgrade.acid.PreUpgradeTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="21949" opendate="2019-7-3 00:00:00" fixdate="2019-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-21232 LLAP: Add a cache-miss friendly split affinity provider</summary>
      <description>HDFS skew issues become LLAP skew issues and issues of skew need to be fixed by running "hdfs balancer" instead of getting a uniform distribution via LLAP.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22059" opendate="2019-7-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar doesn&amp;#39;t contain (fasterxml) jackson library</summary>
      <description>While deploying master branch into a container I've noticed that the jackson libraries are not 100% sure that are available at runtime - this is probably due to the fact that we are still using the "old" codehaus jackson and also the "new" fasterxml one.]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1INFO : Completed executing command(queryId=vagrant_20190729141949_8d8c7f0d-0ac4-4d76-ba12-6ec01561b040); Time taken: 5.127 secondsINFO : Concurrency mode is disabled, not creating a lock managerError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1564408646590_0005_1_00, diagnostics=[Vertex vertex_1564408646590_0005_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: _dummy_table initializer failed, vertex=vertex_1564408646590_0005_1_00 [Map 1], java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapperat org.apache.hadoop.hive.ql.exec.Utilities.&lt;clinit&gt;(Utilities.java:226)at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:428)at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:508)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:488)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:337)at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:122)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.ObjectMapperat java.net.URLClassLoader.findClass(URLClassLoader.java:382)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 19 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 (state=08S01,code=2)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22084" opendate="2019-8-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement exchange partitions related methods on temporary tables</summary>
      <description>IMetaStoreClient exposes the following methods related to exchanging partitions:Partition exchange_partition(Map&lt;String, String&gt; partitionSpecs, String sourceDb, String sourceTable, String destdb, String destTableName);Partition exchange_partition(Map&lt;String, String&gt; partitionSpecs, String sourceCat, String sourceDb, String sourceTable, String destCat, String destdb, String destTableName);List&lt;Partition&gt; exchange_partitions(Map&lt;String, String&gt; partitionSpecs, String sourceDb, String sourceTable, String destdb, String destTableName);List&lt;Partition&gt; exchange_partitions(Map&lt;String, String&gt; partitionSpecs, String sourceCat, String sourceDb, String sourceTable, String destCat, String destdb, String destTableName);In order to support partitions on temporary tables, these methods must be implemented.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22089" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson to 2.9.9</summary>
      <description></description>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22090" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jetty to 9.3.27</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22115" opendate="2019-8-14 00:00:00" fixdate="2019-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent the creation of query-router logger in HS2 as per property</summary>
      <description>Avoid the creation and registration of query-router logger if the Hive server Property is set to false by the userHiveConf.ConfVars.HIVE_SERVER2_LOGGING_OPERATION_ENABLED</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug id="2213" opendate="2011-6-10 00:00:00" fixdate="2011-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize partial specification metastore functions</summary>
      <description>If a table has a large number of partitions, get_partition_names_ps() make take a long time to execute, because we get all of the partition names from the database. This is not very memory efficient, and the operation can be pushed down to the JDO layer without getting all of the names first.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22132" opendate="2019-8-21 00:00:00" fixdate="2019-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-lang3 version to 3.9</summary>
      <description>Upgrade commons lang 3 to 3.9</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22134" opendate="2019-8-21 00:00:00" fixdate="2019-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 3.1 driver includes org.glassfish.jersey.* which can interfer with an application</summary>
      <description>An application that uses JAX-RS 1.1 can be broken by the Hive 3.1 standalone JAR.For example, an application is running in IBM Websphere Liberty Profile (WLP) which detects the classes packaged in the Apache Hive standalone JAR. This results in WLP assuming that the application is providing it's implementation and should not use the default in WLP. Can the Apache Hive JDBC team confirm why these classes are in the JAR.Can the Apache Hive JDBC team schedule to remove them if they are not mandatory.Can the Apache Hive JDBC team confirm which individual JAR files can be copied instead of the uber-standalone JAR which would not include these conflicting classes.This is the class which triggers the problem if all of the jersey stuff is deleted the issue will go awayorg.glassfish.jersey.server.internal.RuntimeDelegateImplorg.glassfish.jersey.*</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22168" opendate="2019-9-4 00:00:00" fixdate="2019-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove excessive logging by llap cache.</summary>
      <description>Llap cache logging is very expensive when it comes to log every request buffers range.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
    </fixedFiles>
  </bug>
  <bug id="22170" opendate="2019-9-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="2218" opendate="2011-6-14 00:00:00" fixdate="2011-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>speedup addInputPaths</summary>
      <description>Speedup the addInputPaths for combined symlink inputformat, and added some other micro optimizations which also work for normal cases.This can help reducing the start time of one query from 5 hours to less than 20 mins.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22199" opendate="2019-9-12 00:00:00" fixdate="2019-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ugrade findbugs to 3.0.5</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">findbugs.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug id="222" opendate="2009-1-10 00:00:00" fixdate="2009-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by on a combination of disitinct and non distinct aggregates can return serialization errors with map side aggregations.</summary>
      <description>For queries of the form (groupby2_map.q in the source)SELECT x, count(DISTINCT y), SUM FROM t GROUP BY xwhen map side aggregation is on hive.map.aggr=true (This is off by default)The following exception can occur: &amp;#91;junit&amp;#93; Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.serialize(DynamicSerDeTypeDouble.java:60) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.serialize(DynamicSerDeFieldList.java:235) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.serialize(DynamicSerDeStructBase.java:81) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.serialize(DynamicSerDe.java:174)</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22209" opendate="2019-9-16 00:00:00" fixdate="2019-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Creating a materialized view with no tables should be handled more gracefully</summary>
      <description>Currently, materialized views without a table reference are not supported. However, instead of printing a clear message about it, when a materialized view is created without a table reference, we fail with an unclear message.&gt; create materialized view mv_test1 as select 5;(...)ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Add request failed :INSERT INTO MV_TABLES_USED (MV_CREATION_METADATA_ID,TBL_ID) VALUES (?,?) )INFO : Completed executing command(queryId=hive_20190916203511_b609cccf-f5e3-45dd-abfd-6e869d94e39a); Time taken: 10.469 secondsError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Add request failed : INSERT INTO MV_TABLES_USED (MV_CREATION_METADATA_ID,TBL_ID) VALUES (?,?) ) (state=08S01,code=1)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
    </fixedFiles>
  </bug>
  <bug id="22210" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization may reuse computation output columns involved in filtering</summary>
      <description>running the following test with TestMiniLlapLocalCliDriver leads to an unexpected results; the coalesce calculated inside the subquery has a value of 1 instead of the correct(922) value.drop table if exists u_table_4;create table u_table_4(smallint_col_22 smallint, int_col_5 int);insert into u_table_4 values(238,922);drop table u_table_7;create table u_table_7 ( bigint_col_3 bigint, int_col_10 int);insert into u_table_7 values (571,198);drop table u_table_19;create table u_table_19 (bigint_col_18 bigint ,int_col_19 int, STRING_COL_7 string);insert into u_table_19 values (922,5,'500');set hive.mapjoin.full.outer=true;set hive.auto.convert.join=true;set hive.query.results.cache.enabled=false;set hive.merge.nway.joins=true;set hive.vectorized.execution.enabled=true;--explain analyze SELECT a5.int_col, 922 as expected, COALESCE(a5.int_col, a5.aa) as expected2, a5.int_col_3 as realityFROM u_table_19 a1 FULL OUTER JOIN ( SELECT a2.int_col_5 AS int_col, a2.smallint_col_22 as aa, COALESCE(a2.int_col_5, a2.smallint_col_22) AS int_col_3 FROM u_table_4 a2 ) a5 ON ( a1.bigint_col_18) = (a5.int_col_3) INNER JOIN ( SELECT a3.bigint_col_3 AS int_col, Cast (COALESCE(a3.bigint_col_3, a3.bigint_col_3, a3.int_col_10) AS BIGINT) * Cast (a3.bigint_col_3 AS BIGINT) AS int_col_3 FROM u_table_7 a3 WHERE bigint_col_3=571 ) a4ON (a1.int_col_19=5) OR ((a5.int_col_3) IN (a4.int_col, 10)) where a1.STRING_COL_7='500'ORDER BY int_col DESC nulls last limit 100;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22211" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change maven phase to generate test sources</summary>
      <description>Some protobuf files are generated in the wrong phase; so I get compile errors because they are not there for eclipse...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22212" opendate="2019-9-17 00:00:00" fixdate="2019-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement append partition related methods on temporary tables</summary>
      <description>The following methods must be implemented in SessionHiveMetastoreClient, in order to support partition append on temporary tables: Partition appendPartition(String dbName, String tableName, List&lt;String&gt; partVals) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String catName, String dbName, String tableName, List&lt;String&gt; partVals) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String dbName, String tableName, String name) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String catName, String dbName, String tableName, String name) throws InvalidObjectException, AlreadyExistsException, MetaException, TException;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22213" opendate="2019-9-18 00:00:00" fixdate="2019-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TxnHander cleanupRecords should only clean records belonging to default catalog</summary>
      <description>Currently it removes record for given database and given table without checking for the catalog, as a result it can end up removing records when it shouldn't.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22214" opendate="2019-9-18 00:00:00" fixdate="2019-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain vectorization should disable user level explain</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22323" opendate="2019-10-10 00:00:00" fixdate="2019-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Desc Table bugs</summary>
      <description>DESC TABLE operation is having the following bugs: Whole table descs have two headers. Table column desc has incorrect long header, while the table is transposed having the headers in the first column. Json formatted data also has the headers. Json formatted data doesn't have the column statistics in it. There is no TestBeeLineDriver test for desc table, thus the actual output is not tested, just some intermediary.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.desc.table.formatted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.desc.table.formatted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partitioned.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tunable.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.analyze.decimal.compare.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.table.update.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.colstats.date.min.max.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.info.DescTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compustat.avro.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.10.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="22327" opendate="2019-10-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl: Ignore read-only transactions in notification log</summary>
      <description>Read txns need not be replicated.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.CommitTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AllocWriteIdEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AbortTxnEvent.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="22328" opendate="2019-10-11 00:00:00" fixdate="2019-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Min value for column in stats is not set correctly for some data types in partitioned tables</summary>
      <description>This is a follow up Jira for HIVE-22248. For partitioned tables the statistics aggregation happens at in the *ColumnStatsAggregator classes instead of the *ColumnStatsMerger classes, and they still fail to handle the unset low values correctly. Beside that they need to be fixed the two classes should use the same codes for merging statistics.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22357" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema mismatch between the Hive table definition and the "hive.sql.query" Parameter</summary>
      <description>The problem is that, for certain types of schema mismatch between the Hive table definition and the "hive.sql.query" parameter, Hive simply returns no rows rather than throwing an error when queried.Ideally Hive would check for schema compatibility during table definition and throw an error if there's a problem - the earlier the better. But even if that cannot be made a requirement, I'd definitely expect an error rather than a silent failure (i.e. zero rows returned) at runtime.I'm attaching investigation of this issue in "Hive-JDBC schema matching sensitivity.txt".</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.JdbcRecordIterator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22358" opendate="2019-10-16 00:00:00" fixdate="2019-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add schedule shorthands for convinience</summary>
      <description>Add shorthands for schedulesevery minuteevery 10 minutesevery 10 secondsevery 4 hoursevery 4 hours offset by '2:03:04'every day offset by '2:03:04'every day at '2:03:04'</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.schq.TestScheduledQueryStatements.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ScheduledQueryAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="2236" opendate="2011-6-23 00:00:00" fixdate="2011-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cli: Print Hadoop&amp;#39;s CPU milliseconds</summary>
      <description>CPU Milliseonds information is available from Hadoop's framework. Printing it out to Hive CLI when executing a job will help users to know more about their jobs.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2248" opendate="2011-6-30 00:00:00" fixdate="2011-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparison Operators convert number types to common type instead of double if possible</summary>
      <description>Now if the two sides of comparison is of different type, we always convert both to double and compare. It was a slight regression from the change in https://issues.apache.org/jira/browse/HIVE-1638. The old UDFOP&lt;Comparison&gt;, using GenericUDFBridge, always tried to find common type first.The worse case is this: If you did "WHERE &lt;BIGINT_COLUMN&gt; = 0 ", we always convert the column and 0 to double and compare, which is wasteful, though it is usually a minor costs in the system. But it is easy to fix.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
    </fixedFiles>
  </bug>
  <bug id="22502" opendate="2019-11-15 00:00:00" fixdate="2019-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConcurrentModificationException in TriggerValidatorRunnable stops trigger processing</summary>
      <description>An other thread is modifying the list the contains the sessions whileTriggerValidatorRunnable is traversing it. This causes theTriggerValidatorRunnable thread to die and triggers are no longer properly monitored.&lt;12&gt;1 2019-11-14T00:31:12.187Z hiveserver2-0.hiveserver2-service.compute-1572769905-6965.svc.cluster.local hiveserver2 1 fa2f30b6-ffb3-11e9-93ba-0a257c2413a2 [mdc@18060 class="tez.TriggerValidatorRunnable" level="WARN" thread="TriggerValidator"] TriggerValidatorRunnable caught exception.&lt;12&gt;1 2019-11-14T00:31:12.187Z hiveserver2-0.hiveserver2-service.compute-1572769905-6965.svc.cluster.local hiveserver2 1 fa2f30b6-ffb3-11e9-93ba-0a257c2413a2 [mdc@18060 class="tez.TriggerValidatorRunnable" level="WARN" thread="TriggerValidator"] TriggerValidatorRunnable caught exception.java.util.ConcurrentModificationException at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966) at java.util.LinkedList$ListItr.next(LinkedList.java:888) at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044) at org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.run(TriggerValidatorRunnable.java:49) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)     </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22506" opendate="2019-11-18 00:00:00" fixdate="2019-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Read-only transactions feature flag</summary>
      <description>Introduce a feature flag, so that read-only transaction functionality could be conditionally turned on/off.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22635" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable scheduled query executor for unittests</summary>
      <description>HIVE-21884 missed to set the default to off; so it may sometime interfere with unit tests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22770" opendate="2020-1-24 00:00:00" fixdate="2020-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip interning of MapWork fields during deserialization</summary>
      <description>HIVE-19937 introduced some interning logic into mapwork deserialization process, but it's only related to spark, maybe we should skip this for tez, reducing the cpu pressure in tez tasks.UPDATE: Hive on spark is not supported anymore, the MapWorkSerializer can be completely removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="22803" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark scheduled queries executions to help end-user identify it easier</summary>
      <description>scheduled queries are executed as-is;it might probably help a lot in field deployments to have a hint where that query is coming from; I'm thinking of prefixing the query with some comment:/*schedule:sc1*/ select 1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22804" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure ANSI quotes are used for mysql connections</summary>
      <description>Recent changes in direct sql queries to resolve postgres issues(e.g. TxnHandler inHIVE-22663) break compatibility with mysql backend db.A workaround for these issues is to add a session config to the mysql connection string, e.g.:jdbc:mysql://localhost:3306/db?sessionVariables=sql_mode=ANSI_QUOTES</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.HikariCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.DbCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.BoneCPDataSourceProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="22995" opendate="2020-3-6 00:00:00" fixdate="2020-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for location for managed tables on database</summary>
      <description>I have attached the initial spec to this jira.Default location for database would be the external table base directory. Managed location can be optionally specified.CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path][MANAGEDLOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];ALTER (DATABASE|SCHEMA) database_name SET MANAGEDLOCATION hdfs_path;</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestCatalogOldClient.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.BaseReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestWarehouseExternalDir.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.alter.location.AlterDatabaseSetLocationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.alter.location.AlterDatabaseSetLocationOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.create.CreateDatabaseOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.desc.DescDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.desc.DescDatabaseOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.change.db.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.db.owner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.owner.actions.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.db.ddl.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Database.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.client.builder.DatabaseBuilder.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MDatabase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="22997" opendate="2020-3-7 00:00:00" fixdate="2020-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Copy external table to target during Repl Dump operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadCompleteAckWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadCompleteAckTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ExternalTableCopyTaskBuilder.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTablesMetaDataOnly.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOnHDFSEncryptedZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.ReplicationTestUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22998" opendate="2020-3-8 00:00:00" fixdate="2020-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dump partition info if hive.repl.dump.metadata.only.for.external.table conf is enabled</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTablesMetaDataOnly.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="23004" opendate="2020-3-10 00:00:00" fixdate="2020-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Decimal64 operations across multiple vertices</summary>
      <description>Support Decimal64 operations across multiple vertices</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.trailing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal64.case.when.nvl.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal64.case.when.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.vectorized.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.keep.uniform.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23005" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consider Default JDBC Fetch Size From HS2</summary>
      <description>Create two fetch sizes: an "init" fetch size and a "default" fetch size. The "init" fetch size comes from the JDBC connection stringfetchSize(if present) and the "default" fetch size comes from HIVE_SERVER2_THRIFT_RESULTSET_DEFAULT_FETCH_SIZE the server response to the open session request. When a Statement is created, its starting fetch size is the "init" fetch size (may be 0) Manually setting the fetch size on the Statement to 0, sets the fetch size to be the server defaultSetting to zero defaults to the server's instructed default and adheres to:If the value specified is zero, then the hint is ignored. The default value is zero.That is to say, if the fetch size is 0, the default from the server is used, otherwise the user can pass a 'hint' and that will be the number of rows fetched instead.https://docs.oracle.com/javase/8/docs/api/java/sql/Statement.html#setFetchSize-int-</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestHiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="23007" opendate="2020-3-10 00:00:00" fixdate="2020-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do Not Consider Client Session For Default Fetch Size</summary>
      <description>Right now, when a client makes a new session, it's value override's the server's value for hive.server2.thrift.resultset.default.fetch.size. This is a server-side configuration and the client should not dictate what the server's default (preferred) size is. Remove superfluous null check Use a Collection's Singleton Map instead of creating a full-sized HashMap Code simplification</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="23011" opendate="2020-3-11 00:00:00" fixdate="2020-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared work optimizer should check residual predicates when comparing joins</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23035" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="23046" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate housekeeping thread from initiator flag</summary>
      <description>If hive.compactor.initiator.on=false, the housekeeping thread is not started, therefore certain resources are not cleaned up. HK thread should be configurable from another parameter.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.MetastoreHousekeepingLeaderTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="23048" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use sequences for TXN_ID generation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.Oracle.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.DatabaseRule.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.DbInstallBase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SQLGenerator.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands3.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.ITestDbTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="23058" opendate="2020-3-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction task reattempt fails with FileAlreadyExistsException</summary>
      <description>Issue occurs when compaction attempt is relaunched after first task attempt failure due to preemption by Scheduler or any other reason.Since _tmp directory was created by first attempt and was left uncleaned after task attempt failure. Second attempt of the the task fails with "FileAlreadyExistsException" exception.Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /warehouse/tablespace/managed/hive/default.db/compaction_test/_tmp_3670bbef-ba7a-4c10-918d-9a2ee17cbd22/base_0000186/bucket_00005 for client 10.xx.xx.xxx already exists</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23059" opendate="2020-3-20 00:00:00" fixdate="2020-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In constraint name uniqueness query use the MTable instead of it&amp;#39;s id</summary>
      <description>The constraint name uniqueness query uses the MTable instead of it's id. This will solve the immediate problem of not being able to create the sys db in Oracle, but not solve the same problem about the rest of the queries with integer number parameters (Long, Integer, etc).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="23060" opendate="2020-3-20 00:00:00" fixdate="2020-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query failing with error "Grouping sets expression is not in GROUP BY key. Error encountered near token"</summary>
      <description>Synopsis:=========Query failing with error "Grouping sets expression is not in GROUP BY key. Error encountered near token"Problem:========A Hive query in a view which fails with the following error:Error while compiling statement: FAILED: SemanticException 35:21 &amp;#91;Error 10213&amp;#93;: Grouping sets expression is not in GROUP BY key. Error encountered near token 'l0_equities_region_id'Reproduction case:create database test; create table test.case665558 (c1 string, c2 string);-- Working query select case when GROUPING__ID = 255 then `c1` end as `col_1`, case when GROUPING__ID = 255 then 3 end as `col_2`, `c1`, `c2`from `test`.`case665558`group by `c1`, `c2`GROUPING SETS ( (`c1`), (`c1`, `c2`) ); create view test.viewcase665558 asselect case when GROUPING__ID = 255 then `c1` end as `col_1`, case when GROUPING__ID = 255 then 3 end as `col_2`, `c1`, `c2`from `test`.`case665558`group by `c1`, `c2`GROUPING SETS ( (`c1`), (`c1`, `c2`) ); Select * from test.viewcase665558 ;Error: Error while compiling statement: FAILED: SemanticException 17:1 [Error 10213]: Grouping sets expression is not in GROUP BY key. Error encountered near token 'c1' (state=42000,code=40000)The issue is because when the view is created, it adds the name of the table to the columns. This seems to be confusing Hive:+-------------------------------------------------+--+| createtab_stmt |+-------------------------------------------------+--+| CREATE VIEW `test.viewcase665558` AS select || case || when GROUPING__ID = 255 then `case665558`.`c1` || end as `col_1`, || case || when GROUPING__ID = 255 then 3 || end as `col_2`, || `case665558`.`c1`, || `case665558`.`c2` || from || `test`.`case665558` || group by || `case665558`.`c1`, || `case665558`.`c2` || GROUPING SETS || ( || (c1), || (c1, c2) || ) |+-------------------------------------------------+--+</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23062" opendate="2020-3-20 00:00:00" fixdate="2020-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive to check Yarn RM URL in TLS and Yarn HA mode for custom Tez queue</summary>
      <description>Currently if custom Tez queue is used, Hive will only check the Http port, so it is not handling TLS and Yarn HA mode URL.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.YarnQueueHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="23064" opendate="2020-3-21 00:00:00" fixdate="2020-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calls to printStackTrace in Module hive-exec</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryViewer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="23065" opendate="2020-3-21 00:00:00" fixdate="2020-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calls to printStackTrace in Module hive-service</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="23178" opendate="2020-4-10 00:00:00" fixdate="2020-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tez Total Order Partitioner</summary>
      <description></description>
      <version>3.1.0,3.1.1,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTotalOrderPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23181" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23184" opendate="2020-4-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade druid to 0.17.1</summary>
      <description>Upgrade to druid latest release 0.17.1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.semijoin.reduction.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.delimited.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidKafkaUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">data.scripts.kafka.init.data.csv</file>
    </fixedFiles>
  </bug>
  <bug id="2319" opendate="2011-7-28 00:00:00" fixdate="2011-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling alter_table after changing partition comment throws an exception</summary>
      <description>Altering a table's partition key comments raises an InvalidOperationException. The partition key name and type should not be mutable, but the comment should be able to get changed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23230" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"get_splits" UDF ignores limit clause while creating splits.</summary>
      <description>Issue: Running the query select * from &lt;table&gt; limit n from spark via hive warehouse connector may return more rows than "n".This happens because "get_splits" udf creates splits ignoring the limit constraint. These splits when submitted to multiple llap daemons will return "n" rows each.How to reproduce: Needs spark-shell, hive-warehouse-connector and hive on llap with more that 1 llap daemons running.run below commands via beeline to create and populate the tablecreate table test (id int);insert into table test values (1);insert into table test values (2);insert into table test values (3);insert into table test values (4);insert into table test values (5);insert into table test values (6);insert into table test values (7);delete from test where id = 7;now running below query via spark-shellimport com.hortonworks.hwc.HiveWarehouseSession val hive = HiveWarehouseSession.session(spark).build() hive.executeQuery("select * from test limit 1").show()will return more than 1 rows.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestNewGetSplitsFormatReturnPath.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestNewGetSplitsFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapVectorArrowBatch.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapVectorArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapRow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.AbstractTestJdbcGenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug id="23235" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing in repl dump failing for orc format</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="23239" opendate="2020-4-17 00:00:00" fixdate="2020-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution via transitive dependency</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23241" opendate="2020-4-19 00:00:00" fixdate="2020-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce transitive dependencies</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23243" opendate="2020-4-19 00:00:00" fixdate="2020-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accept SQL type like pattern for Show Databases</summary>
      <description>Show Databases pattern accepts java like pattern with * and ., use SQL like instead with % and _.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.describe.database.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.database.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.describe.database.json.q</file>
      <file type="M">ql.src.test.queries.clientpositive.database.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.show.ShowDatabasesOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="23247" opendate="2020-4-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase timeout for some tez tests</summary>
      <description>we have a few tez tests which run for ~3.4 seconds with a 5sec timeoutIn case these tests run in a more saturated environment they became unrelyable...and fail regularily</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestCustomPartitionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="23267" opendate="2020-4-21 00:00:00" fixdate="2020-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on groovy</summary>
      <description>Transitively pulled where its unneeded.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23268" opendate="2020-4-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate beanutils transitive dependency</summary>
      <description>Transitively retrieved from hadoop-commons</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23272" opendate="2020-4-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix and reenable timestamptz_2.q</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestTimestampTZWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="23275" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Represent UNBOUNDED in window functions in CBO correctly</summary>
      <description>Currently we use a bounded representation with bound set to Integer.MAX_VALUE, which works correctly since that is the Hive implementation. However, Calcite has a specific boundary class RexWindowBoundUnbounded that we should be using instead.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="23278" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on bouncycastle</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23347" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MSCK REPAIR cannot discover partitions with upper case directory names.</summary>
      <description>For the following scenario, we expect MSCK REPAIR to discover partitions but it couldn't.1. Have partitioned data path as follows.hdfs://mycluster/datapath/t1/Year=2020/Month=03/Day=10hdfs://mycluster/datapath/t1/Year=2020/Month=03/Day=112. create external table t1 (key int, value string) partitioned by (Year int, Month int, Day int) stored as orc location hdfs://mycluster/datapath/t1'';3. msck repair table t1;4. show partitions t1; --&gt; Returns zero partitions5. select * from t1; --&gt; Returns empty data.When the partition directory names are changed to lower case, this works fine.hdfs://mycluster/datapath/t1/year=2020/month=03/day=10hdfs://mycluster/datapath/t1/year=2020/month=03/day=11</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.Msck.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreChecker.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.CheckResult.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckCreatePartitionsInBatches.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2335" opendate="2011-8-2 00:00:00" fixdate="2011-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexes are still automatically queried when out of sync with their source tables</summary>
      <description>The automatic index usage does not check whether or not the indexes are still up-to-date when generating the index queries. This can be addressed in two stages, the first is to add a check before generating the index query to ensure that the index is still valid. The next stage may be to add some sort of mode where indexes are automatically updated on table writes.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="23350" opendate="2020-5-1 00:00:00" fixdate="2020-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade DBCP To DBCP 2.7.0</summary>
      <description>In hive-standalone-metastore-common the current version is 1.4 which is very antiquated and apparently (emphasis mine):DBCP 1.4 compiles and runs under Java 6 only (JDBC 4)Hive is on Java 8&lt;commons-dbcp2.version&gt;2.6.0&lt;/commons-dbcp2.version&gt;&lt;commons-dbcp.version&gt;1.4&lt;/commons-dbcp.version&gt;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.datasource.TestDataSourceProviderFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.DbCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23351" opendate="2020-5-1 00:00:00" fixdate="2020-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Replication Scheduling</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23414" opendate="2020-5-8 00:00:00" fixdate="2020-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detail Hive Java Compatibility</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
    </fixedFiles>
  </bug>
  <bug id="23435" opendate="2020-5-11 00:00:00" fixdate="2020-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Full outer join result is missing rows</summary>
      <description>Full Outer join result has missing rows. Appears to be a bug with the full outer join logic. Expected output is receiving when we do a left and right outer join.Reproducible steps are mentioned below.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~SUPPORT ANALYSISSteps to Reproduce:1. Create a table and insert data:create table x (z char(5), x int, y int);insert into x values ('one', 1, 50), ('two', 2, 30), ('three', 3, 30), ('four', 4, 60), ('five', 5, 70), ('six', 6, 80);2. Try full outer with the below command. The result is incomplete, it is missing the row:NULL NULL NULL three 3 30.0 Full Outer Join:select x1.`z`, x1.`x`, x1.`y`, x2.`z`, x2.`x`, x2.`y` from `x` x1 full outer join `x` x2 on (x1.`x` &gt; 3) and (x2.`x` &lt; 4) and (x1.`x` = x2.`x`);Result:----------------------------------+x1.z x1.x x1.y x2.z x2.x x2.y ----------------------------------+one 1 50 NULL NULL NULL NULL NULL NULL one 1 50 two 2 30 NULL NULL NULL NULL NULL NULL two 2 30 three 3 30 NULL NULL NULL four 4 60 NULL NULL NULL NULL NULL NULL four 4 60 five 5 70 NULL NULL NULL NULL NULL NULL five 5 70 six 6 80 NULL NULL NULL NULL NULL NULL six 6 80 ----------------------------------+3. Expected output is coming when we use left/right join + union:select x1.`z`, x1.`x`, x1.`y`, x2.`z`, x2.`x`, x2.`y` from `x` x1 left outer join `x` x2 on (x1.`x` &gt; 3) and (x2.`x` &lt; 4) and (x1.`x` = x2.`x`) union select x1.`z`, x1.`x`, x1.`y`, x2.`z`, x2.`x`, x2.`y` from `x` x1 right outer join `x` x2 on (x1.`x` &gt; 3) and (x2.`x` &lt; 4) and (x1.`x` = x2.`x`);Result:------------------------------------+z x y _col3 _col4 _col5 ------------------------------------+NULL NULL NULL five 5 70 NULL NULL NULL four 4 60 NULL NULL NULL one 1 50 four 4 60 NULL NULL NULL one 1 50 NULL NULL NULL six 6 80 NULL NULL NULL three 3 30 NULL NULL NULL two 2 30 NULL NULL NULL NULL NULL NULL six 6 80 NULL NULL NULL three 3 30 NULL NULL NULL two 2 30 five 5 70 NULL NULL NULL ------------------------------------+</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.join.1to1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23436" opendate="2020-5-11 00:00:00" fixdate="2020-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Staging directory is not removed for stats gathering tasks</summary>
      <description>When running a query which generates stats, then the staging directory is not removed when the query is finished</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsAutoGatherContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="23555" opendate="2020-5-27 00:00:00" fixdate="2020-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cancel compaction jobs when hive.compactor.worker.timeout is reached</summary>
      <description>Currently when a compactor worker thread is stuck, or working too long on a compaction the the initiator might decide to start a new compaction because of a timeout, but old worker might still wait for the results of the job.It would be good to cancel the worker as well after the timeout is reached</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreThread.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.RemoteCompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MetaStoreCompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorTestUtil.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
    </fixedFiles>
  </bug>
  <bug id="23668" opendate="2020-6-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Task for Hive Metrics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestReplicationMetrics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MReplicationMetrics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.metric.MetricSink.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="2411" opendate="2011-8-26 00:00:00" fixdate="2011-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore server tries to connect to NN without authenticating itself</summary>
      <description>When metastore server is launching it first calls new HMSHandler() even before it has done a login. That results in createDefaultDB() gets called which then results in getFileSystem() calls which tries to create a connection to NN and then it fails since NN cannot authenticate metastore.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2602" opendate="2011-11-22 00:00:00" fixdate="2011-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add support for insert partition overwrite(...) if not exists</summary>
      <description>INSERT OVERWRITE TABLE X PARTITION (a=b, c=d) IF NOT EXISTS ...The partition should be created and written if and only if it's not there already.The support can be added for dynamic partitions in the future, but this jira is for adding this support for static partitions.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="2634" opendate="2011-12-8 00:00:00" fixdate="2011-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>revert HIVE-2566</summary>
      <description>This is leading to some problems.I will upload the offending testcase in a new jira.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2650" opendate="2011-12-12 00:00:00" fixdate="2011-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel test commands that include cd fail</summary>
      <description>Parallel test commands that include cd fail.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.Ssh.py</file>
    </fixedFiles>
  </bug>
</bugrepository>
