<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10104" opendate="2015-3-26 00:00:00" fixdate="2015-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Generate consistent splits and locations for the same split across jobs</summary>
      <description>Locations for splits are currently randomized. Also, the order of splits is random - depending on how threads end up generating the splits.Add an option to sort the splits, and generate repeatable locations - assuming all other factors are the same.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10129" opendate="2015-3-28 00:00:00" fixdate="2015-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fix ordering of execution modes</summary>
      <description>uber &gt; llap &gt; container execution modes. Fix the ordering in in-place update UI.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12734" opendate="2015-12-22 00:00:00" fixdate="2015-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundancy in HiveConfs serialized to UDFContext</summary>
      <description>HCatLoader lands up serializing one HiveConf instance per table-alias, to Pig's UDFContext. This lands up bloating the UDFContext.To reduce the footprint, it makes sense to serialize a default-constructed HiveConf once, and one "diff" per HCatLoader. This should reduce the time taken to kick off jobs from pig -useHCatalog scripts.(Note_to_self: YHIVE-540).</description>
      <version>1.2.1,2.0.0,2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13316" opendate="2016-3-19 00:00:00" fixdate="2016-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Calcite 1.10</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.HiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveDefaultCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveRelMdCost.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidIntervalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidQueryType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidRules.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.HiveDruidConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveDateGranularity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTSTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdCollation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdUniqueKeys.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBOMaxNumToCNF.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
    </fixedFiles>
  </bug>
  <bug id="13666" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Provide the log url for a task attempt to display on the UI</summary>
      <description>The log url needs to be provided for task attempts, to display on the Tez UI associated with a query.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13667" opendate="2016-5-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve performance for ServiceInstanceSet.getByHost</summary>
      <description>ServiceInstanceSet.getByHost is used for scheduling local tasks as well as constructing the log URL.It ends up traversing all hosts on each lookup. This should be avoided.cc prasanth_j</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13669" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: io.enabled config is ignored on the server side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="1367" opendate="2010-5-25 00:00:00" fixdate="2010-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cluster by multiple columns does not work if parenthesis is present</summary>
      <description>The following query:select ... from src cluster by (key, value)throws a compile error:whereas the queryselect ... from src cluster by key, valueworks fine</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
    </fixedFiles>
  </bug>
  <bug id="13744" opendate="2016-5-12 00:00:00" fixdate="2016-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO - add complex types support</summary>
      <description>Recently, complex type column vectors were added to Hive. We should use them in IO elevator.Vectorization itself doesn't support complex types (yet), but this would be useful when it does, also it will enable LLAP IO elevator to be used in non-vectorized context with complex types after HIVE-13617</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13857" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert overwrite select from some table fails throwing org.apache.hadoop.security.AccessControlException - II</summary>
      <description>HIVE-13810 missed a fix, tracking it here.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13901" opendate="2016-6-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hivemetastore add partitions can be slow depending on filesystems</summary>
      <description>Depending on FS, creating external tables &amp; adding partitions can be expensive (e.g msck which adds all partitions).</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.external2.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13927" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding missing header to Java files</summary>
      <description></description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.ThriftCliServiceMessageSizeTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="13954" opendate="2016-6-6 00:00:00" fixdate="2016-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet logs should go to STDERR</summary>
      <description>Parquet uses java util logging. When java logging is not configured using default logging.properties file, parquet's default fallback handler writes to STDOUT at INFO level. Hive writes all logging to STDERR and writes only the query output to STDOUT. Writing logs to STDOUT may cause issues when comparing query results. If we provide default logging.properties for parquet then we can configure it to write to file or stderr.</description>
      <version>2.2.0</version>
      <fixedVersion>1.3.0,2.1.0,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="13961" opendate="2016-6-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Major compaction fails to include the original bucket files if there&amp;#39;s no delta directory</summary>
      <description>The issue can be reproduced by steps below:1. Insert a row to Non-ACID table2. Convert Non-ACID to ACID table (i.e. set transactional=true table property)3. Perform Major compaction</description>
      <version>1.3.0,2.1.0,2.2.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
    </fixedFiles>
  </bug>
  <bug id="13982" opendate="2016-6-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extensions to RS dedup: execute with different column order and sorting direction if possible</summary>
      <description>Pointed out by gopalv.RS dedup should kick in for these cases, avoiding an additional shuffle stage.select state, city, sum(sales) from tablegroup by state, cityorder by state, citylimit 10;select state, city, sum(sales) from tablegroup by city, stateorder by state, citylimit 10;select state, city, sum(sales) from tablegroup by city, stateorder by state desc, citylimit 10;</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.queries.clientpositive.correlationoptimizer13.q</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptfgroupbyjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join3.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13985" opendate="2016-6-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC improvements for reducing the file system calls in task side</summary>
      <description>HIVE-13840 fixed some issues with addition file system invocations during split generation. Similarly, this jira will fix issues with additional file system invocations on the task side. To avoid reading footers on the task side, users can set hive.orc.splits.include.file.footer to true which will serialize the orc footers on the splits. But this has issues with serializing unwanted information like column statistics and other metadata which are not really required for reading orc split on the task side. We can reduce the payload on the orc splits by serializing only the minimum required information (stripe information, types, compression details). This will decrease the payload on the orc splits and can potentially avoid OOMs in application master (AM) during split generation. This jira also address other issues concerning the AM cache. The local cache used by AM is soft reference cache. This can introduce unpredictability across multiple runs of the same query. We can cache the serialized footer in the local cache and also use strong reference cache which should avoid memory pressure and will have better predictability.One other improvement that we can do is when hive.orc.splits.include.file.footer is set to false, on the task side we make one additional file system call to know the size of the file. If we can serialize the file length in the orc split this can be avoided.</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileFormatProxy.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.LocalCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ExternalCache.java</file>
      <file type="M">orc.src.protobuf.orc.proto.proto</file>
      <file type="M">orc.src.java.org.apache.orc.Reader.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ReaderImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.FileMetaInfo.java</file>
      <file type="M">orc.src.gen.protobuf-java.org.apache.orc.OrcProto.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13986" opendate="2016-6-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: kill Tez AM on token errors from plugin</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14007" opendate="2016-6-13 00:00:00" fixdate="2016-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace ORC module with ORC release</summary>
      <description>This completes moving the core ORC reader &amp; writer to the ORC project.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">orc.src.java.org.apache.orc.impl.OrcTail.java</file>
      <file type="M">testutils.ptest2.src.test.resources.test-configuration2.properties</file>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.remove.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.schema.evolution.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.remove.cols.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">orc.src.test.resources.orc-file-has-null.out</file>
      <file type="M">orc.src.test.resources.orc-file-dump.out</file>
      <file type="M">orc.src.test.resources.orc-file-dump.json</file>
      <file type="M">orc.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">orc.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">orc.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">orc.src.test.resources.orc-file-11-format.orc</file>
      <file type="M">orc.src.test.org.apache.orc.tools.TestJsonFileDump.java</file>
      <file type="M">orc.src.test.org.apache.orc.tools.TestFileDump.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestVectorOrcFile.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestUnrolledBitPack.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestTypeDescription.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestStringDictionary.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestOrcTimezone3.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestOrcTimezone2.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestOrcTimezone1.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestOrcNullOptimization.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestNewIntegerEncoding.java</file>
      <file type="M">orc.src.test.org.apache.orc.TestColumnStatistics.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestZlib.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestStringRedBlackTree.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestStreamName.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestSerializationUtils.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestSchemaEvolution.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestRunLengthIntegerReader.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestRunLengthByteReader.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestRLEv2.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestRecordReaderImpl.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestReaderImpl.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestOutStream.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestOrcWideTable.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestMemoryManager.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestIntegerCompressionReader.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestInStream.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestDynamicArray.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestDataReaderProperties.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestColumnStatisticsImpl.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestBitPack.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestBitFieldReader.java</file>
      <file type="M">orc.src.protobuf.orc.proto.proto</file>
      <file type="M">orc.src.java.org.apache.orc.Writer.java</file>
      <file type="M">orc.src.java.org.apache.orc.TypeDescription.java</file>
      <file type="M">orc.src.java.org.apache.orc.tools.JsonFileDump.java</file>
      <file type="M">orc.src.java.org.apache.orc.tools.FileDump.java</file>
      <file type="M">orc.src.java.org.apache.orc.TimestampColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.StripeStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.StripeInformation.java</file>
      <file type="M">orc.src.java.org.apache.orc.StringColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.RecordReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.Reader.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcConf.java</file>
      <file type="M">orc.src.java.org.apache.orc.IntegerColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ZlibCodec.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ZeroCopyShims.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.TreeReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.StringRedBlackTree.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.StreamName.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.SnappyCodec.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.SettableUncompressedStream.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.SerializationUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.SchemaEvolution.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerWriterV2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerWriter.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerReaderV2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthByteWriter.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthByteReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RedBlackTree.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RecordReaderUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RecordReaderImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ReaderImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.PositionRecorder.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.PositionProvider.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.PositionedOutputStream.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.PhysicalWriter.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.PhysicalFsWriter.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.OutStream.java</file>
      <file type="M">common.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.GenericColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.VertorDeserializeOrcWriter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">orc.pom.xml</file>
      <file type="M">orc.src.gen.protobuf-java.org.apache.orc.OrcProto.java</file>
      <file type="M">orc.src.java.org.apache.orc.BinaryColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.BloomFilterIO.java</file>
      <file type="M">orc.src.java.org.apache.orc.BooleanColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.ColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.CompressionCodec.java</file>
      <file type="M">orc.src.java.org.apache.orc.CompressionKind.java</file>
      <file type="M">orc.src.java.org.apache.orc.DataReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.DateColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.DecimalColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.DoubleColumnStatistics.java</file>
      <file type="M">orc.src.java.org.apache.orc.FileFormatException.java</file>
      <file type="M">orc.src.java.org.apache.orc.FileMetadata.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.AcidStats.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.BitFieldReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.BitFieldWriter.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.BufferChunk.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ColumnStatisticsImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DataReaderProperties.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DirectDecompressionCodec.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DynamicByteArray.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DynamicIntArray.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShims.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShimsCurrent.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShims.2.2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.InStream.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.IntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.IntegerWriter.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.MemoryManager.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.OrcAcidUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.OrcIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="14012" opendate="2016-6-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some ColumnVector-s are missing ensureSize</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="14013" opendate="2016-6-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table doesn&amp;#39;t show unicode properly</summary>
      <description>Describe table output will show comments incorrectly rather than the unicode itself.hive&gt; desc formatted t1;# Detailed Table Information Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\"} comment \u8868\u4E2D\u6587\u6D4B\u8BD5 numFiles 0</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14018" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make IN clause row selectivity estimation customizable</summary>
      <description>After HIVE-13287 went in, we calculate IN clause estimates natively (instead of just dividing incoming number of rows by 2). However, as the distribution of values of the columns is considered uniform, we might end up heavily underestimating/overestimating the resulting number of rows.This issue is to add a factor that multiplies the IN clause estimation so we can alleviate this problem. The solution is not very elegant, but it is the best we can do until we have histograms to improve our estimate.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14021" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When converting to CNF, fail if the expression exceeds a threshold</summary>
      <description>When converting to conjunctive normal form (CNF), fail if the expression exceeds a threshold. CNF can explode exponentially in the size of the input expression, but rarely does so in practice. Add a maxNodeCount parameter to RexUtil.toCnf and throw or return null if it is exceeded.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14023" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make the Hive query id available in ContainerRunner</summary>
      <description>Needed to generate logs per query.We can use the dag identifier for now, but that isn't very useful. (The queryId may not be too useful either if users cannot find it - but that's better than a dagIdentifier)The queryId is available right now after the Processor starts, which is too late for log changes.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="14024" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>setAllColumns is called incorrectly after some changes</summary>
      <description>h/t gopalv</description>
      <version>None</version>
      <fixedVersion>2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14027" opendate="2016-6-15 00:00:00" fixdate="2016-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL values produced by left outer join do not behave as NULL</summary>
      <description>Consider the following setup:create table tbl (n bigint, t string); insert into tbl values (1, 'one'); insert into tbl values(2, 'two');select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a left outer join (select * from tbl where 1 = 2) b on a.n = b.n;1 one false trueThe query should return true for isnull(b.n).I've tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case.</description>
      <version>1.2.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14055" opendate="2016-6-18 00:00:00" fixdate="2016-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>directSql - getting the number of partitions is broken</summary>
      <description>Noticed while looking at something else. If the filter cannot be pushed down it just returns 0</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14056" opendate="2016-6-18 00:00:00" fixdate="2016-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden file updates for few tests</summary>
      <description>Click to add description</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.globallimit.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="14057" opendate="2016-6-19 00:00:00" fixdate="2016-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option in llapstatus to generate output to a file</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14060" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: Remove bogus "localhost" from Hive splits</summary>
      <description>On remote filesystems like Azure, GCP and S3, the splits contain a filler location of "localhost".This is worse than having no location information at all - on large clusters yarn waits upto 200&amp;#91;1&amp;#93; seconds for heartbeat from "localhost" before allocating a container.To speed up this process, the split affinity provider should scrub the bogus "localhost" from the locations and allow for the allocation of "*" containers instead on each heartbeat.&amp;#91;1&amp;#93; - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14069" opendate="2016-6-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update curator version to 2.12.0</summary>
      <description>curator-2.10.0 has several bug fixes over current version (2.6.0), updating would help improve stability.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14078" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP input split should get task attempt number from conf if available</summary>
      <description>Currently the attempt number is hard-coded to 0. If the split is being fetched as part of a hadoop job we can get the task attempt ID from the conf if it has been set, and use the attempt number from that.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14079" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove file, method and line number from pattern layout</summary>
      <description>Using %F%M and %L in pattern layouts need location information which is expensive to get and is disabled by default. We should remove them from the default layouts. This will avoid creating empty brackets like belowlockmgr.DbTxnManager (:())</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.hive-exec-log4j2.properties</file>
      <file type="M">llap-server.src.test.resources.log4j2.properties</file>
      <file type="M">llap-server.src.test.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.main.resources.llap-cli-log4j2.properties</file>
      <file type="M">hcatalog.src.test.e2e.templeton.deployers.config.hive.hive-log4j2.properties</file>
      <file type="M">data.conf.spark.log4j2.properties</file>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">common.src.test.resources.hive-log4j2-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j2-test.properties</file>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14080" opendate="2016-6-23 00:00:00" fixdate="2016-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.metastore.schema.verification should check for schema compatiblity</summary>
      <description>The check done when hive.metastore.schema.verification=true should be based on compatibility of schema instead of exact version equiality.See similar change done in schematool - HIVE-12261</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
    </fixedFiles>
  </bug>
  <bug id="14083" opendate="2016-6-23 00:00:00" fixdate="2016-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER INDEX in Tez causes NullPointerException</summary>
      <description>ALTER INDEX causes a NullPointerException when run under TEZ execution engine. Query runs without issue when submitted using MR execution mode.To reproduce:1. CREATE INDEX sample_08_index ON TABLE sample_08 (code) AS 'COMPACT' WITH DEFERRED REBUILD; 2. ALTER INDEX sample_08_index ON sample_08 REBUILD; Stacktrace from Hive 1.2.1ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1460577396252_0005_1_00, diagnostics=[Task failed, taskId=task_1460577396252_0005_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.NullPointerException at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:196) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.&lt;init&gt;(TezGroupedSplitsInputFormat.java:135) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:101) at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:149) at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:80) at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:650) at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:621) at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:145) at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:109) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:390) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:128) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147) ... 14 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:269) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:233) at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:193) ... 25 more</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14093" opendate="2016-6-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP output format connection should wait for all writes to finish before closing channel</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.ChannelOutputStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14118" opendate="2016-6-28 00:00:00" fixdate="2016-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the alter partition exception more meaningful</summary>
      <description>Right now when the alter partitions fails, "alter is not possible" is shown in the log while the real exception/failure is hidden.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="14119" opendate="2016-6-28 00:00:00" fixdate="2016-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external recordreader not returning non-ascii string properly</summary>
      <description>Strings with non-ascii chars showing up with "\�\�\� "</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug id="14123" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add beeline configuration option to show database in the prompt</summary>
      <description>There are several jira issues complaining that, the Beeline does not respect hive.cli.print.current.db.This is partially true, since in embedded mode, it uses the hive.cli.print.current.db to change the prompt, since HIVE-10511.In beeline mode, I think this function should use a beeline command line option instead, like for the showHeader option emphasizing, that this is a client side option.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestClientCommandHookFactory.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ReflectiveCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ClientCommandHookFactory.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14135" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline output not formatted correctly for large column widths</summary>
      <description>If the column width is too large then beeline uses the maximum column width when normalizing all the column widths. In order to reproduce the issue, run set -v; Once the configuration variables is classpath which can be extremely large width (41k characters in my environment).</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BufferedRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14142" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.ClassNotFoundException for the jar in hive.reloadable.aux.jars.path for Hive on Spark</summary>
      <description>Similar to HIVE-14037, seems HOS also has the same issue. The jars in hive.reloadable.aux.jars.path are not available during runtime.java.lang.RuntimeException: Reduce operator initialization failed at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.init(SparkReduceRecordHandler.java:232) at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.call(HiveReduceFunction.java:46) at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.call(HiveReduceFunction.java:28) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:192) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:192) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306) at org.apache.spark.rdd.RDD.iterator(RDD.scala:270) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: xudf.XAdd at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:134) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(FunctionRegistry.java:1365) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isDeterministic(FunctionRegistry.java:1328) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isDeterministic(ExprNodeGenericFuncEvaluator.java:153) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.iterate(ExprNodeEvaluatorFactory.java:100) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.toCachedEvals(ExprNodeEvaluatorFactory.java:74) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:59) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425) at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:406) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.init(SparkReduceRecordHandler.java:217) ... 15 moreCaused by: java.lang.ClassNotFoundException: xudf.XAdd at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:270) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:132) ... 27 more</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="14146" opendate="2016-7-1 00:00:00" fixdate="2016-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column comments with "\n" character "corrupts" table metadata</summary>
      <description>Create a table with the following(noting the \n in the COMMENT):CREATE TABLE commtest(first_nm string COMMENT 'Indicates First name\nof an individual’);Describe shows that now the metadata is messed up:beeline&gt; describe commtest;+-------------------+------------+-----------------------+--+| col_name | data_type | comment |+-------------------+------------+-----------------------+--+| first_nm | string | Indicates First name || of an individual | NULL | NULL |+-------------------+------------+-----------------------+--+</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.invalidate.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14147" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive PPD might remove predicates when they are defined as a simple expr e.g. WHERE &amp;#39;a&amp;#39;</summary>
      <description>Click to add description</description>
      <version>2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="14148" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add branch-2.1 branch to pre-commit tests</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-submit-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14155" opendate="2016-7-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Custom UDF Vectorization annotations are ignored</summary>
      <description>@VectorizedExpressions(value = { VectorStringRot13.class })in a custom UDF Is ignored because the check for annotations happens after custom UDF detection.The custom UDF codepath is on the fail-over track of annotation lookups, so the detection during validation of SEL is sufficient, instead of during expression creation.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14172" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: force evict blocks by size to handle memory fragmentation</summary>
      <description>In the long run, we should replace buddy allocator with a better scheme. For now do a workaround for fragmentation that cannot be easily resolved. It's still not perfect but works for practical ORC cases, where we have the default size and smaller blocks, rather than large allocations having trouble.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14176" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO nesting windowing function within each other when merging Project operators</summary>
      <description>The translation into a physical plan does not support this way of expressing windowing functions. Instead, we will not merge the Project operators when we find this pattern.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="14177" opendate="2016-7-6 00:00:00" fixdate="2016-2-6 01:00:00" resolution="Not A Bug">
    <buginformation>
      <summary>AddPartitionEvent contains the table location, but not the partition location</summary>
      <description>AddPartitionEvent contains the table location, but not the partition location</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug id="14196" opendate="2016-7-8 00:00:00" fixdate="2016-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable LLAP IO when complex types are involved</summary>
      <description>Let's exclude vector_complex_* tests added for llap which is currently broken and fails in all test runs. We can re-enable it with HIVE-14089 patch.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14197" opendate="2016-7-8 00:00:00" fixdate="2016-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP service driver precondition failure should include the values</summary>
      <description>LLAP service driver's precondition failure message are like belowWorking memory + cache has to be smaller than the container sizingIt will be better to include the actual values for the sizes in the precondition failure message.NO PRECOMMIT TESTS</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14198" opendate="2016-7-8 00:00:00" fixdate="2016-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor aux jar related code to make them more consistent</summary>
      <description>There are some redundancy and inconsistency between hive.aux.jar.paths and hive.reloadable.aux.jar.paths and also between MR and spark. Refactor the code to reuse the same code.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14199" opendate="2016-7-8 00:00:00" fixdate="2016-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Bucket Pruning for ACID tables</summary>
      <description>Currently, ACID tables do not benefit from the bucket pruning feature introduced in HIVE-11525. The reason for this has been the fact that bucket pruning happens at split generation level and for ACID, traditionally the delta files were never split. The parallelism for ACID was then restricted to the number of buckets. There would be as many splits as the number of buckets and each worker processing one split would inevitably read all the delta files for that bucket, even when the query may have originally required only one of the buckets to be read.However, HIVE-14035 now enables even the delta files to be also split. What this means is that now we have enough information at the split generation level to determine appropriate buckets to process for the delta files. This can efficiently allow us to prune unnecessary buckets for delta files and will lead to good performance gain for a large number of selective queries on ACID tables.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="142" opendate="2008-12-9 00:00:00" fixdate="2008-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a metastore check command</summary>
      <description>We need a command to verify that the information in the metastore reflects the data that is on hdfs. For example partitions can be deleted on hdfs but still be in the metastore.From Joydeep Sen Sarma, see ticket HIVE-126 for the full comment:for a command line interface - one might want to check the entire database or just a table or even just one partition. other metadata checks will also be added over time (for example - do the file types on disk agree with metadata records, bucketing information etc). So, here's a strawman proposal for a new command:alter table &lt;DB&gt;[.TABLE &amp;#91;PARTITION-SPEC&amp;#93;] check &amp;#91;TYPE-LIST&amp;#93;where TYPE by default is 'all' (check for all kinds of errors), but can be specified to a specific type. For example - in this case - we can have a type called 'partitions' (and then over time we can add other types like 'fileformat' etc.). for v1 - we can just drop the type-list altogether.the check command can produce a list of things that need to be done to fix the format (like adding any directories not in the metastore - but in hdfs - to the metastore). actually performing of such steps would require a user confirmation (y/n).</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.dropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14200" opendate="2016-7-9 00:00:00" fixdate="2016-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez: disable auto-reducer parallelism when reducer-count * min.partition.factor &lt; 1.0</summary>
      <description>The min/max factors offer no real improvement when the fractions are meaningless, for example when 0.25 * 2 is applied as the min.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14202" opendate="2016-7-9 00:00:00" fixdate="2016-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change tez version used to 0.8.4</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14209" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some logging info for session and operation management</summary>
      <description>It's hard to track the session and operation open and close in multiple user env. Add some logging info.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1421" opendate="2010-6-22 00:00:00" fixdate="2010-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>problem with sequence and rcfiles are mixed for null partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14215" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Displaying inconsistent CPU usage data with MR execution engine</summary>
      <description>If the MR task is finished after printing the cumulative CPU time then there is the possibility to print inconsistent CPU usage information.Correct one:2016-07-12 11:31:42,961 Stage-3 map = 0%, reduce = 0%2016-07-12 11:31:48,237 Stage-3 map = 100%, reduce = 0%, Cumulative CPU 2.5 secMapReduce Total cumulative CPU time: 2 seconds 500 msecEnded Job = job_1468321038188_0003MapReduce Jobs Launched: Stage-Stage-3: Map: 1 Cumulative CPU: 2.5 sec HDFS Read: 5864 HDFS Write: 103 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 500 msecOne type of inconsistent data (easily reproducible one):2016-07-12 11:39:00,540 Stage-3 map = 0%, reduce = 0%Ended Job = job_1468321038188_0004MapReduce Jobs Launched: Stage-Stage-3: Map: 1 Cumulative CPU: 2.51 sec HDFS Read: 5864 HDFS Write: 103 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 510 msec</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14224" opendate="2016-7-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP rename query specific log files once a query is complete</summary>
      <description>Once a query is complete, rename the query specific log file so that YARN can aggregate the logs (once it's configured to do so).</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14278" opendate="2016-7-19 00:00:00" fixdate="2016-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate TestHadoop23SAuthBridge.java from Unit3 to Unit4</summary>
      <description>Migrate TestHadoop23SAuthBridge.java from unit3 to unit4</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
    </fixedFiles>
  </bug>
  <bug id="14299" opendate="2016-7-20 00:00:00" fixdate="2016-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log serialized plan size</summary>
      <description>It will be good to log the size of the serialized plan. This can help identifying cases where large objects are accidentally serialized.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="14302" opendate="2016-7-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez: Optimized Hashtable can support DECIMAL keys of same precision</summary>
      <description>Decimal support in the optimized hashtable was decided on the basis of the fact that Decimal(10,1) == Decimal(10, 2) when both contain "1.0" and "1.00".However, the joins now don't have any issues with decimal precision because they cast to common.create temporary table x (a decimal(10,2), b decimal(10,1)) stored as orc;insert into x values (1.0, 1.0); &gt; explain logical select count(1) from x, x x1 where x.a = x1.b;OK LOGICAL PLAN:$hdt$_0:$hdt$_0:x TableScan (TS_0) alias: x filterExpr: (a is not null and true) (type: boolean) Filter Operator (FIL_18) predicate: (a is not null and true) (type: boolean) Select Operator (SEL_2) expressions: a (type: decimal(10,2)) outputColumnNames: _col0 Reduce Output Operator (RS_6) key expressions: _col0 (type: decimal(11,2)) sort order: + Map-reduce partition columns: _col0 (type: decimal(11,2)) Join Operator (JOIN_8) condition map: Inner Join 0 to 1 keys: 0 _col0 (type: decimal(11,2)) 1 _col0 (type: decimal(11,2)) Group By Operator (GBY_11) aggregations: count(1) mode: hash outputColumnNames: _col0See cast up to Decimal(11, 2) in the plan, which normalizes both sides of the join to be able to compare HiveDecimal as-is.</description>
      <version>2.2.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14311" opendate="2016-7-22 00:00:00" fixdate="2016-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to schedule Heartbeat task if the query doesn&amp;#39;t require locks</summary>
      <description>Otherwise the Heartbeat task will just stay there and not be cleaned up, which may cause OOM eventually.</description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14324" opendate="2016-7-25 00:00:00" fixdate="2016-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC PPD for floats is broken</summary>
      <description>ORC stores min/max stats, bloom filters by passing floats as doubles using java's widening conversion. So if we write a float value of 0.22 to ORC file, the min/max stats and bloom filter will use 0.2199999988079071 double value.But when we do PPD, SARG creates literals by converting float to string and then to double which compares 0.22 to 0.2199999988079071 and fails PPD evaluation. hive&gt; create table orc_float (f float) stored as orc;hive&gt; insert into table orc_float values(0.22);hive&gt; set hive.optimize.index.filter=true;hive&gt; select * from orc_float where f=0.22;OKhive&gt; set hive.optimize.index.filter=false;hive&gt; select * from orc_float where f=0.22;OK0.22This is not a problem for doubles and decimals.This issue was introduced in HIVE-8460 but back then there was no strict type check when SARGs are created and also PPD evaluation does not convert to column type. But now predicate leaf creation in SARG enforces strict type check for boxed literals and predicate type and PPD evaluation converts stats and constants to column type (predicate).</description>
      <version>1.3.0,2.0.0,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.basic.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
    </fixedFiles>
  </bug>
  <bug id="14333" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC schema evolution from float to double changes precision and breaks filters</summary>
      <description>ORC vs text schema evolution from float to double changes precisionText Schema Evolutionhive&gt; create table float_text(f float);hive&gt; insert into float_text values(74.72);hive&gt; select f from float_text;OK74.72hive&gt; alter table float_text change column f f double;hive&gt; select f from float_text;OK74.72Orc Schema Evolutionhive&gt; create table float_orc(f float) stored as orc;hive&gt; insert into float_orc values(74.72);hive&gt; select f from float_orc;OK74.72hive&gt; alter table float_orc change column f f double;hive&gt; select f from float_orc;OK74.72000122070312This will break all filters on the evolved column "f"Filter returning no resultshive&gt; set hive.optimize.index.filter=false;hive&gt; select f from float_orc where f=74.72;OK</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestSchemaEvolution.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14340" opendate="2016-7-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new hook triggers before query compilation and after query execution</summary>
      <description>In some cases we may need to have a hook that activates before a query compilation and after its execution. For instance, dynamically generate a UDF specifically for the running query and clean up the resource after the query is done. The current hooks only covers pre &amp; post semantic analysis, pre &amp; post query execution, which doesn't fit the requirement.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14345" opendate="2016-7-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline result table has erroneous characters</summary>
      <description>Beeline returns query results with erroneous characters. For example:0: jdbc:hive2://xxxx:10000/def&gt; select 10;+------+--+| _c0 |+------+--+| 10 |+------+--+1 row selected (3.207 seconds)</description>
      <version>1.1.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14349" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: LIKE should anchor the regexes</summary>
      <description>RLIKE works like contains() and LIKE works like matches().The UDFLike LIKE -&gt; Regex conversion returns unanchored regexes making the vectorized LIKE behave like RLIKE.create temporary table x (a string) stored as orc;insert into x values('XYZa'), ('badXYZa');select * from x where a LIKE 'XYZ%a%' order by 1;OKXYZabadXYZaTime taken: 4.029 seconds, Fetched: 2 row(s)</description>
      <version>1.2.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.udf2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
    </fixedFiles>
  </bug>
  <bug id="1435" opendate="2010-6-25 00:00:00" fixdate="2010-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgraded naming scheme causes JDO exceptions</summary>
      <description>We recently upgraded from Datanucleus 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. Because of this change, connecting to an existing database would throw exceptions such as:2010-06-24 17:59:09,854 ERROR exec.DDLTask (SessionState.java:printError(277)) - FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list'org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list' at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:325) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:2012) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:144) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:633) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:506) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:384) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:302) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14355" opendate="2016-7-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema evolution for ORC in llap is broken for int to string conversion</summary>
      <description>When schema is evolved from any integer type to string then following exceptions are thrown in LLAP (Works fine in Tez). I guess this should happen even for other conversions.hive&gt; create table orc_integer(b bigint) stored as orc;hive&gt; insert into orc_integer values(100);hive&gt; select count(*) from orc_integer where b=100;OK1hive&gt; alter table orc_integer change column b b string;hive&gt; select count(*) from orc_integer where b=100;// FAIL with following exceptionWhen vectorization is enabled2016-07-27T01:48:05,611 INFO [TezTaskRunner ()] vector.VectorReduceSinkOperator: RECORDS_OUT_INTERMEDIATE_Map_1:0,2016-07-27T01:48:05,611 ERROR [TezTaskRunner ()] tez.TezProcessor: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:866) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 18 moreCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringGroupColEqualStringGroupScalarBase.evaluate(FilterStringGroupColEqualStringGroupScalarBase.java:42) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:110) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:774) ... 19 moreWhen vectorization is disabled2016-07-27T01:52:43,328 INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.ReduceSinkOperator: Using tag = -12016-07-27T01:52:43,328 INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.OperatorUtils: Setting output collector: RS[4] --&gt; Reducer 22016-07-27T01:52:43,329 ERROR [TezTaskRunner (1469608604787_0002_26_00_000000_0)] io.BatchToRowReader: Error at row 0/1, column 0/1 org.apache.hadoop.hive.ql.exec.vector.LongColumnVector@7630e56ajava.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextString(BatchToRowReader.java:334) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextValue(BatchToRowReader.java:602) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:149) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:78) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151) ~[tez-mapreduce-0.8.4.jar:0.8.4] at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) ~[tez-mapreduce-0.8.4.jar:0.8.4] at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_91] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_91] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) ~[hadoop-common-2.6.0.jar:?] at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.jar:0.8.4] at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) ~[hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_91] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14364" opendate="2016-7-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update timeouts for llap comparator tests</summary>
      <description>The tests timeout occasionally. Increasing to 60 seconds from 5 seconds. NO_PRECOMMIT_TESTS</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14377" opendate="2016-7-28 00:00:00" fixdate="2016-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: issue with how estimate cache removes unneeded buffers</summary>
      <description>Results in NPE when reading</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileEstimateErrors.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14378" opendate="2016-7-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data size may be estimated as 0 if no columns are being projected after an operator</summary>
      <description>in those cases we still emit rows.. but they may not have any columns within it. We shouldn't estimate 0 data size in such cases.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partial.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14382" opendate="2016-7-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the Functionality of Reverse FOR Statement</summary>
      <description>According to SQL Standards, Reverse FOR Statement should be like this:-FOR index IN Optional&amp;#91;Reverse&amp;#93; Lower_Bound Upper_Boundbut in hive it is like this :- FOR index IN Optional&amp;#91;Reverse&amp;#93; Upper_Bound Lower_Boundso i m just trying to improve the functionality for Reverse FOR StatementREFERNCES :- https://docs.oracle.com/cloud/latest/db112/LNPLS/for_loop_statement.htm#LNPLS1536</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.queries.local.for.range.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
    </fixedFiles>
  </bug>
  <bug id="14386" opendate="2016-7-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UGI clone shim also needs to clone credentials</summary>
      <description>Discovered while testing HADOOP-13081</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="14388" opendate="2016-7-29 00:00:00" fixdate="2016-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add number of rows inserted message after insert command in Beeline</summary>
      <description>Currently, when you run insert command on beeline, it returns a message saying "No rows affected .."A better and more intuitive msg would be "xxx rows inserted (26.068 seconds)"</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TGetOperationStatusResp.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.input.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.non.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dp.counter.mm.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.MapRedStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14389" opendate="2016-7-29 00:00:00" fixdate="2016-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should not output query and prompt to stdout</summary>
      <description>It seems that the Beeline prints the query along with the results in the stdout when a script file is passed. The output file in the example below needs to only have the results and not the query..vihang-MBP:bin vihang$ ./beeline --showheader=false --outformat=tsv2 -u "jdbc:hive2://localhost:10000" -f /tmp/query.sql &gt; /tmp/query.out 2&gt; /tmp/query.errOK$ cat /tmp/query.out1: jdbc:hive2://localhost:10000/default&gt; select * from likes limit 4;+-----------+--------------+--+| 1 | chocolate || 1 | car || 1 | games || 1 | chess |+-----------+--------------+--+1: jdbc:hive2://localhost:10000/default&gt;1: jdbc:hive2://localhost:10000/default&gt;$A lot of people use HiveCLI and in order to transition from HiveCLI scripts to Beeline, this needs to be taken care of. The output files generated by beeline should contain only the results and nothing else.Similarly, when not in silent mode, query are being printed out on stdout, which is adding garbage along with results, as just like HIVE CLI does, users would like to have only the results on stdout, not errors/debugging info/etc, like the full query. Query could be printed out, no problem, as long as it is not on stdout (with results), instead, it must be printed out along with the debugging info.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="14397" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries ran after reopening of tez session launches additional sessions</summary>
      <description>Say we have configured hive.server2.tez.default.queues with 2 queues q1 and q2 with default expiry interval of 5 mins.After 5 mins of non-usage the sessions corresponding to queues q1 and q2 will be expired. When new set of queries are issue after this expiry, the default sessions backed by q1 and q2 and reopened again. Now when we run more queries the reopened sessions are not used instead new session is opened. At this point there will be 4 sessions running (2 abandoned sessions and 2 current sessions).</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14400" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle concurrent insert with dynamic partition</summary>
      <description>With multiple users concurrently issuing insert statements on the same partition has a side effect that some queries may not see a partition at the time when they're issued, but will realize the partition is actually there when it is trying to add such partition to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).For example, imagine such a table is created:create table T (name char(50)) partitioned by (ds string);and the following two queries are launched at the same time, from different sessions:insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="14403" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP node specific preemption will only preempt once on a node per AM</summary>
      <description>Query hang reported by cartershanklinTurns out that once an AM has preempted a task on a node for locality, it will not be able to preempt another task on the same node (specifically for local requests)Manifests as a query hanging. It's possible for a previous query to interfere with a subsequent query since the AM is shared.</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14405" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Have tests log to the console along with hive.log</summary>
      <description>When running tests from the IDE (not itests), logs end up going to hive.log - making it difficult to debug tests.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14408" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>thread safety issue in fast hashtable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="14421" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FS.deleteOnExit holds references to _tmp_space.db files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14422" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IF: when using LLAP IF from multiple threads in secure cluster, tokens can get mixed up</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="14432" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP signing unit test may be timing-dependent</summary>
      <description>Seems like it's possible for slow background thread to roll the key after we have signed with it.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14433" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor LLAP plan cache avoidance and fix issue in merge processor</summary>
      <description>Map and reduce processors do this: if (LlapProxy.isDaemon()) { cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache(); // do not cache plan...but merge processor just gets the plan. If it runs in LLAP, it can get a cached plan. Need to move this logic into ObjectCache itself, via a isPlan arg or something. That will also fix this issue for merge processor</description>
      <version>2.0.1,2.1.1,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14434" opendate="2016-8-4 00:00:00" fixdate="2016-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: BytesBytes lookup capped count can be =0, =1, &gt;=2</summary>
      <description>BytesBytesHashmap does not implement deep counters for the depth of a probe - but however it does distinguish between 0, 1 and &gt; 1 rows.This information can be used in the vectorized hash join to avoid copying the probe side keys.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="14435" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: missed vectorization for const varchar()</summary>
      <description>2016-08-05T09:45:16,488 INFO [main] physical.Vectorizer: Failed to vectorize2016-08-05T09:45:16,488 INFO [main] physical.Vectorizer: Cannot vectorize select expression: Const varchar(1) fThe constant throws an illegal argument because the varchar precision is lost in the pipeline.</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
    </fixedFiles>
  </bug>
  <bug id="14439" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LlapTaskScheduler should try scheduling tasks when a node is disabled</summary>
      <description>When a node is disabled - try scheduling pending tasks. Tasks which may have been waiting for the node to become available could become candidates for scheduling on alternate nodes depending on the locality delay and disable duration.This is what is causing an occasional timeout on testDelayedLocalityNodeCommErrorImmediateAllocation</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14446" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add switch to control BloomFilter in Hybrid grace hash join and make the FPP adjustable</summary>
      <description>When row count exceeds certain limit, it doesn't make sense to generate a bloom filter, since its size will be a few hundred MB or even a few GB.</description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14447" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set HIVE_TRANSACTIONAL_TABLE_SCAN to the correct job conf for FetchOperator</summary>
      <description></description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14453" opendate="2016-8-6 00:00:00" fixdate="2016-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor physical writing of ORC data and metadata to FS from the logical writers</summary>
      <description>ORC data doesn't have to go directly into an HDFS stream via buffers, it can go somewhere else (e.g. a write-thru cache, or an addressable system that doesn't require the stream blocks to be held in memory before writing them all together).To that effect, it would be nice to abstract the data block/metadata structure creating from the physical file concerns.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestOrcWideTable.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RecordReaderUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ReaderImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.OrcTail.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14455" opendate="2016-8-6 00:00:00" fixdate="2016-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade httpclient, httpcore to match updated hadoop dependency</summary>
      <description>Hive was having a newer version of httpclient and httpcore since 1.2.0 (HIVE-9709), when compared to Hadoop 2.x versions, to be able to make use of newer apis in httpclient 4.4.There was security issue in the older version of httpclient and httpcore that hadoop was using, and as a result moved to httpclient 4.5.2 and httpcore 4.4.4 (HADOOP-12767).As hadoop was using the older version of these libraries and they often end up earlier in the classpath, we have had bunch of difficulties in different environments with class/method not found errors. Now, that hadoops dependencies in versions with security fix are newer and have the API that hive needs, we can be on the same version. For older versions of hadoop this version update doesn't matter as the difference is already there.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14459" opendate="2016-8-7 00:00:00" fixdate="2016-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestBeeLineDriver - migration and re-enable</summary>
      <description>this test have been left behind in HIVE-14444 because it had some compile issues.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.DisabledTestBeeLineDriver.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.util.QFileClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="14460" opendate="2016-8-7 00:00:00" fixdate="2016-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AccumuloCliDriver migration to junit4</summary>
      <description>This test have been left behind in HIVE-14444</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloTestSetup.java</file>
      <file type="M">accumulo-handler.src.test.templates.TestAccumuloCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug id="14462" opendate="2016-8-8 00:00:00" fixdate="2016-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of partition check calls in add_partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.CheckResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="14479" opendate="2016-8-8 00:00:00" fixdate="2016-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some join tests for acid table</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
    </fixedFiles>
  </bug>
  <bug id="14487" opendate="2016-8-9 00:00:00" fixdate="2016-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add REBUILD statement for materialized views</summary>
      <description>Support for rebuilding existing materialized views. The statement is the following:ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.multi.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BasicStatsWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.ConstraintsSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14493" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioning support for materialized views</summary>
      <description>We should support defining a partitioning specification for materialized views and that the results of the materialized view evaluation are stored meeting the partitioning spec. The syntax should be extended as follows:CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] -- NEW! [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] AS select_statement;</description>
      <version>2.2.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14495" opendate="2016-8-9 00:00:00" fixdate="2016-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SHOW MATERIALIZED VIEWS statement</summary>
      <description>In the spirit of SHOW TABLES, we should support the following statement:SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];In contrast to SHOW TABLES, this command would only list the materialized views.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14519" opendate="2016-8-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi insert query bug</summary>
      <description>When running multi-insert queries, when one of the query is not returning results, the other query is not returning the right result.For example:After following query, there is no value in /tmp/emp/dir3/000000_0From (select * from src) ainsert overwrite directory '/tmp/emp/dir1/'select key, valueinsert overwrite directory '/tmp/emp/dir2/'select 'header'where 1=2insert overwrite directory '/tmp/emp/dir3/'select key, value where key = 100;where clause in the second insert should not affect the third insert.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14527" opendate="2016-8-12 00:00:00" fixdate="2016-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema evolution tests are not running in TestCliDriver</summary>
      <description>HIVE-14376 broke something that makes schema evolution tests being excluded from TestCliDriver test suite.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14534" opendate="2016-8-13 00:00:00" fixdate="2016-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>modify tables in tests in HIVE-14479 to use transactional_properties=default</summary>
      <description>only need to do this for 2.2</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
    </fixedFiles>
  </bug>
  <bug id="14554" opendate="2016-8-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Download the spark-assembly file on itests only if the MD5 checksum file is different</summary>
      <description>The itests/thridparty directory is created by hive on spark when downloading the spark-assembly file. Hive ptest should delete this directory everytime it runs a new set of tests to avoid conflicts when a new spark tarball is submitted.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14559" opendate="2016-8-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove setting hive.execution.engine in qfiles</summary>
      <description>Some qfiles are explicitly setting execution engine. If we run those tests on different Mini CliDriver's it could be very slow.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.tez.fsstat.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.join.partition.key.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.mr.pathalias.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.acid.non.acid.q</file>
      <file type="M">ql.src.test.queries.clientpositive.decimal.skewjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constprog.dpp.q</file>
    </fixedFiles>
  </bug>
  <bug id="14560" opendate="2016-8-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support exchange partition between s3 and hdfs tables</summary>
      <description>alter table s3_tbl exchange partition (country='USA', state='CA') with table hdfs_tbl;results in:Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: java.lang.IllegalArgumentException Wrong FS: s3a://hive-on-s3/s3_tbl/country=USA/state=CA, expected: hdfs://localhost:9000) (state=08S01,code=1)because the check for whether the s3 destination table path exists occurs on the hdfs filesystem.Furthermore, exchanging between s3 to hdfs fails because the hdfs rename operation is not supported across filesystems. Fix uses copy + deletion in the case that the file systems differ.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14566" opendate="2016-8-18 00:00:00" fixdate="2016-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO reads timestamp wrongly</summary>
      <description>HIVE-10127 is causing incorrect results when orc_merge12.q is run in llap.It reads timestamp wrongly.LLAP IO Enabledhive&gt; select atimestamp1 from alltypesorc3xcols limit 10;OK1969-12-31 15:59:46.674NULL1969-12-31 15:59:55.7871969-12-31 15:59:44.1871969-12-31 15:59:50.4341969-12-31 16:00:15.0071969-12-31 16:00:07.0211969-12-31 16:00:04.9631969-12-31 15:59:52.1761969-12-31 15:59:44.569LLAP IO Disabledhive&gt; select atimestamp1 from alltypesorc3xcols limit 10;OK1969-12-31 15:59:46.674NULL1969-12-31 15:59:55.7871969-12-31 15:59:44.1871969-12-31 15:59:50.4341969-12-31 16:00:14.0071969-12-31 16:00:06.0211969-12-31 16:00:03.9631969-12-31 15:59:52.1761969-12-31 15:59:44.569</description>
      <version>2.0.1,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.TreeReaderFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14570" opendate="2016-8-18 00:00:00" fixdate="2016-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table with column names ROW__ID, INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE sucess but query fails</summary>
      <description>0: jdbc:hive2://189.39.151.74:21066/&gt; create table foo1(ROW__ID string);No rows affected (0.281 seconds)0: jdbc:hive2://189.39.151.74:21066/&gt; create table foo2(BLOCK__OFFSET__INSIDE__FILE string);No rows affected (0.323 seconds)0: jdbc:hive2://189.39.151.74:21066/&gt; create table foo3(INPUT__FILE__NAME string);No rows affected (0.307 seconds)0: jdbc:hive2://189.39.151.74:21066/&gt; select * from foo1;Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference 'TOK_ALLCOLREF' (state=42000,code=40000)0: jdbc:hive2://189.39.151.74:21066/&gt; select * from foo2;Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference 'TOK_ALLCOLREF' (state=42000,code=40000)0: jdbc:hive2://189.39.151.74:21066/&gt; select * from foo3;Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Invalid column reference 'TOK_ALLCOLREF' (state=42000,code=40000)We should prevent user from creating table with column names the same as Virtual Column names</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="14574" opendate="2016-8-18 00:00:00" fixdate="2016-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use consistent hashing for LLAP consistent splits to alleviate impact from cluster changes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="14579" opendate="2016-8-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for date extract</summary>
      <description>https://www.postgresql.org/docs/9.1/static/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="14588" opendate="2016-8-19 00:00:00" fixdate="2016-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add S3 credentials to the hidden configuration variable supported on HIVE-14207</summary>
      <description>Here's the list of S3 values we should hide:fs.s3.awsAccessKeyIdfs.s3.awsSecretAccessKeyfs.s3n.awsAccessKeyIdfs.s3n.awsSecretAccessKeyfs.s3a.access.keyfs.s3a.secret.keyfs.s3a.proxy.password</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14589" opendate="2016-8-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add consistent node replacement to LLAP for splits</summary>
      <description>See HIVE-14574. (copied from the comment below) This basically creates the nodes in ZK for "slots" in the cluster. The LLAPs try to take the lowest available slot, starting from 0. Unlike worker-... nodes, the slots are reused, which is the intent. The LLAPs are always sorted by the slot number for splits.The idea is that as long as LLAP is running, it will retain the same position in the ordering, regardless of other LLAPs restarting, without knowing about each other, the predecessors location (if restarted in a different place), or the total size of the cluster.The restarting LLAPs may not take the same positions as their predecessors (i.e. if two LLAPs restart they can swap slots) but it shouldn't matter because they have lost their cache anyway.I.e. if you have LLAPs with slots 1-2-3-4 and I nuke and restart 1, 2, and 4, they will take whatever slots, but 3 will stay the 3rd and retain cache locality.This also handles size increase, as new LLAPs will always be added to the end of the sequence, which is what consistent hashing needs.One case it doesn't handle is permanent cluster size reduction. There will be a permanent gap if LLAPs are removed that have the slots in the middle; until some are restarted, it will result in misses</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="146" opendate="2008-12-10 00:00:00" fixdate="2008-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix builds for non-default build directory</summary>
      <description>Some build paths are specified as "${hive.root}/build" instead of "${build.dir.hive}". Correct these, including "build.dir.hadoop" (it remains relative to "build.dir.hive" by default). This allows builds to work when a non-default "build.dir.hive" is specified.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14600" opendate="2016-8-22 00:00:00" fixdate="2016-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP zookeeper registry failures do not fail the daemon</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14608" opendate="2016-8-23 00:00:00" fixdate="2016-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: slow scheduling due to LlapTaskScheduler not removing nodes on kill</summary>
      <description>See comments; this can result in a slowdown esp. if some critical task gets unlucky. public void workerNodeRemoved(ServiceInstance serviceInstance) { // FIXME: disabling this for now// instanceToNodeMap.remove(serviceInstance.getWorkerIdentity());</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14613" opendate="2016-8-23 00:00:00" fixdate="2016-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move schema evolution tests to MiniLlap and disable LLAP IO</summary>
      <description>Move the slow schema evolution tests from TestCliDriver to TestMiniLlapCliDriver and disable LLAP IO for these tests so that non-llap reader codepath is used.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.stats.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.part.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14614" opendate="2016-8-23 00:00:00" fixdate="2016-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite local directory fails with IllegalStateException</summary>
      <description>insert overwrite local directory .... select * from table; fails with "java.lang.IllegalStateException: Cannot create staging directory" when the path sent to the getTempDirForPath(Path path) is a local fs path.This is a regression caused by the fix for HIVE-14270</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="1463" opendate="2010-7-13 00:00:00" fixdate="2010-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive output file names are unnecessarily large</summary>
      <description>Hive's output files are named like this:attempt_201006221843_431854_r_000000_0out of all of this goop - only one character '0' would have sufficed. we should fix this. This would help environments with namenode memory constraints.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14647" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo fixes in Beeline help</summary>
      <description>https://github.com/apache/hive/pull/99</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14648" opendate="2016-8-25 00:00:00" fixdate="2016-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Avoid private pages in the SSD cache</summary>
      <description>There's no reason for the SSD cache to have private mappings to the cache file, there's only one reader and the memory overheads aren't worth it.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="14652" opendate="2016-8-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect results for not in on partition columns</summary>
      <description>create table foo (i int) partitioned by (s string);insert overwrite table foo partition(s='foo') select cint from alltypesorc limit 10;insert overwrite table foo partition(s='bar') select cint from alltypesorc limit 10;select * from foo where s not in ('bar');No results. IN ... works correctly</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14655" opendate="2016-8-26 00:00:00" fixdate="2016-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP input format should escape the query string being passed to getSplits()</summary>
      <description>Query may not be parsed correctly by get_splits() otherwise.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug id="14656" opendate="2016-8-26 00:00:00" fixdate="2016-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up driver instance in get_splits</summary>
      <description>get_splits() creates a Driver instance that needs to be closed/cleaned up after use.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug id="14658" opendate="2016-8-27 00:00:00" fixdate="2016-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF abs throws NPE when input arg type is string</summary>
      <description>I know this is not the right use case, but NPE is not exptected.0: jdbc:hive2://10.64.35.144:21066/&gt; select abs("foo");Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
    </fixedFiles>
  </bug>
  <bug id="14678" opendate="2016-8-31 00:00:00" fixdate="2016-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-on-MR deprecation warning is not diplayed when engine is set to capital letter &amp;#39;MR&amp;#39;</summary>
      <description>hive&gt; SET hive.execution.engine=mr;Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt; hive&gt; SET hive.execution.engine=MR;hive&gt;This warning seems to be generated only with small letter 'mr' wand not with capital letter "MR".generateMrDeprecationWarning() seem to be displaying this warning and is being called in different classes. Ex: https://github.com/apache/hive/blob/master/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java#L754https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/Driver.java#L1900</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14693" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some partitions will be left out when partition number is a multiple of the option hive.msck.repair.batch.size</summary>
      <description>For example, bactch_size = 5, and no of partitions = 9, it will skip the last 4 partitions from being added.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.msck.repair.batchsize.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.msck.repair.batchsize.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14702" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAPIF: after a long period of inactivity, signing key may be removed from local store</summary>
      <description>Then, an attempt to get and use it would NPE</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SigningSecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14707" opendate="2016-9-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Insert shuffle sort-merges on blank KEY</summary>
      <description>The ACID insert codepath uses a sorted shuffle, while they key used for shuffle is always 0 bytes long.hive (sales_acid)&gt; explain insert into sales values(1, 2, '3400-0000-0000-009', 1, null);STAGE PLANS: Stage: Stage-1 Tez DagId: gopal_20160906172626_80261c4c-79cc-4e02-87fe-3133be404e55:2 Edges: Reducer 2 &lt;- Map 1 (SIMPLE_EDGE)... Vertices: Map 1 Map Operator Tree: TableScan alias: values__tmp__table__2 Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: tmp_values_col1 (type: string), tmp_values_col2 (type: string), tmp_values_col3 (type: string), tmp_values_col4 (type: string), tmp_values_col5 (type: string) outputColumnNames: _col0, _col1, _col2, _col3, _col4 Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator sort order: Map-reduce partition columns: UDFToLong(_col1) (type: bigint) Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: string) Execution mode: vectorized, llap LLAP IO: no inputsNote the missing "+" / "-" in the Sort Order fields.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.multi.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.include.no.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.remove.26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.count.distinct.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.pushdown3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.offset.limit.ppd.optimizer.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.notin.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="14751" opendate="2016-9-14 00:00:00" fixdate="2016-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for date truncation</summary>
      <description>Add support for floor (&lt;time&gt; to &lt;timeunit&gt;), which is equivalent to date_trunc(&lt;timeunit&gt;, &lt;time&gt;).https://www.postgresql.org/docs/9.1/static/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFDateFormatGranularity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDateFloor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="14753" opendate="2016-9-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track the number of open/closed/abandoned sessions in HS2</summary>
      <description>We should be able to track the nr. of sessions since the startup of the HS2 instance as well as the average lifetime of a session.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.LegacyMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.Metrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="14754" opendate="2016-9-14 00:00:00" fixdate="2016-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track the queries execution lifecycle times</summary>
      <description>We should be able to track the nr. of queries being compiled/executed at any given time, as well as the duration of the execution and compilation phase.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.LegacyMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.Metrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="14783" opendate="2016-9-17 00:00:00" fixdate="2016-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketing column should be part of sorting for delete/update operation when spdo is on</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14831" opendate="2016-9-23 00:00:00" fixdate="2016-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing Druid dependencies at runtime</summary>
      <description>Excluded some packages when shading in the initial patch that should have been included.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14835" opendate="2016-9-23 00:00:00" fixdate="2016-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve ptest2 build time</summary>
      <description>NO PRECOMMIT TESTS2 things can be improved1) ptest2 always downloads jars for compiling its own directory which takes about 1m30s which should take only 5s with cache jars. The reason for that is maven.repo.local is pointing to a path under WORKSPACE which will be cleaned by jenkins for every run.2) For hive build we can make use of parallel build and quite the output of build which should shave off another 15-30s.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug id="14837" opendate="2016-9-24 00:00:00" fixdate="2016-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: standalone jar is missing hadoop core dependencies</summary>
      <description>2016/09/24 00:31:57 ERROR - jmeter.threads.JMeterThread: Test failed! java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration at org.apache.hive.jdbc.HiveConnection.createUnderlyingTransport(HiveConnection.java:418) at org.apache.hive.jdbc.HiveConnection.createBinaryTransport(HiveConnection.java:438) at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:225) at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:182) at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14849" opendate="2016-9-28 00:00:00" fixdate="2016-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support google-compute-engine provider on Hive ptest framework</summary>
      <description>NO PRECOMMIT TESTSCurrently, Hive ptest only supports AWS EC2 to create a cluster. We should add support for GCE as well.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudComputeService.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1487" opendate="2010-7-25 00:00:00" fixdate="2010-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parallelize test query runs</summary>
      <description>HIVE-1464 speeded up serial runs somewhat - but looks like it's still too slow. we should use parallel junit or some similar setup to run test queries in parallel. this should be really easy as well need to just use a separate warehouse/metadb and potentiall mapred system dir location.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.Report.py</file>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="14878" opendate="2016-10-3 00:00:00" fixdate="2016-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>integrate MM tables into ACID: add separate ACID type</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14921" opendate="2016-10-10 00:00:00" fixdate="2016-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move slow CliDriver tests to MiniLlap - part 2</summary>
      <description>Continuation to HIVE-14877</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14922" opendate="2016-10-10 00:00:00" fixdate="2016-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add perf logging for post job completion steps</summary>
      <description>Mostly FS related operations.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14929" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding JDBC test for query cancellation scenario</summary>
      <description>There is some functional testing for query cancellation using JDBC which is missing in unit tests.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="14932" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle bucketing for MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="14933" opendate="2016-10-12 00:00:00" fixdate="2016-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>include argparse with LLAP scripts to support antique Python versions</summary>
      <description>The module is a standalone file, and it's under Python license that is compatible with Apache. In the long term we should probably just move LlapServiceDriver code entirely to Java, as right now it's a combination of part-py, part-java.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
    </fixedFiles>
  </bug>
  <bug id="14935" opendate="2016-10-12 00:00:00" fixdate="2016-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for beeline force option</summary>
      <description>Add unit test for beeline with force option to ensure continuation of running script even after errors.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="14938" opendate="2016-10-12 00:00:00" fixdate="2016-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add deployed ptest properties file to repo, update to remove isolated tests</summary>
      <description>The intent is to checkin the original file, and then modify it to remove isolated tests (and move relevant ones to the skipBatching list), which normally lead to stragglers, and sub-optimal resource utilization.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14942" opendate="2016-10-13 00:00:00" fixdate="2016-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 UI: Canceled queries show up in "Open Queries"</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14943" opendate="2016-10-13 00:00:00" fixdate="2016-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Base Implementation</summary>
      <description>Create the 1st pass functional implementation of MERGEThis should run e2e and produce correct results.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SelectClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="14948" opendate="2016-10-13 00:00:00" fixdate="2016-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>properly handle special characters in identifiers</summary>
      <description>The treatment of quoted identifiers in HIVE-14943 is inconsistent. Need to clean this up and if possible only quote those identifiers that need to be quoted in the generated SQL statement</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="14973" opendate="2016-10-15 00:00:00" fixdate="2016-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: TestJdbcWithSQLAuthorization.testBlackListedUdfUsage</summary>
      <description>Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table test_jdbc_sql_auth_udf already exists) at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:854) at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:862) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4052) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:340) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1988) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1679) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1410) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1136) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:248)</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthUDFBlacklist.java</file>
    </fixedFiles>
  </bug>
  <bug id="14992" opendate="2016-10-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate several common libraries in hive jdbc uber jar</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14996" opendate="2016-10-18 00:00:00" fixdate="2016-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle load for MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15025" opendate="2016-10-20 00:00:00" fixdate="2016-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure-Socket-Layer (SSL) support for HMS</summary>
      <description>HMS server should support SSL encryption. When the server is keberos enabled, the encryption can be enabled. But if keberos is not enabled, then there is no encryption between HS2 and HMS. Similar to HS2, we should support encryption in both cases.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIServiceFeatures.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15039" opendate="2016-10-24 00:00:00" fixdate="2016-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A better job monitor console output for HoS</summary>
      <description>When there're many stages, it's very difficult to read the console output of job progress of HoS. Attached screenshot is an example.We may learn from HoT as it does much better than HoS.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.InPlaceUpdates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15087" opendate="2016-10-27 00:00:00" fixdate="2016-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>integrate MM tables into ACID: replace "hivecommit" property with ACID property</summary>
      <description>Previously declared DDLcreate table t1 (key int, key2 int) tblproperties("hivecommit"="true");should be replaced with:create table t1 (key int, key2 int) tblproperties("transactional"="true", "transactional_properties"="insert_only");</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mm.concatenate.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.concatenate.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="15090" opendate="2016-10-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporary DB failure can stop ExpiredTokenRemover thread</summary>
      <description>In HIVE-13090 we decided that we should not close the metastore if there is an unexpected exception during the expired token removal process, but that fix leaves a running metastore without ExpiredTokenRemover thread.To fix this I will move the catch inside the running loop, and hope the thread could recover from the exception</description>
      <version>1.3.0,2.0.1,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="151" opendate="2008-12-10 00:00:00" fixdate="2008-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveQL Query execution bug: java.lang.NullPointerException</summary>
      <description>Executing a query ------------------------------------- query start ----------------------------------------------------SELECT t11.subject, t22.object , t33.subject , t55.object, t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66ON (t66.subject=t55.object)------------------------------------- query end ----------------------------------------------------on table ------------------------------------- table start ----------------------------------------------------CREATE TABLE triples (foo string,subject string, predicate string, object string, foo2 string)------------------------------------- table end -----------------------------------------------------gives the foolowing output ------------------------------------ console output ---------------------------------------------- INFO &amp;#91;main&amp;#93; (Driver.java:156) - Starting command: SELECT t11.subject, t22.object , t33.subject , t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33 ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44 ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object as obh FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55 ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66 ON (t66.subject=t55.obh) INFO &amp;#91;main&amp;#93; (ParseDriver.java:249) - Parsing command: SELECT t11.subject, t22.object , t33.subject , t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33 ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44 ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object as obh FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55 ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66 ON (t66.subject=t55.obh) INFO &amp;#91;main&amp;#93; (ParseDriver.java:263) - Parse Completed INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:126) - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore INFO &amp;#91;main&amp;#93; (ObjectStore.java:124) - ObjectStore, initialize called INFO &amp;#91;main&amp;#93; (ObjectStore.java:146) - found resource jpox.properties at file:/home/vseledkin/workspace/HiveDrv/bin/jpox.properties WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - Bundle "org.jpox" has an optional dependency to "org.eclipse.equinox.registry" but it cannot be resolved WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - Bundle "org.jpox" has an optional dependency to "org.eclipse.core.runtime" but it cannot be resolved INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - ================= Persistence Configuration =============== INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - JPOX Persistence Factory - Vendor: "JPOX" Version: "1.2.2" INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - JPOX Persistence Factory initialised for datastore URL="jdbc:derby:;databaseName=metastore_db;create=true" driver="org.apache.derby.jdbc.EmbeddedDriver" userName="APP" INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - =========================================================== INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Initialising Catalog "", Schema "APP" using "SchemaTable" auto-start option INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MDatabase since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - No manager for annotations was found in the CLASSPATH so all annotations are ignored. WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - MetaData Parser encountered an error in file "jar:file:/home/vseledkin/workspace/hive/build/hive_metastore.jar!/package.jdo" at line 282, column 13 : The content of element type "class" must match "(extension*,implements*,datastore-identity?,primary-key?,inheritance?,version?,join*,foreign-key*,index*,unique*,column*,field*,property*,query*,fetch-group*,extension*)". - Please check your specification of DTD and the validity of the MetaData XML that you have specified. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MStorageDescriptor since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MSerDeInfo since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MTable since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MPartition since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MDatabase &amp;#91;Table : DBS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MSerDeInfo &amp;#91;Table : SERDES, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStorageDescriptor &amp;#91;Table : SDS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTable &amp;#91;Table : TBLS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartition &amp;#91;Table : PARTITIONS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters &amp;#91;Table : SERDE_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.parameters &amp;#91;Table : PARTITION_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.values &amp;#91;Table : PARTITION_KEY_VALS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.parameters &amp;#91;Table : TABLE_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.partitionKeys &amp;#91;Table : PARTITION_KEYS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols &amp;#91;Table : BUCKETING_COLS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cols &amp;#91;Table : COLUMNS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters &amp;#91;Table : SD_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols &amp;#91;Table : SORT_COLS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 0 foreign key(s) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 index(es) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 foreign key(s) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 4 index(es) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 foreign key(s) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 4 index(es) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 0 foreign key(s) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Catalog "", Schema "APP" initialised - managing 14 classes INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - &gt;&gt; Found StoreManager org.jpox.store.rdbms.RDBMSManager INFO &amp;#91;main&amp;#93; (ObjectStore.java:110) - Initialized ObjectStore INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3086) - Starting Semantic Analysis INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3088) - Completed phase 1 of Semantic Analysis INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3091) - Completed getting MetaData in Semantic Analysis INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4, string reducesinkvalue5, string reducesinkvalue6, string reducesinkvalue7} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4, string reducesinkvalue5, string reducesinkvalue6, string reducesinkvalue7} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3107) - Completed partition pruning INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3111) - Completed sample pruning INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string temporarycol0, string temporarycol1, string temporarycol2, string temporarycol3, string temporarycol4} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3120) - Completed plan generation INFO &amp;#91;main&amp;#93; (Driver.java:173) - Semantic Analysis CompletedTotal MapReduce jobs = 3 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Total MapReduce jobs = 3Number of reducers = 1 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Number of reducers = 1In order to change numer of reducers use: INFO &amp;#91;main&amp;#93; (SessionState.java:254) - In order to change numer of reducers use: set mapred.reduce.tasks = &lt;number&gt; INFO &amp;#91;main&amp;#93; (SessionState.java:254) - set mapred.reduce.tasks = &lt;number&gt; WARN &amp;#91;main&amp;#93; (ExecDriver.java:109) - Number of reduce tasks not specified. Defaulting to jobconf value of: 1 INFO &amp;#91;main&amp;#93; (ExecDriver.java:238) - Adding input file /user/hive/warehouse/triples WARN &amp;#91;main&amp;#93; (JobClient.java:547) - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. INFO &amp;#91;main&amp;#93; (FileInputFormat.java:181) - Total input paths to process : 1Starting Job = job_200812091129_0144, Tracking URL = http://ubunder.avicomp.com:50030/jobdetails.jsp?jobid=job_200812091129_0144 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Starting Job = job_200812091129_0144, Tracking URL = http://ubunder.avicomp.com:50030/jobdetails.jsp?jobid=job_200812091129_0144Kill Command = /home/vseledkin/workspace/HiveDrv/programs/hadoop-0.19.0 job -Dmapred.job.tracker=ubunder.avicomp.com:9001 -kill job_200812091129_0144 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Kill Command = /home/vseledkin/workspace/HiveDrv/programs/hadoop-0.19.0 job -Dmapred.job.tracker=ubunder.avicomp.com:9001 -kill job_200812091129_0144 map = 0%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 0%, reduce =0% map = 50%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 50%, reduce =0% map = 100%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 100%, reduce =0% map = 100%, reduce =100% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 100%, reduce =100%ERROR &amp;#91;main&amp;#93; (SessionState.java:263) - Ended Job = job_200812091129_0144 with errorsEnded Job = job_200812091129_0144 with errorsFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.ExecDriverERROR &amp;#91;main&amp;#93; (SessionState.java:263) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.ExecDriver------------------------------------ console output end ----------------------------------------and the stack trace in hadoop logs ------------------------------------ stack trace ---------------------------------------------------java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:81) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:83) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:337) at org.apache.hadoop.mapred.Child.main(Child.java:155)------------------------------------ stack trace end ---------------------------------------------attached file contains table data to test problematic query</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15119" opendate="2016-11-3 00:00:00" fixdate="2016-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support standard syntax for ROLLUP &amp; CUBE</summary>
      <description>Standard ROLLUP and CUBE syntax is GROUP BY ROLLUP (expression list)... and GROUP BY CUBE (expression list) respectively. Currently HIVE only allows GROUP BY &lt;expression list&gt; WITH ROLLUP/CUBE syntax. We would like HIVE to support standard ROLLUP/CUBE syntax.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.id1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.grouping.sets.q</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.pushdown2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.infer.bucket.sort.grouping.operators.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.rollup1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.cube.multi.gby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.cube1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cte.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.annotate.stats.groupby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="15125" opendate="2016-11-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Parallelize slider package generator</summary>
      <description>The metastore init + download of functions takes approx 4 seconds.This is enough time to complete all the other operations in parallel.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="15129" opendate="2016-11-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP : Enhance cache hits for stripe metadata across queries</summary>
      <description>When multiple queries are run in LLAP, stripe metadata cache misses were observed even though enough memory was available. https://github.com/apache/hive/blob/master/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java#L655. Even in cases when data was found in cache, it wasn't getting used as globalnc changed from query to query. Creating a superset of existing indexes with globalInc would be helpful. This would be lot more beneficial in cloud storage where opening and reading small of data can be expensive compared to HDFS.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
    </fixedFiles>
  </bug>
  <bug id="1513" opendate="2010-8-5 00:00:00" fixdate="2010-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive starter scripts should load admin/user supplied script for configurability</summary>
      <description>it's difficult to add environment variables to Hive starter scripts except by modifying the scripts directly. this is undesirable (since they are source code). Hive starter scripts should load a admin supplied shell script for configurability. This would be similar to what hadoop does with hadoop-env.sh</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.util.execHiveCmd.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15137" opendate="2016-11-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore add partitions background thread should use current username</summary>
      <description>The background thread used in HIVE-13901 for adding partitions needs to be reinitialized with current UGI for each invocation. Otherwise the user in context while thread was created would be the current UGI during the actions in the thread.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.external2.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="1514" opendate="2010-8-5 00:00:00" fixdate="2010-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Be able to modify a partition&amp;#39;s fileformat and file location information.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.mix.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.diff.part.input.formats.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterPartitionProtectModeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15147" opendate="2016-11-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: use LLAP cache for non-columnar formats in a somewhat general way</summary>
      <description>The primary goal for the first pass is caching text files. Nothing would prevent other formats from using the same path, in principle, although, as was originally done with ORC, it may be better to have native caching support optimized for each particular format.Given that caching pure text is not smart, and we already have ORC-encoded cache that is columnar due to ORC file structure, we will transform data into columnar ORC.The general idea is to treat all the data in the world as merely ORC that was compressed with some poor compression codec, such as csv. Using the original IF and serde, as well as an ORC writer (with some heavyweight optimizations disabled, potentially), we can "uncompress" the csv/whatever data into its "original" ORC representation, then cache it efficiently, by column, and also reuse a lot of the existing code.Various other points:1) Caching granularity will have to be somehow determined (i.e. how do we slice the file horizontally, to avoid caching entire columns). As with ORC uncompressed files, the specific offsets don't really matter as long as they are consistent between reads. The problem is that the file offsets will actually need to be propagated to the new reader from the original inputformat. Row counts are easier to use but there's a problem of how to actually map them to missing ranges to read from disk.2) Obviously, for row-based formats, if any one column that is to be read has been evicted or is otherwise missing, "all the columns" have to be read for the corresponding slice to cache and read that one column. The vague plan is to handle this implicitly, similarly to how ORC reader handles CB-RG overlaps - it will just so happen that a missing column in disk range list to retrieve will expand the disk-range-to-read into the whole horizontal slice of the file.3) Granularity/etc. won't work for gzipped text. If anything at all is evicted, the entire file has to be re-read. Gzipped text is a ridiculous feature, so this is by design.4) In future, it would be possible to also build some form or metadata/indexes for this cached data to do PPD, etc. This is out of the scope for now.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.LlapWrappableInputFormatInterface.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.DebugUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.TreeReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RecordReaderImpl.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.PhysicalWriter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapDataBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionDispatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapIo.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15160" opendate="2016-11-8 00:00:00" fixdate="2016-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t order by an unselected column</summary>
      <description>If a grouping key hasn't been selected, Hive complains. For comparison, Postgres does not.Example. Notice i_item_id is not selected:select i_item_desc ,i_category ,i_class ,i_current_price ,sum(cs_ext_sales_price) as itemrevenue ,sum(cs_ext_sales_price)*100/sum(sum(cs_ext_sales_price)) over (partition by i_class) as revenueratio from catalog_sales ,item ,date_dim where cs_item_sk = i_item_sk and i_category in ('Jewelry', 'Sports', 'Books') and cs_sold_date_sk = d_date_sk and d_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30 days) group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratiolimit 100;</description>
      <version>2.0.0,2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15190" opendate="2016-11-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Field names are not preserved in ORC files written with ACID</summary>
      <description>To repro:drop table if exists orc_nonacid;drop table if exists orc_acid;create table orc_nonacid (a int) clustered by (a) into 2 buckets stored as orc;create table orc_acid (a int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');insert into table orc_nonacid values(1), (2);insert into table orc_acid values(1), (2);Running hive --service orcfiledump &lt;file&gt; on the files created by the insert statements above, you'll see that for orc_nonacid, the files have schema struct&lt;a:int&gt; whereas for orc_acid, the files have schema struct&lt;operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct&lt;_col0:int&gt;&gt;. The last field row should have schema struct&lt;a:int&gt;.</description>
      <version>2.1.0,2.2.0,3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15202" opendate="2016-11-15 00:00:00" fixdate="2016-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrent compactions for the same partition may generate malformed folder structure</summary>
      <description>If two compactions run concurrently on a single partition, it may generate folder structure like this: (nested base dir)drwxr-xr-x - root supergroup 0 2016-11-14 22:23 /user/hive/warehouse/test/z=1/base_0000007/base_0000007-rw-r--r-- 3 root supergroup 201 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00000-rw-r--r-- 3 root supergroup 611 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00001-rw-r--r-- 3 root supergroup 614 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00002-rw-r--r-- 3 root supergroup 621 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00003-rw-r--r-- 3 root supergroup 621 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00004-rw-r--r-- 3 root supergroup 201 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00005-rw-r--r-- 3 root supergroup 201 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00006-rw-r--r-- 3 root supergroup 201 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00007-rw-r--r-- 3 root supergroup 201 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00008-rw-r--r-- 3 root supergroup 201 2016-11-14 21:46 /user/hive/warehouse/test/z=1/base_0000007/bucket_00009</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="15207" opendate="2016-11-15 00:00:00" fixdate="2016-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a capability to detect incorrect sequence numbers</summary>
      <description>We have seen next sequence number is smaller than the max(id) for certain tables. Seems it's caused by thread-safe issue in HMS, but still not sure if it has been fully fixed. Try to detect such issue.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15208" opendate="2016-11-15 00:00:00" fixdate="2016-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query string should be HTML encoded for Web UI</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.jamon.org.apache.hive.tmpl.QueryProfileTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="15211" opendate="2016-11-15 00:00:00" fixdate="2016-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide support for complex expressions in ON clauses for INNER joins</summary>
      <description>Currently, we have some restrictions on the predicates that we can use in ON clauses for inner joins (we have those restrictions for outer joins too, but we will tackle that in a follow-up). Semantically equivalent queries can be expressed if the predicate is introduced in the WHERE clause, but we would like that user can express it both in ON and WHERE clause, as in standard SQL.This patch is an extension to overcome these restrictions for inner joins.It will allow to write queries that currently fail in Hive such as:-- DisjunctionsSELECT *FROM src1 JOIN srcON (src1.key=src.key OR src1.value between 100 and 102 OR src.value between 100 and 102)LIMIT 10;-- Conjunction with multiple inputs references in one sideSELECT *FROM src1 JOIN srcON (src1.key+src.key &gt;= 100 AND src1.key+src.key &lt;= 102)LIMIT 10;-- Conjunct with no referencesSELECT *FROM src1 JOIN srcON (src1.value between 100 and 102 AND src.value between 100 and 102 AND true)LIMIT 10;</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinCondTypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="15212" opendate="2016-11-15 00:00:00" fixdate="2016-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>merge branch into master</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mm.truncate.cols.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.truncate.cols.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ImportCommitTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.insertonly.acid.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.bucket.convert.q</file>
      <file type="M">ql.src.test.results.clientnegative.mm.bucket.convert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15217" opendate="2016-11-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add watch mode to llap status tool</summary>
      <description>There is few seconds overhead for launching the llap status command. To avoid we can add "watch" mode to llap status tool that refreshes the status after configured interval.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15220" opendate="2016-11-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat test driver not capturing end time of test accurately</summary>
      <description>Webhcat e2e testsuite prints message while ending test run:Ending test &lt;testcase&gt; at 1479264720Currently it is not capturing the end time correctly.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="15233" opendate="2016-11-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF UUID() should be non-deterministic</summary>
      <description>The UUID() function should be non-deterministic.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUUID.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUUID.java</file>
    </fixedFiles>
  </bug>
  <bug id="15242" opendate="2016-11-18 00:00:00" fixdate="2016-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Act on Node update notifications from registry, fix isAlive checks</summary>
      <description>isAlive checks are currently completely broken, since the ZK registry does not update existing ServiceIsntances. Instead it creates new instances each time.This causes non-existant nodes to be used for scheduling in case of node failures.Also, act on the notifications sent by the registry about nodes going down, or new nodes being added. (new nodes already handled)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstance.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.InactiveServiceInstance.java</file>
    </fixedFiles>
  </bug>
  <bug id="15263" opendate="2016-11-22 00:00:00" fixdate="2016-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detect the values for incorrect NULL values</summary>
      <description>We have seen the incorrect NULL values for SD_ID in TBLS for the hive tables. That column can be null since it will be NULL for hive views.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15273" opendate="2016-11-23 00:00:00" fixdate="2016-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid http client not configured correctly</summary>
      <description>Current used http client by the druid-hive record reader is constructed with default values. Default values of numConnection and ReadTimeout are very small which can lead to following exception " ERROR &amp;#91;2ee34a2b-c8a5-4748-ab91-db3621d2aa5c main&amp;#93; CliDriver: Failed with exception java.io.IOException:java.io.IOException: java.io.IOException: org.apache.hive.druid.org.jboss.netty.channel.ChannelException: Channel disconnected"Full stack can be found here.https://gist.github.com/b-slim/384ca6a96698f5b51ad9b171cff556a2</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.HiveDruidQueryBasedInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15279" opendate="2016-11-24 00:00:00" fixdate="2016-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>map join dummy operators are not set up correctly in certain cases with merge join</summary>
      <description>As a result, MapJoin is not initialized and there's NPE later.Tez-specific.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15284" opendate="2016-11-25 00:00:00" fixdate="2016-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add junit test to test replication scenarios</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15295" opendate="2016-11-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HCatalog javadoc generation with Java 8</summary>
      <description>Realized while generating artifacts for Hive 2.1.1 release.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug id="15307" opendate="2016-11-29 00:00:00" fixdate="2016-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive MERGE: "when matched then update" allows invalid column names.</summary>
      <description>create table target ( id int, val int)CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ("transactional"="true");create table source2 ( id int, val int);insert into source2 values (2, 25), (3, 35), (4, 45);merge into targetusing source2 sub on sub.id = target.idwhen matched then update set invalid = sub.val;</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="15327" opendate="2016-12-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Outerjoin might produce wrong result depending on joinEmitInterval value</summary>
      <description>If joinEmitInterval is smaller than the group size, outerjoins might produce records with NULL appended values multiple times (once per group).HIVE-4689 targeted the same problem. However, the fix does not seem to cover all cases (in particular, it will not apply to left outer joins with filter conditions on the left input). The solution in HIVE-4689 was to disable (override) joinEmitInterval value for those cases. This fix follows the same approach.To reproduce the problem:set hive.strict.checks.cartesian.product=false;set hive.join.emit.interval=1;CREATE TABLE test1 (key INT, value INT, col_1 STRING);INSERT INTO test1 VALUES (99, 0, 'Alice');INSERT INTO test1 VALUES (99, 2, 'Mat');INSERT INTO test1 VALUES (100, 1, 'Bob');INSERT INTO test1 VALUES (101, 2, 'Car');CREATE TABLE test2 (key INT, value INT, col_2 STRING);INSERT INTO test2 VALUES (102, 2, 'Del');INSERT INTO test2 VALUES (103, 2, 'Ema');INSERT INTO test2 VALUES (104, 3, 'Fli');-- Equi-condition and condition on one input (left outer join)SELECT *FROM test1 LEFT OUTER JOIN test2ON (test1.value=test2.value AND test1.key between 100 and 102)LIMIT 10;-- Condition on one input (left outer join)SELECT *FROM test1 LEFT OUTER JOIN test2ON (test1.key between 100 and 102)LIMIT 10;For the first query, current (incorrect) result is: 99 0 Alice NULL NULL NULL 100 1 Bob NULL NULL NULL 101 2 Car 103 2 Ema 99 2 Mat NULL NULL NULL 101 2 Car 102 2 Del 99 2 Mat NULL NULL NULLExpected (correct) result is: 99 0 Alice NULL NULL NULL 100 1 Bob NULL NULL NULL 101 2 Car 103 2 Ema 101 2 Car 102 2 Del 99 2 Mat NULL NULL NULLFor the second query, current (incorrect) result is: 101 2 Car 104 3 Fli 100 1 Bob 104 3 Fli 99 2 Mat NULL NULL NULL 99 0 Alice NULL NULL NULL 101 2 Car 103 2 Ema 100 1 Bob 103 2 Ema 99 2 Mat NULL NULL NULL 99 0 Alice NULL NULL NULL 101 2 Car 102 2 Del 100 1 Bob 102 2 DelExpected (correct) result is: 101 2 Car 104 3 Fli 101 2 Car 103 2 Ema 101 2 Car 102 2 Del 100 1 Bob 104 3 Fli 100 1 Bob 103 2 Ema 100 1 Bob 102 2 Del 99 2 Mat NULL NULL NULL 99 0 Alice NULL NULL NULL</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15347" opendate="2016-12-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Executor memory and Xmx should have some headroom for other services</summary>
      <description>If executor memory + cache memory is configured close or equal to Xmx, the task attempts that is causing OOM can take down the LLAP daemon. Provide some leeway for other services during memory crunch.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1535" opendate="2010-8-13 00:00:00" fixdate="2010-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter partition should throw exception if the specified partition does not exist.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15351" opendate="2016-12-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable vectorized VectorUDFAdaptor usage with non-column or constant parameters</summary>
      <description>Vectorization using VectorUDFAdaptor is broken and produces wrong results when the parameter(s) have vectorized expressions that allocate scratch columns. So, for now, we restrict VectorUDFAdaptor usage to columns or constant expressions.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.adaptor.usage.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.when.case.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15355" opendate="2016-12-4 00:00:00" fixdate="2016-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues during parallel moveFile due to HDFSUtils.setFullFileStatus</summary>
      <description>It is possible to run into concurrency issues during multi-threaded moveFile issued when processing queries like INSERT OVERWRITE TABLE ... SELECT .. when there are multiple files in the staging directory which is a subdirectory of the target directory. The issue is hard to reproduce but following stacktrace is one such example:INFO : Loading data to table functional_text_gzip.alltypesaggmultifilesnopart from hdfs://localhost:20500/test-warehouse/alltypesaggmultifilesnopart_text_gzip/.hive-staging_hive_2016-12-01_19-58-21_712_8968735301422943318-1/-ext-10000ERROR : Failed with exception java.lang.ArrayIndexOutOfBoundsExceptionorg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2858) at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:3124) at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1701) at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:313) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1200) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)Getting log thread is interrupted, since query is done! at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ArrayIndexOutOfBoundsException at java.lang.System.arraycopy(Native Method) at java.util.ArrayList.removeRange(ArrayList.java:616) at java.util.ArrayList$SubList.removeRange(ArrayList.java:1021) at java.util.AbstractList.clear(AbstractList.java:234) at com.google.common.collect.Iterables.removeIfFromRandomAccessList(Iterables.java:213) at com.google.common.collect.Iterables.removeIf(Iterables.java:184) at org.apache.hadoop.hive.shims.Hadoop23Shims.removeBaseAclEntries(Hadoop23Shims.java:865) at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:757) at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2835) at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2828) ... 4 moreERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTaskQuick online search also shows some other instances like the one mentioned in http://stackoverflow.com/questions/38900333/get-concurrentmodificationexception-in-step-2-create-intermediate-flat-hive-tabThe issue seems to be coming from the below code :if (aclEnabled) { aclStatus = sourceStatus.getAclStatus(); if (aclStatus != null) { LOG.trace(aclStatus.toString()); aclEntries = aclStatus.getEntries(); removeBaseAclEntries(aclEntries); //the ACL api's also expect the tradition user/group/other permission in the form of ACL aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.USER, sourcePerm.getUserAction())); aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.GROUP, sourcePerm.getGroupAction())); aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.OTHER, sourcePerm.getOtherAction())); } }removeBaseAclEntries removes objects from List&lt;AclEntry&gt; aclEntries When HDFSUtils.setFullFileStatus() method is called from multiple threads like from https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2835 it is possible that multiple threads try to modify the List&lt;AclEntry&gt; aclEntries leading to concurrency issues. We should either move that block into a thread-safe region or call setFullFileStatus when all the threads converge.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15360" opendate="2016-12-5 00:00:00" fixdate="2016-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nested column pruning: add pruned column paths to explain output</summary>
      <description>We should add the pruned nested column paths to the explain output for easier tracing and debugging.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="15362" opendate="2016-12-5 00:00:00" fixdate="2016-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the missing fields for 2.2.0 upgrade scripts</summary>
      <description>The 2.2.0 upgrade scripts were cut on 05/25/16, while HIVE-13354 (which added some fields to upgrade scripts) was committed to master on 05/27/16, and there's no conflict. So we accidentally missed those fields for 2.2.0.cc ekoifman</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.2.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-2.2.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="15363" opendate="2016-12-5 00:00:00" fixdate="2016-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute hive-blobstore tests using ProxyLocalFileSystem</summary>
      <description>The hive-blobstore directory contains tests that an only be executed on blobstorage systems currently. These test are run manually by committers.To automate these tests on HiveQA, we should allow hive-blobstore to use the ProxyLocalFileSystem to run more test coverage on the pre-commit jenkins jobs.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.hive-blobstore.src.test.resources.hive-site.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15366" opendate="2016-12-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental INSERT events</summary>
      <description>Follow up of HIVE-15294. Once we capture the metadata, we should implement the LOAD/DUMP part</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15370" opendate="2016-12-6 00:00:00" fixdate="2016-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include Join residual filter expressions in user level EXPLAIN</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="15383" opendate="2016-12-7 00:00:00" fixdate="2016-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add additional info to &amp;#39;desc function extended&amp;#39; output</summary>
      <description>Add additional info to the output to 'desc function extended'. The resources would be helpful for the user to check which jars are referred.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.replicate.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.short.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.long.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.int.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.boolean.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.weekofyear.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.var.samp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.var.pop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.variance.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.upper.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unhex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ucase.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.utc.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.tinyint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.tan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sum.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.subtract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substring.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substring.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.struct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.stddev.samp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.stddev.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sqrt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.soundex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.smallint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sign.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sha2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sha1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rtrim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rlike.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.replace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.regexp.replace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.regexp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rand.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.radians.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.quarter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.printf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.power.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pow.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.positive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.PI.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nullif.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.notequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.next.day.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.named.struct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.months.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.month.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.modulo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.min.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.md5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.max.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.show.last.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.show.first.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.last.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.first.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ltrim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.logged.in.user.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.log2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.log10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.log.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ln.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.levenshtein.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lessthanorequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lessthan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lcase.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.last.day.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.java.method.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.int.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.initcap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greaterthanorequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greaterthan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.utc.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.format.number.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.floor.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.find.in.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.field.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.factorial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.exp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.equal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.E.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.divide.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.div.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.degrees.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.decode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.dayofmonth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.day.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.date.sub.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.date.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.date.add.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.datediff.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.current.user.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.crc32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.cos.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.conv.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.row.sequence.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udf.percentile2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.udf.max.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udaf.collect.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udf.max.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udf.min.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.collect.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.corr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.covar.pop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.covar.samp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.acos.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.add.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.add.months.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.aes.decrypt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.aes.encrypt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.array.contains.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.asin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.atan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.avg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bigint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.shiftleft.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.shiftright.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.shiftrightunsigned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.xor.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.boolean.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.cbrt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ceil.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ceiling.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.chr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15391" opendate="2016-12-8 00:00:00" fixdate="2016-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Location validation for table should ignore the values for view.</summary>
      <description>When use schematool to do location validation, we got error message for views, for example:n DB with Name: viewaNULL Location for TABLE with Name: viewaIn DB with Name: viewaNULL Location for TABLE with Name: viewbIn DB with Name: viewa</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15393" opendate="2016-12-8 00:00:00" fixdate="2016-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Guava version</summary>
      <description>Druid base code is using newer version of guava 16.0.1 that is not compatible with the current version used by Hive.FYI Hadoop project is moving to Guava 18 not sure if it is better to move to guava 18 or even 19.https://issues.apache.org/jira/browse/HADOOP-10101</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonIOInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.util.ElapsedTimeLoggingWrapper.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCliDriver.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmPauseMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmMetricsInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="15395" opendate="2016-12-8 00:00:00" fixdate="2016-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t try to intern strings from empty map</summary>
      <description>Otherwise it unnecessarily create another map object.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15403" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Login with kerberos before starting the daemon</summary>
      <description>In LLAP cluster, if some of the nodes are kinit'ed with some user (other than default hive user) and some nodes are kinit'ed with hive user, both will end up in different paths under zk registry and may not be reported by the llap status tool. The reason for that is when creating zk paths we use UGI.getCurrentUser() but current user may not be same across all nodes (someone has to do global kinit). Before bringing up the daemon, if security is enabled each daemons should login based on specified kerberos principal and keytab for llap daemon service and update the current logged in user.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15407" opendate="2016-12-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add distcp to classpath by default, because hive depends on it.</summary>
      <description>when i run hive queries, i get errors as followjava.lang.NoClassDefFoundError: org/apache/hadoop/tools/DistCpOptions...I dig into code, and find that hive depends on distcp ,but distcp is not in classpath by default.I think if adding distcp to hadoop classpath by default in hadoop project, but hadoop committers will not do that. discussions in HADOOP-13865 . They propose that Resolving this problem on HIVESo i add distcp to classpath on HIVE</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="15409" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for GROUPING function with grouping sets</summary>
      <description>The grouping(col_expr) function indicates whether a given column is aggregated in each row.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.multi.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query70.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query67.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query27.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query22.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="15410" opendate="2016-12-9 00:00:00" fixdate="2016-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat supports get/set table property with its name containing period and hyphen</summary>
      <description>Hive table properties could have period (.) or hyphen in their names, auto.purge is one of the examples. But WebHCat APIs does not support either set or get these properties, and they throw out the error msg ""Invalid DDL identifier :property". For example:[root@ctang-1 ~]# curl -s 'http://ctang-1.gce.cloudera.com:7272/templeton/v1/ddl/database/default/table/sample_07/property/prop.key1?user.name=hiveuser'{"error":"Invalid DDL identifier :property"}[root@ctang-1 ~]# curl -s -X PUT -HContent-type:application/json -d '{ "value": "true" }' 'http://ctang-1.gce.cloudera.com:7272/templeton/v1/ddl/database/default/table/sample_07/property/prop.key2?user.name=hiveuser/'{"error":"Invalid DDL identifier :property"}This patch is going to add the supports to the property name containing period and/or hyphen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestServer.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug id="15417" opendate="2016-12-12 00:00:00" fixdate="2016-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Glitches using ACID&amp;#39;s row__id hidden column</summary>
      <description>This only works if you turn PPD off.drop table if exists hello_acid;create table hello_acid (key int, value int)partitioned by (load_date date)clustered by(key) into 3 bucketsstored as orc tblproperties ('transactional'='true');insert into hello_acid partition (load_date='2016-03-01') values (1, 1);insert into hello_acid partition (load_date='2016-03-02') values (2, 2);insert into hello_acid partition (load_date='2016-03-03') values (3, 3);hive&gt; set hive.optimize.ppd=true;hive&gt; select tid from (select row__id.transactionid as tid from hello_acid) sub where tid = 15;FAILED: SemanticException MetaException(message:cannot find field row__id from [0:load_date])hive&gt; set hive.optimize.ppd=false;hive&gt; select tid from (select row__id.transactionid as tid from hello_acid) sub where tid = 15;OKtid15Time taken: 0.075 seconds, Fetched: 1 row(s)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15419" opendate="2016-12-12 00:00:00" fixdate="2016-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate out storage-api to be released independently</summary>
      <description>Currently, the Hive project releases a single monolithic release, but this makes file formats reading directly into Hive's vector row batches a circular dependence. Storage-api is a small module with the vectorized row batches and SearchArgument that are necessary for efficient vectorized read and write. By releasing storage-api independently, we can make an interface that the file formats can read and write from.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestStructColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.TimestampUtils.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimalImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.Pool.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.MemoryBuffer.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DataCache.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.Allocator.java</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">orc.pom.xml</file>
      <file type="M">common.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15420" opendate="2016-12-12 00:00:00" fixdate="2016-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP UI: Relativize resources to allow proxied/secured views</summary>
      <description>If the UI is secured behind a gateway firewall instance, this allows for the UI to function with a base URL like http://&lt;gateway&gt;/proxy/NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.metrics.js</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.index.html</file>
    </fixedFiles>
  </bug>
  <bug id="15425" opendate="2016-12-13 00:00:00" fixdate="2016-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate conflicting output from schematool&amp;#39;s table validator.</summary>
      <description>Running the schemaTool's validate command against Derby DB yields in the following output.Validating tables in the schema for version 2.2.0Expected (from schema definition) 57 tables, Found (from HMS metastore) 58 tablesSchema table validation successfulThe output above creates some confusion when there are extra tables (not part of hive schema) in the database. The intention was to report the total tables found and did not expect the schema namespace to contain additional tables. Even as the validation is successful, the output is confusing.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15426" opendate="2016-12-13 00:00:00" fixdate="2016-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix order guarantee of event executions for REPL LOAD</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="15428" opendate="2016-12-14 00:00:00" fixdate="2016-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS DPP doesn&amp;#39;t remove cyclic dependency</summary>
      <description>More details in HIVE-15357</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkRemoveDynamicPruningBySize.java</file>
    </fixedFiles>
  </bug>
  <bug id="15430" opendate="2016-12-14 00:00:00" fixdate="2016-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change SchemaTool table validator to test based on the dbType</summary>
      <description>Currently the validator parses the "oracle" schema file to determine what tables are expected in the database. (mostly because of ease of parsing the schema file compared to other syntax). We have learnt from HIVE-15118, that not all schema files have the same amount of tables. For example, derby has an old table that is never used that other DBs do not contain).</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15439" opendate="2016-12-15 00:00:00" fixdate="2016-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support INSERT OVERWRITE for internal druid datasources.</summary>
      <description>Add support for SQL statement INSERT OVERWRITE TABLE druid_internal_table.In order to add this support will need to add new post insert hook to update the druid metadata. Creation of the segment will be the same as CTAS.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.DruidRecordWriterTest.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDerbyConnector.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DruidStorageHandlerTest.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15445" opendate="2016-12-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Subquery failing with ClassCastException</summary>
      <description>To reproduce:CREATE TABLE table_7 (int_col INT);SELECT(t1.int_col) * (t1.int_col) AS int_colFROM (SELECTMIN(NULL) OVER () AS int_colFROM table_7) t1WHERE(False) NOT IN (SELECTFalse AS boolean_colFROM (SELECTMIN(NULL) OVER () AS int_colFROM table_7) tt1WHERE(t1.int_col) = (tt1.int_col));The problem seems to be in the method that tries to resolve the subquery column MIN(NULL). It checks the column inspector and ends up returning a constant expression instead of a column expression for min(null).</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15446" opendate="2016-12-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive fails in recursive debug</summary>
      <description>When running hive recursive debug mode, for example,./bin/hive --debug:port=10008,childSuspend=yIt fails with error msg:&amp;#8211;ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.Error occurred during initialization of VMagent library failed to init: jdwp&amp;#8211;It is because HADOOP_OPTS and HADOOP_CLIENT_OPTS both have jvm debug options when invoking HADOOP.sh for the child process. The HADOOP_CLIENT_OPTS is appended to HADOOP_OPTS in HADOOP.sh which leads to the duplicated debug options.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15448" opendate="2016-12-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ChangeManager for replication</summary>
      <description>The change manager implementation as described in https://cwiki.apache.org/confluence/display/Hive/HiveReplicationv2Development#HiveReplicationv2Development-Changemanagement. This issue tracks the infrastructure code. Hooking to actions will be tracked in other ticket.ReplChangeManager includes: method to generate checksum method to convert file path to cm path method to move table/partition/file into cm thread to clear cm files if expires</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15459" opendate="2016-12-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix unit test failures on master</summary>
      <description>Golden file updates missed in HIVE-15397 and HIVE-15192</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.subquery.shared.alias.q</file>
      <file type="M">ql.src.test.queries.clientnegative.subquery.nested.subquery.q</file>
    </fixedFiles>
  </bug>
  <bug id="15472" opendate="2016-12-20 00:00:00" fixdate="2016-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Standalone jar is missing ZK dependencies</summary>
      <description>Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/curator/RetryPolicy at org.apache.hive.jdbc.Utils.configureConnParams(Utils.java:514) at org.apache.hive.jdbc.Utils.parseURL(Utils.java:434) at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:132) at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107) at java.sql.DriverManager.getConnection(DriverManager.java:664) at java.sql.DriverManager.getConnection(DriverManager.java:247) at JDBCExecutor.getConnection(JDBCExecutor.java:65) at JDBCExecutor.executeStatement(JDBCExecutor.java:104) at JDBCExecutor.executeSQLFile(JDBCExecutor.java:81) at JDBCExecutor.main(JDBCExecutor.java:183)Caused by: java.lang.ClassNotFoundException: org.apache.curator.RetryPolicy at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15478" opendate="2016-12-20 00:00:00" fixdate="2016-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add file + checksum list for create table/partition during notification creation (whenever relevant)</summary>
      <description>Currently, file list is being generated during REPL DUMP which will result in inconsistent data getting captured. This ticket is used for event dumping. Bootstrap dump checksum will be in a different Jira.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15482" opendate="2016-12-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: When pre-emption is disabled task scheduler gets into loop</summary>
      <description>When pre-emption is disabled and when number of slots is 0, the scheduler can get into a loop trying to schedule the tasks without actually waiting for free slots.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="15483" opendate="2016-12-21 00:00:00" fixdate="2016-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Database and table name is case sensitive when used in show grant</summary>
      <description>When use SQLStdAuth, db name and table name is case sensitive when use show grant command.0: jdbc:hive2://localhost:21066/&gt; show grant on table p1;+-----------+--------+------------+---------+----------------------+-----------------+------------+---------------+----------------+----------+--+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |+-----------+--------+------------+---------+----------------------+-----------------+------------+---------------+----------------+----------+--+| default | p1 | | | userx | USER | DELETE | true | 1481872357000 | userx || default | p1 | | | userx | USER | INSERT | true | 1481872357000 | userx || default | p1 | | | userx | USER | SELECT | true | 1481872357000 | userx || default | p1 | | | userx | USER | UPDATE | true | 1481872357000 | userx |+-----------+--------+------------+---------+----------------------+-----------------+------------+---------------+----------------+----------+--+7 rows selected (0.158 seconds)0: jdbc:hive2://localhost:21066/&gt; show grant on table P1;+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------+----------+--+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------+----------+--++-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------+----------+--+No rows selected (9.608 seconds)0: jdbc:hive2://localhost:21066/&gt; show grant on table defaulT.p1;+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------+----------+--+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------+----------+--++-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------+----------+--+No rows selected (0.06 seconds)</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="15484" opendate="2016-12-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix PerfCliDriver test failures in master</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15485" opendate="2016-12-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate the DoAs failure in HoS</summary>
      <description>With DoAs enabled, HoS failed with following errors:Exception in thread "main" org.apache.hadoop.security.AccessControlException: systest tries to renew a token with renewer hive at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.renewToken(AbstractDelegationTokenSecretManager.java:484) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:7543) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:555) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.renewDelegationToken(AuthorizationProviderProxyClientProtocol.java:674) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:999) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2141) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1783) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2135)It is related to the change from HIVE-14383. It looks like that SparkSubmit logs in Kerberos with passed in hive principal/keytab and then tries to create a hdfs delegation token for user systest with renewer hive.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15487" opendate="2016-12-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Improvements to random selection while scheduling</summary>
      <description>Currently llap scheduler, picks up random host when no locality information is specified or when all requested hosts are busy serving other requests with forced locality. In such cases, we can pick up the next available node in consistent order to get better locality instead of random selection.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="15488" opendate="2016-12-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Native Vector MapJoin fails when trying to serialize BigTable rows that have (unreferenced) complex types</summary>
      <description>When creating VectorSerializeRow we need to exclude any complex types.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15489" opendate="2016-12-21 00:00:00" fixdate="2016-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alternatively use table scan stats for HoS</summary>
      <description>For MapJoin in HoS, we should provide an option to only use stats in the TS rather than the populated stats in each of the join branch. This could be pretty conservative but more reliable.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkRemoveDynamicPruningBySize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15525" opendate="2016-12-30 00:00:00" fixdate="2016-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hooking ChangeManager to "drop table", "drop partition"</summary>
      <description>When Hive "drop table"/"drop partition", we will move data files into cmroot in case the replication destination will need it.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestWarehousePartitionHelper.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15529" opendate="2017-1-2 00:00:00" fixdate="2017-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: TaskSchedulerService can get stuck when scheduling tasks as disabled node is not re-enabled in NodeEnablerCallable</summary>
      <description>Easier way to simulate the issue:1. Start hive cli with "--hiveconf hive.execution.mode=llap"2. Run a sql script file (e.g sql script containing tpc-ds queries)3. In the middle of the run, press "ctrl+C" which would interrupt the current job. This should not exit the hive cli yet.4. After sometime, launch the same SQL script in same cli. This would get stuck indefinitely (waiting for computing the splits).Even when cli is quit, AM runs forever until explicitly killed. Issue seems to be around LlapTaskSchedulerService::schedulePendingTasks dealing with the loop when it encounters DELAYED_RESOURCES on task scheduling.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15538" opendate="2017-1-4 00:00:00" fixdate="2017-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test HIVE-13884 with more complex query predicates</summary>
      <description>HIVE-13884 introduced a new property hive.metastore.limit.partition.request. It would be good to have more tests to cover the cases where the query predicates (such as like, in) could not be pushed down to see if the fail back from directsql to ORM works properly if hive.metastore.try.direct.sql is enabled.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="15543" opendate="2017-1-5 00:00:00" fixdate="2017-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t try to get memory/cores to decide parallelism when Spark dynamic allocation is enabled</summary>
      <description>Presently Hive tries to get numbers for memory and cores from the Spark application and use them to determine RS parallelism. However, this doesn't make sense when Spark dynamic allocation is enabled because the current numbers doesn't represent available computing resources, especially when SparkContext is initially launched.Thus, it makes send not to do that when dynamic allocation is enabled.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
    </fixedFiles>
  </bug>
  <bug id="15579" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support HADOOP_PROXY_USER for secure impersonation in hive metastore client</summary>
      <description>Hadoop clients support HADOOP_PROXY_USER for secure impersonation. It would be useful to have similar feature for hive metastore client.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15580" opendate="2017-1-11 00:00:00" fixdate="2017-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate unbounded memory usage for orderBy and groupBy in Hive on Spark</summary>
      <description>Currently, orderBy (sortBy) and groupBy in Hive on Spark uses unbounded memory. For orderBy, Hive accumulates key groups using ArrayList (described in HIVE-15527). For groupBy, Hive currently uses Spark's groupByKey operator, which has a shortcoming of not being able to spill to disk within a key group. Thus, for large key group, memory usage is also unbounded.It's likely that this will impact performance. We will profile and optimize afterwards. We could also make this change configurable.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.top.level.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ReduceTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GroupByShuffler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15590" opendate="2017-1-12 00:00:00" fixdate="2017-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add separate spnego principal config for LLAP Web UI</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15612" opendate="2017-1-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include Calcite dependency in Druid storage handler jar</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15619" opendate="2017-1-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column pruner should handle DruidQuery</summary>
      <description>Even when we cannot push any operator into Druid, we might be able to prune some of the columns that are read from the Druid sources.One solution would be to extend the ColumnPruner so it can push the needed columns into DruidQuery.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druid.basic2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15621" opendate="2017-1-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Hive&amp;#39;s own JvmPauseMonitor instead of Hadoop&amp;#39;s in LLAP</summary>
      <description>This is to avoid dependency on Hadoop's JvmPauseMonitor since Hive already has its own implementation. HiveServer2 is already using Hive's .Need to follow up in HIVE-15644 to add the 3 missing JVM metrics for LLAP.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="15623" opendate="2017-1-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use customized version of netty for llap</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15625" opendate="2017-1-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>escape1 test fails on Mac</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.files.escapetest.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15630" opendate="2017-1-15 00:00:00" fixdate="2017-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add operation handle before operation.run instead of after operation.run</summary>
      <description>Add operation handle before operation.run instead of after operation.run. So when session is closed, all the running operations from operation.run can also be closed.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15631" opendate="2017-1-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize for hive client logs , you can filter the log for each session itself.</summary>
      <description>We have several hadoop cluster, about 15 thousand nodes. Every day we use hive to submit above 100 thousand jobs. So we have a large file of hive logs on every client host every day, but i don not know the logs of my session submitted was which line. So i hope to print the hive.session.id on every line of logs, and then i could use grep to find the logs of my session submitted.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="15642" opendate="2017-1-16 00:00:00" fixdate="2017-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replicate Insert Overwrites, Dynamic Partition Inserts and Loads</summary>
      <description>1. Insert Overwrites to a new partition should not capture new files as part of insert event but instead use the subsequent add partition event to capture the files + checksums.2. Insert Overwrites to an existing partition should capture new files as part of the insert event. Similar behaviour for DP inserts and loads.This will need changes from HIVE-15478</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="15644" opendate="2017-1-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect LLAP&amp;#39;s JVM metrics via Hive&amp;#39;s JvmPauseMonitor</summary>
      <description>Similar to what Hadoop's JvmMetrics is doing</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="15645" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez session pool may restart sessions in a wrong queue</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="15649" opendate="2017-1-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO may NPE on all-column read</summary>
      <description>It seems like very few paths use READ_ALL_COLUMNS config, but some do. LLAP IO doesn't account for that.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15650" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Set perflogger to DEBUG level for llap daemons</summary>
      <description>During Hive2 dev, the PerfLogger was moved to DEBUG levels only making it impossible to debug timings from LLAP logs without manually editing log4j2.properties and redeploying LLAP.Enable PerfLogger by default on LLAP.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15651" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: llap status tool enhancements</summary>
      <description>Per sseth following enhancements can be made to llap status tool1) If state changes from an ACTIVE state to STOPPED - terminate the script immediately (fail fast)2) Add a threshold of what is acceptable in terms of the running state - RUNNING_PARTIAL may be ok if 80% nodes are up for example.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15655" opendate="2017-1-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer: Allow config option to disable n-way JOIN merging</summary>
      <description>N-way Joins in Tez produce bad runtime plans whenever they are left-outer joins with map-joins.This is something which should have a safety setting.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15661" opendate="2017-1-18 00:00:00" fixdate="2017-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add security error logging to LLAP</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
    </fixedFiles>
  </bug>
  <bug id="15663" opendate="2017-1-19 00:00:00" fixdate="2017-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more interval tests to HivePerfCliDriver</summary>
      <description>following HIVE-13557</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query12.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query98.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query82.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query80.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query72.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query12.q</file>
    </fixedFiles>
  </bug>
  <bug id="15664" opendate="2017-1-19 00:00:00" fixdate="2017-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP text cache: improve first query perf I</summary>
      <description>1) Don't use ORC dictionary.2) Use VectorDeserialize.3) Don't parse the columns that are not included (cannot avoid reading them).4) Send VRB to the pipeline and write ORC in parallel (in background). HIVE-15672Also add an option to disable the encoding pipeline server-side.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.GenericColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionDispatcher.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15668" opendate="2017-1-19 00:00:00" fixdate="2017-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change REPL DUMP syntax to use "LIMIT" instead of "BATCH" keyword</summary>
      <description>Currently, REPL DUMP syntax goes:REPL DUMP [&lt;dbname&gt;[.&lt;tblname&gt;]] [FROM &lt;eventid&gt; [BATCH &lt;batchSize&gt;]]The BATCH directive says that when doing an event dump, to not dump out more than batchSize number of events. However, there is a clearer keyword for the same effect, and that is LIMIT. Thus, rephrasing the syntax as follows makes it clearer:REPL DUMP [&lt;dbname&gt;[.&lt;tblname&gt;]] [FROM &lt;eventid&gt; [LIMIT &lt;maxEventLimit&gt;]]</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="15669" opendate="2017-1-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Improve aging in shortest job first scheduler</summary>
      <description>Under high concurrency, some jobs can gets starved for longer time when hive.llap.task.scheduler.locality.delay is set to -1 (infinitely wait for locality).</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1567" opendate="2010-8-19 00:00:00" fixdate="2010-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase hive.mapjoin.maxsize to 10 million</summary>
      <description>i saw in a very wide table, hive can process 1million rows in less than one minute (select all columns).setting the hive.mapjoin.maxsize to 100k is kind of too restrictive. Let's increase this to 10 million.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15679" opendate="2017-1-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some metastore metrics always contains column=null (write_column_statistics, write_partition_column_statistics)</summary>
      <description>HIVE-14764 refactored the metrics start functions outside the for loop. This caused that the colName is always null in this case.We should remove these extra elements, since they provide no extra information.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="15680" opendate="2017-1-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results when hive.optimize.index.filter=true and same ORC table is referenced twice in query</summary>
      <description>To repro:set hive.optimize.index.filter=true;create table test_table(number int) stored as ORC;-- Two insertions will create two files, with one stripe eachinsert into table test_table VALUES (1);insert into table test_table VALUES (2);-- This should and does return 2 recordsselect * from test_table;-- These should and do each return 1 recordselect * from test_table where number = 1;select * from test_table where number = 2;-- This should return 2 records but only returns 1 recordselect * from test_table where number = 1union allselect * from test_table where number = 2;What's happening is only the last predicate is being pushed down.</description>
      <version>1.1.0,2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15683" opendate="2017-1-20 00:00:00" fixdate="2017-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make what&amp;#39;s done in HIVE-15580 for group by configurable</summary>
      <description>HIVE-15580 changed the way the data is shuffled for group by: instead of using Spark's groupByKey to shuffle data, Hive on Spark now uses repartitionAndSortWithinPartitions(), which generates (key, value) pairs instead of original (key, value iterator). This might have some performance implications, but it's needed to get rid of unbound memory usage by groupByKey.Here we'd like to compare group by performance with or w/o HIVE-15580. If the impact is significant, we can provide a configuration that allows user to switch back to the original way of shuffling.This work should be ideally done after HIVE-15682 as the optimization there should help the performance here as well.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lateral.view.explode2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ReduceTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GroupByShuffler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15691" opendate="2017-1-21 00:00:00" fixdate="2017-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create StrictRegexWriter to work with RegexSerializer for Flume Hive Sink</summary>
      <description>Create StrictRegexWriter to work with RegexSerializer for Flume Hive Sink.It is similar to StrictJsonWriter available in hive.Dependency is there in flume to commit.FLUME-3036 : Create a RegexSerializer for Hive Sink.Patch is available for Flume, Please verify the below linkhttps://github.com/kalyanhadooptraining/flume/commit/1c651e81395404321f9964c8d9d2af6f4a2aaef9</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.0,2.3.0,3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
    </fixedFiles>
  </bug>
  <bug id="15708" opendate="2017-1-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade calcite version to 1.12</summary>
      <description>Currently we are on 1.10 Need to upgrade calcite version to 1.11</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.constant.in.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.filter.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelShuttleImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExtractDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewFilterScanRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.scalar.multi.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15723" opendate="2017-1-25 00:00:00" fixdate="2017-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should report a warning about missing table/column statistics to user.</summary>
      <description>Many Hive performance issues are due to missing statistics. Either all, table or column statistics are missing. Potentially a new partition has been added and customer forgot to gather stats for that partition.A simple warning about a table or column missing statistics can be very helpful and makes hive more user friendly. Hive already has this information, its a matter of printing it out.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15727" opendate="2017-1-25 00:00:00" fixdate="2017-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pre insert work to give storage handler the possibility to perform pre insert checking</summary>
      <description>Add pre insert work stage to give storage handler the possibility to perform pre insert checking. For instance for the druid storage handler this will block the statement INSERT INTO statement.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.InsertTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaHookV2.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15743" opendate="2017-1-27 00:00:00" fixdate="2017-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>vectorized text parsing: speed up double parse</summary>
      <description>Double.parseDouble( new String(bytes, fieldStart, fieldLength, StandardCharsets.UTF_8));This takes ~25% of the query time in some cases.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.serde.LazySimpleSerDeBench.java</file>
    </fixedFiles>
  </bug>
  <bug id="15745" opendate="2017-1-27 00:00:00" fixdate="2017-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver. vector_varchar_simple,vector_char_simple</summary>
      <description>TestMiniLlapLocalCliDriver. vector_varchar_simple,vector_char_simple are failing occasionallyvector_varchar_simple failed in https://builds.apache.org/job/PreCommit-HIVE-Build/3204/testReport/vector_char_simple failed in https://builds.apache.org/job/PreCommit-HIVE-Build/3205/testReport/org.apache.hadoop.hive.cli/TestMiniLlapLocalCliDriver/testCliDriver_vector_char_simple_/</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15751" opendate="2017-1-30 00:00:00" fixdate="2017-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to run findbugs for itest modules as well</summary>
      <description>Remove relative paths from the findbugs configuration, so it could be run for every module.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15754" opendate="2017-1-30 00:00:00" fixdate="2017-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>exchange partition is not generating notifications</summary>
      <description>exchange partition event is not generating notifications in notification_log.There should multiple events generated. one add_partition event and several drop_partition events.for example:ALTER TABLE tab1 EXCHANGE PARTITION (part=1) WITH TABLE tab2;There should be the following events:ADD_PARTITION on tab2 on partition (part=1)DROP_PARTITION on tab1 on partition (part=1)</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15777" opendate="2017-2-1 00:00:00" fixdate="2017-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>propagate LLAP app ID to ATS and log it</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15782" opendate="2017-2-1 00:00:00" fixdate="2017-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>query on parquet table returns incorrect result when hive.optimize.index.filter is set to true</summary>
      <description>When hive.optimize.index.filter is set to true, the parquet table is filtered using the parquet column index. set hive.optimize.index.filter=true;CREATE TABLE t1 ( name string, dec decimal(5,0)) stored as parquet;insert into table t1 values('Jim', 3);insert into table t1 values('Tom', 5);select * from t1 where (name = 'Jim' or dec = 5);Only one row Jim, 3 is returned, but both should be returned.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetFilterPredicateConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15784" opendate="2017-2-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Turn on text vectorization by default (vector serde)</summary>
      <description>Turn ON text vectorization related variable hive.vectorized.use.vector.serde.deserialize by default.Row-mode (hive.vectorized.use.row.serde.deserialize) is handled by a separate JIRA HIVE-16222)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf.octet.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf.character.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.structin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf.octet.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf.character.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.groupby.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.parquet.types.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15786" opendate="2017-2-1 00:00:00" fixdate="2017-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide additional information from the llapstatus command</summary>
      <description>Slider is making enhancements to provide additional information like completed containers, pending containers etc.Integrate with this to provide additional details in llapstatus.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.main.resources.llap-cli-log4j2.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15787" opendate="2017-2-2 00:00:00" fixdate="2017-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make druid jars available for llap</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="15789" opendate="2017-2-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: limit reduce vectorization to 32Mb chunks</summary>
      <description>Reduce vectorization accumulates 1024 rows before forwarding it into the reduce processor.Add a safety limit for 32Mb of writables, so that shorter sequences can be forwarded into the operator trees. rowIdx++; if (rowIdx &gt;= BATCH_SIZE) { VectorizedBatchUtil.setBatchSize(batch, rowIdx); reducer.process(batch, tag);</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15790" opendate="2017-2-2 00:00:00" fixdate="2017-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused beeline golden files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.beelinepositive.avro.joins.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.uniquejoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.script.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.null.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union29.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union27.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.short.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.long.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.int.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.float.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.double.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.boolean.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.weekofyear.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.var.samp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.var.pop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.variance.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.upper.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.unhex.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ucase.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.trim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.to.date.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.tinyint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.tan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sum.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.subtract.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.substring.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.stddev.samp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.stddev.pop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.stddev.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.std.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sqrt.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.smallint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sign.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rtrim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rpad.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.round.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rlike.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.regexp.replace.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.regexp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.reflect.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rand.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.radians.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.power.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.pow.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.positive.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.PI.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.or.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.notop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.notequal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.not.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.month.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.modulo.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ltrim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lpad.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.logic.java.boolean.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.log2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.log10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.log.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ln.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lessthanorequal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lessthan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lcase.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.last.day.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.java.method.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.isnull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.in.file.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.int.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.initcap.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.index.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.in.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.hex.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.greaterthanorequal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.greaterthan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.floor.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.float.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.find.in.set.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.field.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.exp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.equal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.E.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.double.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.divide.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.div.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.degrees.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.dayofmonth.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.day.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.date.sub.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.date.add.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.datediff.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.cos.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.conv.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.insert2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.insert1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.compare.java.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ceiling.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ceil.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.boolean.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.xor.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.or.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.not.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.and.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitmap.empty.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bigint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.avg.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.atan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.asin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.array.contains.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.add.months.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.add.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.acos.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.covar.samp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.covar.pop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.corr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.type.cast.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.transform2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.touch.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.lazy.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.comparison.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.subq.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.publisher.error.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.empty.partition.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.empty.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.aggregator.error.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.tablestatus.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.partitions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.indexes.edge.cases.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.describe.func.quotes.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.columns.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.showparts.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.set.variable.sub.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.serde.regex.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.script.env.var2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.repair.hadoop23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.repair.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rename.partition.location.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.reduce.deduplicate.exclude.gby.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.toleratecorruptions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.lazydecompress.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.quote2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.query.result.fileformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.query.properties.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.progress.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.print.header.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.pushdown.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.special.char.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.serde.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partitions.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partcols1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.parenthesis.star.by.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.overridden.confs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.order2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.order.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ops.comparison.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullinput2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullinput.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nomore.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.newline.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nestedvirtual.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.misc.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mi.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.overwrite.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.loadpart1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.literal.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.literal.ints.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.literal.double.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.lateral.view.ppd.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.keyword.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.filters.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.empty.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.casesensitive.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.1to1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join40.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join39.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join38.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join37.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join36.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join35.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join34.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join33.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join32.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join29.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join27.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.compressed.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.dfs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input4.cb.delim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input49.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input45.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input44.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input43.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input42.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input41.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input40.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input39.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input38.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input37.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input36.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input35.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input34.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input33.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input32.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.compact.binary.search.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.hook.order.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.hook.context.cs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.having.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.neg.float.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.bigdata.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.fileformat.mix.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.explode.null.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.sortby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.orderby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.clusterby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.enforce.order.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.udf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.table.removes.partition.dirs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.table2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.multi.partitions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.index.removes.partition.dirs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.index.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.function.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.driverhook.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.disable.file.format.check.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.diff.part.input.formats.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.desc.non.existent.tbl.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.table.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.delimiter.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.default.partition.name.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ct.case.insensitive.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.merge.compressed.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.cp.mj.rc.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.count.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.concatenate.inherit.table.location.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.combine1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketizedhiveinputformat.auto.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.binary.constant.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.schema.literal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.sanity.test.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.merge.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.numbuckets.partitioned.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.partition.protect.mode.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.archive.excludeHadoop20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.authorization.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.change.schema.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.evolved.schemas.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15791" opendate="2017-2-2 00:00:00" fixdate="2017-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused ant files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GetVersionPref.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorTestCode.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.DistinctElementsClassPath.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.antlib.xml</file>
      <file type="M">ant.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15795" opendate="2017-2-2 00:00:00" fixdate="2017-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Accumulo Index Tables in Hive Accumulo Connector</summary>
      <description>Ability to specify an accumulo index table for an accumulo-hive table.This would greatly improve performance for non-rowid query predicates</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloIndexLexicoder.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloIndexLexicoder.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15796" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: poor reducer parallelism when operator stats are not accurate</summary>
      <description>In HoS we use currently use operator stats to determine reducer parallelism. However, it is often the case that operator stats are not accurate, especially if column stats are not available. This sometimes will generate extremely poor reducer parallelism, and cause HoS query to run forever. This JIRA tries to offer an alternative way to compute reducer parallelism, similar to how MR does. Here's the approach we are suggesting:1. when computing the parallelism for a MapWork, use stats associated with the TableScan operator;2. when computing the parallelism for a ReduceWork, use the maximum parallelism from all its parents.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15797" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>separate the configs for gby and oby position alias usage</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.decimal.stats.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.groupby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15798" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP run.sh should use stop --force</summary>
      <description>It's both faster, and avoids slider issues when the app survives across kerberization and cannot be stopped by regular stop, which assumes it should have some token or other because the cluster is now secure.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="158" opendate="2008-12-11 00:00:00" fixdate="2008-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>table aliases dont work for sampled tables in joins</summary>
      <description>The following query:FROM table1 TABLESAMPLE (BUCKET 1 OUT OF 512 ON col1) a JOIN table2 b ON (a.j1 = b.j2) SELECT a.col1, a.j1, b.j2;results in the error:ERROR ql.Driver (SessionState.java:printError(263)) - FAILED: Error in semantic analysis: line 1:88 Invalid Table Alias a</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15801" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some logging improvements in LlapTaskScheduler</summary>
      <description>Excessive logging in some places. Not enough otherwise.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="15802" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changes to expected entries for dynamic bloomfilter runtime filtering</summary>
      <description>Estimate bloom filter size based on distinct values from column stats if available</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15803" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>msck can hang when nested partitions are present</summary>
      <description>Steps to reproduce. CREATE TABLE `repairtable`( `col` string) PARTITIONED BY ( `p1` string, `p2` string)hive&gt; dfs -mkdir -p /apps/hive/warehouse/test.db/repairtable/p1=c/p2=a/p3=b;hive&gt; dfs -touchz /apps/hive/warehouse/test.db/repairtable/p1=c/p2=a/p3=b/datafile;hive&gt; set hive.mv.files.thread;hive.mv.files.thread=15hive&gt; set hive.mv.files.thread=1;hive&gt; MSCK TABLE repairtable;</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.msck.repair.batchsize.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.msck.repair.batchsize.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="15805" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some minor improvement on the validation tool</summary>
      <description>To correct some types and make the output neat. And also add the validation servers to the command line.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15806" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid schema inference for Select queries might produce wrong type for metrics</summary>
      <description>We inferred float automatically, instead of emitting a metadata query to Druid and checking the type of the metric.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="15808" opendate="2017-2-3 00:00:00" fixdate="2017-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove semijoin reduction branch if it is on bigtable along with hash join</summary>
      <description>If there is a semijoin branch on the same operator pipeline which contains a hash join then it is by design on big table which is not optimal. The operator cycle detection logic may not find a cycle as there is no cycle at operator level. However, once Tez builds its task there can be a cycle at task level causing the query to fail.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="15809" opendate="2017-2-4 00:00:00" fixdate="2017-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in the PostgreSQL database name for druid service</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1581" opendate="2010-8-22 00:00:00" fixdate="2010-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CompactIndexInputFormat should create split only for files in the index output file.</summary>
      <description>We can get a list of files from the index file, so no need to create splits based on all files in the base table/partition</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15810" opendate="2017-2-4 00:00:00" fixdate="2017-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>llapstatus should wait for ZK node to become available when in wait mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15827" opendate="2017-2-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: status tool breaks out of watch mode when live instances is 0</summary>
      <description>When llap status tool is used in watch mode and when live instances is 0, the tool breaks out of loop.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="15829" opendate="2017-2-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP text cache: disable memory tracking on the writer</summary>
      <description>See ORC-141 and HIVE-15672 for context</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15830" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow additional ACLs for tez jobs</summary>
      <description>Allow users to grant view access to additional users when running tez jobs.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15839" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t force cardinality check if only WHEN NOT MATCHED is specified</summary>
      <description>should've been part of HIVE-14949if only WHEN NOT MATCHED is specified then the join is basically an anti-join and we are not retrieving any values from target side. So the cardinality check is just overhead (though presumably very minor since the filter above the join will filter everything and thus there is nothing to group)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sqlmerge.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1584" opendate="2010-8-23 00:00:00" fixdate="2010-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong log files in contrib client positive</summary>
      <description>TestContribCliDriver still gets some diffs</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15840" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webhcat test TestPig_5 failing with Pig on Tez at check for percent complete of job</summary>
      <description>TestPig_5 is failing at percentage check if the job is Pig on Tez:check_job_percent_complete failed. got percentComplete , expected 100% completeTest command:curl -d user.name=daijy -d arg=-p -d arg=INPDIR=/tmp/templeton_test_data -d arg=-p -d arg=OUTDIR=/tmp/output -d file=loadstore.pig -X POST http://localhost:50111/templeton/v1/pigcurl http://localhost:50111/templeton/v1/jobs/job_1486502484681_0003?user.name=daijyThis is similar to HIVE-9351, which fixes Hive on Tez.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15841" opendate="2017-2-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to ORC 1.3.3</summary>
      <description>Hive needs ORC-141 and ORC-135, so we should upgrade to ORC 1.3.3 once it releases.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15843" opendate="2017-2-7 00:00:00" fixdate="2017-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable slider YARN resource normalization for LLAP</summary>
      <description>The normalization can lead to LLAP starting with invalid configuration with regard to cache size, jmx and container size. If the memory configuration is invalid, it should fail immediately.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="15846" opendate="2017-2-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate more dependencies (e.g. org.apache.zookeeper) for JDBC uber jar</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15847" opendate="2017-2-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In Progress update refreshes seem slow</summary>
      <description>After HIVE-15473, the refresh rates for in place progress bar seems to be slow on hive cli. As pointed out by prasanth_j The refresh rate is slow. Following video will show itbefore patch: https://asciinema.org/a/2fgcncxg5gjavcpxt6lfb8jg9after patch: https://asciinema.org/a/2tht5jf6l9b2dc3ylt5gtztqg</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
    </fixedFiles>
  </bug>
  <bug id="15850" opendate="2017-2-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper handling of timezone in Druid storage handler</summary>
      <description>We need to make sure that filters on timestamp are passed to Druid with correct timezone.After CALCITE-1617, Calcite will generate a Druid query with intervals without timezone specification. In Druid, these intervals will be assumed to be in UTC (if Druid is running in UTC, which is currently the recommendation). However, in Hive, those intervals should be assumed to be in the user timezone. Thus, we should respect Hive semantics and include the user timezone in the intervals passed to Druid.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="15858" opendate="2017-2-9 00:00:00" fixdate="2017-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline ^C doesn&amp;#39;t close the session in HTTP mode</summary>
      <description>When open multiple connections through Beeline to Hiveserver2 and if tries to close the client using !quit or ^C command, it looks like all the connections/sessions are not getting closed.!quit seems to close the current active connection but fails to close other open sessions.^C doesn't close any session.This behaviour is noticed only with the HTTP mode of transport (hive.server2.transport.mode=http). In case of BINARY mode, server triggers the close session when a tcp connection is closed by peer.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="15864" opendate="2017-2-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo introduced in HIVE-14754</summary>
      <description>hs2_suceeded_queries needs another "c": hs2_succeeded_queries.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.operation.TestSQLOperationMetrics.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="15871" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot insert into target table because column number/types are different with hive.merge.cardinality.check=false</summary>
      <description>Merge statement with WHEN MATCHED and hive.merge.cardinality.check=falsecauses errors likeFAILED: SemanticException [Error 10044]: Line 11:12 Cannot insert into target table because column number/types are different 'part_0': Table insclause-0 has 3 columns, but query has 4 columns.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15877" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upload dependency jars for druid storage handler</summary>
      <description>Upload dependency jars for druid storage handler</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15878" opendate="2017-2-10 00:00:00" fixdate="2017-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP text cache: bug in last merge</summary>
      <description>While rebasing the last patch, a bug was introduced.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15880" opendate="2017-2-10 00:00:00" fixdate="2017-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow insert overwrite and truncate table query to use auto.purge table property</summary>
      <description>It seems inconsistent that auto.purge property is not considered when we do a INSERT OVERWRITE while it is when we do a DROP TABLEDrop table doesn't move table data to Trash when auto.purge is set to true&gt; create table temp(col1 string, col2 string);No rows affected (0.064 seconds)&gt; alter table temp set tblproperties('auto.purge'='true');No rows affected (0.083 seconds)&gt; insert into temp values ('test', 'test'), ('test2', 'test2');No rows affected (25.473 seconds)# hdfs dfs -ls /user/hive/warehouse/tempFound 1 items-rwxrwxrwt 3 hive hive 22 2017-02-09 13:03 /user/hive/warehouse/temp/000000_0#&gt; drop table temp;No rows affected (0.242 seconds)# hdfs dfs -ls /user/hive/warehouse/templs: `/user/hive/warehouse/temp': No such file or directory## sudo -u hive hdfs dfs -ls /user/hive/.Trash/Current/user/hive/warehouse#INSERT OVERWRITE query moves the table data to Trash even when auto.purge is set to true&gt; create table temp(col1 string, col2 string);&gt; alter table temp set tblproperties('auto.purge'='true');&gt; insert into temp values ('test', 'test'), ('test2', 'test2');# hdfs dfs -ls /user/hive/warehouse/tempFound 1 items-rwxrwxrwt 3 hive hive 22 2017-02-09 13:07 /user/hive/warehouse/temp/000000_0#&gt; insert overwrite table temp select * from dummy;# hdfs dfs -ls /user/hive/warehouse/tempFound 1 items-rwxrwxrwt 3 hive hive 26 2017-02-09 13:08 /user/hive/warehouse/temp/000000_0# sudo -u hive hdfs dfs -ls /user/hive/.Trash/Current/user/hive/warehouseFound 1 itemsdrwx------ - hive hive 0 2017-02-09 13:08 /user/hive/.Trash/Current/user/hive/warehouse/temp#While move operations are not very costly on HDFS it could be significant overhead on slow FileSystems like S3. This could improve the performance of INSERT OVERWRITE TABLE queries especially when there are large number of partitions on tables located on S3 should the user wish to set auto.purge property to trueSimilarly TRUNCATE TABLE query on a table with auto.purge property set true should not move the data to Trash</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15886" opendate="2017-2-12 00:00:00" fixdate="2017-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Provide logs URL for in-progress and completed task attemtps</summary>
      <description>YARN provides a webservice to access logs with YARN-6011. This can be used to populate the in-progress and completed task attempts logs.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15894" opendate="2017-2-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logical semijoin config in sqlstd safe list</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15895" opendate="2017-2-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HDFS for stats collection temp dir on blob storage</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15896" opendate="2017-2-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: improved failures when security is set up incorrectly</summary>
      <description>Right now LLAP may fail in ZK ACL check when the ACLs are invalid. We can try to fail earlier, and also improve the message.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="15900" opendate="2017-2-13 00:00:00" fixdate="2017-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline prints tez job progress in stdout instead of stderr</summary>
      <description>Tez job progress messages are getting updated to stdout instead of stderrAttaching output file for below command, with the tez job status printed$HIVE_HOME/bin/beeline -n &lt;username&gt; -p &lt;password&gt; -u "&lt;jdbc-connection-url" --outputformat=tsv -e "analyze table studenttab10k compute statistics;" &gt; stdout</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="15901" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: incorrect usage of gap cache</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15906" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>thrift code regeneration to include new protocol version</summary>
      <description>HIVE-15473 changed the protocol version in thrift file.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TProtocolVersion.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TOpenSessionResp.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TOpenSessionReq.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="15915" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Emit progress percentage in getting operation status</summary>
      <description>When running a query asynchronously, client may want to check the progress periodically. HIVE-15473 is to support progressing bar on beeline for Tez. For this issue, we just want the progress percentage.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="15928" opendate="2017-2-15 00:00:00" fixdate="2017-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallelization of Select queries in Druid handler</summary>
      <description>Even if we split a Select query along its time dimension, parallelization is limited as all queries will hit the broker node. Instead, we can interrogate the broker to get the Druid nodes that contain the data, and query those nodes directly.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.HiveDruidSplit.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15932" opendate="2017-2-15 00:00:00" fixdate="2017-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for: "explain ast"</summary>
      <description>AST was removed in explain extended in HIVE-13533; that makes sense from users perspective for the common case, but it would be useful for Hive developers and advanced users to see the AST to diagnose issues. "Explain ast" command can be added to dump the AST.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainConfiguration.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15934" opendate="2017-2-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Downgrade Maven surefire plugin from 2.19.1 to 2.18.1</summary>
      <description>Surefire 2.19.1 has some issue (https://issues.apache.org/jira/browse/SUREFIRE-1255) which caused debugging session to abort after a short period of time. Many IntelliJ users have seen this, although it looks fine for Eclipse users. Version 2.18.1 works fine.We'd better make the change to not impact the development for IntelliJ guys. We can upgrade again once the root cause is figured out.cc kgyrtkirk ashutoshc</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15935" opendate="2017-2-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACL is not set in ATS data</summary>
      <description>When publishing ATS info, Hive does not set ACL, that make Hive ATS entries visible to all users. On the other hand, Tez ATS entires is using Tez DAG ACL which limit both view/modify ACL to end user only. We shall make them consistent. In the Jira, I am going to limit ACL to end user for both Tez ATS and Hive ATS, also provide config "hive.view.acls" and "hive.modify.acls" if user need to overridden.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="15947" opendate="2017-2-16 00:00:00" fixdate="2017-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance Templeton service job operations reliability</summary>
      <description>Currently Templeton service doesn't restrict number of job operation requests. It simply accepts and tries to run all operations. If more number of concurrent job submit requests comes then the time to submit job operations can increase significantly. Templetonused hdfs to store staging file for job. If HDFS storage can't respond to large number of requests and throttles then the job submission can take very large times in order of minutes.This behavior may not be suitable for all applications and client applications may be looking for predictable and low response for successful request or send throttle response to client to wait for some time before re-requesting job operation.In this JIRA, I am trying to address following job operations 1) Submit new Job2) Get Job Status3) List jobsThese three operations has different complexity due to variance in use of cluster resources like YARN/HDFS.The idea is to introduce a new config templeton.parallellism.job.submit which controls maximum number of concurrent active job submissions within Templeton and use this config to control better response times. If a new job submission request sees that there are already templeton.parallellism.job.submit jobs getting submitted concurrently then the request will fail with Http error 503 with reason “Too many concurrent job submission requests received. Please wait for some time before retrying.”The client is expected to catch this response and retry after waiting for some time. The default value for the config templeton.parallellism.job.submit is set to ‘0’. This means by default job submission requests are always accepted. The behavior needs to be enabled based on requirements.We can have similar behavior for Status and List operations with configs templeton.parallellism.job.status and templeton.parallellism.job.list respectively.Once the job operation is started, the operation can take longer time. The client which has requested for job operation may not be waiting for indefinite amount of time. This work introduces configurationstempleton.job.submit.timeouttempleton.job.status.timeouttempleton.job.list.timeoutto specify maximum amount of time job operation can execute. If time out happens then list and status job requests returns to client with message"List job request got timed out. Please retry the operation after waiting for some time."If submit job request gets timed out then i) The job submit request thread which receives time out will check if valid job id is generated in job request. ii) If it is generated then issue kill job request on cancel thread pool. Don't wait for operation to complete and returns to client with time out message. Side effects of enabling time out for submit operations1) This has a possibility for having active job for some time by the client gets response and a list operation from client could potential show the newly created job before it gets killed.2) We do best effort to kill the job and no guarantees. This means there is a possibility of duplicate job created. One possible reason for this could be a case where job is created and then operation timed out but kill request failed due to resource manager unavailability. When resource manager restarts, it will restarts the job which got created.Fixing this scenario is not part of the scope of this JIRA. The job operation functionality can be enabled only if above side effects are acceptable.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SqoopDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="1595" opendate="2010-8-25 00:00:00" fixdate="2010-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>job name for alter table &lt;T&gt; archive partition &lt;P&gt; is not correct</summary>
      <description>For some internal runs, I saw the job name as hadoop-0.20.1-tools.jar, which makes it difficult to identify</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15951" opendate="2017-2-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure base persist directory is unique and deleted</summary>
      <description>In some cases the base persist directory will contain old data or shared between reducer in the same physical VM.That will lead to the failure of the job till that the directory is cleaned.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15953" opendate="2017-2-16 00:00:00" fixdate="2017-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>better error messages for LLAP registry properties</summary>
      <description>int memory = Integer.parseInt(srv.get(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname)); int vCores = Integer.parseInt(srv.get(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname)); Needs to provide a useful error message when it fails because the config is bad. There may be other similar places.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15954" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some Tez INFO logs are too noisy</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15956" opendate="2017-2-17 00:00:00" fixdate="2017-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverflowError when drop lots of partitions</summary>
      <description>Repro steps:1. Create partitioned table and add 10000 partitionscreate table test_partition(id int) partitioned by (dt int);alter table test_partition add partition(dt=1);alter table test_partition add partition(dt=3);alter table test_partition add partition(dt=4);...alter table test_partition add partition(dt=10000);2. Drop 9000 partitions:alter table test_partition drop partition(dt&lt;9000);Step 2 will fail with StackOverflowError:Exception in thread "pool-7-thread-161" java.lang.StackOverflowError at org.datanucleus.query.expression.ExpressionCompiler.isOperator(ExpressionCompiler.java:819) at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:190) at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179) at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192) at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179) at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192) at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)Exception in thread "pool-7-thread-198" java.lang.StackOverflowError at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:83) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="15957" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Follow Hive&amp;#39;s rules for type inference instead of Calcite</summary>
      <description>Mostly those rules are same, but in case they diverge we should pick Hive's rule for type inference.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.interval.arithmetic.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15958" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: IPC connections are not being reused for umbilical protocol</summary>
      <description>During concurrency testing, observed 1000s of ipc thread creations. Ideally, the connections to same hosts should be reused.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15959" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: fix headroom calculation and move it to daemon</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15964" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15971" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: logs urls should use daemon container id instead of fake container id</summary>
      <description>The containerId used for log url generation is fake. It should be replaced by the container id of the llap daemon.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
    </fixedFiles>
  </bug>
  <bug id="15972" opendate="2017-2-18 00:00:00" fixdate="2017-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime filtering not vectorizing for decimal/timestamp/char/varchar</summary>
      <description>Looks like versions of vectorized BetweenDynamicValue that use Java objects needs to be initialized with non-null values</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.semijoin.reduction2.q</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15974" opendate="2017-2-20 00:00:00" fixdate="2017-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support real, double precision and numeric data types</summary>
      <description>Support the standard names for these datatypes, which map to existing Hive datatypes.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="15975" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the MOD function</summary>
      <description>SQL defines the mod expression as a function allowing 2 numeric value expressions. Hive allows the infix notation using %. It would be good for Hive to support the standard approach as well. SQL standard reference T441</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.modulo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.modulo.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="15978" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support regr_* functions</summary>
      <description>Support the standard regr_* functions, regr_slope, regr_intercept, regr_r2, regr_sxx, regr_syy, regr_sxy, regr_avgx, regr_avgy, regr_count. SQL reference section 10.9</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="15979" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support character_length and octet_length</summary>
      <description>SQL defines standard ways to get number of characters and octets. SQL reference: section 6.28. Example:vagrant=# select character_length('欲速则不达'); character_length------------------ 5(1 row)vagrant=# select octet_length('欲速则不达'); octet_length-------------- 15(1 row)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1598" opendate="2010-8-26 00:00:00" fixdate="2010-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use SequenceFile rather than TextFile format for hive query results</summary>
      <description>Hive query's result is written to a temporary directory first and then FetchTask takes the files and display it to the users. Currently the file format used for the resulting file is TextFile format. This could cause incorrect result display if some string typed column contains new lines, which are used as record delimiters in TextInputFormat. Switching to SequenceFile format will solve this problem.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15981" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow empty grouping sets</summary>
      <description>group by () should be treated as equivalent to no group by clause. Currently it throws a parse error</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="15991" opendate="2017-2-21 00:00:00" fixdate="2017-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky Test: TestEncryptedHDFSCliDriver encryption_join_with_different_encryption_keys</summary>
      <description>I ran a git-bisect and seems HIVE-15703 started causing this failure. Not entirely sure why, but I updated the .out file and the diff is pretty straightforward, so I think its safe to just update it.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.with.different.encryption.keys.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15993" opendate="2017-2-21 00:00:00" fixdate="2017-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive REPL STATUS is not returning last event ID</summary>
      <description>While running "REPL STATUS" on target to get last event ID for DB, it returns zero rows.0: jdbc:hive2://localhost:10001/repl&gt; REPL status repl;No rows affected (932.167 seconds)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16002" opendate="2017-2-22 00:00:00" fixdate="2017-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correlated IN subquery with aggregate asserts in sq_count_check UDF</summary>
      <description>Reproducercreate table t(i int, j int);insert into t values(0,1), (0,2);create table tt(i int, j int);insert into tt values(0,3);select * from t where i IN (select count(i) from tt where tt.j = t.j);</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSQCountCheck.java</file>
    </fixedFiles>
  </bug>
  <bug id="16006" opendate="2017-2-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL LOAD Inserts doesn&amp;#39;t operate on the target database if name differs from source database.</summary>
      <description>During "Incremental Load", it is not considering the database name input in the command line. Hence load doesn't happen. At the same time, database with original name is getting modified.Steps:1. INSERT INTO default.tbl values (10, 20);2. REPL DUMP default FROM 52;3. REPL LOAD replDb FROM '/tmp/dump/1487588522621';– This step modifies the default Db instead of replDb.==Additional note - this is happening for INSERT events, not other events.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16013" opendate="2017-2-22 00:00:00" fixdate="2017-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fragments without locality can stack up on nodes</summary>
      <description>When no locality information is provide, task requests can stack up on a node because of consistent no selection. When locality information is not provided we should fallback to random selection for better work distribution.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="16014" opendate="2017-2-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastoreChecker should use hive.metastore.fshandler.threads instead of hive.mv.files.thread for pool size</summary>
      <description>HiveMetastoreChecker uses hive.mv.files.thread configuration value for determining the pool size as below :private void checkPartitionDirs(Path basePath, Set&lt;Path&gt; allDirs, int maxDepth) throws IOException, HiveException { ConcurrentLinkedQueue&lt;Path&gt; basePaths = new ConcurrentLinkedQueue&lt;&gt;(); basePaths.add(basePath); Set&lt;Path&gt; dirSet = Collections.newSetFromMap(new ConcurrentHashMap&lt;Path, Boolean&gt;()); // Here we just reuse the THREAD_COUNT configuration for // HIVE_MOVE_FILES_THREAD_COUNT int poolSize = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 15); // Check if too low config is provided for move files. 2x CPU is reasonable max count. poolSize = poolSize == 0 ? poolSize : Math.max(poolSize, Runtime.getRuntime().availableProcessors() * 2);msck is commonly used to add the missing partitions for the table from the Filesystem. In such a case different pool sizes for HMSHandler and HiveMetastoreChecker can affect the performance. Eg. If hive.metastore.fshandler.threads is set to a lower value like 15 and hive.mv.files.thread is much higher like 100 or vice versa the smaller pool will become the bottleneck. If would be good to use hive.metastore.fshandler.threads to size the pool for HiveMetastoreChecker since the number missing partitions and number of partitions to be added will most likely be the same. In such a case the performance of the query will be optimum when both the pool sizes are same.Since it is possible to tune both the configs individually it will be very likely that they may be different. But since there is a strong co-relation between amount of work done by HiveMetastoreChecker and HiveMetastore.add_partitions call it might be a good idea to use hive.metastore.fshandler.threads for pool size instead of hive.mv.files.thread</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="16015" opendate="2017-2-23 00:00:00" fixdate="2017-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some Tez INFO logs are too noisy II</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16023" opendate="2017-2-23 00:00:00" fixdate="2017-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong estimation for number of rows generated by IN expression</summary>
      <description>Code seems to be wrong, as we are factoring the number of rows to create the multiplying factor, instead of NDV for given column(s) and NDV in IN clause.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16028" opendate="2017-2-23 00:00:00" fixdate="2017-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail UPDATE/DELETE/MERGE queries when Ranger authorization manager is used</summary>
      <description>This is a followup of HIVE-15891. In that jira an error-out logic was added, but the assumption that we need to do row filtering/column masking for entries in a non-empty list of tables returned by applyRowFilterAndColumnMasking is wrong, because on Ranger side, RangerHiveAuthorizer#applyRowFilterAndColumnMasking will unconditionally return a list of tables no matter whether row filtering/column masking is applicable on the tables.The fix for Hive for now will be to move the error-out logic after we figure out there's no replacement text for the query. But ideally we should consider modifying Ranger logic to only return tables that need to be masked.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="16033" opendate="2017-2-24 00:00:00" fixdate="2017-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use PrintGCDateStamps for gc logging</summary>
      <description>This print human readable timestamps instead of timestamp relative to jvm startup</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16037" opendate="2017-2-24 00:00:00" fixdate="2017-1-24 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>with fetch optimization, the query runs after locks are released</summary>
      <description>Other assumptions may also be broken.FetchTask.execute implementation is very curious - it does nothing, and the FetchTask that actually runs the query is put in the same place as the one that normally fetches the results; that is to say, the whole pipeline is run after Driver has "shut down" the query. That releases locks before the query runs, and may also have other implications (for transactions, etc.? I don't think simple fetch can be run for updates)Adding a log line to TSOP process method, and running encrypted_table_insert from EncryptedHDFS cli driver, I get:2017-02-24T14:41:24,521 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] log.PerfLogger: &lt;PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver&gt;[no lines here]2017-02-24T14:41:24,521 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] log.PerfLogger: &lt;/PERFLOG method=runTasks start=1487976084521 end=1487976084521 duration=0 from=org.apache.hadoop.hive.ql.Driver&gt;...2017-02-24T14:41:24,521 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] log.PerfLogger: &lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&gt;2017-02-24T14:41:24,521 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] ZooKeeperHiveLockManager: About to release lock for default/encrypted_table2017-02-24T14:41:24,523 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] ZooKeeperHiveLockManager: About to release lock for default2017-02-24T14:41:24,525 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] log.PerfLogger: &lt;/PERFLOG method=releaseLocks start=1487976084521 end=1487976084525 duration=4 from=org.apache.hadoop.hive.ql.Driver&gt;2017-02-24T14:41:24,525 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] log.PerfLogger: &lt;/PERFLOG method=Driver.run start=1487976084394 end=1487976084525 duration=131 from=org.apache.hadoop.hive.ql.Driver&gt;2017-02-24T14:41:24,525 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] ql.Driver: Shutting down query ...2017-02-24T14:41:24,532 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] mapred.FileInputFormat: Total # of splits generated by getSplits: 1, TimeTaken: 42017-02-24T14:41:24,532 DEBUG [50cde691-3602-4273-a4d9-e35f0c8b6001 main] exec.FetchOperator: Creating fetchTask ......2017-02-24T14:41:24,541 INFO [50cde691-3602-4273-a4d9-e35f0c8b6001 main] exec.TableScanOperator: TODO# calling process2017-02-24T14:41:24,543 INFO [50cde691-3602-4273-a4d9-e35f0c8b6001 main] exec.TableScanOperator: TODO# calling process2017-02-24T14:41:24,543 INFO [50cde691-3602-4273-a4d9-e35f0c8b6001 main] exec.TableScanOperator: Closing operator TS[0]...</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SimpleBufferManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapOomDebugDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionDispatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapIo.java</file>
    </fixedFiles>
  </bug>
  <bug id="16038" opendate="2017-2-25 00:00:00" fixdate="2017-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables: fix (or disable) inferring buckets</summary>
      <description>The following tests on minimr produce diffs if all tables are changed to MM:infer_bucket_sort_dyn_partinfer_bucket_sort_num_bucketsinfer_bucket_sort_mergeinfer_bucket_sort_reducers_power_twoSome of these disable strict checks for bucketing load, which wouldn't work by design; the rest should work. Either that, or we should disable this for MM tables - seems like an obscure feature.</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16044" opendate="2017-2-27 00:00:00" fixdate="2017-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Shuffle Handler keep-alive connections are closed from the server side</summary>
      <description>LLAP's shufflehandler could be closing the keep-alive connections after output is served. This could break the connection from server side. JDK http logs may not be revealing this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16045" opendate="2017-2-27 00:00:00" fixdate="2017-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print progress bar along with operation log</summary>
      <description>allow printing of the operation logs and progress bar such that,allow operations logs to output data once -&gt; block it -&gt; start progress bar -&gt; finish progress bar -&gt; unblock the operations log -&gt; finish operations log -&gt; print query results.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.ServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.logs.InPlaceUpdateStream.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.logs.BeelineInPlaceUpdateStream.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="16047" opendate="2017-2-27 00:00:00" fixdate="2017-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shouldn&amp;#39;t try to get KeyProvider unless encryption is enabled</summary>
      <description>Found lots of following errors in HS2 log:hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!Similar to HDFS-7931</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="16053" opendate="2017-2-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove newRatio from llap JAVA_OPTS_BASE</summary>
      <description>The G1GC is supposed to be able to resize regions as required. Setting the newRatio or other parameters which size the new gen disables this capability.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="16057" opendate="2017-2-28 00:00:00" fixdate="2017-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SchemaTool ignores --passWord argument if hadoop.security.credential.provider.path is configured</summary>
      <description>It the hadoop.security.credential.provider.path is defined in command line, but the correct HADOOP_CREDSTORE_PASSWORD is not provided the SchemaTool fails, even if the correct metastore password is provided with --passWordCould be reproduced if the hive-site.xml contains the following: &lt;property&gt; &lt;name&gt;hadoop.security.credential.provider.path&lt;/name&gt; &lt;value&gt;localjceks://file//Users/petervary/tmp/conf/creds.localjceks&lt;/value&gt; &lt;/property&gt;$ ../schemaTool --dbType=mysql --info --passWord=pwdMetastore connection URL: jdbc:mysql://localhost:3306/hive?useUnicode=true&amp;characterEncoding=UTF-8Metastore Connection Driver : com.mysql.jdbc.DriverMetastore connection User: hiveorg.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.*** schemaTool failed ***The --passWord argument should override the errors from the credential provider</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16058" opendate="2017-2-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable falling back to non-cbo for SemanticException for tests</summary>
      <description>Currently optimizer falls back to non-cbo path if cbo path throws an exception of type SemanticException. This might be eclipsing some genuine issues within cbo-path.We would like to turn off the fall back mechanism for tests to see if there are indeed genuine issues/bugs within cbo path.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.position.alias.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.wrong.column.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.scalar.multi.columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.corr.grandparent.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.InvalidValueBoundary.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.acid.overwrite.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.complex.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.approx.23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.position.alias.test.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.jdbc.handler.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.windowing.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.views.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.unionDistinct.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.semijoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.gby.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.queries.q</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16071" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS RPCServer misuses the timeout in its RPC handshake</summary>
      <description>Based on its property description in HiveConf and the comments in HIVE-12650 (https://issues.apache.org/jira/browse/HIVE-12650?focusedCommentId=15128979&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15128979), hive.spark.client.connect.timeout is the timeout when the spark remote driver makes a socket connection (channel) to RPC server. But currently it is also used by the remote driver for RPC client/server handshaking, which is not right. Instead, hive.spark.client.server.connect.timeout should be used and it has already been used by the RPCServer in the handshaking.The error like following is usually caused by this issue, since the default hive.spark.client.connect.timeout value (1000ms) used by remote driver for handshaking is a little too short.17/02/20 08:46:08 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished. at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37) at org.apache.hive.spark.client.RemoteDriver.&lt;init&gt;(RemoteDriver.java:156) at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:542)Caused by: javax.security.sasl.SaslException: Client closed before SASL negotiation finished. at org.apache.hive.spark.client.rpc.Rpc$SaslClientHandler.dispose(Rpc.java:453) at org.apache.hive.spark.client.rpc.SaslHandler.channelInactive(SaslHandler.java:90)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16072" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add some additional jvm metrics for hadoop-metrics2</summary>
      <description>It will be helpful for debugging to expose some metrics like buffer pool, file descriptors etc. that are not exposed via Hadoop's JvmMetrics. We already a /jmx endpoint that gives out these info but we don't know the timestamp of allocations, number file descriptors to correlated with the logs. This will better suited for graphing tools.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="16075" opendate="2017-3-1 00:00:00" fixdate="2017-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MetaStore needs to reinitialize log4j to allow log specific settings via hiveconf take effect</summary>
      <description>when I start hive metastore with command:hive --service metastore -hiveconf hive.log.file=hivemetastore.log -hiveconf hive.log.dir=/home/yun/hive/logThe two log parameters won't take effect because after metastore copy hive conf parameters into java system properties, it doesn't reinitialize log4j.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16076" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP packaging - include aux libs</summary>
      <description>The old auxlibs (or whatever) should be packaged by default, if present.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16087" opendate="2017-3-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove multi append of log4j.configurationFile in hive script</summary>
      <description>hive script appends -Dlog4j.configurationFile twice.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="16088" opendate="2017-3-2 00:00:00" fixdate="2017-1-2 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Fix hive conf property name introduced in HIVE-12767</summary>
      <description>The configuration property parquet.mr.int96.enable.utc.write.zone should be called hive.parquet.mr.int96.enable.utc.write.zone</description>
      <version>2.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.parquet.int96.timestamp.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16094" opendate="2017-3-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>queued containers may timeout if they don&amp;#39;t get to run for a long time</summary>
      <description>I believe this happened after HIVE-15958 - since we end up keeping amNodeInfo in knownAppMaters, and that can result in the callable not being scheduled on new task registration.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16097" opendate="2017-3-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>minor fixes to metrics and logs in LlapTaskScheduler</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16101" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QTest failure BeeLine escape_comments after HIVE-16045</summary>
      <description>HIVE-16045 committed immediately after HIVE-14459, and added two extra lines to the output which is written there with another thread. We should remove these lines before comparing the out file</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.util.QFileClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16103" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Scheduler timeout monitor never stops with slot nodes</summary>
      <description>The scheduler timeout monitor is started when node count becomes 0 and stopped when node count becomes 1. For node count, we were relying on the paths under llap namespace. With addition of slot znodes, every node creates 2 paths (worker and slot). As a result, the size of the instances cache will never be 1 (always multiple of 2) which leads to condition where timeout monitor is not stopped.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16104" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: preemption may be too aggressive if the pre-empted task doesn&amp;#39;t die immediately</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16115" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stop printing progress info from operation logs with beeline progress bar</summary>
      <description>when in progress bar is enabled, we should not print the progress information via the operations logs.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.RenderStrategy.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16116" opendate="2017-3-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline throws NPE when beeline.hiveconfvariables={} in beeline.properties</summary>
      <description>Env: hive masterSteps to reproduce:1. clear previous beeline.properties (rm -rf ~/.beeline/beeline.properties)2. Launch beeline, "!save" and exit. This would create new "~/.beeline/beeline.properties", which would have "beeline.hiveconfvariables={}"3. Launch "beeline --hiveconf hive.tmp.dir=/tmp". This would throw NPEException in thread "main" java.lang.NullPointerException at org.apache.hive.beeline.BeeLine.setHiveConfVar(BeeLine.java:885) at org.apache.hive.beeline.BeeLine.connectUsingArgs(BeeLine.java:832) at org.apache.hive.beeline.BeeLine.initArgs(BeeLine.java:775) at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1009) at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:519) at org.apache.hive.beeline.BeeLine.main(BeeLine.java:501) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="16117" opendate="2017-3-6 00:00:00" fixdate="2017-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SortProjectTransposeRule should check for monotonicity preserving CAST</summary>
      <description>Due to CALCITE-1618, we need to move to new Calcite release to fix it.Due to this, SortProjectTransposeRule ignores CAST in the Project operator.For instance:...HiveSortLimit(sort0=$4,sort1=$2,dir0=ASC-nulls-first,dir1=DESC-nulls-last,fetch=10) HiveProject(robot=$0,_o__c1=$2,m=$3,s=$4,(tok_function tok_int (tok_table_or_col robot))=CAST($0):INTEGER))...will be transformed into:...HiveProject(robot=$0,_o__c1=$2,m=$3,s=$4,(tok_function tok_int (tok_table_or_col robot))=CAST($0):INTEGER)) HiveSortLimit(sort0=$0,sort1=$2,dir0=ASC-nulls-first,dir1=DESC-nulls-last,fetch=10)...which is incorrect.The problem seems to be in the permutation method in RelOptUtil, which is called in L87. The method actually considers a CAST on a reference as a valid column permutation of the column referenced; probably it should not.permutation is only called by this rule and UnionPullUpConstantsRule, thus it seems it is safe to fix the semantics of the method.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="16120" opendate="2017-3-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Remove empty grouping sets restriction</summary>
      <description>Queries with empty grouping sets, such as the following:SELECT a FROM T1 GROUP BY a GROUPING SETS (());are not allowed in Hive. The restriction was added in HIVE-3471, together with some negative tests. However, the reason why this restriction is included is not described in the JIRA case, and the review board link (where there might be some additional information) does not work anymore. After running some tests myself, empty grouping sets seems to be working perfectly even when it is on its own; thus, it seems we could lift this restriction.</description>
      <version>2.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16122" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE Hive Druid split introduced by HIVE-15928</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.HiveDruidSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="16123" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let user pick the granularity of bucketing and max in row memory</summary>
      <description>Currently we index the data with granularity of none which puts lot of pressure on the indexer.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="16124" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop the segments data as soon it is pushed to HDFS</summary>
      <description>Drop the pushed segments from the indexer as soon as the HDFS push is done.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="16125" opendate="2017-3-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split work between reducers.</summary>
      <description>Split work between reducer.currently we have one reducer per segment granularity even if the interval will be partitioned over multiple partitions.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="16127" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate database initialization from actual query run in TestBeeLineDriver</summary>
      <description>Improve the TestBeeLineDriver, so when running multiple tests, then reuse the default database for multiple runs. This helps to keep the runtimes in check.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.util.QFileClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16137" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default value of hive config hive.auto.convert.join.hashtable.max.entries should be set to 40m instead of 4m</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16148" opendate="2017-3-8 00:00:00" fixdate="2017-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: schema_evol_text_vec_table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.StringToDouble.java</file>
    </fixedFiles>
  </bug>
  <bug id="1615" opendate="2010-9-4 00:00:00" fixdate="2010-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web Interface JSP needs Refactoring for removed meta store methods</summary>
      <description>Some meta store methods being called from JSP have been removed. Really should prioritize compiling jsp into servlet code again.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.web.show.databases.jsp</file>
      <file type="M">hwi.web.show.database.jsp</file>
      <file type="M">hwi.web.session.result.jsp</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16152" opendate="2017-3-9 00:00:00" fixdate="2017-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestBeeLineDriver logging improvements</summary>
      <description>During the review of HIVE-16127 we agreed, that it would be great to have improved logging and error messages during the TestBeeLineDriver run.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.qfile.QFileBeeLineClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.qfile.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16166" opendate="2017-3-10 00:00:00" fixdate="2017-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 may still waste up to 15% of memory on duplicate strings</summary>
      <description>A heap dump obtained from one of our users shows that 15% of memory is wasted on duplicate strings, despite the recent optimizations that I made. The problematic strings just come from different sources this time. See the excerpt from the jxray (www.jxray.com) analysis attached.Adding String.intern() calls in the appropriate places reduces the overhead of duplicate strings with this workload to ~6%. The remaining duplicates come mostly from JDK internal and MapReduce data structures, and thus are more difficult to fix.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StringInternUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16167" opendate="2017-3-10 00:00:00" fixdate="2017-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transitive dependency on mysql connector jar</summary>
      <description>Brought in by druid storage handler transitively.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16168" opendate="2017-3-10 00:00:00" fixdate="2017-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>llap log links should use the NM nodeId port instead of web port</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16170" opendate="2017-3-10 00:00:00" fixdate="2017-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude relocation of org.apache.hadoop.security.* in the JDBC standalone jar</summary>
      <description>There has been a use case that core-site.xml file is used along with the JDBC jar, which sets "hadoop.security.group.mapping" using the class names such as "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback". This will cause CNF errors due to the renaming. So we need to exclude those security related classes in the relocation part.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16171" opendate="2017-3-10 00:00:00" fixdate="2017-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replication of truncate table</summary>
      <description>Need to support truncate table for replication. Key points to note.1. For non-partitioned table, truncate table will remove all the rows from the table.2. For partitioned tables, need to consider how truncate behaves if truncate a partition or the whole table.3. Bootstrap load with truncate table must work as it is just loadTable/loadPartition with empty dataset.4. It is suggested to re-use the alter table/alter partition events to handle truncate.5. Need to consider the case where insert event happens before truncate table which needs to see their data files through change management. The data files should be recycled to the cmroot path before trashing it.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="16176" opendate="2017-3-10 00:00:00" fixdate="2017-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SchemaTool should exit with non-zero exit code when one or more validator&amp;#39;s fail.</summary>
      <description>Currently schematool exits with a code of 0 when one or more schema tool validation fail. Ideally, it should return a non-zero exit code when any of the validators fail.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16183" opendate="2017-3-12 00:00:00" fixdate="2017-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix potential thread safety issues with static variables</summary>
      <description>Many concurrency issues (HIVE-12768, HIVE-16175, HIVE-16060) have been found with respect to class static variable usages. With fact that HS2 supports concurrent compilation and task execution as well as some backend engines (such as Spark) running multiple tasks in a single JVM, traditional assumption (or mindset) of single threaded execution needs to be abandoned.This purpose of this JIRA is to do a global scan of static variables in Hive code base, and correct potential thread-safety issues. However, it's not meant to be exhaustive.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">testutils.src.java.org.apache.hive.testutils.jdbc.HiveBurnInClient.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.RandomTypeUtil.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimalImpl.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.StringToDouble.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CommonFastHashTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CheckFastRowHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInternalInterval.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.RCFileCat.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestRCFileCat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ArchiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.VectorizerReason.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractVectorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorAppMasterEventDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorFileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorFilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorGroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorLimitDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSelectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSMBJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSparkHashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorSparkPartitionPruningSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorTableScanDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="16188" opendate="2017-3-13 00:00:00" fixdate="2017-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline should block the connection if given invalid database name.</summary>
      <description>When using beeline shell to connect to HS2 or impalaD as below -Connection to HS2 using beeline tool on port 10000 -beeline -u "jdbc:hive2://HS2-host-name:10000/default;principal=hive/HS2-host-name@DOMAIN.EXAMPLE.COM"Connection to ImpalaD using beeline tool on port 21050 -beeline -u "jdbc:hive2://impalad-host-name.com:21050/XXX;principal=impala/impalad-host-name.com@DOMAIN.EXAMPLE.COM" Providing a invalid database name as XXX - the connection is made.It should ideally stop the connection to be successfull.Even though, the beeline tool does not allow to move forward, unless you provide a valid DB name, likeUse &lt;Database-Name&gt;;</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="16189" opendate="2017-3-13 00:00:00" fixdate="2017-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table column stats might be invalidated in a failed table rename</summary>
      <description>If the table rename does not succeed due to its failure in moving the data to the new renamed table folder, the changes in TAB_COL_STATS are not rolled back which leads to invalid column stats.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.encryption.move.tbl.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16190" opendate="2017-3-13 00:00:00" fixdate="2017-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support expression in merge statement</summary>
      <description>Right now, we only support atomExpression, rather than expression in values in MergeStatement.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestMergeStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="16193" opendate="2017-3-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive show compactions not reflecting the status of the application</summary>
      <description>In a test for HIVE-13354, we set properties to make the compaction fail. Recently show compactions indicates that compactions have been succeeding on the tables though the corresponding application gets killed as expected.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
    </fixedFiles>
  </bug>
  <bug id="16194" opendate="2017-3-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables: most of the the parquet tests fail (w/o MM enabled)</summary>
      <description>See HIVE-15212, most of the parquet tests in CliDriver and MiniLlapLocalCliDriver fail with this or similar stack2017-03-13T17:51:41,820 DEBUG [a12774a2-0e25-4453-84a8-4965debd6fb5 main] parquet.MapredParquetInputFormat: Using row-mode record reader2017-03-13T17:51:41,821 ERROR [a12774a2-0e25-4453-84a8-4965debd6fb5 main] CliDriver: Failed with exception java.io.IOException:java.io.IOException: cannot find dir = file:/Users/sergey/git/hivegit/itests/qtest/target/localfs/warehouse/tbl_pred in pathToPartitionInfo: []java.io.IOException: java.io.IOException: cannot find dir = file:/Users/sergey/git/hivegit/itests/qtest/target/localfs/warehouse/tbl_pred in pathToPartitionInfo: [] at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:563) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:470) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:148) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2262) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:253)These parts did change on the branch, but they should have changed towards finding more stuff, not less stuff. Need to investigate.</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16208" opendate="2017-3-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: ProcessingModeHashAggregate::sumBatchSize is never reset</summary>
      <description>When processing &gt;2x the hash-table size in the vectorized group-by, the check for fall-back to streaming is wrong because sumBatchSize*minReduction is not reset when processing the next split.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16212" opendate="2017-3-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables: suspicious ORC HDFS counter changes</summary>
      <description>org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_llap_counters1&amp;#93; (batchId=136)org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_llap_counters&amp;#93; (batchId=139)org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_ppd_basic&amp;#93; (batchId=136)org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_ppd_schema_evol_3a&amp;#93; (batchId=137)HDFS counters for operation counts go up (which I can repro locally).</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16219" opendate="2017-3-15 00:00:00" fixdate="2017-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore notification_log contains serialized message with non functional fields</summary>
      <description>the event notification logs stored in hive metastore have json serialized messages stored in NOTIFICATION_LOG table, these messages also store the serialized Thrift API objects in them. when doing a reply dump we are however serializing both the metadata for replication event + event Message + additional helper method getters representing the thrift objects.We should only serialize metadata for replication event + event Message for ex for create table :{ "eventType": "CREATE_TABLE", "server": "", "servicePrincipal": "", "db": "default", "table": "a", "tableObjJson": "{\"1\":{\"str\":\"a\"},\"2\":{\"str\":\"default\"},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1489552350},\"5\":{\"i32\":0},\"6\":{\"i32\":0},\"7\":{\"rec\":{\"1\":{\"lst\":[\"rec\",1,{\"1\":{\"str\":\"name\"},\"2\":{\"str\":\"string\"}}]},\"2\":{\"str\":\"file:/tmp/warehouse/a\"},\"3\":{\"str\":\"org.apache.hadoop.mapred.TextInputFormat\"},\"4\":{\"str\":\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"},\"5\":{\"tf\":0},\"6\":{\"i32\":-1},\"7\":{\"rec\":{\"2\":{\"str\":\"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"},\"3\":{\"map\":[\"str\",\"str\",2,{\"field.delim\":\"\\n\",\"serialization.format\":\"\\n\"}]}}},\"8\":{\"lst\":[\"str\",0]},\"9\":{\"lst\":[\"rec\",0]},\"10\":{\"map\":[\"str\",\"str\",0,{}]},\"11\":{\"rec\":{\"1\":{\"lst\":[\"str\",0]},\"2\":{\"lst\":[\"lst\",0]},\"3\":{\"map\":[\"lst\",\"str\",0,{}]}}},\"12\":{\"tf\":0}}},\"8\":{\"lst\":[\"rec\",0]},\"9\":{\"map\":[\"str\",\"str\",7,{\"totalSize\":\"0\",\"EXTERNAL\":\"TRUE\",\"numRows\":\"0\",\"rawDataSize\":\"0\",\"COLUMN_STATS_ACCURATE\":\"{\\\"BASIC_STATS\\\":\\\"true\\\"}\",\"numFiles\":\"0\",\"transient_lastDdlTime\":\"1489552350\"}]},\"12\":{\"str\":\"EXTERNAL_TABLE\"},\"13\":{\"rec\":{\"1\":{\"map\":[\"str\",\"lst\",1,{\"anagarwal\":[\"rec\",4,{\"1\":{\"str\":\"INSERT\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}},{\"1\":{\"str\":\"SELECT\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}},{\"1\":{\"str\":\"UPDATE\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}},{\"1\":{\"str\":\"DELETE\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}}]}]}}},\"14\":{\"tf\":0}}", "timestamp": 1489552350, "files": [], "tableObj": { "tableName": "a", "dbName": "default", "owner": "anagarwal", "createTime": 1489552350, "lastAccessTime": 0, "retention": 0, "sd": { "cols": [ { "name": "name", "type": "string", "comment": null, "setName": true, "setType": true, "setComment": false } ], "location": "file:/tmp/warehouse/a", "inputFormat": "org.apache.hadoop.mapred.TextInputFormat", "outputFormat": "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat", "compressed": false, "numBuckets": -1, "serdeInfo": { "name": null, "serializationLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe", "parameters": { "serialization.format": "\n", "field.delim": "\n" }, "setName": false, "parametersSize": 2, "setParameters": true, "setSerializationLib": true }, "bucketCols": [], "sortCols": [], "parameters": {}, "skewedInfo": { "skewedColNames": [], "skewedColValues": [], "skewedColValueLocationMaps": {}, "setSkewedColNames": true, "setSkewedColValues": true, "setSkewedColValueLocationMaps": true, "skewedColNamesSize": 0, "skewedColNamesIterator": [], "skewedColValuesSize": 0, "skewedColValuesIterator": [], "skewedColValueLocationMapsSize": 0 }, "storedAsSubDirectories": false, "setSkewedInfo": true, "parametersSize": 0, "colsSize": 1, "setParameters": true, "setLocation": true, "setInputFormat": true, "setCols": true, "setOutputFormat": true, "setSerdeInfo": true, "setBucketCols": true, "setSortCols": true, "colsIterator": [ { "name": "name", "type": "string", "comment": null, "setName": true, "setType": true, "setComment": false } ], "bucketColsSize": 0, "bucketColsIterator": [], "sortColsSize": 0, "sortColsIterator": [], "setStoredAsSubDirectories": true, "setCompressed": true, "setNumBuckets": true }, "partitionKeys": [], "parameters": { "totalSize": "0", "EXTERNAL": "TRUE", "numRows": "0", "rawDataSize": "0", "COLUMN_STATS_ACCURATE": "{\"BASIC_STATS\":\"true\"}", "numFiles": "0", "transient_lastDdlTime": "1489552350" }, "viewOriginalText": null, "viewExpandedText": null, "tableType": "EXTERNAL_TABLE", "privileges": { "userPrivileges": { "anagarwal": [ { "privilege": "INSERT", "createTime": -1, "grantor": "anagarwal", "grantorType": "USER", "grantOption": true, "setCreateTime": true, "setGrantOption": true, "setPrivilege": true, "setGrantor": true, "setGrantorType": true }, { "privilege": "SELECT", "createTime": -1, "grantor": "anagarwal", "grantorType": "USER", "grantOption": true, "setCreateTime": true, "setGrantOption": true, "setPrivilege": true, "setGrantor": true, "setGrantorType": true }, { "privilege": "UPDATE", "createTime": -1, "grantor": "anagarwal", "grantorType": "USER", "grantOption": true, "setCreateTime": true, "setGrantOption": true, "setPrivilege": true, "setGrantor": true, "setGrantorType": true }, { "privilege": "DELETE", "createTime": -1, "grantor": "anagarwal", "grantorType": "USER", "grantOption": true, "setCreateTime": true, "setGrantOption": true, "setPrivilege": true, "setGrantor": true, "setGrantorType": true } ] }, "groupPrivileges": null, "rolePrivileges": null, "rolePrivilegesSize": 0, "setUserPrivileges": true, "setGroupPrivileges": false, "setRolePrivileges": false, "userPrivilegesSize": 1, "groupPrivilegesSize": 0 }, "temporary": false, "rewriteEnabled": false, "setTableName": true, "setDbName": true, "setOwner": true, "setViewOriginalText": false, "setViewExpandedText": false, "setTableType": true, "setPrivileges": true, "setCreateTime": true, "setLastAccessTime": true, "setRetention": true, "partitionKeysIterator": [], "parametersSize": 7, "setTemporary": true, "setRewriteEnabled": false, "setParameters": true, "setPartitionKeys": true, "setSd": true, "partitionKeysSize": 0 }}it should only be the json message required as :{ "eventType": "CREATE_TABLE", "server": "", "servicePrincipal": "", "db": "default", "table": "a", "tableObjJson": "{\"1\":{\"str\":\"a\"},\"2\":{\"str\":\"default\"},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1489552350},\"5\":{\"i32\":0},\"6\":{\"i32\":0},\"7\":{\"rec\":{\"1\":{\"lst\":[\"rec\",1,{\"1\":{\"str\":\"name\"},\"2\":{\"str\":\"string\"}}]},\"2\":{\"str\":\"file:/tmp/warehouse/a\"},\"3\":{\"str\":\"org.apache.hadoop.mapred.TextInputFormat\"},\"4\":{\"str\":\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"},\"5\":{\"tf\":0},\"6\":{\"i32\":-1},\"7\":{\"rec\":{\"2\":{\"str\":\"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"},\"3\":{\"map\":[\"str\",\"str\",2,{\"field.delim\":\"\\n\",\"serialization.format\":\"\\n\"}]}}},\"8\":{\"lst\":[\"str\",0]},\"9\":{\"lst\":[\"rec\",0]},\"10\":{\"map\":[\"str\",\"str\",0,{}]},\"11\":{\"rec\":{\"1\":{\"lst\":[\"str\",0]},\"2\":{\"lst\":[\"lst\",0]},\"3\":{\"map\":[\"lst\",\"str\",0,{}]}}},\"12\":{\"tf\":0}}},\"8\":{\"lst\":[\"rec\",0]},\"9\":{\"map\":[\"str\",\"str\",7,{\"totalSize\":\"0\",\"EXTERNAL\":\"TRUE\",\"numRows\":\"0\",\"rawDataSize\":\"0\",\"COLUMN_STATS_ACCURATE\":\"{\\\"BASIC_STATS\\\":\\\"true\\\"}\",\"numFiles\":\"0\",\"transient_lastDdlTime\":\"1489552350\"}]},\"12\":{\"str\":\"EXTERNAL_TABLE\"},\"13\":{\"rec\":{\"1\":{\"map\":[\"str\",\"lst\",1,{\"anagarwal\":[\"rec\",4,{\"1\":{\"str\":\"INSERT\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}},{\"1\":{\"str\":\"SELECT\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}},{\"1\":{\"str\":\"UPDATE\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}},{\"1\":{\"str\":\"DELETE\"},\"2\":{\"i32\":-1},\"3\":{\"str\":\"anagarwal\"},\"4\":{\"i32\":1},\"5\":{\"tf\":1}}]}]}}},\"14\":{\"tf\":0}}", "timestamp": 1489552350, "files": [],}this will require adding serialization features to mapper use such that it only serializes the annotated fields.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.PartitionFiles.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageDeserializer.java</file>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1622" opendate="2010-9-8 00:00:00" fixdate="2010-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use CombineHiveInputFormat for the merge job if hive.merge.mapredfiles=true</summary>
      <description>Currently map-only merge (using CombineHiveInputFormat) is only enabled for merging files generated by mappers. It should be used for files generated at readers as well.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.merge3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16242" opendate="2017-3-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run BeeLine tests parallel</summary>
      <description>Provide the ability for BeeLine tests to run parallel against the MiniHS2 cluster</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.qfile.QFile.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16245" opendate="2017-3-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Does not handle non-column key expressions in MERGEPARTIAL mode</summary>
      <description>When the planner is able to make a column a constant, MERGEPARTIAL mode in VectorGroupByOperator is broken because it doesn't evaluate the key expression. One result is execution cast exception errors.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestStructColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16254" opendate="2017-3-20 00:00:00" fixdate="2017-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metadata for values temporary tables for INSERTs are getting replicated during bootstrap</summary>
      <description>create table a (age int);insert into table a values (34),(4);repl dump default;there is a temporary table created as values_tmptable_&amp;#91;nmber&amp;#93;, which is also present in the dumped information with only metadata, this should not be processed.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.EventUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16256" opendate="2017-3-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: TestCliDriver.testCliDriver[comments]</summary>
      <description>Test has been failing for 6 consecutive runs. Most recent:https://builds.apache.org/job/PreCommit-HIVE-Build/4245/testReport/Diff:147a148&gt; COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}167a169&gt; COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.updateAccessTime.q</file>
    </fixedFiles>
  </bug>
  <bug id="16266" opendate="2017-3-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable function metadata to be written during bootstrap</summary>
      <description></description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16267" opendate="2017-3-21 00:00:00" fixdate="2017-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable bootstrap function metadata to be loaded in repl load</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.events.TestEventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.DefaultHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.VersionCompatibleSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.ReplicationSpecSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.JsonWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.FunctionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.DBSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16268" opendate="2017-3-21 00:00:00" fixdate="2017-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable incremental repl dump to handle functions metadata</summary>
      <description>this is created separately to ensure that any other metadata related to replication which comes from replication spec, if they are needed as part of the function dump output when doing incremental update.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.events.TestEventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.DefaultHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.AbstractHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16269" opendate="2017-3-21 00:00:00" fixdate="2017-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable incremental function dump to be loaded via repl load</summary>
      <description>depends if there is additional spec elements we put out as part of HIVE-16268</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DefaultHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DefaultHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16272" opendate="2017-3-21 00:00:00" fixdate="2017-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support for drop function in incremental replication</summary>
      <description>drop function should work in incremental dump and incremental load</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandlerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug id="16273" opendate="2017-3-21 00:00:00" fixdate="2017-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Make non-column key expressions work in MERGEPARTIAL mode</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16281" opendate="2017-3-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade master branch to JDK8</summary>
      <description>This is to track the JDK 8 upgrade work for the master branch.Here are threads for the discussion:https://lists.apache.org/thread.html/83d8235bc9547cc94a0d689580f20db4b946876b6d0369e31ea12b51@1460158490@%3Cdev.hive.apache.org%3Ehttps://lists.apache.org/thread.html/dcd57844ceac7faf8975a00d5b8b1825ab5544d94734734aedc3840e@%3Cdev.hive.apache.org%3EJDK7 is end of public update and some newer version of dependent libraries like jetty require newer JDK. Seems it's reasonable to upgrade to JDK8 in 2.x.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.build.properties</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug id="16282" opendate="2017-3-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin: Disable slow-start for the bloom filter aggregate task</summary>
      <description>The slow-start of the bloom filter vertex is a scheduling problem which causes more pre-emption than is useful.When the bloom filters are arranged as followsMap 1(10 tasks)&gt;Reducer 2(1 task)&gt;Map 3(100 tasks)Map 3 and Map 1 are immediately active since Reducer 2 -&gt; Map 3 is a broadcast edge.Once 3 tasks in Map 1 finish, the engine kills one active task from Map 3 to make room for Reducer 2.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16285" opendate="2017-3-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Servlet for dynamically configuring log levels</summary>
      <description>Many long running services like HS2, LLAP etc. will benefit from having an endpoint to dynamically change log levels for various loggers. This will help greatly with debuggability without requiring a restart of the service.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ExternalCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16286" opendate="2017-3-23 00:00:00" fixdate="2017-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log canceled query id</summary>
      <description>Currently, just a generic message is logged when a query is canceled. It is better to log the query id as well.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="16305" opendate="2017-3-27 00:00:00" fixdate="2017-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM</summary>
      <description>This is a followup for HIVE-16160. We see additional ClassLoaderResolverImpl leaks even with the patch.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16307" opendate="2017-3-28 00:00:00" fixdate="2017-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add IO memory usage report to LLAP UI</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1633" opendate="2010-9-13 00:00:00" fixdate="2010-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineHiveInputFormat fails with "cannot find dir for emptyFile"</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16334" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query lock contains the query string, which can cause OOM on ZooKeeper</summary>
      <description>When there are big number of partitions in a query this will result in a huge number of locks on ZooKeeper. Since the query object contains the whole query string this might cause serious memory pressure on the ZooKeeper services.It would be good to have the possibility to truncate the query strings that are written into the locks</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16335" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline user HS2 connection file should use /etc/hive/conf instead of /etc/conf/hive</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clientssays: BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/conf/hive in that order.shouldn't it be?BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/hive/conf in that order?Most distributions I've used have a /etc/hive/conf dir.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="16336" opendate="2017-3-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename hive.spark.use.file.size.for.mapjoin to hive.spark.use.ts.stats.for.mapjoin</summary>
      <description>The name hive.spark.use.file.size.for.mapjoin is confusing. It indicates that HoS uses file size for mapjoin but in fact it still uses (in-memory) data size. We should change it.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16347" opendate="2017-3-31 00:00:00" fixdate="2017-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastoreChecker should skip listing partitions which are not valid when hive.msck.path.validation is set to skip or ignore</summary>
      <description>HIVE-16299 improves msck query so that sub-directories which do not follow the partition column order as defined when table is created should not be added. It needs to skip these partitions when hive.msck.path.validation is not set to "throw". Currently it goes ahead and adds them.This was unfortunately found late in the review of HIVE-16299 and the patch was commit before it could be fixed.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="16348" opendate="2017-3-31 00:00:00" fixdate="2017-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS query is canceled but error message shows RPC is closed</summary>
      <description>When a HoS query is interrupted in getting app id, it keeps trying to get status till timedout, and return some RPC is closed error message which is misleading.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
    </fixedFiles>
  </bug>
  <bug id="16465" opendate="2017-4-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointer Exception when enable vectorization for Parquet file format</summary>
      <description>NullPointer Exception when enable vectorization for Parquet file format. It is caused by the null value of the InputSplit.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16468" opendate="2017-4-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BeeLineDriver should be able to run tests against an externally created cluster</summary>
      <description>It should be possible to run the query tests against an externally created cluster using the BeeLineDriver, and the query files and results.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16482" opendate="2017-4-19 00:00:00" fixdate="2017-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Ser/Des need to use dimension output name</summary>
      <description>Druid Ser/Desr need to use dimension output name in order to function with Extraction function.Some part of the Ser/Desr code uses the method DimensionSpec.getDimension() although when extraction function are in game the name of the dimension will be defined by DimensionSpec.getOutputName()</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16483" opendate="2017-4-19 00:00:00" fixdate="2017-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS should populate split related configurations to HiveConf</summary>
      <description>There are several split related configurations, such as MAPREDMINSPLITSIZE, MAPREDMINSPLITSIZEPERNODE, MAPREDMINSPLITSIZEPERRACK, etc., that should be populated to HiveConf. Currently we only do this for MAPREDMINSPLITSIZE.All the others, if not set, will be using the default value, which is 1.Without these, Spark sometimes will not merge small files for file formats such as text.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16485" opendate="2017-4-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable outputName for RS operator in explain formatted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.formatted.oid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explain.formatted.oid.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AnnotateReduceSinkOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Vertex.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Stage.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="16504" opendate="2017-4-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addition of binary licenses broke rat check</summary>
      <description>The clean up of Hive's licenses (HIVE-15035) broke the rat check, as all the license files get reported as invalid licenses. The rat check needs to be modified to ignore those files.</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16505" opendate="2017-4-21 00:00:00" fixdate="2017-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "unknown" boolean truth value</summary>
      <description>according to the standard, boolean truth value might be: TRUE|FALSE|UNKNOWN.similar queries to the following should be supported:select 1 where null is unknown;select 1 where (select cast(null as boolean) ) is unknown;"unknown" behaves similarily to null. (null=null) is null "All boolean values and SQL truth values are comparable and all are assignable to a site of type boolean. The value True is greater than the value False, and any comparison involving the null value or an Unknown truth value will return an Unknown result. The values True and False may be assigned to any site having a boolean data type; assignment of Unknown, or the null value, is subject to the nullability characteristic of the target." Truth table for the AND boolean operatorAND True False UnknownTrue True False UnknownFalse False False FalseUnknown Unknown False UnknownTruth table for the OR boolean operatorOR True False UnknownTrue True True TrueFalse True False UnknownUnknown True Unknown UnknownTruth table for the IS boolean operatorIS TRUE FALSE UNKNOWNTrue True False FalseFalse False True FalseUnknown False False True </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="16511" opendate="2017-4-22 00:00:00" fixdate="2017-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO looses inner casts on constants of complex type</summary>
      <description>type for map &lt;10, cast(null as int)&gt; becomes map &lt;int,string&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16530" opendate="2017-4-25 00:00:00" fixdate="2017-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HS2 operation logs and improve logs for REPL commands</summary>
      <description>This is the log format that is being proposed for Hive Repl query logsFor bootstrap case:Hive will log a message for each object as it is being bootstrapped and it will be in the following sequence Tables first (views are tables for this purpose) at time including partitions (depth first), followed by functions, constraints The ordering is based on the ordering of listStatus API of HDFS For each object, a message at the beginning of the replication will be logged Every partition bootstrapped will be followed by a message saying the number of partitions bootstrapped so far (for the table) and the partition name And a message at the end of bootstrap of an objectIncremental case: We will have DB Name, event id and event type will be part of the log header (for debugging/troubleshooting) We will have information of current event ID and total number of events to replicate for every event replicated.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug id="16542" opendate="2017-4-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make merge that targets acid 2.0 table fail-fast</summary>
      <description>Until HIVE-14947 is fixed, need to add a check so that acid 2.0 tables are not written to by Merge stmt that has both Insert and Update clauses</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16647" opendate="2017-5-11 00:00:00" fixdate="2017-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the validation output to make the output to stderr and stdout more consistent</summary>
      <description>Some output are printed to stderr or stdout inconsistently. Here are some of them. Update to make them more consistent. Version table validation When the version table is missing, the err msg goes to stderr When the version table is not valid, the err msg goes to stdout with a message like "Failed in schema version validation: &lt;err_msg&gt; Metastore/schema table validation When the version table contains the wrong version or there are no rows in the version table, err msg goes to stderr When there diffs between the schema and metastore tables, the err msg goes to stdout</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="16764" opendate="2017-5-26 00:00:00" fixdate="2017-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support numeric as same as decimal</summary>
      <description>for example numeric(12,2) -&gt; decimal(12,2) This will make Numeric reserved keyword</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.perf.query69.q</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query98.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query97.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query96.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query95.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query94.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query93.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query92.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query91.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query90.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query89.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query88.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query87.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query86.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query85.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query84.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query83.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query82.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query81.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query80.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query79.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query76.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query75.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query73.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query72.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query71.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query70.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query7.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query22.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query24.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query25.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query26.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query27.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query28.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query29.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query30.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query31.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query32.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query33.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query34.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query36.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query37.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query39.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query42.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query43.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query46.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query48.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query50.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query51.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query52.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query54.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query55.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query56.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query58.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query60.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query64.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query65.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query66.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query67.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query68.q</file>
    </fixedFiles>
  </bug>
  <bug id="17066" opendate="2017-7-10 00:00:00" fixdate="2017-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query78 filter wrong estimatation is generating bad plan</summary>
      <description>Filter operator is estimating 1 row following a left outer join causing bad estimatesReducer 12 Execution mode: vectorized, llap Reduce Operator Tree: Map Join Operator condition map: Left Outer Join0 to 1 keys: 0 KEY.reducesinkkey0 (type: bigint), KEY.reducesinkkey1 (type: bigint) 1 KEY.reducesinkkey0 (type: bigint), KEY.reducesinkkey1 (type: bigint) outputColumnNames: _col0, _col1, _col3, _col4, _col5, _col6, _col8 input vertices: 1 Map 14 Statistics: Num rows: 71676270660 Data size: 3727166074320 Basic stats: COMPLETE Column stats: COMPLETE Filter Operator predicate: _col8 is null (type: boolean) Statistics: Num rows: 1 Data size: 52 Basic stats: COMPLETE Column stats: COMPLETE Select Operator expressions: _col0 (type: bigint), _col1 (type: bigint), _col3 (type: int), _col4 (type: double), _col5 (type: double), _col6 (type: bigint) outputColumnNames: _col0, _col1, _col3, _col4, _col5, _col6 Statistics: Num rows: 1 Data size: 52 Basic stats: COMPLETE Column stats: COMPLETE</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="17181" opendate="2017-7-27 00:00:00" fixdate="2017-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatOutputFormat should expose complete output-schema (including partition-keys) for dynamic-partitioning MR jobs</summary>
      <description>Map/Reduce jobs that use HCatalog APIs to write to Hive tables using Dynamic partitioning are expected to call the following API methods: HCatOutputFormat.setOutput() to indicate which table/partitions to write to. This call populates the OutputJobInfo with details fetched from the Metastore. HCatOutputFormat.setSchema() to indicate the output-schema for the data being written.It is a common mistake to invoke HCatOUtputFormat.setSchema() as follows:HCatOutputFormat.setSchema(conf, HCatOutputFormat.getTableSchema(conf));Unfortunately, getTableSchema() returns only the record-schema, not the entire table's schema. We'll need a better API for use in M/R jobs to get the complete table-schema.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17208" opendate="2017-7-30 00:00:00" fixdate="2017-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl dump should pass in db/table information to authorization API</summary>
      <description>"repl dump" does not provide db/table information. That is necessary for authorization replication in ranger.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.repl.load.requires.admin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.repl.dump.requires.admin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="17233" opendate="2017-8-2 00:00:00" fixdate="2017-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set "mapred.input.dir.recursive" for HCatInputFormat-based jobs.</summary>
      <description>This has to do with HIVE-15575. TezCompiler seems to set mapred.input.dir.recursive to true. This is acceptable for Hive jobs, since this allows Hive to consume its peculiar UNION ALL output, where the output of each relation is stored in a separate sub-directory of the output-dir.For such output to be readable through HCatalog (via Pig/HCatLoader), mapred.input.dir.recursive should be set from HCatInputFormat as well. Otherwise, one gets zero records for that input.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="17241" opendate="2017-8-3 00:00:00" fixdate="2017-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change metastore classes to not use the shims</summary>
      <description>As part of moving the metastore into a standalone package, it will no longer have access to the shims. This means we need to either copy them or access the underlying Hadoop operations directly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.StorageBasedMetastoreTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
    </fixedFiles>
  </bug>
  <bug id="1731" opendate="2010-10-19 00:00:00" fixdate="2010-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve miscellaneous error messages</summary>
      <description>This is a place for accumulating error message improvements so that we can update a bunch in batch.</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.function.param2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.dot.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.insert.wrong.number.columns.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.in.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.not.bool.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.function.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.index.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby.key.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.nospec.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.noof.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sample.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17352" opendate="2017-8-17 00:00:00" fixdate="2017-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveSever2 error with "Illegal Operation state transition from CLOSED to FINISHED"</summary>
      <description>HiveSever2 error with "Illegal Operation state transition from CLOSED to FINISHED"Many cases like CANCELED, TIMEDOUT AND CLOSED are handled. Need to handle FINISHED in runQuery() method.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="17391" opendate="2017-8-25 00:00:00" fixdate="2017-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction fails if there is an empty value in tblproperties</summary>
      <description>create table t1 (a int) tblproperties ('serialization.null.format'='');alter table t1 compact 'major';fails</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StringableMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="17429" opendate="2017-9-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC doesn&amp;#39;t return rows when querying Impala</summary>
      <description>The Hive JDBC driver used to return a result set when querying Impala. Now, instead, it gets data back but interprets the data as query logs instead of a resultSet. This causes many issues (we see complaints about beeline as well as test failures).This appears to be a regression introduced with asynchronous operation against Hive.Ideally, we could make both behaviors work. I have a simple patch that should fix the problem.</description>
      <version>2.1.0,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>2.1.0,2.1.1,2.2.1,2.3.4,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="1743" opendate="2010-10-22 00:00:00" fixdate="2010-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group-by to determine equals of Keys in reverse order</summary>
      <description>When processing group-by, in reduce side, keys are ordered. Comparing equality of two keys can be more efficient in reverse order.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17459" opendate="2017-9-6 00:00:00" fixdate="2017-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>View deletion operation failed to replicate on target cluster</summary>
      <description>View dropping is not replicated during incremental repl.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17472" opendate="2017-9-6 00:00:00" fixdate="2017-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop-partition for multi-level partition fails, if data does not exist.</summary>
      <description>Raising this on behalf of cdrome and selinazh. Here's how to reproduce the problem:CREATE TABLE foobar ( foo STRING, bar STRING ) PARTITIONED BY ( dt STRING, region STRING ) STORED AS RCFILE LOCATION '/tmp/foobar';ALTER TABLE foobar ADD PARTITION ( dt='1', region='A' ) ;dfs -rm -R -skipTrash /tmp/foobar/dt=1;ALTER TABLE foobar DROP PARTITION ( dt='1' );This causes a client-side error as follows:15/02/26 23:08:32 ERROR exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: Unknown error. Please check logs.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.3.2,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17473" opendate="2017-9-7 00:00:00" fixdate="2017-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement workload management pools</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17475" opendate="2017-9-7 00:00:00" fixdate="2017-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable mapjoin using hint</summary>
      <description>Using hint disable mapjoin for a given query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17534" opendate="2017-9-14 00:00:00" fixdate="2017-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a config to turn off parquet vectorization</summary>
      <description>It should be a good addition to give an option for users to turn off parquet vectorization without affecting vectorization on other file formats.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17536" opendate="2017-9-14 00:00:00" fixdate="2017-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StatsUtil::getBasicStatForTable doesn&amp;#39;t distinguish b/w absence of statistics or zero stats</summary>
      <description>This method returns zero for both of the following cases: Statistics are missing in metastore Actual stats e.g. number of rows are zeroIt'll be good for this method to return e.g. -1 in absence of statistics instead of assuming it to be zero.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.number.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.nway.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.table.with.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.is.not.distinct.from.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge6.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17538" opendate="2017-9-14 00:00:00" fixdate="2017-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance estimation of stats to estimate even if only one column is missing stats</summary>
      <description>HIVE-16811 provided support for estimating statistics in absence of stats. But that estimation is done if and only if statistics are missing for all columns.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17574" opendate="2017-9-21 00:00:00" fixdate="2017-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid multiple copies of HDFS-based jars when localizing job-jars</summary>
      <description>Raising this on behalf of Selina Zhang. (For my own reference: YHIVE-1035.)This has to do with the classpaths of Hive actions run from Oozie, and affects scripts that adds jars/resources from HDFS locations.As part of Oozie's "sharelib" deploys, foundation jars (such as Hive jars) tend to be stored in HDFS paths, as are any custom user-libraries used in workflows. An ADD JAR|FILE|ARCHIVE statement in a Hive script causes the following steps to occur: Files are downloaded from HDFS to local temp dir. UDFs are resolved/validated. All jars/files, including those just downloaded from HDFS, are shipped right back to HDFS-based scratch-directories, for job submission.For HDFS-based files, this is wasteful and time-consuming. #3 above should skip shipping HDFS-based resources, and add those directly to the Tez session.We have a patch that's being used internally at Yahoo.</description>
      <version>2.2.0,2.4.0,3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestAddResource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17601" opendate="2017-9-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve error handling in LlapServiceDriver</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17604" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add druid properties to conf white list</summary>
      <description>Currently throws:Error: Error while processing statement: Cannot modify hive.druid.select.distribute at runtime. It is not in list of params that are allowed to be modified at runtime (state=42000,code=1)</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17606" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve security for DB notification related APIs</summary>
      <description>The purpose is to make sure only the superusers which are specified in the proxyuser settings can make the db notification related API calls, since this is supposed to be called by superuser/admin instead of any end user.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestExportImport.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestCopyUtils.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17607" opendate="2017-9-26 00:00:00" fixdate="2017-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove ColumnStatsDesc usage from columnstatsupdatetask</summary>
      <description>it's not entirely connected to this task...it should either has its own descriptor; or work sould take on the: tablename/coltype/colname payload</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="17609" opendate="2017-9-26 00:00:00" fixdate="2017-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to manipulate delegation tokens</summary>
      <description>This was precipitated by OOZIE-2797. We had a case in production where the number of active metastore delegation tokens outstripped the ZooKeeper jute.maxBuffer size. Delegation tokens could neither be fetched, nor be cancelled. The root-cause turned out to be a miscommunication, causing delegation tokens fetched by Oozie not to be cancelled automatically from HCat. This was sorted out as part of OOZIE-2797.The issue exposed how poor the log-messages were, in the code pertaining to token fetch/cancellation. We also found need for a tool to query/list/purge delegation tokens that might have expired already. This patch introduces such a tool, and improves the log-messages.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.Security.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1761" opendate="2010-11-1 00:00:00" fixdate="2010-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support show locks for a particular table</summary>
      <description>Currently, only show locks is supported - it would be very useful to show locks for a particular table</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lock2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lock1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lock2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.lock1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17610" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: an exception in exception handling can hide the original exception</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17613" opendate="2017-9-27 00:00:00" fixdate="2017-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove object pools for short, same-thread allocations</summary>
      <description>Objects pools probably don't have much effect in this case.They are only useful when allocations are shared between threads.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="17621" opendate="2017-9-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-site settings are ignored during HCatInputFormat split-calculation</summary>
      <description>Another one that Selina Zhang and Chris Drome worked on.The production hive-site.xml could well contain settings that differ from the defaults in HiveConf.java. In our case, we introduced a custom ORC split-strategy, which we introduced as the site-wide default.We noticed that during HCatInputFormat::getSplits(), if the user-script did not contain the setting, the site-wide default was ignored in favour of the HiveConf default. HCat would not convey hive-site settings to the input-format (or anywhere downstream).The forthcoming patch fixes this problem.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="17661" opendate="2017-9-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBTxnManager.acquireLocks() - MM tables should use shared lock for Insert</summary>
      <description>case INSERT: assert t != null; if(AcidUtils.isFullAcidTable(t)) { compBuilder.setShared(); } else { if (conf.getBoolVar(HiveConf.ConfVars.HIVE_TXN_STRICT_LOCKING_MODE)) {if(AcidUtils.isFullAcidTable(t)) { should probably be if(AcidUtils.isAcidTable(t)) {</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.mm.truncate.cols.q</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.convert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17664" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description></description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17669" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache to optimize SearchArgument deserialization</summary>
      <description>And another, from selinazh and cdrome. (YHIVE-927)When a mapper needs to process multiple ORC files, it might land up having use essentially the same SearchArgument over several files. It would be good not to have to deserialize from string, over and over again. Caching the object against the string-form should speed things up.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1767" opendate="2010-11-4 00:00:00" fixdate="2010-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge files does not work with dynamic partition</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17756" opendate="2017-10-10 00:00:00" fixdate="2017-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable subquery related Qtests for Hive on Spark</summary>
      <description>HIVE-15456 and HIVE-15192 using Calsite to decorrelate and plan subqueries. This JIRA is to indroduce subquery test and verify the subqueries plan for Hive on Spark</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17771" opendate="2017-10-11 00:00:00" fixdate="2017-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement commands to manage resource plan</summary>
      <description>Please see parent jira about llap workload management.This jira is to implement create and show resource plan commands in hive to configure resource plans for llap workload. The following are the proposed commands implemented as part of the jira:CREATE RESOURCE PLAN plan_name WITH QUERY_PARALLELISM parallelism;SHOW RESOURCE PLAN plan_name;SHOW RESOURCE PLANS;ALTER RESOURCE PLAN plan_name SET QUERY_PARALLELISM = parallelism;ALTER RESOURCE PLAN plan_name RENAME TO new_name;ALTER RESOURCE PLAN plan_name ACTIVATE;ALTER RESOURCE PLAN plan_name DISABLE;ALTER RESOURCE PLAN plan_name ENABLE;DROP RESOURCE PLAN;It will be followed up with more jiras to manage pools, triggers and copy resource plans.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug id="17792" opendate="2017-10-12 00:00:00" fixdate="2017-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Bucket Map Join when there are extra keys other than bucketed columns</summary>
      <description>Currently this wont go through Bucket Map Join(BMJ)CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;select a.key, a.value, b.valuefrom tab a join tab_part b on a.key = b.key and a.value = b.value;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="17874" opendate="2017-10-22 00:00:00" fixdate="2017-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet vectorization fails on tables with complex columns when there are no projected columns</summary>
      <description>When a parquet table contains an unsupported type like Map, LIST or UNION simple queries like select count from table fails with unsupported type exception even though vectorized reader doesn't really need read the complex type into batches.</description>
      <version>2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18251" opendate="2017-12-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loosen restriction for some checks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18273" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LLAP-level counters for WM</summary>
      <description>On query fragment level (like IO counters)time queued as guaranteed;time running as guaranteed;time running as speculative.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="18274" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add AM level metrics for WM</summary>
      <description>Unused guaranteed tasks (1 metric); guaranteed/speculative tasks x updated/update in progress (4 metrics).It should be possible to view those over time as the query is (was) running, to detect any anomalies. This jira is just to save the correct metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="18275" opendate="2017-12-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HS2-level WM metrics</summary>
      <description>E.g. time spent in pool queue. Some existing UIs use perflogger output, so we should also include that.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.QueryExecutionBreakdownSummary.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="18306" opendate="2017-12-19 00:00:00" fixdate="2017-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spark smb tests</summary>
      <description>seems to me that TestSparkCliDriver#testCliDriver[auto_sortmerge_join_10] and TestSparkCliDriver#testCliDriver[bucketsortoptimize_insert_7] is failing since HIVE-18208 is in.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="18415" opendate="2018-1-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower "Updating Partition Stats" Logging Level</summary>
      <description>org.apache.hadoop.hive.metastore.utils.MetaStoreUtilsLOG.warn("Updating partition stats fast for: " + part.getTableName());...LOG.warn("Updated size to " + params.get(StatsSetupConst.TOTAL_SIZE));This logging produces many lines of WARN log messages in my log file and it's not clear to me what the issue is here. Why is this a warning and how should I respond to address this warning?DEBUG is probably more appropriate for a utility class. Please lower.</description>
      <version>1.2.2,2.2.0,2.3.2,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18421" opendate="2018-1-10 00:00:00" fixdate="2018-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized execution handles overflows in a different manner than non-vectorized execution</summary>
      <description>In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.Simple test case to reproduce this issueset hive.vectorized.execution.enabled=true;create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;insert into parquettable values (-104, 25), (-112, 24), (54, 9);select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) &lt; 50 order by diff desc;+-------+-----+-------+| t1 | t2 | diff |+-------+-----+-------+| -104 | 25 | 127 || -112 | 24 | 120 || 54 | 9 | 45 |+-------+-----+-------+When vectorization is turned off the same query produces only one row.</description>
      <version>2.1.1,2.2.0,2.3.2,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorTestCode.java</file>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestUnaryMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.java</file>
      <file type="M">ql.src.gen.vectorization.TestTemplates.TestClass.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ScalarArithmeticColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnUnaryMinus.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnDivideColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.ColumnArithmeticColumn.txt</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizedArithmeticBench.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18443" opendate="2018-1-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure git gc finished in ptest prep phase before copying repo</summary>
      <description>In ptest's prep phase script first we checkout the latest Hive code from git, and then we make copy of its contents (along .git folder) for that will serve as Yetus' working directory.In some cases we can see errors such as+ cp -R . ../yetuscp: cannot stat ?./.git/gc.pid?: No such file or directorye.g. hereThis is caused by git running its gc feature in the background when our prep script has already started copying. In cases where gc finishes while cp is running, we'll get this error</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug id="19083" opendate="2018-3-30 00:00:00" fixdate="2018-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19302" opendate="2018-4-25 00:00:00" fixdate="2018-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logging Too Verbose For TableNotFound</summary>
      <description>There is way too much logging when a user submits a query against a table which does not exist.  In an ad-hoc setting, it is quite normal that a user fat-fingers a table name.  Yet, from the perspective of the Hive administrator, there was perhaps a major issue based on the volume and severity of logging.  Please change the logging to INFO level, and do not present a stack trace, for such a trivial error. See the attached file for a sample of what logging a single "table not found" query generates.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19306" opendate="2018-4-25 00:00:00" fixdate="2018-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow batch serializer</summary>
      <description>Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19716" opendate="2018-5-25 00:00:00" fixdate="2018-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set spark.local.dir for a few more HoS integration tests</summary>
      <description>There are a few more flaky tests that are failing because they run HoS queries that writes some temp data to /tmp/. These tests are regular JUnit tests, so they weren't covered in the previous attempts to do this.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.exec.spark.TestSparkStatistics.java</file>
    </fixedFiles>
  </bug>
  <bug id="21009" opendate="2018-12-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP - Specify binddn for ldap-search</summary>
      <description>When user accounts cannot do an LDAP search, there is currently no way of specifying a custom binddn to use for the ldap-search.So I'm missing something like that:hive.server2.authentication.ldap.bindn=cn=ldapuser,ou=user,dc=examplehive.server2.authentication.ldap.bindnpw=password</description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2225" opendate="2011-6-16 00:00:00" fixdate="2011-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Purge expired events</summary>
      <description>HIVE-2215 adds the ability to add events in metastore. These events needs to be purged as they have limited life.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMarkPartition.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22606" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AvroSerde logs avro.schema.literal under INFO level</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="22673" opendate="2019-12-28 00:00:00" fixdate="2019-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in contrib Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="22675" opendate="2019-12-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hive-standalone-metastore Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DBTokenStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22679" opendate="2019-12-29 00:00:00" fixdate="2019-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in metastore-common Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge.java</file>
    </fixedFiles>
  </bug>
  <bug id="2385" opendate="2011-8-17 00:00:00" fixdate="2011-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local Mode can be more aggressive if LIMIT optimization is on</summary>
      <description>Local mode now depends on total input data, but for LIMIT queries with no filtering, the data actually scanned can be much less and it's relatively predictable. We can place local mode more aggressively.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8472" opendate="2014-10-15 00:00:00" fixdate="2014-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ALTER DATABASE SET LOCATION</summary>
      <description>Similarly to ALTER TABLE tablename SET LOCATION, it would be helpful if there was an equivalent for databases.</description>
      <version>2.2.0,2.4.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
