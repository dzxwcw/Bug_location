<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="1039" opendate="2010-1-11 00:00:00" fixdate="2010-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>multi-insert doesn&amp;#39;t work for local directories</summary>
      <description>As wd pointed out in hive-user, the following query only load data to the first local directory. Multi-insert to tables works fine. hive&gt; from test &gt; INSERT OVERWRITE LOCAL DIRECTORY '/home/stefdong/tmp/0' select *where a = 1 &gt; INSERT OVERWRITE LOCAL DIRECTORY '/home/stefdong/tmp/1' select *where a = 3;</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1045" opendate="2010-1-12 00:00:00" fixdate="2010-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>(bigint % int) should return bigint instead of double</summary>
      <description>This expression should return bigint instead of double.CREATE TABLE test (a BIGINT);EXPLAIN SELECT a % 3 FROM test;There must be something wrong in FunctionRegistry.getMethodInternal</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1056" opendate="2010-1-16 00:00:00" fixdate="2010-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Predicate push down does not work with UDTF&amp;#39;s</summary>
      <description>Predicate push down does not work with UDTF's in lateral viewshive&gt; SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS k WHERE k=1;FAILED: Unknown exception: nullhive&gt;</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen-php.queryplan.types.php</file>
      <file type="M">ql.src.gen-javabean.org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10603" opendate="2015-5-4 00:00:00" fixdate="2015-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase default permgen space for HS2 on windows</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.hiveserver2.cmd</file>
    </fixedFiles>
  </bug>
  <bug id="1086" opendate="2010-1-23 00:00:00" fixdate="2010-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "-Doffline=true" option to ant</summary>
      <description>Currently I am seeing ivy retrieve for 4 times, each time for 4 of the hadoop versions.It takes a long time.ivy-retrieve-hadoop-source:[ivy:retrieve] :: Ivy 2.0.0-rc2 - 20081028224207 :: http://ant.apache.org/ivy/ :::: loading settings :: file = /hive/trunk/VENDOR.hive/trunk/ivy/ivysettings.xml[ivy:retrieve] :: resolving dependencies :: org.apache.hadoop.hive#shims;working@zshao.com[ivy:retrieve] confs: [default][ivy:retrieve] found hadoop#core;0.17.2.1 in hadoop-source[ivy:retrieve] found hadoop#core;0.18.3 in hadoop-source...We should fix this problem. Also it will help if we can add an option "offline" like what hadoop has.</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1116" opendate="2010-1-28 00:00:00" fixdate="2010-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bug with alter table rename when table has property EXTERNAL=FALSE</summary>
      <description>if the location is not an external location - this would be safer.the problem right now is that it's tricky to use the drop and rename way of writing new data into a table. consider:Initialization block:drop table a_tmpcreate table a_tmp like a;Loading block:load data &lt;newdata&gt; into a_tmp;drop table a;alter table a_tmp rename to a;this looks safe. but it's not. if one runs this multiple times - then data is lost (since 'a' is pointing to 'a_tmp''s location after any iteration. and dropping table 'a' blows away loaded data in the next iteration). if the location is being managed by Hive - then 'rename' should switch location as well.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter3.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11290" opendate="2015-7-17 00:00:00" fixdate="2015-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cursor attributes %ISOPEN, %FOUND, %NOTFOUND and SYS_REFCURSOR variable</summary>
      <description>Cursor attributes allow you to get information about the current cursor state. Cursor variable can be used to pass result sets between routines.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Query.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="11291" opendate="2015-7-17 00:00:00" fixdate="2015-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid allocation storm while doing rule matching on operator/expression trees</summary>
      <description>RuleRegExMatch repeatedly allocates string while trying to find a matching pattern. This results in huge GC churn for large trees.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleExactMatch.java</file>
    </fixedFiles>
  </bug>
  <bug id="11390" opendate="2015-7-28 00:00:00" fixdate="2015-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Fix table alias propagation for windowing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="1158" opendate="2010-2-11 00:00:00" fixdate="2010-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introducing a new parameter for Map-side join bucket size</summary>
      <description>Map-side join cache the small table in memory and join with the split of the large table at the mapper side. If the small table is too large, it uses RowContainer to cache a number of rows indicated by parameter hive.join.cache.size, whose default value is 25000. This parameter is also used for regular reducer-side joins to cache all input tables except the streaming table. This default value is too large for map-side join bucket size, resulting in OOM exceptions sometimes. We should define a different parameter to separate these two cache sizes.</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1188" opendate="2010-2-22 00:00:00" fixdate="2010-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when running TestJdbcDriver/TestHiveServer</summary>
      <description>% ant test -Dtestcase=TestJdbcDriverBUILD FAILED/Users/carl/Projects/hive/hd11/hive/build.xml:154: The following error occurred while executing this line:/Users/carl/Projects/hive/hd11/hive/build.xml:93: The following error occurred while executing this line:/Users/carl/Projects/hive/hd11/hive/contrib/build.xml:77: java.lang.NullPointerException at java.util.Arrays$ArrayList.&lt;init&gt;(Arrays.java:3357) at java.util.Arrays.asList(Arrays.java:3343) at org.apache.hadoop.hive.ant.QTestGenTask.execute(QTestGenTask.java:248) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:390) at org.apache.tools.ant.Target.performTasks(Target.java:411) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360) at org.apache.tools.ant.helper.SingleCheckExecutor.executeTargets(SingleCheckExecutor.java:38) at org.apache.tools.ant.Project.executeTargets(Project.java:1212) at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:441) at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:302) at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:221) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:68) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.taskdefs.MacroInstance.execute(MacroInstance.java:398) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:390) at org.apache.tools.ant.Target.performTasks(Target.java:411) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360) at org.apache.tools.ant.Project.executeTarget(Project.java:1329) at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41) at org.apache.tools.ant.Project.executeTargets(Project.java:1212) at org.apache.tools.ant.Main.runBuild(Main.java:801) at org.apache.tools.ant.Main.startAnt(Main.java:218) at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280) at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)TestHiveServer throws the same error.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1255" opendate="2010-3-18 00:00:00" fixdate="2010-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add mathematical UDFs PI, E, degrees, radians, tan, sign, and atan</summary>
      <description>Add support for PI, E, degrees, radians, tan, sign and atan</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1271" opendate="2010-3-23 00:00:00" fixdate="2010-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Case sensitiveness of type information specified when using custom reducer causes type mismatch</summary>
      <description>Type information specified while using a custom reduce script is converted to lower case, and causes type mismatch during query semantic analysis . The following REDUCE query where field name = "userId" failed.hive&gt; CREATE TABLE SS (&gt; a INT,&gt; b INT,&gt; vals ARRAY&lt;STRUCT&lt;userId:INT, y:STRING&gt;&gt;&gt; );OKhive&gt; FROM (select * from srcTable DISTRIBUTE BY id SORT BY id) s&gt; INSERT OVERWRITE TABLE SS&gt; REDUCE *&gt; USING 'myreduce.py'&gt; AS&gt; (a INT,&gt; b INT,&gt; vals ARRAY&lt;STRUCT&lt;userId:INT, y:STRING&gt;&gt;&gt; )&gt; ;FAILED: Error in semantic analysis: line 2:27 Cannot insert intotarget table because column number/types are different SS: Cannotconvert column 2 from array&lt;struct&lt;userId:int,y:string&gt;&gt; toarray&lt;struct&lt;userid:int,y:string&gt;&gt;.The same query worked fine after changing "userId" to "userid".</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12710" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add better logging for Tez session creation thread failures</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="12711" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document howto disable web ui in config of hive.server2.webui.port</summary>
      <description>hive.server2.webui.port config does not say that it can be used to disable webui as well.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1281" opendate="2010-3-24 00:00:00" fixdate="2010-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucketing column names in create table should be case-insensitive</summary>
      <description>This create table fails because 'userId' != 'userid'CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1296" opendate="2010-4-9 00:00:00" fixdate="2010-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI set and set -v commands should dump properties in alphabetical order</summary>
      <description>This makes it easier to find properties by name when dumping them all at once.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12960" opendate="2016-1-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Column Stats Extrapolation and UniformDistribution to HBaseStore</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCacheWithBitVector.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BooleanColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BinaryColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.StatsCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12970" opendate="2016-1-31 00:00:00" fixdate="2016-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add total open connections in HiveServer2</summary>
      <description>I add the metrics to HiveServer2 in order to confirm the change per unit time. I will be able to use the information at the time of monitoring.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="1335" opendate="2010-5-3 00:00:00" fixdate="2010-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataNucleus should use connection pooling</summary>
      <description>Currently each Data Nucleus operation disconnects and reconnects to the MetaStore over jdbc. Queries fail to even explain properly in cases where a table has many partitions. This is fixed by enabling one parameter and including several jars.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1350" opendate="2010-5-18 00:00:00" fixdate="2010-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.query.id is not unique</summary>
      <description>if commands are executed by the same user within a second</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13501" opendate="2016-4-13 00:00:00" fixdate="2016-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invoke failure hooks if query fails on exception</summary>
      <description>When a query fails on some exception, failure hooks are not called currently. It's better to invoke such hooks so that we know the query is failed.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13583" opendate="2016-4-21 00:00:00" fixdate="2016-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>E061-14: Search Conditions</summary>
      <description>This is a part of the SQL:2011 Analytics Complete Umbrella JIRA HIVE-13554. Support for various forms of search conditions are mandatory in the SQL standard. For example, "&lt;predicate&gt; is not true;" Hive should support those forms mandated by the standard.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestMergeStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="1360" opendate="2010-5-21 00:00:00" fixdate="2010-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow UDFs to access constant parameter values at compile time</summary>
      <description>UDFs should be able to access constant parameter values at compile time.</description>
      <version>0.5.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug id="1364" opendate="2010-5-24 00:00:00" fixdate="2010-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the maximum length of various metastore fields, and remove TYPE_NAME from COLUMNS primary key</summary>
      <description>The value component of a SERDEPROPERTIES key/value pair is currently limitedto a maximum length of 767 characters. I believe that the motivation for limiting the length to 767 characters is that this value is the maximum allowed length of an index ina MySQL database running on the InnoDB engine: http://bugs.mysql.com/bug.php?id=13315 The Metastore OR mapping currently limits many fields (including SERDEPROPERTIES.PARAM_VALUE) to a maximum length of 767 characters despite the fact that these fields are not indexed. The maximum length of a VARCHAR value in MySQL 5.0.3 and later is 65,535. We can expect many users to hit the 767 character limit on SERDEPROPERTIES.PARAM_VALUE when using the hbase.columns.mapping serdeproperty to map a table that has many columns.I propose increasing the maximum allowed length of SERDEPROPERTIES.PARAM_VALUE to 8192.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13800" opendate="2016-5-20 00:00:00" fixdate="2016-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable auth enabled by default on LLAP UI for secure clusters</summary>
      <description>There's no sensitive information that I'm aware of. (The logs would be the most sensitive).Similar to the HS2 UI, the LLAP UI can be default unprotected even on secure clusters.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13921" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix constprog_partitioner for HoS</summary>
      <description>index_bitmap3 and constprog_partitioner have been failing. Let's fix them here.EDIT: index_bitmap3 will be fixed via HIVE-13997.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="1401" opendate="2010-6-11 00:00:00" fixdate="2010-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web Interface can ony browse default</summary>
      <description></description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.web.show.database.jsp</file>
      <file type="M">hwi.web.set.processor.jsp</file>
      <file type="M">hwi.web.session.result.jsp</file>
      <file type="M">hwi.web.index.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14041" opendate="2016-6-17 00:00:00" fixdate="2016-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>llap scripts add hadoop and other libraries from the machine local install to the daemon classpath</summary>
      <description>`hadoop classpath` ends up getting added to the classpath of llap daemons. This essentially means picking up the classpath from the local deploy.This isn't required since the slider package includes relevant libraries (shipped from the client)</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1405" opendate="2010-6-14 00:00:00" fixdate="2010-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive command line option -i to run an init file before other SQL commands</summary>
      <description>When deploying hive, it would be nice to have a .hiverc file containing statements that would be automatically run whenever hive is launched. This way, we can automatically add JARs, create temporary functions, set flags, etc. for all users quickly. This should ideally be set up like .bashrc and the like with a global version and a user-local version.</description>
      <version>0.5.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1411" opendate="2010-6-15 00:00:00" fixdate="2010-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataNucleus throws NucleusException if core-3.1.1 JAR appears more than once on CLASSPATH</summary>
      <description>DataNucleus barfs when the core-3.1.1 JAR file appears more than once on the CLASSPATH:2010-03-06 12:33:25,565 ERROR exec.DDLTask (SessionState.java:printError(279)) - FAILED: Error in metadata: javax.jdo.JDOFatalInter nalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:258) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:879) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:103) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:379) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:285) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156) Caused by: javax.jdo.JDOFatalInternalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1186)at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:803) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:698) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:164) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:181)at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:125) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:104) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:130)at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:146)at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:118) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.(HiveMetaStore.java:100) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:74) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:783) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:794) at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:252) ... 12 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:597) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1956) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1951) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1159)... 28 more Caused by: org.datanucleus.exceptions.NucleusException: Plugin (Bundle) "org.eclipse.jdt.core" is already registered. Ensure you do nt have multiple JAR versions of the same plugin in the classpath. The URL "file:/Users/hadop/hadoop-0.20.1+152/build/ivy/lib/Hadoo p/common/core-3.1.1.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/Users/hado p/hadoop-0.20.1+152/lib/core-3.1.1.jar." at org.datanucleus.plugin.NonManagedPluginRegistry.registerBundle(NonManagedPluginRegistry.java:437)at org.datanucleus.plugin.NonManagedPluginRegistry.registerBundle(NonManagedPluginRegistry.java:343)at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensions(NonManagedPluginRegistry.java:227)at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensionPoints(NonManagedPluginRegistry.java:159)at org.datanucleus.plugin.PluginManager.registerExtensionPoints(PluginManager.java:82) at org.datanucleus.OMFContext.(OMFContext.java:164) at org.datanucleus.OMFContext.(OMFContext.java:145) at org.datanucleus.ObjectManagerFactoryImpl.initialiseOMFContext(ObjectManagerFactoryImpl.java:143)at org.datanucleus.jdo.JDOPersistenceManagerFactory.initialiseProperties(JDOPersistenceManagerFactory.java:317)at org.datanucleus.jdo.JDOPersistenceManagerFactory.(JDOPersistenceManagerFactory.java:261)at org.datanucleus.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:174)... 36 more 2010-03-06 12:33:25,575 ERROR ql.Driver (SessionState.java:printError(279)) - FAILED: Execution Error, return code 1 from org.apach e.hadoop.hive.ql.exec.DDLTask 2010-03-06 12:42:30,457 ERROR exec.DDLTask (SessionState.java:printError(279)) - FAILED: Error in metadata: javax.jdo.JDOFatalInter nalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Unexpected exception caught. NestedThrowables:</description>
      <version>0.4.0,0.4.1,0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1414" opendate="2010-6-18 00:00:00" fixdate="2010-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>automatically invoke .hiverc init script</summary>
      <description>Similar to .bashrc but run Hive SQL commands.</description>
      <version>0.5.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.stylesheets.project.xml</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14140" opendate="2016-6-30 00:00:00" fixdate="2016-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: package codec jars</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="1415" opendate="2010-6-18 00:00:00" fixdate="2010-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add CLI command for executing a SQL script</summary>
      <description>Suggestion in HIVE-1405 was "source", e.g.source somescript.sql;</description>
      <version>0.5.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.xdocs.language.manual.cli.xml</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14180" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable LlapZookeeperRegistry ZK auth setup for external clients</summary>
      <description>Caused by: org.apache.hadoop.service.ServiceStateException: java.io.IOException: Llap Kerberos keytab is emptyat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)at org.apache.hadoop.service.AbstractService.start(AbstractService.java:204)at org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.getClient(LlapRegistryService.java:67)at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:238)at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:142)at org.apache.hadoop.hive.llap.LlapRowInputFormat.getRecordReader(LlapRowInputFormat.java:51)Using the LLAP ZK registry in environments other than the LLAP daemon (such as external LLAP clients), there should be a way to skip this ZK auth setup.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14182" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert "HIVE-13084: Vectorization add support for PROJECTION Multi-AND/OR</summary>
      <description>To many issues with scratch column allocation.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14262" opendate="2016-7-17 00:00:00" fixdate="2016-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inherit writetype from partition WriteEntity for table WriteEntity</summary>
      <description>For partitioned table operations, a Table WriteEntity is being added to the list to be authorized if there is a partition in the output list from semantic analyzer. However, it is being added with a default WriteType of DDL_NO_TASK.The new Table WriteEntity should be created with the WriteType of the partition WriteEntity.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14263" opendate="2016-7-18 00:00:00" fixdate="2016-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log message when HS2 query is waiting on compile lock</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14444" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade qtest execution framework to junit4 - migrate most of them</summary>
      <description>this is the second step..migrating all exiting qtestgen generated tests to junit4it might be possible that not all will get migrated in this ticket...I will leave out the problematic ones...</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestPerfCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestBeeLineDriver.vm</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.antlib.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14445" opendate="2016-8-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade maven surefire to 2.19.1</summary>
      <description>newer maven surefire has a great feature: it is possible to select testmethods by regular expressions...and there are also improvements in using '#' to address testmethodsi've looked into this earlier...the upgrade is "almost" seemless...i'm already using 2.19.1, but the spark modules don't really like the empty spark.home variable</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1465" opendate="2010-7-14 00:00:00" fixdate="2010-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-site.xml ${user.name} not replaced for local-file derby metastore connection URL</summary>
      <description>Seems that for this parameter&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:derby:;databaseName=/var/lib/hive/metastore/${user.name}_db;create=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;${user.name} is never replaced by the actual user name:$ ls -la /var/lib/hive/metastore/total 24drwxrwxrwt 3 root root 4096 Apr 30 12:37 .drwxr-xr-x 3 root root 4096 Apr 30 12:25 ..drwxrwxr-x 5 hadoop hadoop 4096 Apr 30 12:37 ${user.name}_db</description>
      <version>0.5.0,0.6.0,0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14830" opendate="2016-9-23 00:00:00" fixdate="2016-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move a majority of the MiniLlapCliDriver tests to use an inline AM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15530" opendate="2017-1-3 00:00:00" fixdate="2017-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the column stats update logic in table alteration</summary>
      <description>Currently when a table is altered, if any of below conditions is true, HMS would try to update column statistics for the table: database name is changed table name is changed old columns and new columns are not the sameAs a result, when a column is added to a table, Hive also tries to update column statistics, which is not necessary. We can loose the last condition by checking whether all existing columns are changed or not. If not, we don't have to update stats info.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="1594" opendate="2010-8-25 00:00:00" fixdate="2010-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo of hive.merge.size.smallfiles.avgsize prevents change of value</summary>
      <description>The setting is described as &lt;name&gt;hive.merge.size.smallfiles.avgsize&lt;/name&gt;, however common/src/java/org/apache/hadoop/hive/conf/HiveConf.java reads it as "hive.merge.smallfiles.avgsize" (note the missing '.size.') so the user's setting has no effect and the value is stuck at the default of 16MB.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16130" opendate="2017-3-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove jackson classes from hive-jdbc standalone jar</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16131" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive building with Hadoop 3 - additional stuff broken recently</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="16132" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSize stats don&amp;#39;t seem correct in semijoin opt branch</summary>
      <description>For the following operator tree snippet, the second Select is the start of a semijoin optimization branch. Take a look at the Data size - it is the same as the data size for its parent Select, even though the second select has only a single bigint column in its projection (the parent has 2 columns). I would expect the size to be 533328 (16 bytes * 33333).Fixing this estimate may become important if we need to estimate the cost of generating the min/max/bloomfilter.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug id="16133" opendate="2017-3-7 00:00:00" fixdate="2017-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Footer cache in Tez AM can take too much memory</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.LocalCache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1659" opendate="2010-9-21 00:00:00" fixdate="2010-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parse_url_tuple: a UDTF version of parse_url</summary>
      <description>The UDF parse_url take s a URL, parse it and extract QUERY/PATH etc from it. However it can only extract an atomic value from the URL. If we want to extract multiple piece of information, we need to call the function many times. It is desirable to parse the URL once and extract all needed information and return a tuple in a UDTF.</description>
      <version>0.5.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1693" opendate="2010-10-5 00:00:00" fixdate="2010-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the compile target depend on thrift.home</summary>
      <description>Per http://wiki.apache.org/hadoop/Hive/HiveODBC the ant compile targets require thrift.home be set. Rather than fail to compile fail with a message indicating it should be set.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1973" opendate="2011-2-8 00:00:00" fixdate="2011-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting error when join on tables where name of table has uppercase letters</summary>
      <description>When execute a join query on tables containing Uppercase letters in the table names hit an exception Ex: create table a(b int); create table tabForJoin(b int,c int); select * from a join tabForJoin on(a.b=tabForJoin.b); Got an exception like this FAILED: Error in semantic analysis: Invalid Table Alias tabForJoinBut if i give without capital letters ,It is working</description>
      <version>0.5.0,0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1975" opendate="2011-2-8 00:00:00" fixdate="2011-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"insert overwrite directory" Not able to insert data with multi level directory path</summary>
      <description>Below query execution is failedEx: insert overwrite directory '/HIVEFT25686/chinna/' select * from dept_j;</description>
      <version>0.5.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1976" opendate="2011-2-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception should be thrown when invalid jar,file,archive is given to add command</summary>
      <description>When executed add command with non existing jar it should throw exception through HiveStatementEx: add jar /root/invalidpath/testjar.jarHere testjar.jar is not exist so it should throw exception.</description>
      <version>0.5.0,0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="21670" opendate="2019-4-30 00:00:00" fixdate="2019-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replacing mockito-all with mockito-core dependency</summary>
      <description>The mockito-all dependency contains an old version of Hamcrest core which can collide with other Hamcrest dependencies. Replacint it with mockito-core should be straightforward.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21762" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP to support new format for replication policy input to take included tables list.</summary>
      <description>REPL DUMP syntax:REPL DUMP &lt;repl_policy&gt; [FROM &lt;last_repl_id&gt; WITH &lt;key_values_list&gt;; New format for the Replication policy have 3 parts all separated with Dot (.).1. First part is DB name.2. Second part is included list. Comma separated table names/regex with in square brackets[]. If square brackets are not there, then it is treated as single table replication which skips DB level events.3. Third part is excluded list. Comma separated table names/regex with in square brackets[].&lt;db_name&gt; -- Full DB replication which is currently supported&lt;db_name&gt;.['.*?'] -- Full DB replication&lt;db_name&gt;.[] -- Replicate just functions and not include any tables.&lt;db_name&gt;.['t1', 't2'] -- DB replication with static list of tables t1 and t2 included.&lt;db_name&gt;.['t1*', 't2', '*t3'].['t100', '5t3', 't4'] -- DB replication with all tables having prefix t1, with suffix t3 and include table t2 and exclude t100 which has the prefix t1, 5t3 which suffix t3 and t4. Need to support regular expression of any format. A table is included in dump only if it matches the regular expressions in included list and doesn't match the excluded list.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.DatabaseAndTableFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.BasicFilter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.ReplConst.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplLastIdInfo.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.ReplicationTestUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadDatabase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.TableContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DeletePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DeleteTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.OpenTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReplRemoveFirstIncLoadPendFlagDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21763" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental replication to allow changing include/exclude tables list in replication policy.</summary>
      <description>REPL DUMP takes 2 inputs along with existing FROM and WITH clause.- REPL DUMP &lt;current_repl_policy&gt; [REPLACE &lt;previous_repl_policy&gt; FROM &lt;last_repl_id&gt; WITH &lt;key_values_list&gt;;- current_repl_policy and previous_repl_policy can be any format mentioned in Point-4.- REPLACE clause to be supported to take previous repl policy as input. If REPLACE clause is not there, then the policy remains unchanged.- Rest of the format remains same. Now, REPL DUMP on this DB will replicate the tables based on current_repl_policy. Single table replication of format &lt;db_name&gt;.t1 doesnt allow changing the policy dynamically. So REPLACE clause is not allowed if previous_repl_policy of this format. If any table is added dynamically either due to change in regular expression or added to include list should be bootstrapped using independant table level replication policy.- Hive will automatically figure out the list of tables newly included in the list by comparing the current_repl_policy &amp; previous_repl_policy inputs and combine bootstrap dump for added tables as part of incremental dump. "_bootstrap" directory can be created in dump dir to accommodate all tables to be bootstrapped.- If any table is renamed, then it may gets dynamically added/removed for replication based on defined replication policy + include/exclude list. So, Hive will perform bootstrap for the table which is just included after rename. REPL LOAD should check for changes in repl policy and drop the tables/views excluded in the new policy compared to previous policy. It should be done before performing incremental and bootstrap load from the current dump. REPL LOAD on incremental dump should load events directories first and then check for "_bootstrap" directory and perform bootstrap load on them.Rename table is not in scope of this jira.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.repl.ReplScope.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AllocWriteIdHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="21764" opendate="2019-5-21 00:00:00" fixdate="2019-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP should detect and bootstrap any rename table events where old table was excluded but renamed table is included.</summary>
      <description>REPL DUMP fetches the events from NOTIFICATION_LOG table based on regular expression + inclusion/exclusion list. So, in case of rename table event, the event will be ignored if old table doesn't match the pattern but the new table should be bootstrapped. REPL DUMP should have a mechanism to detect such tables and automatically bootstrap with incremental replication.Also, if renamed table is excluded from replication policy, then need to drop the old table at target as well.There are 4 scenarios that needs to be handled. Both new name and old name satisfies the table name pattern filter. No need to do anything. The incremental event for rename should take care of the replication. Both the names does not satisfy the table name pattern filter. Both the names are not in the scope of the policy and thus nothing needs to be done. New name satisfies the pattern but the old name does not. The table will not be present at the target. Rename event handler for dump should detect this case and add the new table name to the list of table for bootstrap. All the events related to the table (new name) should be ignored. If there is a drop event for the table (with new name), then remove the table from the list of tables to be bootstrapped. In case of rename (double rename) If the new name satisfies the table pattern, then add the new name to the list of tables to be bootstrapped and remove the old name from the list of tables to be bootstrapped. If the new name does not satisfies then just removed the table name from the list of tables to be bootstrapped. New name does not satisfies the pattern but the old name satisfies. Change the rename event to a drop event.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.event.filters.ReplEventFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.OpenTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbortTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.DumpType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="2178" opendate="2011-5-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log related Check style Comments fixes</summary>
      <description>Fix Log related Check style Comments</description>
      <version>0.5.0,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.SimpleCharStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="21801" opendate="2019-5-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests using miniHS2 with HTTP as transport are creating miniHS2 with binary transport</summary>
      <description>Even though tests using miniHS2 set the confighive.server2.transport.mode is set to http, miniHS2 is created with binary transport.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.MiniHiveKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="21981" opendate="2019-7-10 00:00:00" fixdate="2019-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When LlapDaemon capacity is set to 0 and the waitqueue is not empty then the queries are stuck</summary>
      <description>When an LlapDaemon executor capacity is set to 0 then the already queued tasks are not handled causing the queries to stuck</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="22340" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent shaded imports</summary>
      <description>Make sure that hive developers don't import the shaded version of some class by accident.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FileUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFBinarySetFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.RelTreeSignature.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.metrics.ReadWriteLockMetrics.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduSerDe.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduPredicateHandler.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduOutputFormat.java</file>
      <file type="M">kudu-handler.src.test.org.apache.hadoop.hive.kudu.TestKuduInputFormat.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.kudu.KuduTestSetup.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22630" opendate="2019-12-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not retrieve Materialized View definitions for rebuild if query is test SQL</summary>
      <description>for the query like select 1, select current_timestamp, select current_datehive retrieve all the Materialized view from metastore, if the one of databases are too large then this call take lots of time, the situation becomes worse if there are too frequent if hive server receives frequent "select 1" query ( connection pool uses it to check if the connection is valid or not).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22631" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid deep copying partition list in listPartitionsByExpr</summary>
      <description>This is an expensive call, I am not sure why deepCopy is required.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22632" opendate="2019-12-12 00:00:00" fixdate="2019-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve estimateRowSizeFromSchema</summary>
      <description>estimateRowSizeFromSchema un-necessarily iterate and do look-up. This could be avoided.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="2307" opendate="2011-7-26 00:00:00" fixdate="2011-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema creation scripts for PostgreSQL use bit(1) instead of boolean</summary>
      <description>The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.hive&gt; create table test (id int); FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7" using statement "INSERT INTO "SDS" ("SD_ID","INPUT_FORMAT","OUTPUT_FORMAT","LOCATION","SERDE_ID","NUM_BUCKETS","IS_COMPRESSED") VALUES (?,?,?,?,?,?,?)" failed : ERROR: column "IS_COMPRESSED" is of type bit but expression is of type boolean</description>
      <version>0.5.0,0.6.0,0.7.0,0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0-to-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.5.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.1.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.3.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="23073" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade netty and upgrade to netty 4.1.48.Final</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="307" opendate="2009-2-25 00:00:00" fixdate="2009-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"LOAD DATA LOCAL INPATH" fails when the table already contains a file of the same name</summary>
      <description>Failed with exception checkPaths: /user/zshao/warehouse/tmp_user_msg_history/test_user_msg_history already existsFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3149" opendate="2012-6-16 00:00:00" fixdate="2012-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamically generated paritions deleted by Block level merge</summary>
      <description>When creating partitions in a table using dynamic partitions and a Block level merge is executed at the end of the query, some partitions may be lost. Specifically if the values of two or more dynamic partition keys end in the same sequence of numbers, all but the largest will be dropped.I was not able to confirm it, but I suspect that if a map reduce job is speculated as part of the merge, the duplicate data will not be deleted either.E.g.insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)select key, value, if(key % 2 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';In this query, if a Block level merge is executed at the end, only one of the partitions ds=2008-04-08/hr=a1 and ds=2008-04-08/hr=b1 will appear in the final table.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5578" opendate="2013-10-17 00:00:00" fixdate="2013-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcat script doesn&amp;#39;t include jars from HIVE_AUX_JARS_PATH</summary>
      <description>hcat script include jars from $HIVE_HOME/lib but not from HIVE_AUX_JARS_PATH.</description>
      <version>0.5.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.bin.hcat</file>
    </fixedFiles>
  </bug>
  <bug id="5591" opendate="2013-10-18 00:00:00" fixdate="2013-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use TezGroupedSplits to combine splits based on headroom in Tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7732" opendate="2014-8-14 00:00:00" fixdate="2014-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:JoinOrder Algo update to use HiveRels</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveFilterRel.java</file>
    </fixedFiles>
  </bug>
  <bug id="7740" opendate="2014-8-15 00:00:00" fixdate="2014-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>qfile and qfile_regex should override includeFiles</summary>
      <description>qfile and qfile_regex should override include files so they can be used by devs to run tests speculatively.</description>
      <version>None</version>
      <fixedVersion>spark-branch</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7750" opendate="2014-8-16 00:00:00" fixdate="2014-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: PPD should Push Predicates on On clause if possible</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="7755" opendate="2014-8-16 00:00:00" fixdate="2014-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable avro* tests [Spark Branch]</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7757" opendate="2014-8-16 00:00:00" fixdate="2014-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 separates test files with spaces while QTestGen uses commas</summary>
      <description>I noticed in HIVE-7749 that even after the testconfiguration.properties file is updated TestSparkCliDriver is not being generated correctly. Basically it doesn't include any tests. The issue appears to be that in the pom file properties are separated by comma and the PTest2 properties files are separated by spaces. Since both comma and space are not used in the qtest properties files let's update all parsing code to use both comma and space.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7758" opendate="2014-8-16 00:00:00" fixdate="2014-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 separates test files with spaces while QTestGen uses commas [Spark Branch]</summary>
      <description>HIVE-7757 for spark branch</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="7860" opendate="2014-8-22 00:00:00" fixdate="2014-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Query on partitioned table which filter out all partitions fails</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="8010" opendate="2014-9-5 00:00:00" fixdate="2014-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle nested types</summary>
      <description>need to handle ExprNodeFieldDesc</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="8021" opendate="2014-9-8 00:00:00" fixdate="2014-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: support CTAS and insert ... select</summary>
      <description>Need to send only the select part to CBO for now</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.decimal.serde.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.colname.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8410" opendate="2014-10-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in DOAP - incorrect category URL</summary>
      <description>NO PRECOMMIT TESTSThe DOAP contains the following:&lt;category rdf:resource="http://www.apache.org/category/database" /&gt;However, the URL is incorrect; it must be&lt;category rdf:resource="http://projects.apache.org/category/database" /&gt;Please fix this</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">doap.Hive.rdf</file>
    </fixedFiles>
  </bug>
  <bug id="8411" opendate="2014-10-9 00:00:00" fixdate="2014-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partial partition spec for certain ALTER PARTITION statements</summary>
      <description>To help address concerns hagleitn had about having to update many partitions here</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.change.col.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.partition.change.col.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="8412" opendate="2014-10-9 00:00:00" fixdate="2014-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make reduce side join work for all join queries [Spark Branch]</summary>
      <description>Regardless all these join related optimizations such as map join, bucket join, skewed join, etc, reduce side join is the fallback. That means, if a join query wasn't taken care of by any of the optimization, it should work with reduce side join (might in a less optimal fashion).It's found that this isn't case at the moment. For instance, auto_sortmerge_join_1.q failed to execute on Spark.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="878" opendate="2009-10-15 00:00:00" fixdate="2009-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the hash table entry before flushing in Group By hash aggregation</summary>
      <description>This is a newly introduced bug from r796133.We should first update the aggregation, and then we can flush the hash table. Otherwise the entry that we update might be already out of the hash table.</description>
      <version>0.4.0,0.5.0</version>
      <fixedVersion>0.4.1,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8780" opendate="2014-11-7 00:00:00" fixdate="2014-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert1.q and ppd_join4.q hangs with hadoop-1 [Spark Branch]</summary>
      <description>In working on HIVE-8758, found these tests hang at java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.startMonitor(SparkJobMonitor.java:129) at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:111) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1404) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1216) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1043) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1033) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345) at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832) at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:3706) at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join4(TestSparkCliDriver.java:2790)Both tests hang at the same place. There could be other hanging tests.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SimpleSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8852" opendate="2014-11-13 00:00:00" fixdate="2014-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update new spark progress API for local submitted job monitoring [Spark Branch]</summary>
      <description>SPARK-2321 has enabled new Spark job progress API, we should update our current job monitoring implementation based on it. This JIRA should only take care of spark job monitoring which submitted through local spark context, job monitoring through remote spark context is tracked with HIVE-8834.CLEAR LIBRARY CACHE</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkStageProgress.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SimpleSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.JobStateListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="8855" opendate="2014-11-13 00:00:00" fixdate="2014-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatic calculate reduce number for spark job [Spark Branch]</summary>
      <description>As the following up work of HIVE-8649, we should enable reduce number automatic calculation for both local spark client and remote spark client.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="923" opendate="2009-11-10 00:00:00" fixdate="2009-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should not use CombineHiveInputFormat by default because it breaks for hadoop 0.20 releases.</summary>
      <description>HADOOP-5759 is committed to Hadoop 0.20 on 19/Oct/09 but not released yet.We should not use CombineHiveInputFormat for hadoop 0.20 for now. Otherwise all users will encounter problems using Hive trunk against Hadoop 0.20.We can switch the default back when a new release from hadoop 0.20 comes out.</description>
      <version>0.5.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="940" opendate="2009-11-18 00:00:00" fixdate="2009-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>restrict creation of partitions with empty partition keys</summary>
      <description>create table pc (a int) partitioned by (b string, c string);alter table pc add partition (b="f", c='');above alter cmd fails but actually creates a partition with name 'b=f/c=' but describe partition on the same name fails. creation of such partitions should not be allowed.</description>
      <version>0.3.0,0.4.0,0.4.1,0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9592" opendate="2015-2-5 00:00:00" fixdate="2015-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix ArrayIndexOutOfBoundsException in date_add and date_sub initialize</summary>
      <description>hive&gt; select date_add('2015-01-01', map(1,1));FAILED: ArrayIndexOutOfBoundsException 2hive&gt; select date_sub('2015-01-01', map(1,1));FAILED: ArrayIndexOutOfBoundsException 2</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
    </fixedFiles>
  </bug>
  <bug id="9781" opendate="2015-2-25 00:00:00" fixdate="2015-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Utilize spark.kryo.classesToRegister [Spark Branch]</summary>
      <description>I noticed in several thread dumps that it appears kyro is serializing the class names associated with our keys and values.Kyro supports pre-registering classes so that you don't have to serialize the class name and spark supports this via the spark.kryo.registrator property. We should do this so we don't have to serialize class names.Thread 12154: (state = BLOCKED) - java.lang.Object.hashCode() @bci=0 (Compiled frame; information may be imprecise) - com.esotericsoftware.kryo.util.ObjectMap.get(java.lang.Object) @bci=1, line=265 (Compiled frame) - com.esotericsoftware.kryo.util.DefaultClassResolver.getRegistration(java.lang.Class) @bci=18, line=61 (Compiled frame) - com.esotericsoftware.kryo.Kryo.getRegistration(java.lang.Class) @bci=20, line=429 (Compiled frame) - com.esotericsoftware.kryo.util.DefaultClassResolver.readName(com.esotericsoftware.kryo.io.Input) @bci=242, line=148 (Compiled frame) - com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(com.esotericsoftware.kryo.io.Input) @bci=65, line=115 (Compiled frame) - com.esotericsoftware.kryo.Kryo.readClass(com.esotericsoftware.kryo.io.Input) @bci=20, line=610 (Compiled frame) - com.esotericsoftware.kryo.Kryo.readClassAndObject(com.esotericsoftware.kryo.io.Input) @bci=21, line=721 (Compiled frame) - com.twitter.chill.Tuple2Serializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class) @bci=6, line=41 (Compiled frame) - com.twitter.chill.Tuple2Serializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class) @bci=4, line=33 (Compiled frame) - com.esotericsoftware.kryo.Kryo.readClassAndObject(com.esotericsoftware.kryo.io.Input) @bci=126, line=729 (Compiled frame) - org.apache.spark.serializer.KryoDeserializationStream.readObject(scala.reflect.ClassTag) @bci=8, line=142 (Compiled frame) - org.apache.spark.serializer.DeserializationStream$$anon$1.getNext() @bci=10, line=133 (Compiled frame) - org.apache.spark.util.NextIterator.hasNext() @bci=16, line=71 (Compiled frame) - org.apache.spark.util.CompletionIterator.hasNext() @bci=4, line=32 (Compiled frame) - scala.collection.Iterator$$anon$13.hasNext() @bci=4, line=371 (Compiled frame) - org.apache.spark.util.CompletionIterator.hasNext() @bci=4, line=32 (Compiled frame) - org.apache.spark.InterruptibleIterator.hasNext() @bci=22, line=39 (Compiled frame) - scala.collection.Iterator$$anon$11.hasNext() @bci=4, line=327 (Compiled frame) - org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator) @bci=191, line=217 (Compiled frame) - org.apache.spark.shuffle.hash.HashShuffleReader.read() @bci=278, line=61 (Interpreted frame) - org.apache.spark.rdd.ShuffledRDD.compute(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=46, line=92 (Interpreted frame) - org.apache.spark.rdd.RDD.computeOrReadCheckpoint(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=26, line=263 (Interpreted frame) - org.apache.spark.rdd.RDD.iterator(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=33, line=230 (Interpreted frame) - org.apache.spark.rdd.MapPartitionsRDD.compute(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=24, line=35 (Interpreted frame) - org.apache.spark.rdd.RDD.computeOrReadCheckpoint(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=26, line=263 (Interpreted frame) - org.apache.spark.rdd.RDD.iterator(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=33, line=230 (Interpreted frame) - org.apache.spark.rdd.MapPartitionsRDD.compute(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=24, line=35 (Interpreted frame) - org.apache.spark.rdd.RDD.computeOrReadCheckpoint(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=26, line=263 (Interpreted frame) - org.apache.spark.rdd.RDD.iterator(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=33, line=230 (Interpreted frame) - org.apache.spark.rdd.UnionRDD.compute(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=22, line=87 (Interpreted frame) - org.apache.spark.rdd.RDD.computeOrReadCheckpoint(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=26, line=263 (Interpreted frame) - org.apache.spark.rdd.RDD.iterator(org.apache.spark.Partition, org.apache.spark.TaskContext) @bci=33, line=230 (Interpreted frame) - org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext) @bci=166, line=68 (Interpreted frame) - org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext) @bci=2, line=41 (Interpreted frame) - org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted frame) - org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196 (Interpreted frame) - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Interpreted frame) - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame) - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
