<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10527" opendate="2015-4-29 00:00:00" fixdate="2015-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in SparkUtilities::isDedicatedCluster [Spark Branch]</summary>
      <description>We should add spark.master to HiveConf when it doesn't exist.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10678" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update sql standard authorization configuration whitelist - more optimization flags</summary>
      <description>hive.exec.parallel and hive.groupby.orderby.position.alias are optimization config parameters that should be settable when sql standard authorization is enabled.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10679" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JsonSerde ignores varchar and char size limit specified during table creation</summary>
      <description>JsonSerde ignores varchar and char size limit specified during table creation and always creates varchar or char column with max length.steps to reproduce the issue:create table jsonserde_1 (v varchar(50), c char(50)) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe';desc jsonserde_1;OKv varchar(65535) from deserializer c char(255) from deserializer Time taken: 0.468 seconds, Fetched: 2 row(s)</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10682" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="10683" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add a mechanism for daemons to inform the AM about killed tasks</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11417" opendate="2015-7-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create shims for the row by row read path that is backed by VectorizedRowBatch</summary>
      <description>I'd like to make the default path for reading and writing ORC files to be vectorized. To ensure that Hive can still read row by row, we'll need shims to support the old API.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestStringDictionary.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.ZeroCopyShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestTimestampWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.json</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestUnrolledBitPack.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestTypeDescription.java</file>
      <file type="M">bin.ext.orcfiledump.cmd</file>
      <file type="M">bin.ext.orcfiledump.sh</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">orc.pom.xml</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShims.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShimsCurrent.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShims.2.2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.IntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">orc.src.java.org.apache.orc.Reader.java</file>
      <file type="M">orc.src.java.org.apache.orc.TypeDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastMillisecondsLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.FileFormatException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ConvertTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.JsonFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestTimestampWritableAndColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestFileDump.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcTimezone1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcTimezone2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRLEv2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestStreamName.java</file>
    </fixedFiles>
  </bug>
  <bug id="12897" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading</summary>
      <description>There are many redundant calls to metastore which is not needed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dynamic.partitions.with.whitelist.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3256" opendate="2012-7-13 00:00:00" fixdate="2012-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update asm version in Hive</summary>
      <description>Hive trunk are currently using asm version 3.1, Hadoop trunk are on 3.2. Anyobjections to bumping the Hive version to 3.2 to be inline with Hadoop</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3257" opendate="2012-7-14 00:00:00" fixdate="2012-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix avro_joins.q testcase failure when building hive on hadoop0.23</summary>
      <description>avro_joins.q is failing when building hive on hadoop0.23 for both MR1 and MR2. It has an execution exception:This query fails when execution:SELECT e.title, e.air_date, d.first_name, d.last_name, d.extra_field, e.air_dateFROM doctors4 d JOIN episodes e ON (d.number=e.doctor)ORDER BY d.last_name, e.titleExecution failed with exit status: 2Obtaining error informationTask failed!Task ID:Stage-1Logs:/home/cloudera/Code/hive/build/ql/tmp//hive.logFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="3365" opendate="2012-8-9 00:00:00" fixdate="2012-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive&amp;#39;s Avro dependency to version 1.7</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3721" opendate="2012-11-19 00:00:00" fixdate="2012-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER TABLE ADD PARTS should check for valid partition spec and throw a SemanticException if part spec is not valid</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3953" opendate="2013-1-28 00:00:00" fixdate="2013-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reading of partitioned Avro data fails because of missing properties</summary>
      <description>After HIVE-3833, reading partitioned Avro data fails due to missing properties. The "avro.schema.(url|literal)" properties are not making it all the way to the SerDe. Non-partitioned data can still be read.</description>
      <version>0.11.0,0.12.0,0.11.1</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4123" opendate="2013-3-5 00:00:00" fixdate="2013-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The RLE encoding for ORC can be improved</summary>
      <description>The run length encoding of integers can be improved: tighter bit packing allow delta encoding allow longer runs</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SerializationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4219" opendate="2013-3-22 00:00:00" fixdate="2013-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>explain dependency does not capture the input table</summary>
      <description>hive&gt; explain dependency select * from srcpart where ds is not null;OK{"input_partitions":[{"partitionName":"default@srcpart@ds=2008-04-08/hr=11"},{"partitionName":"default@srcpart@ds=2008-04-08/hr=12"},{"partitionName":"default@srcpart@ds=2008-04-09/hr=11"},{"partitionName":"default@srcpart@ds=2008-04-09/hr=12"}],"input_tables":[]}input_tables should contain srcpart</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.offline.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.protect.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.16.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.17.part.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.23.import.part.authsuccess.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insertexternal1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="422" opendate="2009-4-15 00:00:00" fixdate="2009-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logo for Hive</summary>
      <description>Greetings fine Hadoop peoples,While working on a few projects here at Cloudera we found ourselves wanting for some sort of icon for both the JobTracker and for Hive. After checking on the project page for Hive (the JobTracker doesn't really have one) and finding that these items have no icons, we rolled up our sleeves and made some. We'd like to contribute these to the project, so if you want 'em, they're all yours.I opened a ticket here for both Hive and JobTracker logos: https://issues.apache.org/jira/browse/HADOOP-5683But Ashish Thusoo suggested I open a ticket here for the hive logo specifically. See attached.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.images.hive-logo.jpg</file>
    </fixedFiles>
  </bug>
  <bug id="4430" opendate="2013-4-27 00:00:00" fixdate="2013-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semantic analysis fails in presence of certain literals in on clause</summary>
      <description>When users include a bigint literal (a number suffixed with 'L') in the conditions in the on clause the query will fail with, e.g.FAILED: SemanticException 0L encountered with 0 childrenI haven't tried it yet, but I suspect the same is true for other, lesser used literals.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4431" opendate="2013-4-27 00:00:00" fixdate="2013-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized string concatenation</summary>
      <description>Include Col-Col, Scalar-Col, and Col-Scalar logic</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.ColumnArithmeticColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColDivideDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColSubtractLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColSubtractDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColMultiplyLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColMultiplyDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColDivideDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColAddLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColAddDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="4436" opendate="2013-4-27 00:00:00" fixdate="2013-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.exec.parallel=true doesn&amp;#39;t work on hadoop-2</summary>
      <description>While running a hive query with multiple independent stages, hive.exec.parallel is a valid optimization to use.The query tested has 3 MR jobs - the first job is the root dependency and the 2 further job depend on the first one.When hive.exec.parallel is turned on, the job fails with the following exceptionjava.io.IOException: java.lang.InterruptedException at org.apache.hadoop.ipc.Client.call(Client.java:1214) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202) at $Proxy12.mkdirs(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83) at $Proxy12.mkdirs(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:447) at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2165) at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2136) at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:544) at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1916) at org.apache.hadoop.hive.ql.exec.ExecDriver.createTmpDirs(ExecDriver.java:222) at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:444) at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:138) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:145) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57) at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)Caused by: java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1279) at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218) at java.util.concurrent.FutureTask.get(FutureTask.java:83) at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:921) at org.apache.hadoop.ipc.Client.call(Client.java:1208)The query plan is as follows Stage-9 is a root stage Stage-8 depends on stages: Stage-9 Stage-3 depends on stages: Stage-8 Stage-0 depends on stages: Stage-3 Stage-4 depends on stages: Stage-0 Stage-5 depends on stages: Stage-8 Stage-1 depends on stages: Stage-5 Stage-6 depends on stages: Stage-1STAGE PLANS: Stage: Stage-9 Map Reduce Local Work Stage: Stage-8 Map Reduce Map Join Operator Stage: Stage-3 Map Reduce Stage: Stage-0 Move Operator Stage: Stage-4 Stats-Aggr Operator Stage: Stage-5 Map Reduce Stage: Stage-1 Move Operator Stage: Stage-6 Stats-Aggr OperatorI cannot conclude that this is purely a hive issue, will file a bug on HDFS if that does show up during triage.Triaged - set hive.stats.autogather=false; removes the bug.</description>
      <version>0.9.0,0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="4438" opendate="2013-4-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused join configuration parameter: hive.mapjoin.size.key</summary>
      <description>The config parameter that used to limit the number of cached rows per key is no longer used in the code base. I suggest to remove it to make things less confusing.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4439" opendate="2013-4-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused join configuration parameter: hive.mapjoin.cache.numrows</summary>
      <description>The description says:"How many rows should be cached by jdbm for map join."I can't find any reference to that parameter in the code however.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join40.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join39.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join39.q</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4478" opendate="2013-5-2 00:00:00" fixdate="2013-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In ORC, add boolean noNulls flag to column stripe metadata</summary>
      <description>Currently, the stripe metadata for ORC contains the min and max value for each column in the stripe. This will be used for stripe elimination. However, an additional bit of metadata for each column for each stripe, noNulls (true/false), is needed to help speed up vectorized query execution as much as 30%. The vectorized QE code has a Boolean flag for each column vector called noNulls. If this is true, all the null-checking logic is skipped for that column for a VectorizedRowBatch when an operation is performed on that column. For simple filters and arithmetic expressions, this can save on the order of 30% of the time.Once this noNulls stripe metadata is available, the vectorized iterator (reader) for ORC can be updated to avoid all expense to load the isNull bitmap, and efficiently set the noNulls flag for each column vector.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OutStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="4481" opendate="2013-5-2 00:00:00" fixdate="2013-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized row batch should be initialized with additional columns to hold intermediate output.</summary>
      <description>Vectorized row batch should be initialized with additional columns to hold intermediate output.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.CodeGen.java</file>
    </fixedFiles>
  </bug>
  <bug id="4483" opendate="2013-5-3 00:00:00" fixdate="2013-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Input format to read vector data from RC file</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="4485" opendate="2013-5-3 00:00:00" fixdate="2013-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline prints null as empty strings</summary>
      <description>beeline is printing nulls as emtpy strings. There was no way to distinguish between an empty string an a null value. This is inconsistent with hive cli and other databases, they print null as "NULL" string.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.src.test.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BufferedRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="4492" opendate="2013-5-3 00:00:00" fixdate="2013-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-4322</summary>
      <description>See HIVE-4432 and HIVE-4433. It's possible to work around these issues but a better solution is probably to roll back the "fix" and change the API to use a primitive type as the map key (in a backwards-compatible manner).</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SkewedValueList.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="4493" opendate="2013-5-3 00:00:00" fixdate="2013-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized filter for string column compared to string column</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.CodeGen.java</file>
    </fixedFiles>
  </bug>
  <bug id="4495" opendate="2013-5-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized string substr</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="4496" opendate="2013-5-3 00:00:00" fixdate="2013-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC2 won&amp;#39;t compile with JDK7</summary>
      <description>HiveServer2 related JDBC does not compile with JDK7. Related to HIVE-3384.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveCallableStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="4498" opendate="2013-5-4 00:00:00" fixdate="2013-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestBeeLineWithArgs.testPositiveScriptFile fails</summary>
      <description>TestBeeLineWithArgs.testPositiveScriptFile fails - [junit] 0: jdbc:hive2://localhost:10000&gt; &gt;&gt;&gt; STARTED testBreakOnErrorScriptFile [junit] Output: Connecting to jdbc:hive2://localhost:10000 [junit] Connected to: Hive (version 0.12.0-SNAPSHOT) [junit] Driver: Hive (version 0.12.0-SNAPSHOT) [junit] Transaction isolation: TRANSACTION_REPEATABLE_READ [junit] Beeline version 0.12.0-SNAPSHOT by Apache Hive [junit] +----------------+ [junit] | database_name | [junit] +----------------+ [junit] +----------------+ [junit] No rows selected (0.899 seconds) [junit] Closing: org.apache.hive.jdbc.HiveConnection [junit] [junit] &gt;&gt;&gt; FAILED testPositiveScriptFile (ERROR) (2s)</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4543" opendate="2013-5-12 00:00:00" fixdate="2013-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken link in HCat 0.5 doc (Reader and Writer Interfaces)</summary>
      <description>Due to HCatalog's move from the incubator to Hive, a link to TestReaderWriter.java is broken at the end of the "Reader and Writer Interfaces" doc for HCat 0.5 (here). This should be fixed in the html and pdf files.Thanks to Himanshu Bari for pointing this out.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.readerwriter.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4545" opendate="2013-5-12 00:00:00" fixdate="2013-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 should return describe table results without space padding</summary>
      <description>HIVE-3140 changed behavior of 'DESCRIBE table;' to be like 'DESCRIBE FORMATTED table;'. HIVE-3140 introduced changes to not print header in 'DESCRIBE table;'. But jdbc/odbc calls still get fields padded with space for the 'DESCRIBE table;' query.As the jdbc/odbc results are not for direct human consumption the space padding should not be done for hive server2.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="4572" opendate="2013-5-16 00:00:00" fixdate="2013-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ColumnPruner cannot preserve RS key columns corresponding to un-selected join keys in columnExprMap</summary>
      <description>For a RS of a join operator, if the join key corresponding to this RS does not appear in the SELECT clause, ColumnPruner will drop the entry of this column in colExprMap. Example:SELECT x.key FROM src1 x JOIN src y ON (x.key = y.key);Before CP,colExprMap of RS corresponding to x: {VALUE._col3=Column[INPUT__FILE__NAME], VALUE._col2=Column[BLOCK__OFFSET__INSIDE__FILE], VALUE._col1=Column[value], VALUE._col0=Column[key]};colExprMap of RS corresponding to y: {VALUE._col3=Column[INPUT__FILE__NAME], VALUE._col2=Column[BLOCK__OFFSET__INSIDE__FILE], VALUE._col1=Column[value], VALUE._col0=Column[key]}.After CP,colExprMap of RS corresponding to x: {VALUE._col0=Column[key]};colExprMap of RS corresponding to y: {}.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="4578" opendate="2013-5-20 00:00:00" fixdate="2013-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changes to Pig&amp;#39;s test harness broke HCat e2e tests</summary>
      <description>HCatalog externs the test harness from Pig. Pig recently made some changes to the test harness to work better across Unix and Windows. These changes require new OS specific files. HCatalog will also need these files in order to work with the test harness.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.hcatalog.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4579" opendate="2013-5-20 00:00:00" fixdate="2013-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a SARG interface for RecordReaders</summary>
      <description>I think we should create a SARG (http://en.wikipedia.org/wiki/Sargable) interface for RecordReaders. For a first pass, I'll create an API that uses the value stored in hive.io.filter.expr.serialized.The desire is to define an simpler interface that the direct AST expression that is provided by hive.io.filter.expr.serialized so that the code to evaluate expressions can be generalized instead of put inside a particular RecordReader.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4617" opendate="2013-5-27 00:00:00" fixdate="2013-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Asynchronous execution in HiveServer2 to run a query in non-blocking mode</summary>
      <description>Provide a way to run a queries asynchronously. Current executeStatement call blocks until the query run is complete.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationState.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ICLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.EmbeddedCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOperationState.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TExecuteStatementReq.java</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4658" opendate="2013-6-5 00:00:00" fixdate="2013-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make KW_OUTER optional in outer joins</summary>
      <description>For really trivial migration issue.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="4673" opendate="2013-6-6 00:00:00" fixdate="2013-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use VectorExpessionWriter to write column vectors into Writables.</summary>
      <description>VectorExpressionWriter interface should be used to write column vectors into Writables. VectorExpressionWriter supports all primitive datatypes and this will make vector select operator and vector group by operators consistent.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4706" opendate="2013-6-11 00:00:00" fixdate="2013-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query on Table with partition columns fail with AlreadyBeingCreatedException</summary>
      <description>Table with partition columns fail will AlreadyBeingCreatedException when CombineHiveInputFormat is uses. Below is the exception stack2013-06-07 16:38:51,098 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:ssakala cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/hive-ssakala/hive_2013-06-07_16-36-10_579_6803860768897313738/_task_tmp.-ext-10001/_tmp.000000_0 for DFSClient_attempt_201306071336_0016_m_000000_0 on client 127.0.0.1 because current leaseholder is trying to recreate file.2013-06-07 16:38:51,098 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call create(/tmp/hive-ssakala/hive_2013-06-07_16-36-10_579_6803860768897313738/_task_tmp.-ext-10001/_tmp.000000_0, rwxr-xr-x, DFSClient_attempt_201306071336_0016_m_000000_0, true, true, 1, 67108864) from 127.0.0.1:32905: error: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/hive-ssakala/hive_2013-06-07_16-36-10_579_6803860768897313738/_task_tmp.-ext-10001/_tmp.000000_0 for DFSClient_attempt_201306071336_0016_m_000000_0 on client 127.0.0.1 because current leaseholder is trying to recreate file.org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/hive-ssakala/hive_2013-06-07_16-36-10_579_6803860768897313738/_task_tmp.-ext-10001/_tmp.000000_0 for DFSClient_attempt_201306071336_0016_m_000000_0 on client 127.0.0.1 because current leaseholder is trying to recreate file.at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1386)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1258)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1200)at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:632)at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:597)at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:396)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4712" opendate="2013-6-11 00:00:00" fixdate="2013-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestCliDriver.truncate_* on 0.23</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.truncate.column.merge.q</file>
      <file type="M">ql.src.test.queries.clientpositive.truncate.column.q</file>
    </fixedFiles>
  </bug>
  <bug id="4714" opendate="2013-6-11 00:00:00" fixdate="2013-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Sum of scalar subtract column returns negative result when positive exected</summary>
      <description>Actual: -5701157.669591231Expected: 5701157.663489044drop table LINEITEM_ORC;create external table LINEITEM_ORC(L_DISCOUNT float ) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.CommonOrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';SELECT Sum(1 - l_discount) FROM Lineitem_orc</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="4724" opendate="2013-6-12 00:00:00" fixdate="2013-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC readers should have a better error detection for non-ORC files</summary>
      <description>A customer loaded a text file into a table that is stored as ORC. The error message was very unfriendly.</description>
      <version>None</version>
      <fixedVersion>0.12.0,0.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="4727" opendate="2013-6-13 00:00:00" fixdate="2013-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize ORC StringTreeReader::nextVector to not create dictionary of strings for each call to nextVector</summary>
      <description>Currently ORC StringTreeReader::nextVector creates dictionary of strings for each call to nextVector. This leads to bad perf as there is huge memory allocation and deallocation on each call. Since the dictionary does not change within a stripe, StringTreeReader::nextVector should be optimized to create this dictionary only on stripe read.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="473" opendate="2009-5-6 00:00:00" fixdate="2009-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up after tests</summary>
      <description>The test suite creates a lot of temporary files that aren't cleaned up. For example plan xml files, mapred/local and mapred/system files.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4743" opendate="2013-6-17 00:00:00" fixdate="2013-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve test coverage of package org.apache.hadoop.hive.ql.io</summary>
      <description>The patch improves unit test coverage of package org.apache.hadoop.hive.ql.io up to 80%.</description>
      <version>0.12.0,0.10.1,0.11.1</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4791" opendate="2013-6-26 00:00:00" fixdate="2013-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve test coverage of package org.apache.hadoop.hive.ql.udf.xml</summary>
      <description>improve test coverage of package org.apache.hadoop.hive.ql.udf.xml to 80%.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4796" opendate="2013-6-28 00:00:00" fixdate="2013-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase coverage of package org.apache.hadoop.hive.common.metrics</summary>
      <description></description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.MetricsMBean.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.Metrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="4802" opendate="2013-7-2 00:00:00" fixdate="2013-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix url check for missing "/" or "/&lt;db&gt; after hostname in jdb uri</summary>
      <description>HIVE-4406 added a check for jdbc uri to prevent unintentional use of embedded mode. But that does not correctly check for uri like "jdbc:hive2://localhost:10000;principal=hive/HiveServer2Host@YOUR-REALM.COM" that can also result in embedded mode being used.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="4805" opendate="2013-7-2 00:00:00" fixdate="2013-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance coverage of package org.apache.hadoop.hive.ql.exec.errors</summary>
      <description></description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.DataCorruptErrorHeuristic.java</file>
    </fixedFiles>
  </bug>
  <bug id="4810" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor exec package</summary>
      <description>The exec package contains both operators and classes used to execute the job. Moving the latter into a sub package makes the package slightly more manageable and will make it easier to provide a tez-based implementation.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart.err.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.reflect.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udfnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.publisher.error.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.publisher.error.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.aggregator.error.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.serde.regex2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.minimr.broken.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.turnoff.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.turnoff.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.local.mapred.error.cache.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fatal.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part.max.per.node.q.out</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Throttle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.java</file>
      <file type="M">ql.src.test.results.clientnegative.autolocal1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.cachingprintstream.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.cluster.tasklog.retrieval.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4812" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logical explain plan</summary>
      <description>In various situations it would have been useful to me to glance at the operator plan before we break it into tasks and apply join, total order sort, etc optimizations.I've added this as an options to explain. "Explain logical &lt;QUERY&gt;" will output the full operator tree (not the stage plans, tasks, AST etc).Again, I don't think this has to even be documented for users, but might be useful to developers.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="4814" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust WebHCat e2e tests until HIVE-4703 is addressed</summary>
      <description>right now a number of e2e webhcat test cases fail due to HIVE-4703. This issue in that bug has been around for a long time and the fix is not quick.We need to adjust expected e2e results until HIVE-4703 is fixed.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.ddl.conf</file>
    </fixedFiles>
  </bug>
  <bug id="483" opendate="2009-5-12 00:00:00" fixdate="2009-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>comments not handled properly in trunk</summary>
      <description>Anything after the comment is ignored - it can lead to random errors - it should be handled by the parser</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input16.cc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.q.out</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4844" opendate="2013-7-12 00:00:00" fixdate="2013-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add varchar data type</summary>
      <description>Add new varchar data types which have support for more SQL-compliant behavior, such as SQL string comparison semantics, max length, etc.Char type will be added as another task.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.ParameterizedPrimitiveTypeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.DoubleWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.gen.thrift.gen-rb.serde.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-py.org.apache.hadoop.hive.serde.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-php.org.apache.hadoop.hive.serde.Types.php</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.test.ThriftTestObj.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde2.thrift.test.Complex.java</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.cpp</file>
      <file type="M">serde.if.serde.thrift</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.GenericUDFEncode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">data.files.datatypes.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4846" opendate="2013-7-12 00:00:00" fixdate="2013-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Vectorized Limit Operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="4927" opendate="2013-7-24 00:00:00" fixdate="2013-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When we merge two MapJoin MapRedTasks, the TableScanOperator of the second one should be removed</summary>
      <description>set hive.auto.convert.join=true;set hive.auto.convert.join.noconditionaltask=true;EXPLAINSELECT x1.key AS key FROM src x1 JOIN src1 y1 ON (x1.key = y1.key) JOIN src1 y2 ON (x1.value = y2.value) GROUP BY x1.key;We will get a NPE from MetadataOnlyOptimizer. The reason is that the operator tree of the MapRedTask evaluating two MapJoins is TS1-&gt;MapJoin1-&gt;TS2-&gt;MapJoin2-&gt;...We should remove the TS2...</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="4928" opendate="2013-7-24 00:00:00" fixdate="2013-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date literals do not work properly in partition spec clause</summary>
      <description>The partition spec parsing doesn't do any actual real evaluation of the values in the partition spec, instead just taking the text value of the ASTNode representing the partition value. This works fine for string/numeric literals (expression tree below):(TOK_PARTVAL region 99)But not for Date literals which are of form DATE 'yyyy-mm-dd' (expression tree below:(TOK_DATELITERAL '1999-12-31')In this case the parser/analyzer uses "TOK_DATELITERAL" as the partition column value, when it should really get value of the child of the DATELITERAL token.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="4952" opendate="2013-7-29 00:00:00" fixdate="2013-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When hive.join.emit.interval is small, queries optimized by Correlation Optimizer may generate wrong results</summary>
      <description>If we have a query like this ...SELECT xx.key, xx.cnt, yy.keyFROM(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xxJOIN src yyON xx.key=yy.key;After Correlation Optimizer, the operator tree in the reducer will be JOIN2 | | MUX / \ / \ GBY | | | JOIN1 | \ / \ / DEMUXFor JOIN2, the right table will arrive at this operator first. If hive.join.emit.interval is small, e.g. 1, JOIN2 will output the results even it has not got any row from the left table. The logic related hive.join.emit.interval in JoinOperator assumes that inputs will be ordered by the tag. But, if a query has been optimized by Correlation Optimizer, this assumption may not hold for those JoinOperators inside the reducer.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4959" opendate="2013-7-30 00:00:00" fixdate="2013-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized plan generation should be added as an optimization transform.</summary>
      <description>Currently the query plan is vectorized at the query run time in the map task. It will be much cleaner to add vectorization as an optimization step.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4972" opendate="2013-8-1 00:00:00" fixdate="2013-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update code generated by thrift for DemuxOperator and MuxOperator</summary>
      <description>HIVE-2206 introduces two new operators, which are DemuxOperator and MuxOperator. queryplan.thrift has been updated. But code generated by thrift should be also updated</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
    </fixedFiles>
  </bug>
  <bug id="4985" opendate="2013-8-2 00:00:00" fixdate="2013-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor/clean up partition name pruning to be usable inside metastore server</summary>
      <description>Preliminary for HIVE-4914.The patch is going to be large already, so some refactoring and dead code removal that is non-controversial can be done in advance in a separate patch.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="4987" opendate="2013-8-2 00:00:00" fixdate="2013-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadoc can generate argument list too long error</summary>
      <description>We just to add to useexternalfile="yes" to the javadoc statements.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.build.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4991" opendate="2013-8-3 00:00:00" fixdate="2013-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive build with 0.20 is broken</summary>
      <description>As reported in HIVE-4911 ant clean package -Dhadoop.mr.rev=20Fails with - compile: [echo] Project: ql [javac] Compiling 898 source files to /Users/malakar/code/oss/hive/build/ql/classes [javac] /Users/malakar/code/oss/hive/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:35: package org.apache.commons.io does not exist [javac] import org.apache.commons.io.FileUtils; [javac] ^ [javac] /Users/malakar/code/oss/hive/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java:743: cannot find symbol [javac] symbol : variable FileUtils [javac] location: class org.apache.hadoop.hive.ql.session.SessionState [javac] FileUtils.deleteDirectory(resourceDir); [javac] ^ [javac] Note: Some input files use or override a deprecated API. [javac] Note: Recompile with -Xlint:deprecation for details. [javac] Note: Some input files use unchecked or unsafe operations. [javac] Note: Recompile with -Xlint:unchecked for details. [javac] 2 errors</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.java</file>
    </fixedFiles>
  </bug>
  <bug id="4996" opendate="2013-8-5 00:00:00" fixdate="2013-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unbalanced calls to openTransaction/commitTransaction</summary>
      <description>when we used hiveserver1 based on hive-0.10.0, we found the Exception thrown.It was:FAILED: Error in metadata: MetaException(message:java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that there are unbalanced calls to openTransaction/commitTransaction)FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskhelp</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRawStoreTxn.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5003" opendate="2013-8-6 00:00:00" fixdate="2013-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Localize hive exec jar for tez</summary>
      <description>Tez doesn't expose a distributed cache. JARs are localized via yarn APIs and added to vertices and the dag itself as needed. For hive we need to localize the hive-exec.jar.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5011" opendate="2013-8-6 00:00:00" fixdate="2013-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partitioning in HCatalog broken on external tables</summary>
      <description>Dynamic partitioning with HCatalog has been broken as a result of HCATALOG-500 trying to support user-set paths for external tables.The goal there was to be able to support other custom destinations apart from the normal "hive-style" partitions. However, it is not currently possible for users to set paths for dynamic ptn writes, since we don't support any way for users to specify "patterns"(like, say "${rootdir}/$v1.$v2/") into which writes happen, only "locations", and the values for dyn. partitions are not known ahead of time. Also, specifying a custom path messes with the way dynamic ptn. code tries to determine what was written to where from the output committer, which means that even if we supported patterned-writes instead of location-writes, we still have to do some more deep diving into the output committer code to support it.Thus, my current proposal is that we honour writes to user-specified paths for external tables ONLY for static partition writes - i.e., if we can determine that the write is a dyn. ptn. write, we will ignore the user specification. (Note that this does not mean we ignore the table's external location - we honour that - we just don't honour any HCatStorer/etc provided additional location - we stick to what metadata tells us the root location is.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.mapreduce.FosterStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5015" opendate="2013-8-6 00:00:00" fixdate="2013-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[HCatalog] Fix HCatalog unit tests on Windows</summary>
      <description>Note: To run hcatalog unit tests on windows, we need to use a hadoop.jar for Windows. Checkout branch-1-win branch of hadoop, do a mvn-install. Then you can use the branch-1-win.hadoop for unit test.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestSnapshots.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestHBaseInputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.ManyMiniCluster.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.ZKUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatLoaderStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5072" opendate="2013-8-13 00:00:00" fixdate="2013-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat]Enable directly invoke Sqoop job through Templeton</summary>
      <description>Now it is hard to invoke a Sqoop job through templeton. The only way is to use the classpath jar generated by a sqoop job and use the jar delegator in Templeton. We should implement Sqoop Delegator to enable directly invoke Sqoop job through Templeton.</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.VersionDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LogRetriever.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5073" opendate="2013-8-13 00:00:00" fixdate="2013-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix problem with multiple root tasks in tez</summary>
      <description>Input splits are always created in the same directory. That won't work.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5076" opendate="2013-8-13 00:00:00" fixdate="2013-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Subsequent reduce stages fail when executed in tez</summary>
      <description>Problem is missing calls to configuration apis.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5078" opendate="2013-8-13 00:00:00" fixdate="2013-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Fix e2e tests on Windows plus test cases for new features</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.ddl.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
      <file type="M">hcatalog.src.test.e2e.templeton.conf.default.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5079" opendate="2013-8-13 00:00:00" fixdate="2013-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive compile under Windows</summary>
      <description>Hive compilation failed under Windows. Error message:compile: [echo] Project: common [exec] D:\Program Files (x86)\GnuWin32\bin\xargs.exe: md5sum: No such fileor directory [exec] md5sum: ../serde/src/java/org/apache/hadoop/hive/serde2/io/Timesta:No such file or directory [javac] Compiling 25 source files to D:\Users\Administrator\hive\build\common\classes [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:4: unclosed string literal [javac] @HiveVersionAnnotation(version="0.12.0-SNAPSHOT", revision="80eadd8fa2af5eeba61f921318ab8b2c19980ab3", branch="trunk [javac] ^ [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:5: unclosed string literal [javac] ", [javac] ^ [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:6: class, interface, or enum expected [javac] user="Administrator [javac] ^ [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:6: unclosed string literal [javac] user="Administrator [javac] ^ [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:10: unclosed string literal [javac] ", [javac] ^ [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:11: unclosed string literal [javac] srcChecksum="aadceb95c37a1704aaf19501f46f6e84 [javac] ^ [javac] D:\Users\Administrator\hive\common\src\gen\org\apache\hive\common\package-info.java:12: unclosed string literal [javac] ") [javac] ^ [javac] 7 errors</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.scripts.saveVersion.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5080" opendate="2013-8-13 00:00:00" fixdate="2013-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hook to do additional optimization on the operator plan in tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5091" opendate="2013-8-14 00:00:00" fixdate="2013-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC files should have an option to pad stripes to the HDFS block boundaries</summary>
      <description>With ORC stripes being large, if a stripe straddles an HDFS block, the locality of read is suboptimal. It would be good to add padding to ensure that stripes don't straddle HDFS blocks.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5102" opendate="2013-8-15 00:00:00" fixdate="2013-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC getSplits should create splits based the stripes</summary>
      <description>Currently ORC inherits getSplits from FileFormat, which basically makes a split per an HDFS block. This can create too little parallelism and would be better done by having getSplits look at the file footer and create splits based on the stripes.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StripeInformation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5103" opendate="2013-8-15 00:00:00" fixdate="2013-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job numbers are incorrectly displayed in Tez</summary>
      <description>We display the number of jobs before we run anything. However, we don't look at Tez tasks yet.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5108" opendate="2013-8-16 00:00:00" fixdate="2013-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join on Tez fails in certain cases</summary>
      <description>select * from (select c from foo group by c) f1 join (select c from foo group by c) f2 on (f1.c = f2.c)fails with NPE on the cluster.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5147" opendate="2013-8-25 00:00:00" fixdate="2013-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Newly added test TestSessionHooks is failing on trunk</summary>
      <description>This was recently added via HIVE-4588</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionHookContextImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionHookContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="5148" opendate="2013-8-25 00:00:00" fixdate="2013-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jam sessions w/ Tez</summary>
      <description>Tez introduced a session api that let's you reuse certain resources during a session (AM, localized files, etc).Hive needs to tie these into hive sessions (for both CLI and HS2)NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">conf.hive-default.xml.template</file>
    </fixedFiles>
  </bug>
  <bug id="5151" opendate="2013-8-26 00:00:00" fixdate="2013-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Going green: Container re-cycling in Tez</summary>
      <description>Tez reuses containers to schedule tasks from same and different vertices in the same JVM. It also offers an API to reuse objects across vertices, dags and sessions.For hive we should reuse the operator plan as well as any hash tables (map join).NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5152" opendate="2013-8-26 00:00:00" fixdate="2013-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vector operators should inherit from non-vector operators for code re-use.</summary>
      <description>In many cases vectorized operators could share code from non-vector operators by inheriting.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5156" opendate="2013-8-27 00:00:00" fixdate="2013-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 jdbc ResultSet.close should free up resources on server side</summary>
      <description>ResultSet.close does not free up any resources (tmp files etc) on hive server.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="5158" opendate="2013-8-27 00:00:00" fixdate="2013-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow getting all partitions for table to also use direct SQL path</summary>
      <description>While testing some queries I noticed that getPartitions can be very slow (which happens e.g. in non-strict mode with no partition column filter); with a table with many partitions it can take 10-12s easily. SQL perf path can also be used for this path.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="5197" opendate="2013-9-3 00:00:00" fixdate="2013-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestE2EScenerios.createTaskAttempt should use MapRedUtil</summary>
      <description>Basically we should use HCatMapRedUtil as opposed to new'ing the task attempt context.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestE2EScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="5210" opendate="2013-9-4 00:00:00" fixdate="2013-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCatJTShim implementations are missing Apache license headers</summary>
      <description>During investigation of SQOOP-1190, I've noticed that WebHCatJTShim implementaion files do not have license headers.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.mapred.WebHCatJTShim20S.java</file>
    </fixedFiles>
  </bug>
  <bug id="5213" opendate="2013-9-4 00:00:00" fixdate="2013-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove hcatalog/shims directory</summary>
      <description>hcatalog/shims is no longer relevant. It should have been deleted as part of HIVE-4460 (it's in the .patch) but for some reason it's still in the source tree.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.shims.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5229" opendate="2013-9-5 00:00:00" fixdate="2013-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better thread management for HiveServer2 async threads</summary>
      <description>HIVE-4617 provides support for async execution in HS2. The async (background) thread pool currently creates N threads (server config), which are alive all the time. If all the threads in the pool are busy, a new request is added to a blocking queue. However, we can improve the strategy by not having all the async (background) threads alive when there are no corresponding requests. The async threads should die after a certain timeout if there are no new requests to handle.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5233" opendate="2013-9-5 00:00:00" fixdate="2013-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move hbase storage handler to org.apache.hcatalog package</summary>
      <description>org.apache.hcatalog in hcatalog/storage-handlers/ was erroneously renamed to org.apache.hive.hcatalog in HIVE-4895. This should be reverted as this module is deprecated and should continue to exist in org.apache.hcatalog.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.TestSnapshots.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestSnapshots.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestHBaseInputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestHBaseDirectOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.TestZNodeSetUp.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.TestThriftSerialization.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.TestRevisionManagerEndpoint.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.TestRevisionManager.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.TestIDGenerator.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.lock.TestZNodeName.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.lock.TestWriteLock.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.snapshot.IDGenClient.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.ManyMiniCluster.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.resources.revision-manager-default.xml</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.ZKUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.ZKBasedRevisionManager.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.Transaction.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.TableSnapshot.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RMConstants.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RevisionManagerProtocol.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RevisionManagerFactory.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RevisionManagerEndpoint.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RevisionManagerConfiguration.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.RevisionManager.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.PathUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.package-info.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.lock.ZooKeeperOperation.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.lock.ZNodeName.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.lock.WriteLock.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.lock.ProtocolSupport.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.lock.LockListener.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.IDGenerator.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.snapshot.FamilyRevision.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.ResultConverter.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.ImportSequenceFile.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HCatTableSnapshot.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HbaseSnapshotRecordReader.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseRevisionManagerUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseInputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseHCatStorageHandler.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseDirectOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseConstants.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseBulkOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseBaseOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hcatalog.hbase.HBaseAuthorizationProvider.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.gen-java.org.apache.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevisionList.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.gen-java.org.apache.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevision.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.if.transaction.thrift</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.gen-java.org.apache.hive.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevision.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.gen-java.org.apache.hive.hcatalog.hbase.snapshot.transaction.thrift.StoreFamilyRevisionList.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseAuthorizationProvider.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseBaseOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseBulkOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseConstants.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseDirectOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseHCatStorageHandler.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseInputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseRevisionManagerUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HbaseSnapshotRecordReader.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HBaseUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.HCatTableSnapshot.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.ImportSequenceFile.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.ResultConverter.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.FamilyRevision.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.IDGenerator.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.lock.LockListener.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.lock.ProtocolSupport.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.lock.WriteLock.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.lock.ZNodeName.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.lock.ZooKeeperOperation.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.package-info.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.PathUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RevisionManager.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerConfiguration.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerEndpoint.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerEndpointClient.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerFactory.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RevisionManagerProtocol.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.RMConstants.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.TableSnapshot.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.Transaction.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.ZKBasedRevisionManager.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.java.org.apache.hive.hcatalog.hbase.snapshot.ZKUtil.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.ManyMiniCluster.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.IDGenClient.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.lock.TestWriteLock.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.lock.TestZNodeName.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.TestIDGenerator.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManager.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManagerConfiguration.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.TestRevisionManagerEndpoint.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.TestThriftSerialization.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.snapshot.TestZNodeSetUp.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.TestHBaseBulkOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.TestHBaseDirectOutputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.TestHBaseHCatStorageHandler.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.TestHBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5234" opendate="2013-9-6 00:00:00" fixdate="2013-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition name filtering uses suboptimal datastructures</summary>
      <description>Some DSes used in name-based partition filtering, as well as related methods, are suboptimal, which can cost 100-s of ms on large number of partitions. I noticed while perf testing HIVE-4914, but it can also be applied separately given that the patch over there will take some time to get in.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
    </fixedFiles>
  </bug>
  <bug id="5240" opendate="2013-9-7 00:00:00" fixdate="2013-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column statistics on a partitioned column should fail early with proper error message</summary>
      <description>When computing column statistics on a partitioned table, if one of the columns equals the partitioned column then IndexOutOfBoundsException is thrown. Following analyze query throws IndexOutOfBoundsException during semantic analysis phasehive&gt; analyze table qlog_1m_part partition(year=5) compute statistics for columns year,month,week,type;FAILED: IndexOutOfBoundsException Index: 1, Size: 0 If the partitioned column is specified at last like below then the same exception is thrown at runtimehive&gt; analyze table qlog_1m_part partition(year=5) compute statistics for columns month,week,type,year;Hadoop job information for null: number of mappers: 0; number of reducers: 02013-09-06 18:05:06,587 null map = 0%, reduce = 100%Ended Job = job_local861862820_0001Execution completed successfullyMapred Local Task Succeeded . Convert the Join into MapJoinjava.lang.IndexOutOfBoundsException: Index: 3, Size: 3 at java.util.LinkedList.entry(LinkedList.java:365) at java.util.LinkedList.get(LinkedList.java:315) at org.apache.hadoop.hive.ql.exec.ColumnStatsTask.constructColumnStatsFromPackedRow(ColumnStatsTask.java:262) at org.apache.hadoop.hive.ql.exec.ColumnStatsTask.persistPartitionStats(ColumnStatsTask.java:302) at org.apache.hadoop.hive.ql.exec.ColumnStatsTask.execute(ColumnStatsTask.java:345) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1407) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1187) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1017) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:885) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:160)</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="5260" opendate="2013-9-10 00:00:00" fixdate="2013-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce HivePassThroughOutputFormat that allows Hive to use general purpose OutputFormats instead of HiveOutputFormats in StorageHandlers</summary>
      <description>This is a task being created to address the hive-side of HIVE-4331.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5263" opendate="2013-9-10 00:00:00" fixdate="2013-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query Plan cloning time could be improved by using Kryo</summary>
      <description>In HIVE-1511 we implemented Kryo for serialization but did not use for query plan cloning. As was discussed there it's a possible speed improvement. In addition the current XML serialization method does not work with Java 7 so using Kryo is a workaround for this issue.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcUnion.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardConstantListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardConstantMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="5265" opendate="2013-9-11 00:00:00" fixdate="2013-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Direct SQL fallback broken on Postgres</summary>
      <description>See HIVE-5264. Postgres aborts transaction on any failed query, so the fallback doesn't work.Original code used to do rollback/restart tx on SQL failure, but then it was removed to allow usage in cases like dropTable/etc., where there's external tx present and we cannot partially rollback.Looks like the solution for now is to reinstate the rollback/reopen, and prohibit the usage inside external transactions.</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="5266" opendate="2013-9-11 00:00:00" fixdate="2013-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog checkstyle failure due to HIVE-5225</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HiveClientCache.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.common.HiveClientCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="527" opendate="2009-6-2 00:00:00" fixdate="2009-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inserting into a partitioned table without specifying the partition field should fail</summary>
      <description>The following piece of code should fail but it succeeded - and it deletes all existing partitions of zshao_p.CREATE TABLE zshao_p (a string) PARTITIONED BY (ds string);INSERT OVERWRITE table zshao_p SELECT 1 from zshao_tt;It should output an error saying that the partition key/value pair is not specified.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5276" opendate="2013-9-12 00:00:00" fixdate="2013-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip redundant string encoding/decoding for hiveserver2</summary>
      <description>Current hiveserver2 acquires rows in string format which is used for cli output. Then convert them into row again and convert to final format lastly. This is inefficient and memory consuming.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5277" opendate="2013-9-12 00:00:00" fixdate="2013-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase handler skips rows with null valued first cells when only row key is selected</summary>
      <description>HBaseStorageHandler skips rows with null valued first cells when only row key is selected.SELECT key, col1, col2 FROM hbase_table;key1 cell1 cell2 key2 NULL cell3SELECT COUNT(key) FROM hbase_table;1HiveHBaseTableInputFormat.getRecordReader makes first cell selected to avoid skipping rows. But when the first cell is null, HBase skips that row.http://hbase.apache.org/book/perf.reading.html 12.9.6. Optimal Loading of Row Keys describes how to deal with this problem.I tried to find an existing issue, but I couldn't. If you find a same issue, please make this issue duplicated.</description>
      <version>0.11.0,0.12.0,0.11.1,0.13.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5278" opendate="2013-9-12 00:00:00" fixdate="2013-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move some string UDFs to GenericUDFs, for better varchar support</summary>
      <description>To better support varchar/char types in string UDFs, select UDFs should be converted to GenericUDFs. This allows the UDF to return the resulting char/varchar length in the type metadata.This work is being split off as a separate task from HIVE-4844. The initial UDFs as part of this work are concat/lower/upper.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5296" opendate="2013-9-16 00:00:00" fixdate="2013-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak: OOM Error after multiple open/closed JDBC connections.</summary>
      <description>Multiple connections to Hiveserver2, all of which are closed and disposed of properly show the Java heap size to grow extremely quickly. This issue can be recreated using the following codeimport java.sql.DriverManager;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;import org.apache.hive.service.cli.HiveSQLException;import org.apache.log4j.Logger;/* * Class which encapsulates the lifecycle of a query or statement. * Provides functionality which allows you to create a connection */public class HiveClient { Connection con; Logger logger; private static String driverName = "org.apache.hive.jdbc.HiveDriver"; private String db; public HiveClient(String db) { logger = Logger.getLogger(HiveClient.class); this.db=db; try{ Class.forName(driverName); }catch(ClassNotFoundException e){ logger.info("Can't find Hive driver"); } String hiveHost = GlimmerServer.config.getString("hive/host"); String hivePort = GlimmerServer.config.getString("hive/port"); String connectionString = "jdbc:hive2://"+hiveHost+":"+hivePort +"/default"; logger.info(String.format("Attempting to connect to %s",connectionString)); try{ con = DriverManager.getConnection(connectionString,"",""); }catch(Exception e){ logger.error("Problem instantiating the connection"+e.getMessage()); } } public int update(String query) { Integer res = 0; Statement stmt = null; try{ stmt = con.createStatement(); String switchdb = "USE "+db; logger.info(switchdb); stmt.executeUpdate(switchdb); logger.info(query); res = stmt.executeUpdate(query); logger.info("Query passed to server"); stmt.close(); }catch(HiveSQLException e){ logger.info(String.format("HiveSQLException thrown, this can be valid, " + "but check the error: %s from the query %s",query,e.toString())); }catch(SQLException e){ logger.error(String.format("Unable to execute query SQLException %s. Error: %s",query,e)); }catch(Exception e){ logger.error(String.format("Unable to execute query %s. Error: %s",query,e)); } if(stmt!=null) try{ stmt.close(); }catch(SQLException e){ logger.error("Cannot close the statment, potentially memory leak "+e); } return res; } public void close() { if(con!=null){ try { con.close(); } catch (SQLException e) { logger.info("Problem closing connection "+e); } } } }And by creating and closing many HiveClient objects. The heap space used by the hiveserver2 runjar process is seen to increase extremely quickly, without such space being released.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="5313" opendate="2013-9-18 00:00:00" fixdate="2013-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-4487 breaks build because 0.20.2 is missing FSPermission(string)</summary>
      <description>As per HIVE-4487, 0.20.2 does not contain FSPermission(string) so we'll have to shim it out.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="5322" opendate="2013-9-19 00:00:00" fixdate="2013-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FsPermission is initialized incorrectly in HIVE 5513</summary>
      <description>The change in HIVE-5313 converts the octal string into short using Short.parseShort(scratchDirPermission) but Short.parseShort function expects decimal. So "700" gets converted to 700 instead of 448.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="5351" opendate="2013-9-24 00:00:00" fixdate="2013-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure-Socket-Layer (SSL) support for HiveServer2</summary>
      <description>HiveServer2 and JDBC driver should support encrypted communication using SSL</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5352" opendate="2013-9-24 00:00:00" fixdate="2013-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cast(&amp;#39;1.0&amp;#39; as int) returns null</summary>
      <description>Casting strings to int/smallint/bigint/tinyint yields null if the string isn't a 'pure' integer. '1.0', '2.4' all return null. I think for those cases the cast should return the truncated int (i.e.: if c is string, cast(c as int) should be the same as cast(cast(c as float) as int).This is in line with the standard and is the same behavior as mysql and oracle. (postgres and sql server throw error, see first answer here: http://social.msdn.microsoft.com/Forums/sqlserver/en-US/af3eff9c-737b-42fe-9016-05da9203a667/oracle-does-understand-cast10-as-int-why-sql-server-does-not)</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestToInteger.java</file>
    </fixedFiles>
  </bug>
  <bug id="5363" opendate="2013-9-25 00:00:00" fixdate="2013-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-3978 broke the command line option --auxpath</summary>
      <description>HIVE-3978 changed the seperator for HIVE_AUX_JARS_PATH to : from ,. However, it's expected that it's , later on in the script, specifically here: AUX_CLASSPATH=${HIVE_AUX_JARS_PATH} AUX_PARAM=file://${HIVE_AUX_JARS_PATH} AUX_PARAM=`echo $AUX_PARAM | sed 's/,/,file:\/\//g'</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="5365" opendate="2013-9-25 00:00:00" fixdate="2013-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Boolean constants in the query are not handled correctly.</summary>
      <description>Boolean constants in the query are not handled correctly.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringColumnCompareColumn.txt</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="5367" opendate="2013-9-26 00:00:00" fixdate="2013-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix hive-tez build after tez updates</summary>
      <description>Changes need to be made to work against tez package changes.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5372" opendate="2013-9-26 00:00:00" fixdate="2013-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor TypeInfo and PrimitiveTypeEntry class hierachy to eliminate info repetition</summary>
      <description>TypeInfo with its sub-classes and PrimititiveTypeEntry class seem having repetitive information, such as type names and type params. It will be good if we can streamline the information organization.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeQualifiers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeDescriptor.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeParams.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeSpec.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.ParameterizedPrimitiveTypeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.BaseTypeParams.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.SettableUDF.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMacro.java</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.varchar.length.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.varchar.length.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.varchar.length.3.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveVarchar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="5395" opendate="2013-9-28 00:00:00" fixdate="2013-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various cleanup in ptf code</summary>
      <description>Some minor issues:Implementing classes on left side of equalsStack used instead of ArrayDequeClasses defined statically inside other files (when they do not need to beCheckstyle errors like indenting</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.Noop.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="54" opendate="2008-11-11 00:00:00" fixdate="2008-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a release notes file CHANGES.txt</summary>
      <description>A files CHANGES.txt (similar to Hadoop core) that lists all bug fixes in a particular release.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README</file>
    </fixedFiles>
  </bug>
  <bug id="5409" opendate="2013-10-1 00:00:00" fixdate="2013-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization for Tez</summary>
      <description>Enable the vectorization optimization on TezNO PRECOMMIT TESTS (WIP for Tez)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="541" opendate="2009-6-4 00:00:00" fixdate="2009-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement UDFs: INSTR and LOCATE</summary>
      <description>http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_instrhttp://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_locateThese functions can be directly implemented with Text (instead of String). This will make the test of whether one string contains another string much faster.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5410" opendate="2013-10-1 00:00:00" fixdate="2013-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive command line option --auxpath still does not work post HIVE-5363</summary>
      <description>In short, AUX_PARAM is set to:$ echo "file:///etc/passwd" | sed 's/:/,file:\/\//g'file,file://///etc/passwdwhich is invalid because "file" is not a real file.NO PRECOMMIT TESTS (since this is not tested)</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="5414" opendate="2013-10-2 00:00:00" fixdate="2013-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The result of show grant is not visible via JDBC</summary>
      <description>Currently, show grant / show role grant does not make fetch task, which provides the result schema for jdbc clients.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.keyword.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.authorization.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RoleDDLDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5415" opendate="2013-10-2 00:00:00" fixdate="2013-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove "System.err.println" from vectorization optimization</summary>
      <description>The vectorization optimization prints some stuff to stderr for no good reason (I guess that was useful during debugging).</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5422" opendate="2013-10-2 00:00:00" fixdate="2013-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Kyro to 2.22 now that it is released</summary>
      <description>As noted here v2.22 has been released. We should upgrade to that version as opposed to using the snapshot.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5425" opendate="2013-10-2 00:00:00" fixdate="2013-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a configuration option to control the default stripe size for ORC</summary>
      <description>We should provide a configuration option to control the default stripe size.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5426" opendate="2013-10-2 00:00:00" fixdate="2013-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestThriftBinaryCLIService tests fail on branch 0.12</summary>
      <description>Two tests of TestThriftBinaryCLIService are failing in branch 0.12.See https://builds.apache.org/job/Hive-branch-0.12-hadoop1/lastCompletedBuild/testReport/</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5440" opendate="2013-10-4 00:00:00" fixdate="2013-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 doesn&amp;#39;t apply SQL operation&amp;#39;s config property</summary>
      <description>The HiveServer2 thrift IDL includes an optional config overlay map which is currently not used.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="5441" opendate="2013-10-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Async query execution doesn&amp;#39;t return resultset status</summary>
      <description>For synchronous statement execution (SQL as well as metadata and other), the operation handle includes a boolean flag indicating whether the statement returns a resultset. In case of async execution, that's always set to false.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5442" opendate="2013-10-4 00:00:00" fixdate="2013-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Plumbing for map join in tez</summary>
      <description>Need to set up some classes to differentiate how MR and tez load the hashtables for MapJoins.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5449" opendate="2013-10-4 00:00:00" fixdate="2013-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive schematool info option incorrectly reports error for Postgres metastore</summary>
      <description>The schema tool has an option to verify the schema version stored in the metastore. This is implemented as a simple select query executed via JDBC. The problem is that Postgres requires object names to be quoted due to the way tables are created. It's a similar issues hit by metastore direct SQL (HIVE-5264, HIVE-5265 etc).</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="5459" opendate="2013-10-5 00:00:00" fixdate="2013-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add --version option to hive script</summary>
      <description>Hive jars already contain all the build information, similar to hadoop. This was added as part of HiveServer2 feature.We are still missing the command line wrapper to extract that information</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="5478" opendate="2013-10-7 00:00:00" fixdate="2013-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e testsuite for hcat authorization tests needs some fixes</summary>
      <description>Here are the issues:1. The HARNESS_ROOT in the test-hcat-authorization testsuite needs to be testdist root otherwise the ant command fails to look for resource/default.res.2. A few tests DB_OPS_5 and TABLE_OPS_2 were relying on default permissions on the hive warehouse directory which can vary based on the environment, improved the test to check what is set.3. DB_OPS_18 error message is old, now we get a more specific message, updated to verify the new one.NO PRECOMMIT TESTS</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.hcatperms.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5480" opendate="2013-10-7 00:00:00" fixdate="2013-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e tests for doAs feature are failing</summary>
      <description>WebHCat testsuite have two tests failing:1. doAsTests_6 - The test was assuming that the metadata can be read even if reading data cannot be. As part of the setup we are using the StorageBasedAuthorizationProvider which will not allow for this operation to succeed. Updated the test to check for the failure and verify the error message.2. doAsTests_7 - Updated the error message to reflect the current error message which looks correct.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.doas.conf</file>
    </fixedFiles>
  </bug>
  <bug id="5482" opendate="2013-10-7 00:00:00" fixdate="2013-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC should depend on httpclient.version and httpcore.version 4.1.3 to be consistent with other modules</summary>
      <description>Currently depends on 4.2.4 and 4.2.5 which conflicts with thrift-0.9 which depends on 4.1.3</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5485" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SBAP errors on null partition being passed into partition level authorization</summary>
      <description>SBAP causes an NPE when null is passed in as a partition for partition-level or column-level authorization.Personally, in my opinion, this is not a SBAP bug, but incorrect usage of AuthorizationProviders - one should not be calling the column-level authorize (given that column-level is more basic than partition-level) function and pass in a null as the partition value. However, that happens on code introduced by HIVE-1887, and unless we rewrite that (and possibly a whole bunch more(will need evaluation)), we have to accommodate that null and appropriately attempt to fall back to table-level authorization in that case.The offending code section is in Driver.java:685 678 // if we reach here, it means it needs to do a table authorization 679 // check, and the table authorization may already happened because of other 680 // partitions 681 if (tbl != null &amp;&amp; !tableAuthChecked.contains(tbl.getTableName()) &amp;&amp; 682 !(tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE)) { 683 List&lt;String&gt; cols = tab2Cols.get(tbl); 684 if (cols != null &amp;&amp; cols.size() &gt; 0) { 685 ss.getAuthorizer().authorize(tbl, null, cols, 686 op.getInputRequiredPrivileges(), null); 687 } else { 688 ss.getAuthorizer().authorize(tbl, op.getInputRequiredPrivileges(), 689 null); 690 } 691 tableAuthChecked.add(tbl.getTableName()); 692 }</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="5486" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 should create base scratch directories at startup</summary>
      <description>With impersonation enabled, the same base directory is used by all sessions/queries. For a new deployment, this directory gets created on first invocation by the user running that session. This would cause directory permission conflict for other users.HiveServer2 should create the base scratch dirs if it doesn't exist.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5488" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some files are missing apache license headers</summary>
      <description>Around 29 files that should have apache license headers are missing it in 0.12 branch. There are some more such files in trunk.This needs to be fixed for the 0.12 release.NO PRECOMMIT TESTS</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DBTokenStore.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HiveEventCounter.java</file>
      <file type="M">shims.src.common-secure.test.org.apache.hadoop.hive.thrift.TestDBTokenStore.java</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MDelegationToken.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MMasterKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.OrderDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.OrderExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PartitionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.GenericUDFDecode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.GenericUDFEncode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBase64.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUnbase64.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.udf.Rot13OutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFDecode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFEncode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestToInteger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFBase64.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFHex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUnbase64.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUnhex.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomSerDe1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomSerDe2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomSerDe3.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomSerDe4.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.CustomSerDe5.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SettableUnionObjectInspector.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.AddResourceOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.DeleteResourceOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.DfsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SetOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="549" opendate="2009-6-8 00:00:00" fixdate="2009-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel Execution Mechanism</summary>
      <description>In a massively parallel database system, it would be awesome to also parallelize some of the mapreduce phases that our data needs to go through.One example that just occurred to me is UNION ALL: when you union two SELECT statements, effectively you could run those statements in parallel. There's no situation (that I can think of, but I don't have a formal proof) in which the left statement would rely on the right statement, or vice versa. So, they could be run at the same time...and perhaps they should be. Or, perhaps there should be a way to make this happen...PARALLEL UNION ALL? PUNION ALL?</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input41.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.input.part9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input42.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input41.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5493" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>duplicate jars with different versions for guava, commons-logging</summary>
      <description>Duplicate jars with different versions for guava and commons-logging are present in build/dist/lib .hive should ship with just one version of each jar.guava-r08 and guava-0.11.0.2 are present, and commons-logging has 1.0.4 and 1.1.1 .</description>
      <version>0.12.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5494" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization throws exception with nested UDF.</summary>
      <description>Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Udf: GenericUDFAbs, is not supportedat org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:465)at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:274)at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getAggregatorExpression(VectorizationContext.java:1512)at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.&lt;init&gt;(VectorGroupByOperator.java:133)... 41 moreFAILED: RuntimeException java.lang.reflect.InvocationTargetException</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5496" opendate="2013-10-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcat -e "drop database if exists" fails on authorizing non-existent null db</summary>
      <description>When running a "drop database if exists" call on hcat commandline, it fails authorization with a NPE because it tries to authorize access to a null database. This should be changed to not call authorize if the db for the DropDatabaseDesc is null.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5508" opendate="2013-10-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] ignore log collector e2e tests for Hadoop 2</summary>
      <description>Log collector currently only works with Hadoop 1. If run under Hadoop 2, no log will be collected. Templeton e2e tests check the existence of those logs, so they will fail under Hadoop 2. Need to disable them when run under Hadoop 2.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.streaming.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
      <file type="M">hcatalog.src.test.e2e.templeton.conf.default.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5510" opendate="2013-10-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] GET job/queue return wrong job information</summary>
      <description>GET job/queue of a TempletonController job return weird information. It is a mix of child job and itself. It should only pull the information of the controller job itself.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.QueueStatusBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.streaming.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="5511" opendate="2013-10-10 00:00:00" fixdate="2013-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>percentComplete returned by job status from WebHCat is null</summary>
      <description>In hadoop1 the logging from MR is sent to stderr. In H2, by default, to syslog. templeton.tool.LaunchMapper expects to see the output on stderr to produce 'percentComplete' in job status.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.mapred.WebHCatJTShim20S.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.bin.webhcat.config.sh</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.streaming.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="5519" opendate="2013-10-11 00:00:00" fixdate="2013-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paging mechanism for templeton get requests.</summary>
      <description>Issuing a command to retrieve the jobs field using"https://mwinkledemo.azurehdinsight.net:563/templeton/v1/queue/&lt;job_id&gt;?user.name=admin&amp;fields=*" --user uwill result in timeout in windows machine. The issue happens because of the amount of data that needs to be fetched. The proposal is to use paging based encoding scheme so that we flush the contents regularly and the client does not time out.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
    </fixedFiles>
  </bug>
  <bug id="5533" opendate="2013-10-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-connect Tez session after AM timeout</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5535" opendate="2013-10-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Webhcat e2e test JOBS_2 fail due to permission when hdfs umask setting is 022</summary>
      <description>Complaining no permission to output directory "/tmp/templeton_test_out/$runid". This is because /tmp/templeton_test_out/runid is created with umask 022 with user "test.other.user.name" (the userid of the first test in the group JOBS_1). Other user cannot write to it (JOBS_2, which run as userid "test.user.name")NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="5536" opendate="2013-10-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect Operation Name is passed to hookcontext</summary>
      <description>HS2 passes incorrect operation name to hookcontext.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.hooks.TestHs2Hooks.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="554" opendate="2009-6-10 00:00:00" fixdate="2009-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GenericUDF to create arrays, maps</summary>
      <description>Here is an example:SELECT array(1,2,3)[3], map("a":1,"b":2,"c":3)["a"], struct(user_id:3, revenue: sum(rev))FROM tableGROUP BY user_id;This is relatively easy to do with the GenericUDF framework, and will greatly increase the flexibility of the language.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5547" opendate="2013-10-15 00:00:00" fixdate="2013-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat pig job submission should ship hive tar if -usehcatalog is specified</summary>
      <description>Currently when when a Pig job is submitted through WebHCat and the Pig script uses HCatalog, that means that Hive should be installed on the node in the cluster which ends up executing the job. For large clusters is this a manageability issue so we should use DistributedCache to ship the Hive tar file to the target node as part of job submissionTestPig_11 in hcatalog/src/test/e2e/templeton/tests/jobsubmission.conf has the test case for this</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.TempletonDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5556" opendate="2013-10-16 00:00:00" fixdate="2013-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pushdown join conditions</summary>
      <description>See details in HIVE-5555</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
    </fixedFiles>
  </bug>
  <bug id="5558" opendate="2013-10-16 00:00:00" fixdate="2013-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support alternate join syntax</summary>
      <description>See details in HIVE-5555Allow from clause to join table sources with the `comma' token.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="5561" opendate="2013-10-16 00:00:00" fixdate="2013-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear work map for container reuse on tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5567" opendate="2013-10-16 00:00:00" fixdate="2013-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add better protection code for SARGs</summary>
      <description>Currently, the SARG parser gets a NPE when the push down predicate uses a type like decimal that isn't supported.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.create.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.analyze.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">data.files.orc.create.people.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5595" opendate="2013-10-19 00:00:00" fixdate="2013-1-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized SMB JOIN</summary>
      <description>Vectorized implementation of SMB Map Join.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="5602" opendate="2013-10-22 00:00:00" fixdate="2013-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Micro optimize select operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5604" opendate="2013-10-22 00:00:00" fixdate="2013-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix validation of nested expressions.</summary>
      <description>The bug fixes a few issues related to nested expressions:1) The nested expressions were not being validated at all.2) UDFRegExp was not handled correctly, but issue was not caught because of the previous issue.3) HIVE-5642 will not show up when this jira is fixed, but still added a sanity check to validate the number of arguments.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5629" opendate="2013-10-23 00:00:00" fixdate="2013-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix two javadoc failures in HCatalog</summary>
      <description>I am seeing two javadoc failures on HCatalog. These are not being seen by PTest and indeed I cannot reproduce on my Mac but can on Linux. Regardless they should be fixed.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.InputJobInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="563" opendate="2009-6-16 00:00:00" fixdate="2009-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF for parsing the URL</summary>
      <description>Needs a udf to extract the parts of url from url string.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5638" opendate="2013-10-24 00:00:00" fixdate="2013-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in ConvertJoinMapJoin on Tez</summary>
      <description>getBigTableCandidates will return "null" in the outer join case. The join transformation assumes that it's always a Set (and empty when there are no candidates).</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
    </fixedFiles>
  </bug>
  <bug id="5639" opendate="2013-10-24 00:00:00" fixdate="2013-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow caching of Orc footers in Tez AM</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="567" opendate="2009-6-19 00:00:00" fixdate="2009-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc: integrate hive with pentaho report designer</summary>
      <description>Instead of trying to get a complete implementation of jdbc, its probably more useful to pick reporting/analytics software out there and implement the jdbc methods necessary to get them working. This jira is a first attempt at this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5670" opendate="2013-10-28 00:00:00" fixdate="2013-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>annoying ZK exceptions are annoying</summary>
      <description>when I run tests locally (or on cluster IIRC) there are bunch of ZK-related exceptions in Hive log, such as2013-10-28 09:50:50,851 ERROR zookeeper.ClientCnxn (ClientCnxn.java:processEvent(523)) - Error while calling watcher java.lang.NullPointerException at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497) 2013-10-28 09:51:05,747 DEBUG server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1024)) - ignoring exception during input shutdownjava.net.SocketException: Socket is not connected at sun.nio.ch.SocketChannelImpl.shutdown(Native Method) at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:633) at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360) at org.apache.zookeeper.server.NIOServerCnxn.closeSock(NIOServerCnxn.java:1020) at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:977) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:347) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:224) at java.lang.Thread.run(Thread.java:680)They are annoying when you look for actual problems in logs.Those on DEBUG level should be silenced via log levels for ZK classes by default. Not sure what to do with ERROR level one(s?), I'd need to look if they can be silenced/logged as DEBUG on hive side, or maybe file a bug for ZK...</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.log4j.properties</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">data.conf.hive-log4j.properties</file>
      <file type="M">common.src.test.resources.hive-log4j-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j-test.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5672" opendate="2013-10-28 00:00:00" fixdate="2013-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert with custom separator not supported for non-local directory</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-3682 is great but non local directory don't seem to be supported:insert overwrite directory '/tmp/test-02'row format delimitedFIELDS TERMINATED BY ':'select description FROM sample_07Error while compiling statement: FAILED: ParseException line 2:0 cannot recognize input near 'row' 'format' 'delimited' in select clauseThis works (with 'local'):insert overwrite local directory '/tmp/test-02'row format delimitedFIELDS TERMINATED BY ':'select code, description FROM sample_07</description>
      <version>0.12.0,1.0.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="5681" opendate="2013-10-29 00:00:00" fixdate="2013-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validation doesn&amp;#39;t catch SMBMapJoin</summary>
      <description>SMBMapJoin is currently not supported, but validation doesn't catch it because it has same OperatorType.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5683" opendate="2013-10-29 00:00:00" fixdate="2013-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC support for char</summary>
      <description>Support char type in JDBC, including char length in result set metadata.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeQualifiers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Type.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.constants.rb</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.constants.py</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeId.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.constants.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.constants.cpp</file>
      <file type="M">service.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">data.files.datatypes.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5684" opendate="2013-10-29 00:00:00" fixdate="2013-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serde support for char</summary>
      <description>Update some of the SerDe's with char support</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="5685" opendate="2013-10-29 00:00:00" fixdate="2013-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition column type validation doesn&amp;#39;t work in some cases</summary>
      <description>It seems like it works if there's more than one partition column, and doesn't work if there's just one. At least that's the case that I found. The situation for different types is the same.hive&gt; create table zzz(c string) partitioned by (i int);OKTime taken: 0.41 secondshive&gt; alter table zzz add partition (i='foo');OKTime taken: 0.185 secondshive&gt; create table zzzz(c string) partitioned by (i int,j int); OKTime taken: 0.085 secondshive&gt; alter table zzzz add partition (i='foo',j=5); FAILED: SemanticException [Error 10248]: Cannot add partition column i of type string as it cannot be converted to type inthive&gt; alter table zzzz add partition (i=5,j='foo');FAILED: SemanticException [Error 10248]: Cannot add partition column j of type string as it cannot be converted to type int</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5686" opendate="2013-10-29 00:00:00" fixdate="2013-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition column type validation doesn&amp;#39;t quite work for dates</summary>
      <description>Another interesting issue...hive&gt; create table zzzzz(c string) partitioned by (i date,j date);OKTime taken: 0.099 secondshive&gt; alter table zzzzz add partition (i='2012-01-01', j='foo'); FAILED: SemanticException [Error 10248]: Cannot add partition column j of type string as it cannot be converted to type datehive&gt; alter table zzzzz add partition (i='2012-01-01', j=date 'foo');OKTime taken: 0.119 secondsThe fake date is caught in normal queries:hive&gt; select * from zzzzz where j == date 'foo';FAILED: SemanticException Unable to convert date literal string to date value.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="5687" opendate="2013-10-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Streaming support in Hive</summary>
      <description>Implement support for Streaming data into HIVE. Provide a client streaming API Transaction support: Clients should be able to periodically commit a batch of records atomically Immediate visibility: Records should be immediately visible to queries on commit Should not overload HDFS with too many small filesUse Cases: Streaming logs `into HIVE via Flume Streaming results of computations from Storm</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5698" opendate="2013-10-30 00:00:00" fixdate="2013-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A few test files missing apache license header</summary>
      <description></description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFRound.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="5704" opendate="2013-10-31 00:00:00" fixdate="2013-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A couple of generic UDFs are not in the right folder/package</summary>
      <description>There are two generic UDFs are in the package for non-generic UDFs. I think it's better to be consistent but putting them in the udf.generic package</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFEncode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFDecode.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.GenericUDFEncode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.GenericUDFDecode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5707" opendate="2013-10-31 00:00:00" fixdate="2013-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validate values for ConfVar</summary>
      <description>with set hive.conf.validation=true, hive validates new value can be changed to the type. But it does not check value itself.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.create.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5708" opendate="2013-10-31 00:00:00" fixdate="2013-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 should trim long logs when posting to jira</summary>
      <description>When a build fails we post the build log to JIRA. The issue is that sometimes this can be a couple hundred KB. Since this log is available in the link also mentioned in the JIRA we should trim the message size down to say last 200 lines and then add a warning when we need to.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestJIRAService.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
    </fixedFiles>
  </bug>
  <bug id="5717" opendate="2013-10-31 00:00:00" fixdate="2013-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate javadoc and source jars</summary>
      <description>We should be generating both javadoc and source jars.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5719" opendate="2013-11-1 00:00:00" fixdate="2013-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove some overly noisy perflogger statements from Tez codepath</summary>
      <description>Some of these fire so often they turn into hotspots in profiler runs...</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5721" opendate="2013-11-1 00:00:00" fixdate="2013-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental build is disabled by MCOMPILER-209</summary>
      <description>NO PRECOMMIT TESTSmaven-compiler-plugin-3.1 has bug on incremental build(http://jira.codehaus.org/browse/MCOMPILER-209)</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5722" opendate="2013-11-1 00:00:00" fixdate="2013-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip generating vectorization code if possible</summary>
      <description>Currently, ql module always generates new vectorization code, which might not be changed so frequently.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorTestCode.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="5734" opendate="2013-11-4 00:00:00" fixdate="2013-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable merge/move tasks for Tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5735" opendate="2013-11-4 00:00:00" fixdate="2013-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable noscan/partialscan on Tez</summary>
      <description>analyze ... noscan or partialscan is currently implemented as a physical optimization (GenMRTable1). There's little benefit in porting this over to tez, but we should automatically fall back to MR even if optimize.tez is set.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5737" opendate="2013-11-4 00:00:00" fixdate="2013-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide StructObjectInspector for UDTFs rather than ObjectInspect[]</summary>
      <description>In UDTF, column names can be useful sometimes. For example, complex function with many optional parameters something like,xml_explode('\t' as field, '\n' as line, '=' as mapkey, ':' as items, input)Without column name, it's not easy to discern each parameter is for what.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5765" opendate="2013-11-6 00:00:00" fixdate="2013-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline throws NPE when -e option is used</summary>
      <description>When running Beeline with -e option to pass in sql commands, a null pointer exception is thrown, even though the command completes successfully.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="5766" opendate="2013-11-6 00:00:00" fixdate="2013-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update call to Tez DAG status to reflect updated API</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5768" opendate="2013-11-7 00:00:00" fixdate="2013-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline connection cannot be closed with !close command</summary>
      <description>NO PRECOMMIT TESTS0: jdbc:hive2://localhost:10000/db2&gt; !closeAmbiguous command: [close, closeall]</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="5770" opendate="2013-11-7 00:00:00" fixdate="2013-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch merge tasks to use tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="5772" opendate="2013-11-7 00:00:00" fixdate="2013-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print message for union operators</summary>
      <description>Until unions are implemented print message asking user to run on MR instead of failing on the backend.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5773" opendate="2013-11-7 00:00:00" fixdate="2013-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix build due to conflict between HIVE-5711 and HIVE-5713</summary>
      <description>I should have seen this but HIVE-5711 and HIVE-5713 conflicted. It's a trival patch will have it up in a few minutes time.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5775" opendate="2013-11-7 00:00:00" fixdate="2013-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce Cost Based Optimizer to Hive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5778" opendate="2013-11-7 00:00:00" fixdate="2013-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin hints on Tez</summary>
      <description>Mapjoin hints don't work on tez right now (there's two places we convert joins auto happens late in the compilation mapjoin hints happen early).Since the default is to ignore them, I'll just disable hints altogether on tez for now.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5779" opendate="2013-11-7 00:00:00" fixdate="2013-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Subquery in where clause with distinct fails with mapjoin turned on with serialization error.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="5788" opendate="2013-11-9 00:00:00" fixdate="2013-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select * fails for table after adding new columns using rcfile storage format</summary>
      <description>Given the following tables:select * from two;+-----+----+| id | a |+-----+----+| 1 | a || 2 | b |+-----+----+ select * from three;+-----+----+----+| id | a | b |+-----+----+----+| 1 | a | z || 2 | b | y || 3 | c | x |+-----+----+----+Execute the following steps:create table testrc (id bigint, a string) stored as rcfile;insert into table testrc select * from two;select * from testrc; //returns correctly+-----+----+| id | a |+-----+----+| 1 | a || 2 | b |+-----+----+ alter table testrc add columns (b string); insert into table testrc select * from three;select * from testrc; //new column returns null values+-----+----+----+| id | a | b |+-----+----+----+| 1 | a | || 2 | b | || 1 | a | || 2 | b | || 3 | c | |+-----+----+----+</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5790" opendate="2013-11-9 00:00:00" fixdate="2013-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven test build failure shows wrong error message</summary>
      <description>This is the error message that was correct for ant."See build/ql/tmp/hive.log, or try "ant test ... -Dtest.silent=false" to get more logs."This JIRA is to replace this message with mvn-specific error message.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5799" opendate="2013-11-12 00:00:00" fixdate="2013-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>session/operation timeout for hiveserver2</summary>
      <description>Need some timeout facility for preventing resource leakages from instable or bad clients.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationState.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.conf.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Heartbeater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AutoProgressor.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Validator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="580" opendate="2009-6-24 00:00:00" fixdate="2009-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>command to list available UDF names</summary>
      <description>It would be great if there was some way to list all available UDF names in the CLI.Ideally, there would be some way for the UDFs themselves to provide a description/help that could also be displayed.Would also be nice if the UDF names were available for auto-completion (HIVE-97).As a start, just having the names available would be great.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5808" opendate="2013-11-13 00:00:00" fixdate="2013-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>broadcast join in tez discards duplicate records from the broadcasted table</summary>
      <description>When the small(broadcasted table) in a join has duplicate records for a key, they are getting discarded, resulting in incorrect output.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="5809" opendate="2013-11-13 00:00:00" fixdate="2013-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect stats in some cases with hive.stats.autogather=true</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="581" opendate="2009-6-24 00:00:00" fixdate="2009-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve group by syntax</summary>
      <description>It would be nice if group by allowed either column aliases or column position (like mysql).It can be burdensome to have to repeat UDFs both in the select and in the group by.e.g. instead of:select f1(col1), f2(col2), f3(col3), count(1) group by f1(col1), f2(col2), f3(col3);it would allow:select f1(col1), f2(col2), f3(col3), count(1) group by 1, 2, 3;</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5843" opendate="2013-11-18 00:00:00" fixdate="2013-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transaction manager for Hive</summary>
      <description>As part of the ACID work proposed in HIVE-5317 a transaction manager is required.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockMode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.12.0-to-0.13.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.12.0-to-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.12.0-to-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.12.0-to-0.13.0.postgres.sql</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HiveObjectRef.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrivilegeBag.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Type.java</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="5849" opendate="2013-11-19 00:00:00" fixdate="2013-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the stats of operators based on heuristics in the absence of any column statistics</summary>
      <description>In the absence of any column statistics, operators will simply use the statistics from its parents. It is useful to apply some heuristics to update basic statistics (number of rows and data size) in the absence of any column statistics. This will be worst case scenario.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby.q</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5859" opendate="2013-11-20 00:00:00" fixdate="2013-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create view does not captures inputs</summary>
      <description>For example, CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e;should capture "default.tbl_j5jbymsx8e" as input entity for authorization process but currently it's not.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.inputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5861" opendate="2013-11-20 00:00:00" fixdate="2013-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix exception in multi insert statement on Tez</summary>
      <description>Multi insert statements that have multiple group by clauses aren't handled properly in tez.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5871" opendate="2013-11-22 00:00:00" fixdate="2013-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use multiple-characters as field delimiter</summary>
      <description>By default, hive only allows user to use single character as field delimiter. Although there's RegexSerDe to specify multiple-character delimiter, it can be daunting to use, especially for amateurs.The patch adds a new SerDe named MultiDelimitSerDe. With MultiDelimitSerDe, users can specify a multiple-character field delimiter when creating tables, in a way most similar to typical table creations. For example:create table test (id string,hivearray array&lt;binary&gt;,hivemap map&lt;string,int&gt;) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ("field.delim"="[,]","collection.delim"=":","mapkey.delim"="@");where field.delim is the field delimiter, collection.delim and mapkey.delim is the delimiter for collection items and key value pairs, respectively. Among these delimiters, field.delim is mandatory and can be of multiple characters, while collection.delim and mapkey.delim is optional and only support single character.To use MultiDelimitSerDe, you have to add the hive-contrib jar to the class path, e.g. with the add jar command.</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
    </fixedFiles>
  </bug>
  <bug id="5880" opendate="2013-11-25 00:00:00" fixdate="2013-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename HCatalog HBase Storage Handler artifact id</summary>
      <description>Current the HBase storage handler is named hive-hbase-storage-handler. I think we should rename it to hive-hcatalog-hbase-storage-handler to match the other hcatalog artifacts and to differentiate it from the hive-hbase-handler.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hcatalog.storage-handlers.hbase.pom.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5882" opendate="2013-11-25 00:00:00" fixdate="2013-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce logging verbosity on Tez</summary>
      <description>Running on Tez with debug level set to INFO is very noisy.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5883" opendate="2013-11-25 00:00:00" fixdate="2013-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Plan is deserialized more often than necessary on Tez (in container reuse case)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="5889" opendate="2013-11-26 00:00:00" fixdate="2013-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add counter based stats aggregator for tez</summary>
      <description>In tez counters are retrieved with different API. Need to add a separate implementation of the recently added counter based stats aggregator.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug id="5909" opendate="2013-11-28 00:00:00" fixdate="2013-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>locate and instr throw java.nio.BufferUnderflowException when empty string as substring</summary>
      <description>Locate and instr functions, try:select * from foo_bar where locate("", "foobar")orselect * from foo_bar where instr("foobar", "")Examples (hive cli):hive (default)&gt; select locate("","dfs") from foobar;FAILED: BufferUnderflowException nullLog output from the failed job:Task ID: task_1384392632998_7426_m_000035 URL: http://jobtracker:54311/taskdetails.jsp?jobid=job_1384392632998_7426&amp;tipid=task_1384392632998_7426_m_000035-----Diagnostic Messages for this Task:Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {...} at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:175) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {...} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:544) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:157) ... 8 moreCaused by: java.nio.BufferUnderflowException at java.nio.Buffer.nextGetIndex(Buffer.java:474) at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:117)</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="591" opendate="2009-6-29 00:00:00" fixdate="2009-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create new type of join ( 1 row for a given key from multiple tables) (UNIQUEJOIN)</summary>
      <description>It will be useful to support a new type of join:say:.select .. from JOIN TABLES (A,B,C) WITH KEYS (A.key, B.key, C.key) where ....The semantics are that for a given key only 1 row is created - nulls are present for the the tables which do not contain a row for that key.There is no limit on the number of tables, the number of keys should be the same as the number of tables.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinCond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.joinType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.joinCond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5911" opendate="2013-12-2 00:00:00" fixdate="2013-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recent change to schema upgrade scripts breaks file naming conventions</summary>
      <description>The changes made in HIVE-5700 break the convention for naming schema upgrade scripts.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.12.0-to-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.014-HIVE-3764.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.011-HIVE-3649.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.12.0-to-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.12.0-to-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.014-HIVE-3764.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="5915" opendate="2013-12-2 00:00:00" fixdate="2013-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade Kryo dependency</summary>
      <description>Kryo changes API's often, we should shade it so users can use their own version.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5946" opendate="2013-12-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5957" opendate="2013-12-5 00:00:00" fixdate="2013-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HCatalog Unit tests on Windows</summary>
      <description>org.apache.hcatalog.hbase.TestHBaseHCatStorageHandler fails on Windows. It generates "java.lang.IllegalStateException: Failed to setup cluster at org.apache.hcatalog.hbase.ManyMiniCluster.start(ManyMiniCluster.java:119)". Digging further there is "Please fix invalid configuration for hbase.rootdir file://C:/tmp/build/test/data/test_default_573990410261077827/hbasejava.lang.IllegalArgumentException: Wrong FS: file://C:/tmp/build/test/data/test_default_573990410261077827/hbase, expected: file:///at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)"This was fixed in HIVE-5015 (9/3/13) and and got clobbered by HIVE-5261 (9/12/13).</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hcatalog.mapreduce.TestSequenceFileReadWrite.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.TestHCatHBaseInputFormat.java</file>
      <file type="M">hcatalog.storage-handlers.hbase.src.test.org.apache.hcatalog.hbase.SkeletonHBaseTest.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatLoaderStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.HcatTestUtils.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.TestMultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="596" opendate="2009-6-30 00:00:00" fixdate="2009-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parition pruning not working will nulls</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5960" opendate="2013-12-5 00:00:00" fixdate="2013-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - special handling of PUBLIC role</summary>
      <description>All users should belong to the public role. Users don't have to be (/can't be) added to this role .Metastore api/checks should return this role.set role should allow setting to this role.GRANT/REVOKE on this should be disabled.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.sql.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.7.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5961" opendate="2013-12-5 00:00:00" fixdate="2013-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explain authorize for checking privileges</summary>
      <description>For easy checking of need privileges for a query, explain authorize select * from src join srcpartINPUTS: default@srcpart default@srcpart@ds=2008-04-08/hr=11 default@srcpart@ds=2008-04-08/hr=12 default@srcpart@ds=2008-04-09/hr=11 default@srcpart@ds=2008-04-09/hr=12 default@srcOUTPUTS: file:/home/navis/apache/oss-hive/itests/qtest/target/tmp/localscratchdir/hive_2013-12-04_21-57-53_748_5323811717799107868-1/-mr-10000CURRENT_USER: hive_test_userOPERATION: QUERYAUTHORIZATION_FAILURES: No privilege 'Select' found for inputs { database:default, table:srcpart, columnName:key} No privilege 'Select' found for inputs { database:default, table:src, columnName:key} No privilege 'Select' found for inputs { database:default, table:src, columnName:key}Hopefully good for debugging of authorization, which is in progress on HIVE-5837.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="597" opendate="2009-6-30 00:00:00" fixdate="2009-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc/src/java/org/apache/hadoop/hive/jdbc do not have the apache header</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveCallableStatement.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5975" opendate="2013-12-6 00:00:00" fixdate="2013-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] templeton mapreduce job failed if provide "define" parameters</summary>
      <description>Trying to submit a mapreduce job through templeton failed:curl -k -u user:pass -d user.name=user -d define=JobName=MRPiJob -d class=pi -d arg=16 -d arg=100 -d jar="hadoop-mapreduce-examples.jar" https://xxx/templeton/v1/mapreduce/jarThe error message is:"Usage: org.apache.hadoop.examples.QuasiMonteCarlo &lt;nMaps&gt; &lt;nSamples&gt; Generic options supported are -conf &lt;configuration file&gt; specify an application configuration file -D &lt;property=value&gt; use value for given property -fs &lt;local|namenode:port&gt; specify a namenode -jt &lt;local|jobtracker:port&gt; specify a job tracker -files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster -libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath. -archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.The general command line syntax is bin/hadoop command &amp;#91;genericOptions&amp;#93; &amp;#91;commandOptions&amp;#93;templeton: job failed with exit code 2"Note that if we remove the "define" parameter it works fine.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug id="5976" opendate="2013-12-6 00:00:00" fixdate="2013-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decouple input formats from STORED as keywords</summary>
      <description>As noted in HIVE-5783, we hard code the input formats mapped to keywords. It'd be nice if there was a registration system so we didn't need to do that.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.duplicate.key.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.union.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.genericFileFormat.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.bad.class.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5978" opendate="2013-12-6 00:00:00" fixdate="2013-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rollups not supported in vector mode.</summary>
      <description>Rollups are not supported in vector mode, the query should fail to vectorize. A separate jira will be filed to implement rollups in vector mode.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5982" opendate="2013-12-7 00:00:00" fixdate="2013-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant filesystem operations and methods in FileSink</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5984" opendate="2013-12-9 00:00:00" fixdate="2013-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi insert statement fails on Tez</summary>
      <description>Here's an example statement that doesn't work:rom (SELECT name FROM studenttab10k WHERE name != 'foo') a join studenttab10k on (a.name = studenttab10k.name)insert overwrite table mi1select a.name, a.namewhere a.name &lt;&gt; 'foo'insert overwrite table mi2select a.name, a.namewhere a.name &lt;&gt; 'bar'</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.java</file>
    </fixedFiles>
  </bug>
  <bug id="5985" opendate="2013-12-9 00:00:00" fixdate="2013-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make qfile_regex to accept multiple patterns</summary>
      <description>It's not easy to specify multiple tests by single pattern string.mvn test -Dtest=TestCliDriver -Dqfile_regex=stats_no.*,alter.* -Dtest.output.overwrite=true -Phadoop-1</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="5993" opendate="2013-12-10 00:00:00" fixdate="2013-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC Driver should not hard-code the database name</summary>
      <description>Method HiveDatabaseMetadata.getDatabaseProductName() returns a hard-coded string "hive".This should instead call the existing Hive-server2 api to return the db name.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="5998" opendate="2013-12-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add vectorized reader for Parquet files</summary>
      <description>HIVE-5783 is adding native Parquet support in Hive. As Parquet is a columnar format, it makes sense to provide a vectorized reader, similar to how RC and ORC formats have, to benefit from vectorized execution engine.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6000" opendate="2013-12-10 00:00:00" fixdate="2013-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive build broken on hadoop2</summary>
      <description>When I build on hadoop2 since yesterday, I get[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-it-unit: Compilation failure: Compilation failure:[ERROR] /Users/sergey/git/hive/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java:[28,41] package org.apache.hadoop.hbase.zookeeper does not exist[ERROR] /Users/sergey/git/hive/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java:[40,11] cannot find symbol[ERROR] symbol : class MiniZooKeeperCluster[ERROR] location: class org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore[ERROR] /Users/sergey/git/hive/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java:[53,26] cannot find symbol[ERROR] symbol : class MiniZooKeeperCluster</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6001" opendate="2013-12-10 00:00:00" fixdate="2013-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez: UDFs are not properly localized</summary>
      <description>We try to pick up the udf jars from the hive conf variables, but we never transfer them there.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6004" opendate="2013-12-11 00:00:00" fixdate="2013-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix statistics annotation related test failures in hadoop2</summary>
      <description>Fix test failures that are related to HIVE-5369 and its subtask changes.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6022" opendate="2013-12-12 00:00:00" fixdate="2013-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load statements with incorrect order of partitions put input files to unreadable places</summary>
      <description>Load statements with incorrect order of partitions put input files to incorrect paths. CREATE TABLE test_parts (c1 string, c2 int) PARTITIONED BY (p1 string,p2 string);LOAD DATA LOCAL INPATH '/opt/hive/examples/files/kv1.txt' OVERWRITE INTO TABLE test_parts PARTITION (p2='p1', p1='p2')"The input file is located as below and the data is not readable.% find /user/hive/warehouse/test_parts//user/hive/warehouse/test_parts//user/hive/warehouse/test_parts//p1=p2/user/hive/warehouse/test_parts//p1=p2/p2=p1/user/hive/warehouse/test_parts//p2=p1/user/hive/warehouse/test_parts//p2=p1/p1=p2/user/hive/warehouse/test_parts//p2=p1/p1=p2/.kv1.txt.crc/user/hive/warehouse/test_parts//p2=p1/p1=p2/kv1.txt</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6024" opendate="2013-12-12 00:00:00" fixdate="2013-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data local inpath unnecessarily creates a copy task</summary>
      <description>Load data command creates an additional copy task only when its loading from local It doesn't create this additional copy task while loading from DFS though.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6035" opendate="2013-12-13 00:00:00" fixdate="2013-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windows: percentComplete returned by job status from WebHCat is null</summary>
      <description>HIVE-5511 fixed the same problem on Linux, but it still broke on Windows.</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
      <file type="M">hcatalog.bin.templeton.cmd</file>
    </fixedFiles>
  </bug>
  <bug id="6036" opendate="2013-12-14 00:00:00" fixdate="2013-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A test case for embedded beeline - with URL jdbc:hive2:///default</summary>
      <description>A test case for embedded beeline would have been helpful. ie, with URL jdbc:hive2:///defaultThis causes beeline (JDBC driver) to invoken embedded hive.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.src.test.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="6038" opendate="2013-12-16 00:00:00" fixdate="2013-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Tez branch to properly compile against hadoop-1 profile</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mrr.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6039" opendate="2013-12-16 00:00:00" fixdate="2013-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Round, AVG and SUM functions reject char/varch input while accepting string input</summary>
      <description>An error similar to the following will occur:hive&gt; create table tabs (c char(8), vc varchar(10)) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';hive&gt; select sum(c), avg(c), sum(vc), avg(vc) from tabs;FAILED: UDFArgumentTypeException Only numeric or string type arguments are accepted but char(8) is passed.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
    </fixedFiles>
  </bug>
  <bug id="6064" opendate="2013-12-19 00:00:00" fixdate="2013-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wincompat: windows path substitutions overridden by MiniMrShim.getConfiguration() on hadoop-2</summary>
      <description>On Windows, HiveConf setting hive.exec.scratchdir is changed to remove the drive letter (i.e. "c:") from the start of its path. However, in HadoopShims23, MiniMrShim.setupConfiguration() subsequently overwrites the HiveConf settings and the drive letter is added back to hive.exec.scratchdir, causing path issues in the MiniMR tests.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6066" opendate="2013-12-19 00:00:00" fixdate="2013-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wincompat: describe_comment_nonascii.q failing on windows</summary>
      <description>describe_comment_nonascii.q failing on Windows. Some strings are not being properly converted to utf-8 bytes, both during formatting and when diffing the results.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6095" opendate="2013-12-22 00:00:00" fixdate="2013-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently II</summary>
      <description>This is follow-up of HIVE-3616.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6097" opendate="2013-12-23 00:00:00" fixdate="2013-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sessions on Tez NPE when quitting CLI</summary>
      <description>Reported in HIVE-5148. Code doesn't check whether session exists before attempting to close.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="61" opendate="2008-11-13 00:00:00" fixdate="2008-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implment ORDER BY</summary>
      <description>ORDER BY is in the query language reference but currently is a no-op. We should make it an op.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6106" opendate="2013-12-25 00:00:00" fixdate="2013-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update golden files for tez</summary>
      <description>there's two cases where we need update: the stage id is one off in one file. this isn't a functional problem. in vectorization we codified a wrong result in a couple golden files</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6109" opendate="2013-12-26 00:00:00" fixdate="2013-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support customized location for EXTERNAL tables created by Dynamic Partitioning</summary>
      <description>Currently when dynamic partitions are created by HCatalog, the underlying directories for the partitions are created in a fixed 'Hive-style' format, i.e. root_dir/key1=value1/key2=value2/.... and so on. However in case of external table, user should be able to control the format of directories created for dynamic partitions.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.OutputJobInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6112" opendate="2013-12-27 00:00:00" fixdate="2013-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - support new privileges INSERT, DELETE</summary>
      <description>Includes INSERT, DELETE privileges.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="6116" opendate="2013-12-27 00:00:00" fixdate="2013-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Paths consistently III</summary>
      <description>Another one in patch series to make use of Paths consistently.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.LineageState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="6118" opendate="2013-12-27 00:00:00" fixdate="2013-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 is banned periodically by AWS because JClouds is too agressive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.context.TestCloudComputeService.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudComputeService.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6138" opendate="2014-1-3 00:00:00" fixdate="2014-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez: Add some additional comments to clarify intent</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="6147" opendate="2014-1-6 00:00:00" fixdate="2014-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support avro data stored in HBase columns</summary>
      <description>Presently, the HBase Hive integration supports querying only primitive data types in columns. It would be nice to be able to store and query Avro objects in HBase columns by making them visible as structs to Hive. This will allow Hive to perform ad hoc analysis of HBase data which can be deeply structured.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUnion.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.java</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseCellMap.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseRowSerializer.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ColumnMappings.java</file>
      <file type="M">hbase-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6148" opendate="2014-1-6 00:00:00" fixdate="2014-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support arbitrary structs stored in HBase</summary>
      <description>We should add support to be able to query arbitrary structs stored in HBase.</description>
      <version>0.12.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.DefaultHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.DefaultHBaseKeyFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6159" opendate="2014-1-7 00:00:00" fixdate="2014-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive uses deprecated hadoop configuration in Hadoop 2.0</summary>
      <description>Build hive against hadoop 2.0. Then run hive CLI, you'll see deprecated configurations warnings like this:13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.max.split.size is depre cated. Instead, use mapreduce.input.fileinputformat.split.maxsize 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size is depre cated. Instead, use mapreduce.input.fileinputformat.split.minsize 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.r ack 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.n ode 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks is depreca ted. Instead, use mapreduce.job.reduces 13/12/14 01:00:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculativ e.execution is deprecated. Instead, use mapreduce.reduce.speculative</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.test.results.clientpositive.overridden.confs.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="616" opendate="2009-7-8 00:00:00" fixdate="2009-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make hive work with symbolic linked sub directories</summary>
      <description>Currently, Hive does not work if bin, lib, conf, etc are all symbolic links.Specifically, we are using "bin/.." for HIVE_HOME but we can easily get rid of ".." to make it work with symbolic links.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive-config.sh</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinObjectValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6167" opendate="2014-1-8 00:00:00" fixdate="2014-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow user-defined functions to be qualified with database name</summary>
      <description>Function names in Hive are currently unqualified and there is a single namespace for all function names. This task would allow users to define temporary UDFs (and eventually permanent UDFs) with a database name, such as:CREATE TEMPORARY FUNCTION userdb.myfunc 'myudfclass';</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="6174" opendate="2014-1-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline "set varible" doesn&amp;#39;t show the value of the variable as Hive CLI</summary>
      <description>Currently it displays nothing.0: jdbc:hive2://&gt; set env:TERM; 0: jdbc:hive2://&gt; In contrast, Hive CLI displays the value of the variable.hive&gt; set env:TERM; env:TERM=xterm</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="6177" opendate="2014-1-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix keyword KW_REANME which was intended to be KW_RENAME</summary>
      <description>http://stackoverflow.com/questions/20952865/very-strange-keyword-reanme-in-apache-hive</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="6178" opendate="2014-1-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized reader for DECIMAL datatype for ORC format.</summary>
      <description>Implement vectorized reader for DECIMAL datatype for ORC format.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestUnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.UnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug id="62" opendate="2008-11-13 00:00:00" fixdate="2008-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to describe nested types</summary>
      <description>Add ability to describe nested types.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6200" opendate="2014-1-15 00:00:00" fixdate="2014-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive custom SerDe cannot load DLL added by "ADD FILE" command</summary>
      <description>When custom SerDe need to load a DLL file added using "ADD FILE" command in HIVE, the loading fail with exception like "java.lang.UnsatisfiedLinkError:C:\tmp\admin2_6996@headnode0_201401100431_resources\hello.dll: Access is denied". The reason is when FileSystem creating local copy of the file, the permission of local file is set to default as "666". DLL file need "execute" permission to be loaded successfully.Similar scenario also happens when Hadoop localize files in distributed cache. The solution in Hadoop is to add "execute" permission to the file after localizationl.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="6204" opendate="2014-1-15 00:00:00" fixdate="2014-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The result of show grant / show role should be tabular format</summary>
      <description>hive&gt; show grant role role1 on all;OKdatabase defaulttable srcprincipalName role1principalType ROLEprivilege CreategrantTime Wed Dec 18 14:17:56 KST 2013grantor navisdatabase defaulttable srcpartprincipalName role1principalType ROLEprivilege UpdategrantTime Wed Dec 18 14:18:28 KST 2013grantor navisThis should be something like below, especially for JDBC clients.hive&gt; show grant role role1 on all;OKdefault src role1 ROLE Create false 1387343876000 navisdefault srcpart role1 ROLE Update false 1387343908000 navis</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.keyword.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.sql.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.authorization.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowGrantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RoleDDLDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Role.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6209" opendate="2014-1-16 00:00:00" fixdate="2014-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;LOAD DATA INPATH ... OVERWRITE ..&amp;#39; doesn&amp;#39;t overwrite current data</summary>
      <description>In case where user loads data into table using overwrite, using a different file, it is not being overwritten.$ hdfs dfs -cat /tmp/dataaaabbbccc$ hdfs dfs -cat /tmp/data2dddeeefff$ hivehive&gt; create table test (id string); hive&gt; load data inpath '/tmp/data' overwrite into table test;hive&gt; select * from test;aaabbbccchive&gt; load data inpath '/tmp/data2' overwrite into table test;hive&gt; select * from test;aaabbbcccdddeeefffIt seems it is broken by HIVE-3756 which added another condition to whether "rmr" should be run on old directory, and skips in this case.There is a workaround of set fs.hdfs.impl.disable.cache=true; which sabotages this condition, but this condition should be removed in long-term.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="6217" opendate="2014-1-16 00:00:00" fixdate="2014-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Beeline&amp;#39;s JDBC connection to use property map instead of long url</summary>
      <description>To make a connection to HiverServer2, currently Beeline constructs a long url that contains hiveconf and hivevariables. At JDBC driver side, it parses the url into individual maps. Constructing and decomposing the url is complicated and unnecessary, using a different method from JDBC DriverManager that takes a property map makes the logic simpler, especially when hiveconf and hivevar variables can also come from Beeline command line in addition to those in connection url.This is identified in and required by HIVE-6173.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="6245" opendate="2014-1-21 00:00:00" fixdate="2014-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 creates DBs/Tables with wrong ownership when HMS setugi is true</summary>
      <description>The case with following settings is valid but does not work correctly in current HS2:==hive.server2.authentication=NONE (or LDAP)hive.server2.enable.doAs= truehive.metastore.sasl.enabled=falsehive.metastore.execute.setugi=true==Ideally, HS2 is able to impersonate the logged in user (from Beeline, or JDBC application) and create DBs/Tables with user's ownership.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="6338" opendate="2014-1-30 00:00:00" fixdate="2014-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception handling in createDefaultDb() in Metastore</summary>
      <description>There is a suggestion on HIVE-5959 comment list on possible improvements.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="6342" opendate="2014-1-30 00:00:00" fixdate="2014-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive drop partitions should use standard expr filter instead of some custom class</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.filter.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.drop.partition.filter.failure2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6345" opendate="2014-1-31 00:00:00" fixdate="2014-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DECIMAL support to vectorized JOIN operators</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.UnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6353" opendate="2014-2-3 00:00:00" fixdate="2014-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hadoop-2 golden files after HIVE-6267</summary>
      <description>HIVE-6267 changed explain with lots of changes to golden files. Separate jira because of number of files changed.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6354" opendate="2014-2-3 00:00:00" fixdate="2014-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some index test golden files produce non-deterministic stats in explain</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.bitmap.auto.q</file>
      <file type="M">ql.src.test.queries.clientpositive.index.bitmap3.q</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6356" opendate="2014-2-3 00:00:00" fixdate="2014-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency injection in hbase storage handler is broken</summary>
      <description>Dependent jars for hbase is not added to tmpjars, which is caused by the change of method signature(TableMapReduceUtil.addDependencyJars).</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6358" opendate="2014-2-3 00:00:00" fixdate="2014-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>filterExpr not printed in explain for tablescan operators (ppd)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6359" opendate="2014-2-3 00:00:00" fixdate="2014-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline -f fails on scripts with tabs in them.</summary>
      <description>NO PRECOMMIT TESTSOn a recent trunk build I used beeline -f on a script with tabs in it.Beeline rather unhelpfully attempts to perform tab expansion on the tabs and the query fails. Here's a screendump.Connecting to jdbc:hive2://mymachine:10000/mydbConnected to: Apache Hive (version 0.13.0-SNAPSHOT)Driver: Hive JDBC (version 0.13.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 0.13.0-SNAPSHOT by Apache Hive0: jdbc:hive2://mymachine:10000/mydb&gt; select i_brand_id as brand_id, i_brand as brand,. . . . . . . . . . . . . . . . . . . . . . .&gt; Display all 560 possibilities? (y or n) . . . . . . . . . . . . . . . . . . . . . . .&gt; ager_id=36. . . . . . . . . . . . . . . . . . . . . . .&gt; Display all 560 possibilities? (y or n) . . . . . . . . . . . . . . . . . . . . . . .&gt; d d_moy=12. . . . . . . . . . . . . . . . . . . . . . .&gt; Display all 560 possibilities? (y or n) . . . . . . . . . . . . . . . . . . . . . . .&gt; d d_year=2001. . . . . . . . . . . . . . . . . . . . . . .&gt; and ss_sold_date between '2001-12-01' and '2001-12-31'. . . . . . . . . . . . . . . . . . . . . . .&gt; group by i_brand, i_brand_id. . . . . . . . . . . . . . . . . . . . . . .&gt; order by ext_price desc, brand_id. . . . . . . . . . . . . . . . . . . . . . .&gt; limit 100 ;Error: Error while compiling statement: FAILED: ParseException line 1:65 missing FROM at 'd_moy' near 'd' in from source (state=42000,code=40000)Closing: org.apache.hive.jdbc.HiveConnectionThe same query works fine if I replace tabs with some spaces.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="636" opendate="2009-7-14 00:00:00" fixdate="2009-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>concatenation task does not work if the destination table is partitioned</summary>
      <description>The temporary table that it creates assumes the schema of the final table, which is not correct.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.tableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6372" opendate="2014-2-5 00:00:00" fixdate="2014-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getDatabaseMajor/Minor version returns wrong values</summary>
      <description>Currently getDatabaseMajorVersion returns 13, and getDatabaseMinorVersion returns 0. The index is off by one.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="6385" opendate="2014-2-6 00:00:00" fixdate="2014-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF degrees() doesn&amp;#39;t take decimal as input</summary>
      <description>HIVE-6246 and HIVE-6327 added decimal support in most of the mathematical UDFs, including radians(). However, such support is still missing for UDF degrees(). This fills the gap.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDegrees.java</file>
    </fixedFiles>
  </bug>
  <bug id="6389" opendate="2014-2-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.</summary>
      <description>RCFile tables that use the LazyBinaryColumnarSerDe don't seem to handle look-ups into map-columns when the value of the column is null.When an RCFile table is created with LazyBinaryColumnarSerDe (as is default in 0.12), and queried as follows:select mymap['1024'] from mytable;and if the mymap column has nulls, then one is treated to the following guttural utterance:2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":null,"mymap":null,"isnull":null} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41) at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524) ... 10 moreA patch is on the way, but the short of it is that the LazyBinaryMapOI needs to return nulls if either the map or the lookup-key is null.This is handled correctly for Text data, and for RCFiles using ColumnarSerDe.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="6392" opendate="2014-2-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive (and HCatalog) don&amp;#39;t allow super-users to add partitions to tables.</summary>
      <description>HDFS allows for users to be added to a "supergroup" (identified by the "dfs.permissions.superusergroup" key in hdfs-site.xml). Users in this group are allowed to modify HDFS contents regardless of the path's ogw permissions.However, Hive's StorageBasedAuthProvider disallows such a superuser from adding partitions to any table that doesn't explicitly grant write permissions to said superuser. This causes the odd scenario where the superuser writes data to a partition-directory (under the table's path), but can't register the appropriate partition.I have a patch that brings the Metastore's behaviour in line with what the HDFS allows.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.security.HdfsAuthorizationProvider.java</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6393" opendate="2014-2-8 00:00:00" fixdate="2014-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support unqualified column references in Joining conditions</summary>
      <description>Support queries of the form:create table r1(a int);create table r2(b);select a, bfrom r1 join r2 on a = bThis becomes more useful in old style syntax:select a, bfrom r1, r2where a = b</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6469" opendate="2014-2-20 00:00:00" fixdate="2014-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>skipTrash option in hive command line</summary>
      <description>Th current behavior of hive metastore during a "drop table &lt;table_name&gt;" command is to delete the data from HDFS warehouse and put it into Trash.Currently there is no way to provide a flag to tell the warehouse to skip trash while deleting table data.This ticket is to add skipTrash configuration "hive.warehouse.data.skipTrash" , which when set to true, will skipTrash while dropping table data from hdfs warehouse. This will be set to false by default to keep current behavior.This would be good feature to add, so that an admin of the cluster can specify when not to put data into the trash directory (eg. in a dev environment) and thus not to fill hdfs space instead of relying on trash interval and policy configuration to take care of disk filling issue.</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6486" opendate="2014-2-22 00:00:00" fixdate="2014-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support secure Subject.doAs() in HiveServer2 JDBC client.</summary>
      <description>HIVE-5155 addresses the problem of kerberos authentication in multi-user middleware server using proxy user. In this mode the principal used by the middle ware server has privileges to impersonate selected users in Hive/Hadoop. This enhancement is to support Subject.doAs() authentication in Hive JDBC layer so that the end users Kerberos Subject is passed through in the middle ware server. With this improvement there won't be any additional setup in the server to grant proxy privileges to some users and there won't be need to specify a proxy user in the JDBC client. This version should also be more secure since it won't require principals with the privileges to impersonate other users in Hive/Hadoop setup.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.KerberosSaslHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="6492" opendate="2014-2-24 00:00:00" fixdate="2014-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>limit partition number involved in a table scan</summary>
      <description>To protect the cluster, a new configure variable "hive.limit.query.max.table.partition" is added to hive configuration tolimit the table partitions involved in a table scan. The default value will be set to -1 which means there is no limit by default. This variable will not affect "metadata only" query.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6533" opendate="2014-3-2 00:00:00" fixdate="2014-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to released tez 0.3</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6534" opendate="2014-3-2 00:00:00" fixdate="2014-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Keep guava on v11 in tez branch</summary>
      <description>Needed to upgrade guava for tez - but the 0.3 release rolled that back.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6545" opendate="2014-3-4 00:00:00" fixdate="2014-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>analyze table throws NPE for non-existent tables.</summary>
      <description>Instead of NPE, we should give error message to user.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6546" opendate="2014-3-4 00:00:00" fixdate="2014-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat job submission for pig with -useHCatalog argument fails on Windows</summary>
      <description>On a one-box windows setup, do the following from a powershell prompt:cmd /c curl.exe -s ` -d user.name=hadoop ` -d arg=-useHCatalog ` -d execute="emp = load '/data/emp/emp_0.dat'; dump emp;" ` -d statusdir="/tmp/webhcat.output01" ` 'http://localhost:50111/templeton/v1/pig' -vThe job fails with error code 7, but it should run. I traced this down to the following. In the job configuration for the TempletonJobController, we have templeton.args set tocmd,/c,call,C:\\hadoop\\\\pig-0.11.0.1.3.0.0-0846/bin/pig.cmd,-D_WEBHCAT_TOKEN_FILE_LOCATION_="-useHCatalog",-execute,"emp = load '/data/emp/emp_0.dat'; dump emp;"Notice the = sign before "-useHCatalog". I think this should be a comma.The bad string D_WEBHCAT_TOKEN_FILE_LOCATION_="-useHCatalog" gets created in org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows().It happens at line 434: } else { if (i &lt; args.length - 1) { prop += "=" + args[++i]; // RIGHT HERE! at iterations i = 37, 38 } }Bug is here: if (prop != null) { if (prop.contains("=")) { // -D__WEBHCAT_TOKEN_FILE_LOCATION__ does not contain equal, so else branch is run and appends ="-useHCatalog", // everything good } else { if (i &lt; args.length - 1) { prop += "=" + args[++i]; } } newArgs.add(prop); }One possible fix is to change the string constant org.apache.hcatalog.templeton.tool.TempletonControllerJob.TOKEN_FILE_ARG_PLACEHOLDER to have an "=" sign in it. Or, preProcessForWindows() itself could be changed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6551" opendate="2014-3-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>group by after join with skew join optimization references invalid task sometimes</summary>
      <description>For example,hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.skewjoin = true;hive&gt; set hive.skewjoin.key = 3;hive&gt; &gt; EXPLAIN FROM &gt; (SELECT src.* FROM src) x &gt; JOIN &gt; (SELECT src.* FROM src) Y &gt; ON (x.key = Y.key) &gt; SELECT sum(hash(Y.key)), sum(hash(Y.value));OKSTAGE DEPENDENCIES: Stage-8 is a root stage Stage-6 depends on stages: Stage-8 Stage-5 depends on stages: Stage-6 , consists of Stage-4, Stage-2 Stage-4 Stage-2 depends on stages: Stage-4, Stage-1 Stage-0 is a root stage...Stage-2 references not-existing Stage-1</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6555" opendate="2014-3-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSchemaTool is failing on trunk after branching</summary>
      <description>This is because version was bumped to 0.14 in pom file and there are no metastore scripts for 0.14 yet.</description>
      <version>None</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
    </fixedFiles>
  </bug>
  <bug id="6561" opendate="2014-3-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should accept -i option to Initializing a SQL file</summary>
      <description>Hive CLI has -i option. From Hive CLI help:... -i &lt;filename&gt; Initialization SQL file...However, Beeline has no such option:xzhang@xzlt:~/apa/hive3$ ./packaging/target/apache-hive-0.14.0-SNAPSHOT-bin/apache-hive-0.14.0-SNAPSHOT-bin/bin/beeline -u jdbc:hive2:// -i hive.rc...Connected to: Apache Hive (version 0.14.0-SNAPSHOT)Driver: Hive JDBC (version 0.14.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ-i (No such file or directory)Property "url" is requiredBeeline version 0.14.0-SNAPSHOT by Apache Hive...</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="6573" opendate="2014-3-6 00:00:00" fixdate="2014-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Oracle metastore doesnt come up when hive.cluster.delegation.token.store.class is set to DBTokenStore</summary>
      <description>This config hive.cluster.delegation.token.store.class was introduced in HIVE-3255 and is useful only if oracle metastore is used in secure setup with HA config.</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
    </fixedFiles>
  </bug>
  <bug id="6585" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucket map join fails in presence of _SUCCESS file</summary>
      <description>Reason is missing path filters.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6591" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Importing a table containing hidden dirs fails</summary>
      <description>hidden files should be ignored while exporting</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6609" opendate="2014-3-10 00:00:00" fixdate="2014-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doing Ctrl-C on hive cli doesn&amp;#39;t kill running MR jobs on hadoop-2</summary>
      <description>This is because url based job killing which we use doesn't work on hadoop2. We need to use java api.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6620" opendate="2014-3-11 00:00:00" fixdate="2014-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF printf doesn&amp;#39;t take either CHAR or VARCHAR as the first argument</summary>
      <description>hive&gt; desc vc;OKc char(5) None vc varchar(7) None s string None hive&gt; select printf(c) from vc;FAILED: SemanticException [Error 10016]: Line 1:14 Argument type mismatch 'c': Argument 1 of function PRINTF must be "string", but "char(5)" was found.However, if the argument is string type, the query runs successfully.</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6623" opendate="2014-3-11 00:00:00" fixdate="2014-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "owner" tag to ptest2 created instances</summary>
      <description>We have a new requirement to have an "owner" tag on instances. We need to change ptest2 to support this.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudComputeService.java</file>
    </fixedFiles>
  </bug>
  <bug id="6633" opendate="2014-3-12 00:00:00" fixdate="2014-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pig -useHCatalog with embedded metastore fails to pass command line args to metastore</summary>
      <description>This fails because the embedded metastore can't connect to the database because the command line -D arguments passed to pig are not getting passed to the metastore when the embedded metastore is created. Using hive.metastore.uris set to the empty string causes creation of an embedded metastore.pig -useHCatalog "-Dhive.metastore.uris=" "-Djavax.jdo.option.ConnectionPassword=AzureSQLDBXYZ"The goal is to allow a pig job submitted via WebHCat to specify a metastore to use via job arguments. That is not working because it is not possible to pass Djavax.jdo.option.ConnectionPassword and other necessary arguments to the embedded metastore.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hcatalog.pig.PigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hcatalog.pig.HCatLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="6644" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>document TestStreaming_2 e2e test case for webhcat</summary>
      <description></description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.streaming.conf</file>
    </fixedFiles>
  </bug>
  <bug id="6648" opendate="2014-3-13 00:00:00" fixdate="2014-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permissions are not inherited correctly when tables have multiple partition columns</summary>
      <description>Warehouse.mkdirs() always looks at the immediate parent of the path that it creates when determining what permissions to inherit. However, it may have created that parent directory as well, in which case it will have the default permissions and will not have inherited them.This is a problem when performing an INSERT into a table with more than one partition column. E.g., in an empty table:INSERT INTO TABLE tbl PARTITION(p1=1, p2=2) ... A new subdirectory /p1=1/p2=2 will be created, and with permission inheritance (per HIVE-2504) enabled, the intention is presumably for both new directories to inherit the root table dir's permissions. However, mkdirs() will only set the permission of the leaf directory (i.e. /p2=2/), and then only to the permissions of /p1=1/, which was just created.public boolean mkdirs(Path f) throws MetaException { FileSystem fs = null; try { fs = getFs(f); LOG.debug("Creating directory if it doesn't exist: " + f); //Check if the directory already exists. We want to change the permission //to that of the parent directory only for newly created directories. if (this.inheritPerms) { try { return fs.getFileStatus(f).isDir(); } catch (FileNotFoundException ignore) { } } boolean success = fs.mkdirs(f); if (this.inheritPerms &amp;&amp; success) { // Set the permission of parent directory. // HNR: This is the bug - getParent() may refer to a just-created directory. fs.setPermission(f, fs.getFileStatus(f.getParent()).getPermission()); } return success; } catch (IOException e) { closeFs(fs); MetaStoreUtils.logAndThrowMetaException(e); } return false; }</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="665" opendate="2009-7-21 00:00:00" fixdate="2009-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to query hadoop/mapreduce cluster status from hive server</summary>
      <description>Tools/infra around hadoop/hive need to check cluster status in many cases.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">service.src.gen-py.hive.service.ttypes.py</file>
      <file type="M">service.src.gen-py.hive.service.ThriftHive.py</file>
      <file type="M">service.src.gen-py.hive.service.ThriftHive-remote</file>
      <file type="M">service.src.gen-php.ThriftHive.php</file>
      <file type="M">service.src.gen-php.hive.service.types.php</file>
      <file type="M">service.src.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">service.if.hive.service.thrift</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6658" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify Alter_numbuckets* test to reflect hadoop2 changes</summary>
      <description>Hadoop2 now honors number of reducers config while running in local mode. This affects bucketing tests as the data gets properly bucketed in Hadoop2 (In hadoop1 all data ended up in same bucket while in local mode).</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.numbuckets.partitioned.table2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.numbuckets.partitioned.table.q</file>
    </fixedFiles>
  </bug>
  <bug id="6659" opendate="2014-3-13 00:00:00" fixdate="2014-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update log for list_bucket_* to add pre/post DB</summary>
      <description>On Hadoop2 we now print out Database Name using pre/post hooks.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6671" opendate="2014-3-14 00:00:00" fixdate="2014-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat Job Submission API &amp;#39;enablelog&amp;#39; parameter is only supported with Hadoop 1</summary>
      <description>We should throw a consistent exception if enablelog=true and WebHCat is talking to H2</description>
      <version>0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug id="6681" opendate="2014-3-16 00:00:00" fixdate="2014-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table sometimes shows "from deserializer" for column comments</summary>
      <description></description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.context.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.varchar.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.varchar.nested.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.counter.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.counter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.tblproperty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.date2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.vectorization.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatdir.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lb.fs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.import.exported.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.ignore.protection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.pretty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.custom.input.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.nested.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.colserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.bincolserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats2.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats.empty.partition.q.out</file>
      <file type="M">itests.test-serde.src.main.java.org.apache.hadoop.hive.serde2.TestSerDe.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.2columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidcolname.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidtype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.partialscan.autogether.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6689" opendate="2014-3-17 00:00:00" fixdate="2014-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to not display partition columns separately in describe table output</summary>
      <description>In ancient Hive partition columns were not displayed differently, in newer version they are displayed differently. This has resulted in backward incompatible change for upgrade scenarios.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6694" opendate="2014-3-18 00:00:00" fixdate="2014-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should provide a way to execute shell command as Hive CLI does</summary>
      <description>Hive CLI allows a user to execute a shell command using ! notation. For instance, !cat myfile.txt. Being able to execute shell command may be important for some users. As a replacement, however, Beeline provides no such capability, possibly because ! notation is reserved for SQLLine commands. It's possible to provide this using a slightly syntactic variation such as !sh cat myfilie.txt.</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="6697" opendate="2014-3-19 00:00:00" fixdate="2014-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 secure thrift/http authentication needs to support SPNego</summary>
      <description>Looking to integrating Apache Knox to work with HiveServer2 secure thrift/http.Found that thrift/http uses some form of Kerberos authentication that is not SPNego. Considering it is going over http protocol, expected it to use SPNego protocol.Apache Knox is already integrated with WebHDFS, WebHCat, Oozie and HBase Stargate using SPNego for authentication.Requesting that HiveServer2 secure thrift/http authentication support SPNego.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6711" opendate="2014-3-20 00:00:00" fixdate="2014-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC maps uses getMapSize() from MapOI which is unreliable</summary>
      <description>HIVE-6707 had issues with map size. getMapSize() of LazyMap and LazyBinaryMap does not deserialize the keys and count the number of unique keys. Since getMapSize() may return non-distinct count of keys, the length of maps stored using ORC's map tree writer will not be in sync with actual map size. As a result of this RLE reader will try to read beyond the disk range expecting more map entries and will throw exception.Stack trace will look like:Caused by: java.io.EOFException: Read past end of RLE integer from compressed stream Stream for column 2 kind DATA position: 22059699 length: 22059699 range: 0 offset: 22359014 limit: 22359014 range 0 = 0 to 22059699 uncompressed: 53370 to 53370 at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54) at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:301) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringDictionaryTreeReader.next(RecordReaderImpl.java:1572) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringTreeReader.next(RecordReaderImpl.java:1330) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$MapTreeReader.next(RecordReaderImpl.java:2041) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1772) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2963) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:121)</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6806" opendate="2014-4-1 00:00:00" fixdate="2014-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE should support STORED AS AVRO</summary>
      <description>Avro is well established and widely used within Hive, however creating Avro-backed tables requires the messy listing of the SerDe, InputFormat and OutputFormat classes.Similarly to HIVE-5783 for Parquet, Hive would be easier to use if it had native Avro support.</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.java</file>
      <file type="M">ql.src.main.resources.META-INF.services.org.apache.hadoop.hive.ql.io.StorageFormatDescriptor</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.IOConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6811" opendate="2014-4-2 00:00:00" fixdate="2014-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LOAD command does not work with relative paths on Windows</summary>
      <description>qfile tests on Windows fail when trying to load data, with URISyntaxException: Relative path in absolute URI</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6822" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestAvroSerdeUtils fails with -Phadoop-2</summary>
      <description>Works fine with -Phadoop-1, but with -Phadoop-2 hits the following error:Running org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtilsTests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.603 sec &lt;&lt;&lt; FAILURE! - in org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtilsdetermineSchemaCanReadSchemaFromHDFS(org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils) Time elapsed: 0.688 sec &lt;&lt;&lt; ERROR!java.lang.NoClassDefFoundError: com/sun/jersey/spi/container/servlet/ServletContainer at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) at org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:564) at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.initWebHdfs(NameNodeHttpServer.java:84) at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:121) at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:601) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:500) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:658) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:643) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1259) at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:914) at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:805) at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:663) at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:603) at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:474) at org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.determineSchemaCanReadSchemaFromHDFS(TestAvroSerdeUtils.java:189)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6843" opendate="2014-4-4 00:00:00" fixdate="2014-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSTR for UTF-8 returns incorrect position</summary>
      <description></description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6903" opendate="2014-4-14 00:00:00" fixdate="2014-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of hive.metastore.execute.setugi to true</summary>
      <description>Since its introduction in HIVE-2616 I havent seen any bug reported for it, only grief from users who expect system to work as if this is true by default.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6931" opendate="2014-4-18 00:00:00" fixdate="2014-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windows unit test fixes</summary>
      <description>A few misc fixes for some of the unit tests on Windows.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.win.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.insert.overwrite.local.directory.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.scriptfile1.win.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.WindowsPathUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="6936" opendate="2014-4-18 00:00:00" fixdate="2014-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide table properties to InputFormats</summary>
      <description>Some advanced file formats need the table properties made available to them. Additionally, it would be convenient to provide a unique id for fetch operators and the complete list of directories.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="6937" opendate="2014-4-20 00:00:00" fixdate="2014-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test reporting url&amp;#39;s after jenkins move from bigtop</summary>
      <description>This move co-located HivePtest webserver and Jenkins server. Due to the conflicts, I had to remap some URL's, thus breaking the URL of getting logs and test-reports.The Hive Ptest2 framework makes some assumption about the relative location of logs and REST endpoint URL's, that are no longer true, namely that they are located at endpoint:/logs and endpoint:/hive-ptest/api. This needs to be fixed. Now, the logs are at host/logs, and HivePtest webserver REST endpoints are at: endpoint/hive-ptest/api.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.PTestClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="6976" opendate="2014-4-25 00:00:00" fixdate="2014-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show query id only when there&amp;#39;s jobs on the cluster</summary>
      <description>No need to print the query id for local-only execution.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6977" opendate="2014-4-25 00:00:00" fixdate="2014-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete Hiveserver1</summary>
      <description>See mailing list discussion.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServerSessions.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveInterface.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveClient.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcTable.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveCallableStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestOptionsProcessor.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliSessionState.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6978" opendate="2014-4-25 00:00:00" fixdate="2014-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline always exits with 0 status, should exit with non-zero status on error</summary>
      <description>Was supposed to be fixed in Hive 0.12 (HIVE-4364). Doesn't look fixed from here.&amp;#91;i@p sqoop&amp;#93;$ beeline -u 'jdbc:hive2://p:10000/k;principal=hive/p@L' -e "select * from MEMBERS" --outputformat=verticalscan complete in 3msConnecting to jdbc:hive2://p:10000/k;principal=hive/p@LSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in &amp;#91;jar:file:/opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;SLF4J: Found binding in &amp;#91;jar:file:/opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/lib/avro/avro-tools-1.7.5-cdh5.0.0.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type &amp;#91;org.slf4j.impl.Log4jLoggerFactory&amp;#93;Connected to: Apache Hive (version 0.12.0-cdh5.0.0)Driver: Hive JDBC (version 0.12.0-cdh5.0.0)Transaction isolation: TRANSACTION_REPEATABLE_READ-hiveconf (No such file or directory)hive.aux.jars.path=&amp;#91;redacted&amp;#93;Error: Error while compiling statement: FAILED: SemanticException &amp;#91;Error 10001&amp;#93;: Line 1:14 Table not found 'MEMBERS' (state=42S02,code=10001)Beeline version 0.12.0-cdh5.0.0 by Apache HiveClosing: org.apache.hive.jdbc.HiveConnection&amp;#91;inter@p sqoop&amp;#93;$ echo $?0</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ReflectiveCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.CommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractCommandHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="6991" opendate="2014-4-30 00:00:00" fixdate="2014-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>History not able to disable/enable after session started</summary>
      <description>By default history is disabled, after session started if enable history through this command set hive.session.history.enabled=true. It is not working.I think it will help to this user queryhttp://mail-archives.apache.org/mod_mbox/hive-user/201404.mbox/%3CCAJqy7aFAPa_pjS6bUon0o8zYT2qwfN2WT-mtZnwfmuRav_8ZjA@mail.gmail.com%3E</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="70" opendate="2008-11-18 00:00:00" fixdate="2008-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>provide option to limit standard error output by user scripts</summary>
      <description>runaway user scripts emitting every row to standard error overwhelm our log partitions. We need to provide an option to limit standard error size per task.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="700" opendate="2009-7-28 00:00:00" fixdate="2009-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test error by adding "DROP FUNCTION"</summary>
      <description>Since we added "Show Functions" in HIVE-580, test results will depend on what temporary functions are added to the system.We should add the capability of "DROP FUNCTION", and do that at the end of those "create function" tests to make sure the "show functions" results are deterministic.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.udaf.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.genericudf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FunctionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7000" opendate="2014-5-1 00:00:00" fixdate="2014-8-1 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Several issues with javadoc generation</summary>
      <description>1.Ran 'mvn javadoc:javadoc -Phadoop-2'. Encountered several issues Generated classes are included in the javadoc generation fails in the top level hcatalog folder because its src folder contains no java files.Patch attached to fix these issues.2.Tried mvn javadoc:aggregate -Phadoop-2 cannot get an aggregated javadoc for all of hive tried setting 'aggregate' parameter to true. Didn't workThere are several questions in StackOverflow about multiple project javadoc. Seems like this is broken.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7016" opendate="2014-5-6 00:00:00" fixdate="2014-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive returns wrong results when execute UDF on top of DISTINCT column</summary>
      <description>The following query returns wrong result:select hash(distinct value) from table;This kind of query should be identified as syntax error. However, Hive ignores DISTINCT and returns the result.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="7037" opendate="2014-5-8 00:00:00" fixdate="2014-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add additional tests for transform clauses with Tez</summary>
      <description>Enabling some q tests for Tez wrt to ScriptOperator/Stream/Transform.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7045" opendate="2014-5-12 00:00:00" fixdate="2014-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results in multi-table insert aggregating without group by clause</summary>
      <description>This happens whenever there are more than 1 reducers.The scenario :CREATE TABLE t1 (a int, b int);CREATE TABLE t2 (cnt int) PARTITIONED BY (var_name string);insert into table t1 select 1,1 from asd limit 1;insert into table t1 select 2,2 from asd limit 1;t1 contains :1 12 2from t1insert overwrite table t2 partition(var_name='a') select count(a) cnt insert overwrite table t2 partition(var_name='b') select count(b) cnt ;select * from t2;returns : 2 a2 bas expected.Setting the number of reducers higher than 1 :set mapred.reduce.tasks=2;from t1insert overwrite table t2 partition(var_name='a') select count(a) cntinsert overwrite table t2 partition(var_name='b') select count(b) cnt;select * from t2;1 a1 a1 b1 bWrong results.This happens when ever t1 is big enough to automatically generate more than 1 reducers and without specifying it directly.adding "group by 1" in the end of each insert solves the problem :from t1insert overwrite table t2 partition(var_name='a') select count(a) cnt group by 1insert overwrite table t2 partition(var_name='b') select count(b) cnt group by 1;generates : 2 a2 bThis should work without the group by...The number of rows for each partition will be the amount of reducers.Each reducer calculated a sub total of the count.</description>
      <version>0.10.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7075" opendate="2014-5-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JsonSerde raises NullPointerException when object key is not lower case</summary>
      <description>We have noticed that the JsonSerde produces a NullPointerException if a JSON object has a key value that is not lower case. For example. Assume we have the file "one.json": { "empId" : 123, "name" : "John" } { "empId" : 456, "name" : "Jane" } hive&gt; CREATE TABLE emps (empId INT, name STRING) ROW FORMAT SERDE "org.apache.hive.hcatalog.data.JsonSerDe"; hive&gt; LOAD DATA LOCAL INPATH 'one.json' INTO TABLE emps; hive&gt; SELECT * FROM emps; Failed with exception java.io.IOException:java.lang.NullPointerException -------- Notice, it seems to work if the keys are lower case. Assume we have the file 'two.json': { "empid" : 123, "name" : "John" } { "empid" : 456, "name" : "Jane" } hive&gt; DROP TABLE emps; hive&gt; CREATE TABLE emps (empId INT, name STRING) ROW FORMAT SERDE "org.apache.hive.hcatalog.data.JsonSerDe"; hive&gt; LOAD DATA LOCAL INPATH 'two.json' INTO TABLE emps;hive&gt; SELECT * FROM emps; OK 123 John 456 Jane</description>
      <version>0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatSchema.java</file>
    </fixedFiles>
  </bug>
  <bug id="7076" opendate="2014-5-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Plugin (exec hook) to log to application timeline data to Yarn</summary>
      <description>See: https://issues.apache.org/jira/browse/YARN-1530This is a simple pre/post exec hook to log query + plan information to yarn. This information can be used to build tools and UIs to monitor, track, debug and tune Hive queries.Off by default, but can be enabled via:hive.exec.pre.hooks=ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHookhive.exec.post.hooks=ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHookhive.exec.failure.hooks=ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7140" opendate="2014-5-28 00:00:00" fixdate="2014-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump default hive.metastore.client.socket.timeout to 5 minutes</summary>
      <description>The issue is that OOTB clients often face timeouts when using HMS since many operations in the HMS completes are long running (e.g. many operations on a table with many partitions). A few supporting pieces of information: The default value of hive.metastore.client.socket.timeout is 20 seconds. Since the timeout is client only, the server happy continues doing the requested work Clients retry after a small delay to perform the requested work again, often while the server is still trying to complete the original request A few tests have actually increased this value in order to pass reliably.</description>
      <version>0.10.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7143" opendate="2014-5-29 00:00:00" fixdate="2014-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Streaming support in Windowing mode for more UDAFs (min/max, lead/lag, fval/lval)</summary>
      <description>Provided implementations for Streaming for the above fns.Min/Max based on Alg by Daniel Lemire: http://www.archipel.uqam.ca/309/1/webmaximinalgo.pdf</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEnhancer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
    </fixedFiles>
  </bug>
  <bug id="7169" opendate="2014-6-2 00:00:00" fixdate="2014-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in Http Mode should have a configurable IdleMaxTime timeout</summary>
      <description>Currently, in HiveServer2 we use Jetty Server to start the Http Server. The connector used for this Thrift Http Cli Service has maximum idle time as the default timeout as mentioned in http://grepcode.com/file/repo1.maven.org/maven2/org.eclipse.jetty/jetty-server/7.0.0.v20091005/org/eclipse/jetty/server/AbstractConnector.java#AbstractConnector.0_maxIdleTime.This should be manually configurable using connector.setMaxIdleTime(maxIdleTime);</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7188" opendate="2014-6-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sum(if()) returns wrong results with vectorization</summary>
      <description>1. The tgz file containing the setup is attached.2. Run the following queryselect sum(if(is_returning=true and is_free=false,1,0)) as unpaid_returning from hike_error.ttr_day0;returns 0 rows with vectorization turned on whereas it return 131 rows with vectorization turned off.hive&gt; source insert.sql &gt; ;OKTime taken: 0.359 secondsOKTime taken: 0.015 secondsOKTime taken: 0.069 secondsOKTime taken: 0.176 secondsLoading data to table hike_error.ttr_day0Table hike_error.ttr_day0 stats: &amp;#91;numFiles=1, numRows=0, totalSize=3581, rawDataSize=0&amp;#93;OKTime taken: 0.33 secondshive&gt; select &gt; sum(if(is_returning=true and is_free=false,1,0)) as unpaid_returning &gt; from hike_error.ttr_day0;Query ID = hsubramaniyan_20140606134646_04790d3d-ca9a-427a-8cf9-3174536114edTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapred.reduce.tasks=&lt;number&gt;Execution log at: /var/folders/r0/9x0wltgx2nv4m4b18m71z1y40000gr/T//hsubramaniyan/hsubramaniyan_20140606134646_04790d3d-ca9a-427a-8cf9-3174536114ed.logJob running in-process (local Hadoop)Hadoop job information for null: number of mappers: 0; number of reducers: 02014-06-06 13:47:02,043 null map = 0%, reduce = 100%Ended Job = job_local773704964_0001Execution completed successfullyMapredLocal task succeededOK131Time taken: 5.325 seconds, Fetched: 1 row(s)hive&gt; set hive.vectorized.execution.enabled=true; hive&gt; select &gt; sum(if(is_returning=true and is_free=false,1,0)) as unpaid_returning &gt; from hike_error.ttr_day0;Query ID = hsubramaniyan_20140606134747_1182c765-90ac-4a33-a8b1-760adca6bf38Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapred.reduce.tasks=&lt;number&gt;Execution log at: /var/folders/r0/9x0wltgx2nv4m4b18m71z1y40000gr/T//hsubramaniyan/hsubramaniyan_20140606134747_1182c765-90ac-4a33-a8b1-760adca6bf38.logJob running in-process (local Hadoop)Hadoop job information for null: number of mappers: 0; number of reducers: 02014-06-06 13:47:18,604 null map = 0%, reduce = 100%Ended Job = job_local701415676_0001Execution completed successfullyMapredLocal task succeededOK0Time taken: 5.52 seconds, Fetched: 1 row(s)hive&gt; explain select &gt; sum(if(is_returning=true and is_free=false,1,0)) as unpaid_returning &gt; from hike_error.ttr_day0;OKSTAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1STAGE PLANS: Stage: Stage-1 Map Reduce Map Operator Tree: TableScan alias: ttr_day0 Statistics: Num rows: 447 Data size: 3581 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: is_returning (type: boolean), is_free (type: boolean) outputColumnNames: is_returning, is_free Statistics: Num rows: 447 Data size: 3581 Basic stats: COMPLETE Column stats: NONE Group By Operator aggregations: sum(if(((is_returning = true) and (is_free = false)), 1, 0)) mode: hash outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator sort order: Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE value expressions: _col0 (type: bigint) Execution mode: vectorized Reduce Operator Tree: Group By Operator aggregations: sum(VALUE._col0) mode: mergepartial outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: _col0 (type: bigint) outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE File Output Operator compressed: false Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: ListSinkTime taken: 0.079 seconds, Fetched: 49 row(s)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
    </fixedFiles>
  </bug>
  <bug id="7255" opendate="2014-6-19 00:00:00" fixdate="2014-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow partial partition spec in analyze command</summary>
      <description>So that stats collection can happen for multiple partitions through one statement.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.invalid.values.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.incorrect.num.keys.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.incorrect.num.keys.q</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.dp.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="7262" opendate="2014-6-20 00:00:00" fixdate="2014-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioned Table Function (PTF) query fails on ORC table when attempting to vectorize</summary>
      <description>In ptf.q, create the part table with STORED AS ORC and SET hive.vectorized.execution.enabled=true;Queries fail to find BLOCKOFFSET virtual column during vectorization and suffers an exception.ERROR vector.VectorizationContext (VectorizationContext.java:getInputColumnIndex(186)) - The column BLOCK_OFFSETINSIDE_FILE is not in the vectorization context column map.Jitendra pointed to the routine that returns the VectorizationContext in Vectorize.java needing to add virtual columns to the map, too.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7263" opendate="2014-6-20 00:00:00" fixdate="2014-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing fixes from review of parquet-timestamp</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.utils.NanoTimeUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7274" opendate="2014-6-23 00:00:00" fixdate="2014-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update PTest2 to JClouds 1.7.3</summary>
      <description>Required to use newer instance types</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7314" opendate="2014-6-30 00:00:00" fixdate="2014-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results of UDF when hive.cache.expr.evaluation is set</summary>
      <description>It seems that the expression caching doesn't work when using UDF inside another UDF or a hive function.For example :tbl has one row : 'a','b'The following query : select concat(custUDF(a),' ', custUDF(b)) from tbl; returns 'a a'seems to cache custUDF(a) and use it for custUDF(b).Same query without the concat works fine.Replacing the concat with another custom UDF also returns 'a a'</description>
      <version>0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug id="732" opendate="2009-8-6 00:00:00" fixdate="2009-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store intermediate data in binary using LazyBinarySerDe</summary>
      <description>Follow-up on HIVE-640. We should use LazyBinarySerDe in several places in the code to improve the efficiency: value between map-reduce boundary, temporary tables.We should also allow users to create tables stored as binary format.CREATE TABLE xxx (...)ROW FORMAT BINARYSTORED AS SEQUENCEFILE;</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7323" opendate="2014-7-1 00:00:00" fixdate="2014-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type stats in ORC sometimes go stale</summary>
      <description>I cannot make proper test case but sometimes min/max value in date type stats is changed in runtime. Stats for other type contains non-mutable values in it but date type stats contains DateWritable, which of inner value can be changed anytime.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="7346" opendate="2014-7-3 00:00:00" fixdate="2014-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results caused by hive ppd under specific join condition</summary>
      <description>Assuming two tables : t1(id1 string,id2 string) , t2 (id string,d int) t1 contains 1 row : 'a','a't2 contains 1 row : 'a',2The following query : select a.*,b.d d1,c.d d2from t1 a join t2 b on (a.id1=b.id)join t2 c on (a.id2=b.id)where b.d &lt;=1 and c.d&lt;=1 Returns 0 rows as expected because t2.d = 2Wrapping this query, like so : select * from (select a.*,b.d d1,c.d d2from t1 a join t2 b on (a.id1=b.id)join t2 c on (a.id2=b.id)where b.d &lt;=1 and c.d&lt;=1) z where d1&gt;1 or d2&gt;1 Where another filter was add on the columns causes the plan to lack the filter of the "&lt;=1" and return a single row - Wrong Results.The plan is : ABSTRACT SYNTAX TREE: (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME t1) a) (TOK_TABREF (TOK_TABNAME t2) b) (= (. (TOK_TABLE_OR_COL a) id1) (. (TOK_TABLE_OR_COL b) id))) (TOK_TABREF (TOK_TABNAME t2) c) (= (. (TOK_TABLE_OR_COL a) id2) (. (TOK_TABLE_OR_COL b) id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) d) d1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) d) d2)) (TOK_WHERE (and (&lt;= (. (TOK_TABLE_OR_COL b) d) 1) (&lt;= (. (TOK_TABLE_OR_COL c) d) 1))))) z)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (&gt; (TOK_TABLE_OR_COL d1) 1) (&gt; (TOK_TABLE_OR_COL d2) 1)))))STAGE DEPENDENCIES: Stage-7 is a root stage Stage-5 depends on stages: Stage-7 Stage-0 is a root stageSTAGE PLANS: Stage: Stage-7 Map Reduce Local Work Alias -&gt; Map Local Tables: z:b Fetch Operator limit: -1 z:c Fetch Operator limit: -1 Alias -&gt; Map Local Operator Tree: z:b TableScan alias: b HashTable Sink Operator condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] Position of Big Table: 0 z:c TableScan alias: c HashTable Sink Operator condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] Position of Big Table: 0 Stage: Stage-5 Map Reduce Alias -&gt; Map Operator Tree: z:a TableScan alias: a Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] outputColumnNames: _col0, _col1, _col4, _col5 Position of Big Table: 0 Filter Operator predicate: expr: (_col1 = _col4) type: boolean Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] outputColumnNames: _col1, _col4, _col5, _col9 Position of Big Table: 0 Filter Operator predicate: expr: ((_col1 &gt; 1) or (_col9 &gt; 1)) type: boolean Select Operator expressions: expr: _col4 type: string expr: _col5 type: string expr: _col1 type: int expr: _col9 type: int outputColumnNames: _col0, _col1, _col2, _col3 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Local Work: Map Reduce Local Work Stage: Stage-0 Fetch Operator limit: -1Setting : hive.optimize.ppd=false Results in the following correct plan : ABSTRACT SYNTAX TREE: (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME t1) a) (TOK_TABREF (TOK_TABNAME t2) b) (= (. (TOK_TABLE_OR_COL a) id1) (. (TOK_TABLE_OR_COL b) id))) (TOK_TABREF (TOK_TABNAME t2) c) (= (. (TOK_TABLE_OR_COL a) id2) (. (TOK_TABLE_OR_COL b) id)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME a))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) d) d1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) d) d2)) (TOK_WHERE (and (&lt;= (. (TOK_TABLE_OR_COL b) d) 1) (&lt;= (. (TOK_TABLE_OR_COL c) d) 1))))) z)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (&gt; (TOK_TABLE_OR_COL d1) 1) (&gt; (TOK_TABLE_OR_COL d2) 1)))))STAGE DEPENDENCIES: Stage-7 is a root stage Stage-5 depends on stages: Stage-7 Stage-0 is a root stageSTAGE PLANS: Stage: Stage-7 Map Reduce Local Work Alias -&gt; Map Local Tables: z:b Fetch Operator limit: -1 z:c Fetch Operator limit: -1 Alias -&gt; Map Local Operator Tree: z:b TableScan alias: b HashTable Sink Operator condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] Position of Big Table: 0 z:c TableScan alias: c HashTable Sink Operator condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] Position of Big Table: 0 Stage: Stage-5 Map Reduce Alias -&gt; Map Operator Tree: z:a TableScan alias: a Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {id1} {id2} 1 {id} {d} handleSkewJoin: false keys: 0 [Column[id1]] 1 [Column[id]] outputColumnNames: _col0, _col1, _col4, _col5 Position of Big Table: 0 Filter Operator predicate: expr: (_col1 = _col4) type: boolean Map Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {_col5} {_col0} {_col1} 1 {d} handleSkewJoin: false keys: 0 [] 1 [] outputColumnNames: _col1, _col4, _col5, _col9 Position of Big Table: 0 Filter Operator predicate: expr: ((_col1 &lt;= 1) and (_col9 &lt;= 1)) type: boolean Select Operator expressions: expr: _col4 type: string expr: _col5 type: string expr: _col1 type: int expr: _col9 type: int outputColumnNames: _col0, _col1, _col2, _col3 Filter Operator predicate: expr: ((_col2 &gt; 1) or (_col3 &gt; 1)) type: boolean Select Operator expressions: expr: _col0 type: string expr: _col1 type: string expr: _col2 type: int expr: _col3 type: int outputColumnNames: _col0, _col1, _col2, _col3 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Local Work: Map Reduce Local Work Stage: Stage-0 Fetch Operator limit: -1</description>
      <version>0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="7369" opendate="2014-7-8 00:00:00" fixdate="2014-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Support agg distinct function with GB</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveAggregateRel.java</file>
    </fixedFiles>
  </bug>
  <bug id="7429" opendate="2014-7-16 00:00:00" fixdate="2014-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set replication for archive called before file exists</summary>
      <description>The call to set replication is called prior to uploading the archive file to hdfs, which does not throw an error, but the replication never gets set.This has a significant impact on large jobs (especially hash joins) due to too many tasks hitting the data nodes.</description>
      <version>0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="743" opendate="2009-8-8 00:00:00" fixdate="2009-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>let user specify serde for custom scripts</summary>
      <description>Splitting up https://issues.apache.org/jira/browse/HIVE-708 into this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7450" opendate="2014-7-19 00:00:00" fixdate="2014-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Database should inherit perms of warehouse dir</summary>
      <description>One more ask: the database directory should inherit permission and extended ACL's of the hive warehouse directory.As table dirs are inheriting it, theres no reason that database dirs shouldn't inherit it.Behavior is governed by "hive.warehouse.subdir.inherit.perms" flag.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7451" opendate="2014-7-19 00:00:00" fixdate="2014-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pass function name in create/drop function to authorization api</summary>
      <description>If function names are passed to the authorization api for create/drop function calls, then authorization decisions can be made based on the function names as well.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.fail.nogrant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.udaf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.using.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.logic.java.boolean.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.context.aware.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.compare.java.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.sum.list.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.register.tblfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compile.processor.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.admin.almighty2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.nonexistent.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.local.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.function.does.not.implement.udf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.temp.table.authorize.create.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.native.udf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.udf.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.udaf.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.function.nonudf.class.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.function.nonexistent.class.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorize.create.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.truncate.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.show.parts.nosel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.rolehierarchy.privs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.tab.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.alter.tab.serdeprop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.alter.tab.rename.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insert.noselectpriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insert.noinspriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insertoverwrite.nodel.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.invalid.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.udtf.explode2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.add.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.arraymapstruct.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.format.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udtf.explode2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udtf.output.on.close.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Entity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.addjar.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.addpartition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.alter.db.owner.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.alter.db.owner.default.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.compile.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.createview.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.macro1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.deletejar.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.desc.table.nosel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.dfs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.droppartition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.allpriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.fail1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="7453" opendate="2014-7-19 00:00:00" fixdate="2014-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Partition Pruning enhancements 1</summary>
      <description>1. Handle type casts2. Handle Literal Conversion for Partition Pruning expressions</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7457" opendate="2014-7-21 00:00:00" fixdate="2014-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor HCatalog Pig Adapter test clean up</summary>
      <description>Minor cleanup to the HCatalog Pig Adapter tests in preparation for HIVE-7420: Run through Hive Eclipse formatter. Convert JUnit 3-style tests to follow JUnit 4 conventions.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestPigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestOrcHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.MockLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="7459" opendate="2014-7-21 00:00:00" fixdate="2014-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="7468" opendate="2014-7-22 00:00:00" fixdate="2014-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:UDF translation needs to use Hive UDF name</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7575" opendate="2014-7-31 00:00:00" fixdate="2014-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetTables thrift call is very slow</summary>
      <description>The GetTables thrift call takes a long time when the number of table is large.With around 5000 tables, the call takes around 80 seconds compared to a "Show Tables" query on the same HiveServer2 instance which takes 3-7 seconds.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="7580" opendate="2014-7-31 00:00:00" fixdate="2014-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support dynamic partitioning [Spark Branch]</summary>
      <description>My understanding is that we don't need to do anything special for this. However, this needs to be verified and tested.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7612" opendate="2014-8-5 00:00:00" fixdate="2014-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Add link to parent vertex to mapjoin in explain</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="7615" opendate="2014-8-5 00:00:00" fixdate="2014-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should have an option for user to see the query progress</summary>
      <description>When executing query in Beeline, user should have a option to see the progress through the outputs.Beeline could use the API introduced in HIVE-4629 to get and display the logs to the client.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7634" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Configuration.getPassword() if available to eliminate passwords from hive-site.xml</summary>
      <description>HADOOP-10607 provides a Configuration.getPassword() API that allows passwords to be retrieved from a configured credential provider, while also being able to fall back to the HiveConf setting if no provider is set up. Hive should use this API for versions of Hadoop that support this API. This would give users the ability to remove the passwords from their Hive configuration files.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7662" opendate="2014-8-8 00:00:00" fixdate="2014-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: changes to Cost Model</summary>
      <description>Model Join cost as Sum of Input sizes Fix bug with NDV calculations. For now use Optiq's default formulas. Model Cummulative cost to favor broad Plans over Deep Plans.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveCost.java</file>
    </fixedFiles>
  </bug>
  <bug id="7670" opendate="2014-8-9 00:00:00" fixdate="2014-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: predicates on partition columns get applied twice in cardinality estimates.</summary>
      <description>this causes an underestimation of Part table row counts.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="7788" opendate="2014-8-20 00:00:00" fixdate="2014-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate plans for insert, update, and delete</summary>
      <description>Insert plans needs to be generated differently for ACID tables, plus we need to be able to generate plans in the semantic analyzer for update and delete.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.invalid.cast.from.binary.1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.StorageFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7790" opendate="2014-8-20 00:00:00" fixdate="2014-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update privileges to check for update and delete</summary>
      <description>In the new SQLStdAuth scheme, we need to add UPDATE and DELETE as operations and add ability check for them.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="7791" opendate="2014-8-20 00:00:00" fixdate="2014-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (1) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. alter_merge_orc.q,\ alter_merge_stats_orc.q,\ auto_join0.q,\ auto_join1.q,\ bucket2.q,\ bucket3.q,\ bucket4.q,\ count.q,\ create_merge_compressed.q,\ cross_join.q,\ cross_product_check_1.q,\ cross_product_check_2.q,\ ctas.q,\custom_input_output_format.q,\ disable_merge_for_bucketing.q,\ dynpart_sort_opt_vectorization.q,\ dynpart_sort_optimization.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7792" opendate="2014-8-20 00:00:00" fixdate="2014-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (2) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on.limit_pushdown.q,\ load_dyn_part1.q,\ load_dyn_part2.q,\ load_dyn_part3.q,\ mapjoin_mapjoin.q,\ mapreduce1.q,\ mapreduce2.q,\ merge1.q,\ merge2.q,\ metadata_only_queries.q,\ optimize_nullscan.q,\ orc_analyze.q,\ orc_merge1.q,\ orc_merge2.q,\ orc_merge3.q,\ orc_merge4.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7793" opendate="2014-8-20 00:00:00" fixdate="2014-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (3) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. ptf.q,\ sample1.q,\ script_env_var1.q,\ script_env_var2.q,\ script_pipe.q,\ scriptfile1.q,\ stats_counter.q,\ stats_counter_partitioned.q,\ stats_noscan_1.q,\ subquery_exists.q,\ subquery_in.q,\ temp_table.q,\ transform1.q,\ transform2.q,\ transform_ppr1.q,\ transform_ppr2.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7794" opendate="2014-8-20 00:00:00" fixdate="2014-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (4) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. vector_cast_constant.q,\ vector_data_types.q,\ vector_decimal_aggregate.q,\ vector_left_outer_join.q,\ vector_string_concat.q,\ vectorization_12.q,\ vectorization_13.q,\ vectorization_14.q,\ vectorization_15.q,\ vectorization_9.q,\ vectorization_part_project.q,\ vectorization_short_regress.q,\ vectorized_mapjoin.q,\ vectorized_nested_mapjoin.q,\ vectorized_ptf.q,\ vectorized_shufflejoin.q,\ vectorized_timestamp_funcs.q</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7795" opendate="2014-8-20 00:00:00" fixdate="2014-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable ptf.q and ptf_streaming.q.[Spark Branch]</summary>
      <description>ptf.q and ptf_streaming.q contains join queries, we should enable these qtests in milestone2.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7848" opendate="2014-8-22 00:00:00" fixdate="2014-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refresh SparkContext when spark configuration changes [Spark Branch]</summary>
      <description>Recreate the spark client if spark configurations are updated (through set command).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="8182" opendate="2014-9-18 00:00:00" fixdate="2014-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline fails when executing multiple-line queries with trailing spaces</summary>
      <description>As title indicates, when executing a multi-line query with trailing spaces, beeline reports syntax error: Error: Error while compiling statement: FAILED: ParseException line 1:76 extraneous input ';' expecting EOF near '&lt;EOF&gt;' (state=42000,code=40000)If put this query in one single line, beeline succeeds to execute it.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="8189" opendate="2014-9-19 00:00:00" fixdate="2014-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A select statement with a subquery is failing with HBaseSerde</summary>
      <description>Hive tables in the query are hbase tables, and the subquery is a join statement.Whenset hive.optimize.ppd=true; andset hive.auto.convert.join=false;The query does not return data. While hive.optimize.ppd=true and hive.auto.convert.join=true return values back. See attached query file.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="8313" opendate="2014-9-30 00:00:00" fixdate="2014-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize evaluation for ExprNodeConstantEvaluator and ExprNodeNullEvaluator</summary>
      <description>Consider the following query:SELECT foo, bar, goo, idFROM myTableWHERE id IN ( 'A', 'B', 'C', 'D', ... , 'ZZZZZZ' );One finds that when the IN clause has several thousand elements (and the table has several million rows), the query above takes orders-of-magnitude longer to run on Hive 0.12 than say Hive 0.10.I have a possibly incomplete fix.</description>
      <version>0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8395" opendate="2014-10-8 00:00:00" fixdate="2014-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: enable by default</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.date.trim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.processor.namespaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.print.header.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">ql.src.test.queries.clientnegative.join.nonexistent.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ambiguous.col.q</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constantPropagateForSubQuery.q</file>
      <file type="M">ql.src.test.queries.clientpositive.filter.join.breaktask2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.vc.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.optimize.nullscan.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ppd.gby.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.q</file>
      <file type="M">ql.src.test.queries.clientpositive.query.properties.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.exists.explain.rewrite.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.in.explain.rewrite.q</file>
      <file type="M">ql.src.test.results.clientnegative.join.nonexistent.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.InvalidValueBoundary.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ansi.sql.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="8428" opendate="2014-10-10 00:00:00" fixdate="2014-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PCR doesnt remove filters involving casts</summary>
      <description>e.g.,select key,value from srcpart where hr = cast(11 as double);</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.pcr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.vectorization.ppd.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.decimal.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="8429" opendate="2014-10-10 00:00:00" fixdate="2014-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add records in/out counters</summary>
      <description>We don't do counters for input/output records right now. That would help for debugging though (if it can be done with minimal overhead).</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8435" opendate="2014-10-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add identity project remover optimization</summary>
      <description></description>
      <version>0.9.0,0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="8436" opendate="2014-10-12 00:00:00" fixdate="2014-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify SparkWork to split works with multiple child works [Spark Branch]</summary>
      <description>Based on the design doc, we need to split the operator tree of a work in SparkWork if the work is connected to multiple child works. The way splitting the operator tree is performed by cloning the original work and removing unwanted branches in the operator tree. Please refer to the design doc for details.This process should be done right before we generate SparkPlan. We should have a utility method that takes the orignal SparkWork and return a modified SparkWork.This process should also keep the information about the original work and its clones. Such information will be needed during SparkPlan generation (HIVE-8437).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkTableScanProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMultiInsertionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMergeTaskProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8442" opendate="2014-10-13 00:00:00" fixdate="2014-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-8403</summary>
      <description>HIVE-8403 caused the number of tests run to drop from ~6K to ~4K. Also, the datanucleus repo is back up. So we should revert this change.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8532" opendate="2014-10-21 00:00:00" fixdate="2014-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>return code of "source xxx" clause is missing</summary>
      <description>When executing "source &lt;hql-file&gt;" clause, hive client driver does not catch the return code of this command.This behaviour causes an issue when running hive query in Oozie workflow.When the "source" clause is put into a Oozie workflow, Oozie can not get the return code of this command. Thus, Oozie consider the "source" clause as successful all the time. So, when the "source" clause fails, the hive query does not abort and the oozie workflow does not abort either.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="8533" opendate="2014-10-21 00:00:00" fixdate="2014-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable all q-tests for multi-insertion [Spark Branch]</summary>
      <description>As HIVE-8436 is done, we should be able to enable all multi-insertion related tests. This JIRA is created to track this and record any potential issue encountered.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8535" opendate="2014-10-21 00:00:00" fixdate="2014-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable compile time skew join optimization for spark [Spark Branch]</summary>
      <description>Sub-task of HIVE-8406</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.union.remove.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.union.remove.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8536" opendate="2014-10-21 00:00:00" fixdate="2014-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable SkewJoinResolver for spark [Spark Branch]</summary>
      <description>Sub-task of HIVE-8406</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="8600" opendate="2014-10-24 00:00:00" fixdate="2014-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to log explain output for query</summary>
      <description>When diagnosing issues, it is useful to have the explain output for the query already available in log4j log.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="8603" opendate="2014-10-25 00:00:00" fixdate="2014-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>auto_sortmerge_join_5 is getting stuck on tez</summary>
      <description>test is getting stuck in precommit builds - causes slowdown in all runs.vikram.dixit - will temporarily disable.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8696" opendate="2014-11-2 00:00:00" fixdate="2014-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatClientHMSImpl doesn&amp;#39;t use a Retrying-HiveMetastoreClient.</summary>
      <description>The HCatClientHMSImpl doesn't use a RetryingHiveMetastoreClient. Users of the HCatClient API that log in through keytabs will fail without retry, when their TGTs expire.The fix is inbound.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HiveClientCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="8732" opendate="2014-11-4 00:00:00" fixdate="2014-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC string statistics are not merged correctly</summary>
      <description>Currently ORC's string statistics do not merge correctly causing incorrect maximum values.</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="8746" opendate="2014-11-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC timestamp columns are sensitive to daylight savings time</summary>
      <description>Hive uses Java's Timestamp class to manipulate timestamp columns. Unfortunately the textual parsing in Timestamp is done in local time and the internal storage is in UTC.ORC mostly side steps this issue by storing the difference between the time and a base time also in local and storing that difference in the file. Reading the file between timezones will mostly work correctly "2014-01-01 12:34:56" will read correctly in every timezone.However, when moving between timezones with different daylight saving it creates trouble. In particular, moving from a computer in PST to UTC will read "2014-06-06 12:34:56" as "2014-06-06 11:34:56".</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.static.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="8847" opendate="2014-11-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bugs in jenkins scripts</summary>
      <description>1) Incorrect help message in process_jira function2) Spark builds do not work3) Build "profiles" (which map to a properties file) are hard coded4) A JIRA is required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9059" opendate="2014-12-10 00:00:00" fixdate="2014-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove wrappers for SparkJobInfo and SparkStageInfo [Spark Branch]</summary>
      <description>SPARK-4567 is resolved. We can remove the wrappers we added to solve the serailization issues.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.status.HiveSparkStageInfo.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.status.HiveSparkJobInfo.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.KryoMessageCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9234" opendate="2014-12-30 00:00:00" fixdate="2014-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks FileSystem objects in FileSystem.CACHE</summary>
      <description>Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true), but creating new FileSystem objects is expensive.</description>
      <version>0.12.0,0.12.1,0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
    </fixedFiles>
  </bug>
  <bug id="9513" opendate="2015-1-29 00:00:00" fixdate="2015-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="9514" opendate="2015-1-29 00:00:00" fixdate="2015-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool is broken in hive 1.0.0</summary>
      <description>Schematool gives following error - bin/schematool -dbType derby -initSchemaStarting metastore schema initialization to 1.0org.apache.hadoop.hive.metastore.HiveMetaException: Unknown version specified for initialization: 1.0Metastore schema hasn't changed from 0.14.0 to 1.0.0. So there is no need for new .sql files for 1.0.0. However, schematool needs to be made aware of the metastore schema equivalence.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="9593" opendate="2015-2-5 00:00:00" fixdate="2015-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC Reader should ignore unknown metadata streams</summary>
      <description>ORC readers should ignore metadata streams which are non-essential additions to the main data streams.This will include additional indices, histograms or anything we add as an optional stream.</description>
      <version>0.11.0,0.12.0,0.13.1,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="9877" opendate="2015-3-5 00:00:00" fixdate="2015-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline cannot run multiple statements in the same row</summary>
      <description>I'm trying to switch from hive cli to beeline and found the below working with hive cli, but not with beeline.This works in hive cli:$ hive -e "USE my_db;SHOW TABLES;" The same does not work in beeline:$ beeline -u jdbc:hive2://my_server.com -n my_user -p my_password -e "USE my_db;SHOW TABLES;"Error: Error while compiling statement: FAILED: ParseException line 1:9 missing EOF at ';' near 'my_db' (state=42000,code=40000)Beeline version 0.12.0-cdh5.1.3 by Apache Hive I have also tried with beeline -f &amp;#91;filename&amp;#93;The issue is the same, except when the two statements are listed in separate lines in the file supplied via the -f parameter.So when using beeline -f my.hqlThis works:my.hql:USE my_db;SHOW TABLES;This does not work:my.hql:USE my_db;SHOW TABLES;$ beeline -u jdbc:hive2://my_server.com -n my_user -p my_password -f my.hqlConnected to: Apache Hive (version 0.12.0-cdh5.1.3)Driver: Hive JDBC (version 0.12.0-cdh5.1.3)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 0.12.0-cdh5.1.3 by Apache Hive0: jdbc:hive2://my_server.com&gt; USE my_db;SHOW TABLES;Error: Error while compiling statement: FAILED: ParseException line 1:9 missing EOF at ';' near 'my_db' (state=42000,code=40000)Closing: org.apache.hive.jdbc.HiveConnectionHow to reproduce:Run any type of multiple statements with beeline where the statements are in the same line separated by ; whether using "beeline -e &amp;#91;statement&amp;#93;" or "beeline -f &amp;#91;file&amp;#93;"</description>
      <version>0.12.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
