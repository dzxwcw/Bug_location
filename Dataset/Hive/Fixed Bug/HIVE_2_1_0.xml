<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10129" opendate="2015-3-28 00:00:00" fixdate="2015-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fix ordering of execution modes</summary>
      <description>uber &gt; llap &gt; container execution modes. Fix the ordering in in-place update UI.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12165" opendate="2015-10-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong result when hive.optimize.sampling.orderby=true with some aggregate functions</summary>
      <description>This simple query give wrong result , when , i use the parallel order .select count(*) , count(distinct dummyint ) , min(dummyint),max(dummyint) from foobar_1M ;Current wrong result :c0 c1 c2 c332740 32740 0 163695113172 113172 163700 72955554088 54088 729560 999995Right result :c0 c1 c2 c31000000 1000000 0 999999The sql script for my test drop table foobar_1 ;create table foobar_1 ( dummyint int , dummystr string ) ;insert into table foobar_1 select count(*),'dummy 0' from foobar_1 ;drop table foobar_1M ;create table foobar_1M ( dummyint bigint , dummystr string ) ;insert overwrite table foobar_1M select val_int , concat('dummy ',val_int) from ( select ((((((d_1*10)+d_2)*10+d_3)*10+d_4)*10+d_5)*10+d_6) as val_int from foobar_1 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_1 as d_1 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_2 as d_2 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_3 as d_3 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_4 as d_4 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_5 as d_5 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_6 as d_6 ) as f ;set hive.optimize.sampling.orderby.number=10000;set hive.optimize.sampling.orderby.percent=0.1f;set mapreduce.job.reduces=3 ;set hive.optimize.sampling.orderby=false;select count(*) , count(distinct dummyint ) , min(dummyint),max(dummyint) from foobar_1M ;set hive.optimize.sampling.orderby=true;select count(*) , count(distinct dummyint ) , min(dummyint),max(dummyint) from foobar_1M ;</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12584" opendate="2015-12-3 00:00:00" fixdate="2015-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized join with partition column of type char does not trim spaces</summary>
      <description>When a table is partitioned on a column of type char and if join is performed on partitioned column then following exception gets thrown from hashtable loaderCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5 at org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(ObjectCache.java:82) at org.apache.hadoop.hive.ql.exec.tez.ObjectCache$1.call(ObjectCache.java:92) ... 4 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5 at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:216) at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:293) at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:174) at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:170) at org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(ObjectCache.java:75) ... 5 moreCaused by: org.apache.hadoop.hive.serde2.SerDeException: Unexpected tag: 52 reserialized to 5 at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$LazyBinaryKvWriter.sanityCheckKeyForTag(MapJoinBytesTableContainer.java:276) at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$LazyBinaryKvWriter.getHashFromKey(MapJoinBytesTableContainer.java:247) at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.internalPutRow(HybridHashTableContainer.java:451) at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.putRow(HybridHashTableContainer.java:444) at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:210)</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12609" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove javaXML serialization</summary>
      <description>We use kryo as default serializer and javaXML based serialization is not used in many places and is also not well tested. We should remove javaXML serialization and make kryo as the only serialization option.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.cast.qualified.types.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AggregationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12614" opendate="2015-12-8 00:00:00" fixdate="2015-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RESET command does not close spark session</summary>
      <description></description>
      <version>1.3.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ResetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12628" opendate="2015-12-9 00:00:00" fixdate="2015-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate flakiness in TestMetrics</summary>
      <description>TestMetrics relies on timing of json file dumps. Rewrite these tests to eliminate flakiness.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="12631" opendate="2015-12-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: support ORC ACID tables</summary>
      <description>LLAP uses a completely separate read path in ORC to allow for caching and parallelization of reads and processing. This path does not support ACID. As far as I remember ACID logic is embedded inside ORC format; we need to refactor it to be on top of some interface, if practical; or just port it to LLAP read path.Another consideration is how the logic will work with cache. The cache is currently low-level (CB-level in ORC), so we could just use it to read bases and deltas (deltas should be cached with higher priority) and merge as usual. We could also cache merged representation in future.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.LlapAwareSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12632" opendate="2015-12-9 00:00:00" fixdate="2015-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: don&amp;#39;t use IO elevator for ACID tables</summary>
      <description>Until HIVE-12631 is fixed, we need to avoid ACID tables in IO elevator. Right now, a FileNotFound error is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12633" opendate="2015-12-9 00:00:00" fixdate="2015-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: package included serde jars</summary>
      <description>Some SerDes like JSONSerde are not packaged with LLAP. One cannot localize jars on the daemon (due to security consideration if nothing else), so we should package them.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="12646" opendate="2015-12-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline and HIVE CLI do not parse ; in quote properly</summary>
      <description>Beeline and Cli have to escape ; in the quote while most other shell scripts need not. For example:in Beeline:0: jdbc:hive2://localhost:10000&gt; select ';' from tlb1;select ';' from tlb1;15/12/10 10:45:26 DEBUG TSaslTransport: writing data length: 11515/12/10 10:45:26 DEBUG TSaslTransport: CLIENT: reading data length: 3403Error: Error while compiling statement: FAILED: ParseException line 1:8 cannot recognize input near '&lt;EOF&gt;' '&lt;EOF&gt;while in mysql shell:mysql&gt; SELECT CONCAT(';', 'foo') FROM test limit 3;+--------------------+| ;foo || ;foo || ;foo |+--------------------+3 rows in set (0.00 sec)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="12648" opendate="2015-12-10 00:00:00" fixdate="2015-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO was disabled in CliDriver by accident (and tests are broken)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
    </fixedFiles>
  </bug>
  <bug id="12687" opendate="2015-12-16 00:00:00" fixdate="2015-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Workdirs need to default to YARN local</summary>
      <description>LLAP_DAEMON_WORK_DIRS("hive.llap.daemon.work.dirs", ""is a bad default &amp; fails at startup if not overridden.A better default would be to fall back onto YARN local dirs if this is not configured.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12693" opendate="2015-12-16 00:00:00" fixdate="2015-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use Slider Anti-Affinity scheduling mode for daemon distribution</summary>
      <description>Slider has SLIDER-82 which adds anti-affinity placement policies for containers, to avoid colliding on to the same machine when deploying LLAP instances.NO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="12694" opendate="2015-12-16 00:00:00" fixdate="2015-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Slider destroy semantics require force</summary>
      <description>2015-12-16 20:10:55,118 [main] ERROR main.ServiceLauncher - Destroy will permanently delete directories and registries. Reissue this command with the --force option if you want to proceed.NO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="12733" opendate="2015-12-22 00:00:00" fixdate="2015-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UX improvements for HIVE-12499</summary>
      <description>From some early user feedback, the concept of delta metadata is a bit confusing, and should be separated into the 'created' and 'deleted' metadata metrics. Delta can be inferred from those.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HMSMetricsListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="12743" opendate="2015-12-24 00:00:00" fixdate="2015-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RCFileInputFormat needs to be registered with kryo</summary>
      <description>Ran into an issue with union distinct query that uses RCFile table with the following exceptionCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.RCFileInputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="1276" opendate="2010-3-24 00:00:00" fixdate="2010-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize bucketing</summary>
      <description>If the query results are already clustered by the bucketing column, there is no need for another map-reduce job.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12762" opendate="2015-12-30 00:00:00" fixdate="2015-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Common join on parquet tables returns incorrect result when hive.optimize.index.filter set to true</summary>
      <description>The following query will give incorrect result.CREATE TABLE tbl1(id INT) STORED AS PARQUET;INSERT INTO tbl1 VALUES(1), (2);CREATE TABLE tbl2(id INT, value STRING) STORED AS PARQUET;INSERT INTO tbl2 VALUES(1, 'value1');INSERT INTO tbl2 VALUES(1, 'value2');set hive.optimize.index.filter = true;set hive.auto.convert.join=false;select tbl1.id, t1.value, t2.valueFROM tbl1JOIN (SELECT * FROM tbl2 WHERE value='value1') t1 ON tbl1.id=t1.idJOIN (SELECT * FROM tbl2 WHERE value='value2') t2 ON tbl1.id=t2.id;We are enforcing to use common join and tbl2 will have 2 files after 2 insertions underneath.the map job contains 3 TableScan operators (2 for tbl2 and 1 for tbl1). When hive.optimize.index.filter is set to true, we are incorrectly applying the later filtering condition to each block, which causes no data is returned for the subquery SELECT * FROM tbl2 WHERE value='value1'.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.ExpressionTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="12765" opendate="2015-12-30 00:00:00" fixdate="2015-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Intersect (distinct/all) Except (distinct/all) Minus (distinct/all)</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12792" opendate="2016-1-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-12075 didn&amp;#39;t update operation type for plugins</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
    </fixedFiles>
  </bug>
  <bug id="12802" opendate="2016-1-7 00:00:00" fixdate="2016-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver.vector_join_filters.q failure</summary>
      <description>Discovered as part of running :mvn test -Dtest=TestMiniTezCliDriver -Dqfile_regex=vector.* -Dhive.cbo.returnpath.hiveop=true -Dtest.output.overwrite=trueSELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key &gt; 40 AND a.value &gt; 50 AND a.key = a.value AND b.key &gt; 40 AND b.value &gt; 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key &gt; 40 AND c.value &gt; 50 AND c.key = c.value AND b.key &gt; 40 AND b.value &gt; 50 AND b.key = b.value)2016-01-07T11:16:06,198 ERROR [657fd759-7643-467b-9bd0-17cb4958cb69 main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(309)) - CBO failed, skipping CBO.java.lang.IndexOutOfBoundsException: index (10) must be less than size (6) at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305) ~[guava-14.0.1.jar:?] at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284) ~[guava-14.0.1.jar:?] at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81) ~[guava-14.0.1.jar:?] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitInputRef(ExprNodeConverter.java:109) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitInputRef(ExprNodeConverter.java:79) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) ~[calcite-core-1.5.0.jar:1.5.0] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitCall(ExprNodeConverter.java:128) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.visitCall(ExprNodeConverter.java:79) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.calcite.rex.RexCall.accept(RexCall.java:107) ~[calcite-core-1.5.0.jar:1.5.0] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.convertToExprNode(HiveOpConverter.java:1153) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.translateJoin(HiveOpConverter.java:381) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.visit(HiveOpConverter.java:313) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.dispatch(HiveOpConverter.java:164) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.visit(HiveOpConverter.java:268) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.dispatch(HiveOpConverter.java:162) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.visit(HiveOpConverter.java:397) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.dispatch(HiveOpConverter.java:181) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.convert(HiveOpConverter.java:154) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedHiveOPDag(CalcitePlanner.java:688) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:266) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10094) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:231) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:471) [hive-exec-2.1.0-SNAPSHOT.jar:?]</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12805" opendate="2016-1-7 00:00:00" fixdate="2016-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver skewjoin.q failure</summary>
      <description>Set hive.cbo.returnpath.hiveop=trueFROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))The stack trace:java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:635) at java.util.ArrayList.get(ArrayList.java:411) at org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate$JoinSynthetic.process(SyntheticJoinPredicate.java:183) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:43) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) at org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.transform(SyntheticJoinPredicate.java:100) at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:236) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10170) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:231) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:471)Same error happens in auto_sortmerge_join_6.q.out for select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="12809" opendate="2016-1-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: fast-path for coalesce if input.noNulls = true</summary>
      <description>Coalesce can skip processing other columns, if all the input columns are non-null.Possibly retaining, isRepeating=true.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
    </fixedFiles>
  </bug>
  <bug id="12820" opendate="2016-1-8 00:00:00" fixdate="2016-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the check if carriage return and new line are used for separator or escape character</summary>
      <description>The change in HIVE-11785 doesn't allow \r or \n to be used as separator or escape character which may break some existing tables which uses \r as separator or escape character e.g..This case actually can be supported regardless of SERIALIZATION_ESCAPE_CRLF set or not.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
    </fixedFiles>
  </bug>
  <bug id="12826" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: fix VectorUDAF* suspect isNull checks</summary>
      <description>for isRepeating=true, checking isNull[selected&amp;#91;i&amp;#93;] might return incorrect results (without a heavy array fill of isNull).VectorUDAFSum/Min/Max/Avg and SumDecimal impls need to be reviewed for this pattern. private void iterateHasNullsRepeatingSelectionWithAggregationSelection( VectorAggregationBufferRow[] aggregationBufferSets, int aggregateIndex, &lt;ValueType&gt; value, int batchSize, int[] selection, boolean[] isNull) { for (int i=0; i &lt; batchSize; ++i) { if (!isNull[selection[i]]) { Aggregation myagg = getCurrentAggregationBuffer( aggregationBufferSets, aggregateIndex, i); myagg.sumValue(value); } } }</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12827" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: VectorCopyRow/VectorAssignRow/VectorDeserializeRow assign needs explicit isNull[offset] modification</summary>
      <description>Some scenarios do set Double.NaN instead of isNull=true, but all types aren't consistent.Examples of un-set isNull for the valid values are private class FloatReader extends AbstractDoubleReader { FloatReader(int columnIndex) { super(columnIndex); } @Override void apply(VectorizedRowBatch batch, int batchIndex) throws IOException { DoubleColumnVector colVector = (DoubleColumnVector) batch.cols[columnIndex]; if (deserializeRead.readCheckNull()) { VectorizedBatchUtil.setNullColIsNullValue(colVector, batchIndex); } else { float value = deserializeRead.readFloat(); colVector.vector[batchIndex] = (double) value; } } } private class DoubleCopyRow extends CopyRow { DoubleCopyRow(int inColumnIndex, int outColumnIndex) { super(inColumnIndex, outColumnIndex); } @Override void copy(VectorizedRowBatch inBatch, int inBatchIndex, VectorizedRowBatch outBatch, int outBatchIndex) { DoubleColumnVector inColVector = (DoubleColumnVector) inBatch.cols[inColumnIndex]; DoubleColumnVector outColVector = (DoubleColumnVector) outBatch.cols[outColumnIndex]; if (inColVector.isRepeating) { if (inColVector.noNulls || !inColVector.isNull[0]) { outColVector.vector[outBatchIndex] = inColVector.vector[0]; } else { VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex); } } else { if (inColVector.noNulls || !inColVector.isNull[inBatchIndex]) { outColVector.vector[outBatchIndex] = inColVector.vector[inBatchIndex]; } else { VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex); } } } } private static abstract class VectorDoubleColumnAssign extends VectorColumnAssignVectorBase&lt;DoubleColumnVector&gt; { protected void assignDouble(double value, int destIndex) { outCol.vector[destIndex] = value; } }The pattern to imitate would be the earlier code from VectorBatchUtil case DOUBLE: { DoubleColumnVector dcv = (DoubleColumnVector) batch.cols[offset + colIndex]; if (writableCol != null) { dcv.vector[rowIndex] = ((DoubleWritable) writableCol).get(); dcv.isNull[rowIndex] = false; } else { dcv.vector[rowIndex] = Double.NaN; setNullColIsNullValue(dcv, rowIndex); } } break;</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12828" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Spark version to 1.6</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12836" opendate="2016-1-11 00:00:00" fixdate="2016-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Install wget &amp; curl packages on LXC containers for HMS upgrade tests</summary>
      <description>Install wget &amp; curl packages on LXC containers for HMS upgrade tests.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="12864" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverflowError parsing queries with very large predicates</summary>
      <description>We have seen that queries with very large predicates might fail with the following stacktrace:016-01-12 05:47:36,516|beaver.machine|INFO|552|5072|Thread-22|Exception in thread "main" java.lang.StackOverflowError2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:145)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:36,634|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)2016-01-12 05:47:37,582|main|INFO|552|4568|MainThread|TEST "test_WideQuery" FAILED in 10.95 secondsThe problem could be solved by reimplementing some of the parsing methods so they are iterative instead of recursive.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="12880" opendate="2016-1-15 00:00:00" fixdate="2016-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark-assembly causes Hive class version problems</summary>
      <description>It looks like spark-assembly contains versions of Hive classes (e.g. HiveConf), and these sometimes (always?) come from older versions of Hive.We've seen problems where depending on classpath perturbations, NoSuchField errors may be thrown for recently added ConfVars because the HiveConf class comes from spark-assembly.Would making sure spark-assembly comes last in the classpath solve the problem?Otherwise, can we depend on something that does not package older Hive classes?Currently, HIVE-12179 provides a workaround (in non-Spark use case, at least; I am assuming this issue can also affect Hive-on-Spark).</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="12889" opendate="2016-1-19 00:00:00" fixdate="2016-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support COUNT(DISTINCT) for partitioning query.</summary>
      <description>We need to support avg(distinct), count(distinct), sum(distinct) for the parent jira HIVE-9534. Separate the work for count(distinct) in this subtask.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlCountAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="129" opendate="2008-12-6 00:00:00" fixdate="2008-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix aux.jar packaging to work properly with 0.17 and 0.18 versions of hadoop</summary>
      <description>ant -lib testlibs -Dhadoop.version="0.17" clean-test testleads to failures ininput16.qinput16_cc.qinput3.qas TestSerDe cannot be located.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.gen-php.ThriftHive.php</file>
      <file type="M">service.src.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1290" opendate="2010-4-2 00:00:00" fixdate="2010-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sort merge join does not work with bucketizedhiveinputformat</summary>
      <description>The mappers are assigned in the order of the sizes of the files which violates the output bucketing of the result of sort merge join</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.7.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.BucketMatcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12911" opendate="2016-1-22 00:00:00" fixdate="2016-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD might get exercised even when flag is false if CBO is on</summary>
      <description>Introduced in HIVE-11865.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12915" opendate="2016-1-23 00:00:00" fixdate="2016-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez session pool has concurrency issues during init</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12933" opendate="2016-1-26 00:00:00" fixdate="2016-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline will hang when authenticating with PAM when libjpam.so is missing</summary>
      <description>When we setup PAM authentication, we need to have libjpam.so under java.library.path. If it happens to misplace the .so file, rather than giving an exception, the client will hang forever.Seems we should catch the exception when the lib is missing.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.PamAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12934" opendate="2016-1-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor llap module structure to allow for a usable client</summary>
      <description>The client isn't really usable at the moment, and all of the code resides in the llap-server module. Restructure this so that the daemon execution code and cache code remains in server, common components move to a different module and relevant client pieces sit in the client module.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.tez.dag.app.rm.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapDaemonProtocolClientProxy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.ContainerFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapUmbilicalPolicyProvider.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapDaemonProtocolClientProxy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapContainerLauncher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.Converters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapServerSecurityInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapDaemonPolicyProvider.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.LlapNodeId.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.LlapManagementProtocolBlockingPB.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.LlapDaemonProtocolBlockingPB.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapManagementProtocolClientImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenProvider.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12946" opendate="2016-1-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table should also add default scheme and authority for the location similar to create table</summary>
      <description>When you call create table command (create table ... location '/abc') and check the database, you will notice that the table location has the scheme. While alter table set location '/abc' would throw an error "{0} is not absolute or has no scheme information. Please specify a complete absolute uri with scheme information. /abc".Seems it should be consistent.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="12958" opendate="2016-1-28 00:00:00" fixdate="2016-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make embedded Jetty server more configurable</summary>
      <description>Currently you can't configure embedded jetty within HCatalog. Propose to support an xml configuration which Jetty already supports. A new Web-hcat property will be added to specify the configure file location. If the file doesn't exist, falls back to old behavior. If it exists, such configuration will be loaded to configure embedded Jetty server. Some default parameters for Jetty may not be sufficient for some cases such as request/response buffer size. This improvement allows to make such change.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12964" opendate="2016-1-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestOperationLoggingAPIWithMr,TestOperationLoggingAPIWithTez fail on branch-2.0 (with Java 7, at least)</summary>
      <description></description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug id="12965" opendate="2016-1-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite local directory should perserve the overwritten directory permission</summary>
      <description>In Hive, "insert overwrite local directory" first deletes the overwritten directory if exists, recreate a new one, then copy the files from src directory to the new local directory. This process sometimes changes the permissions of the to-be-overwritten local directory, therefore causing some applications no more to be able to access its content.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="12993" opendate="2016-2-3 00:00:00" fixdate="2016-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>user and password supplied from URL is overwritten by the empty user and password of the JDBC connection string when it&amp;#39;s calling from beeline</summary>
      <description>When we make the call beeline -u "jdbc:hive2://localhost:10000/;user=aaa;password=bbb", the user and password are overwritten by the blank ones since internally it constructs a "connect &lt;url&gt; '' '' &lt;driver&gt;" call with empty user and password.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="12995" opendate="2016-2-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Synthetic file ids need collision checks</summary>
      <description>LLAP synthetic file ids do not have any way of checking whether a collision occurs other than a data-error.Synthetic file-ids have only been used with unit tests so far - but they will be needed to add cache mechanisms to non-HDFS filesystems.In case of Synthetic file-ids, it is recommended that we track the full-tuple (path, mtime, len) in the cache so that a cache-hit for the synthetic file-id can be compared against the parameters &amp; only accepted if those match.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DataCache.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.OrcCacheKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.FileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.NoopCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.Cache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1300" opendate="2010-4-12 00:00:00" fixdate="2010-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support: alter table touch partition</summary>
      <description>In some cases, the user wants to touch a partition, since some other operations might be performed on the hdfs directories.Currently, there is no way to do that.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13015" opendate="2016-2-5 00:00:00" fixdate="2016-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle Log4j2 jars with hive-exec</summary>
      <description>In some of the recent test runs, we are seeing multiple bindings for SLF4j that causes issues with LOG4j2 logger. SLF4J: Found binding in [jar:file:/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1454694331819_0001/container_e06_1454694331819_0001_01_000002/app/install/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]We have added explicit exclusions for slf4j-log4j12 but some library is pulling it transitively and it's getting packaged with hive libs. Also hive currently uses version 1.7.5 for slf4j. We should add dependency convergence for sl4fj and also remove packaging of slf4j-log4j12.*.jar</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13027" opendate="2016-2-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configuration changes to improve logging performance</summary>
      <description>For LLAP and HS2, some configuration changes can be made to improve logging performance1) LOG4j2's async logger claims to have 6-68 times better performance than synchronous logger. https://logging.apache.org/log4j/2.x/manual/async.html2) Replace File appenders with RandomAccessFileAppender that claims to be 20-200% more performant.https://logging.apache.org/log4j/2.x/manual/appenders.html#RandomAccessFileAppenderAlso make async logging configurable.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.tez-container-log4j2.properties</file>
      <file type="M">ql.src.main.resources.hive-exec-log4j2.properties</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-server.src.test.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.main.resources.llap-cli-log4j2.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.bin.llapDaemon.sh</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13029" opendate="2016-2-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NVDIMM support for LLAP Cache</summary>
      <description>LLAP cache has been designed so that the cache can be offloaded easily to a pmem API without restart coherence.The tricky part about NVDIMMs are restart coherence, while most of the cache gains can be obtained without keeping state across refreshes, since LLAP is not the system of record, HDFS is.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Validator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13032" opendate="2016-2-9 00:00:00" fixdate="2016-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive services need HADOOP_CLIENT_OPTS for proper log4j2 initialization</summary>
      <description>HIVE-12497 removed HADOOP_CLIENT_OPTS as it slowed down cli launch time. But it leads to log4j2 not being initialized when using services other than CLI. Other services like metastore, schematool etc. rely on log4j to initialize the logging based on the presence of log4j2.properties file in the classpath. If we use the standard name for log4j configuration file (log4j2.properties) then automatic initialization will happen. If not, we have to tell log4j to look for specific properties file. This is done via -Dlog4j.configurationFile system property. If we pass this system property via HADOOP_CLIENT_OPTS then all hive services will have logging initialized properly. In HIVE-12497, the problem was we had HADOOP_CLIENT_OPTS at the top of the script. As a result, hadoop and hbase commands tries to initialize logging which took long time slowing down the startup time.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.schemaTool.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13034" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add jdeb plugin to build debian</summary>
      <description>It would be nice to also generate a debian as a part of build. This can be done by adding jdeb plugin to dist profile.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13036" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split hive.root.logger separately to make it compatible with log4j1.x (for remaining services)</summary>
      <description>Similar to HIVE-12402 but for HS2 and metastore this time.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.CommonCliOptions.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13043" opendate="2016-2-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reload function has no impact to function registry</summary>
      <description>With HIVE-2573, users should run "reload function" to refresh cached function registry. However, "reload function" has no impact at all. We need to fix this. Otherwise, HS2 needs to be restarted to see new global functions.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13044" opendate="2016-2-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable TLS encryption to HMS backend database</summary>
      <description>When the database like mysql enables TLS/SSL encryption, we should provide some configuration properties like the ones to HS2 to enable that. Right now, I think we can enable that through javaopts and connection url.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13045" opendate="2016-2-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move guava dependency back to 14 after HIVE-12952</summary>
      <description>HIVE-12952 removed usage of EvictingQueue, so we don't need to up dependency to guava 15 at this point - avoid version related conflicts with clients if we can avoid it.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1305" opendate="2010-4-13 00:00:00" fixdate="2010-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add progress in join and groupby</summary>
      <description>The operators join and groupby can consume a lot of rows before producing any output. All operators which do not have a output for every input should report progress periodically.Currently, it is only being done for ScriptOperator and FilterOperator.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13054" opendate="2016-2-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable permanent fns by default (for now)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13068" opendate="2016-2-17 00:00:00" fixdate="2016-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable Hive ConstantPropagate optimizer when CBO has optimized the plan II</summary>
      <description>After HIVE-12543 went in, we need follow-up work to disable the last call to ConstantPropagate in Hive. This probably implies work on extending the constant folding logic in Calcite.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.number.compare.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionall.unbalancedppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.partition.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.recursive.dir.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.nonacid.from.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.folder.predicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.join.view.q</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.excludeHadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.const.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.semijoin.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13069" opendate="2016-2-17 00:00:00" fixdate="2016-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable cartesian product merging</summary>
      <description>Currently we can merge 2-way joins into n-way joins when the joins are executed over the same column.In turn, CBO might produce plans containing cartesian products if the join columns are constant values; after HIVE-12543 went in, this is rather common, as those constant columns are correctly pruned. However, currently we do not merge a cartesian product with two inputs into a cartesian product with multiple inputs, which could result in performance loss.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13070" opendate="2016-2-17 00:00:00" fixdate="2016-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Precommit HMS tests should run in addition to precommit normal tests, not instead of</summary>
      <description>When a certain patch makes changes in the metastore upgrade scripts folder, precommit HMS tests are triggered. The problem is that precommit HMS marks the patch as tested, thus normal precommit tests are never triggered.I hit the issue while testing HIVE-12994.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13077" opendate="2016-2-17 00:00:00" fixdate="2016-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Scrub daemon-site.xml from client configs</summary>
      <description>if (llapMode) { // add configs for llap-daemon-site.xml + localize llap jars // they cannot be referred to directly as it would be a circular depedency conf.addResource("llap-daemon-site.xml");</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="13079" opendate="2016-2-18 00:00:00" fixdate="2016-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Allow reading log4j properties from default JAR resources</summary>
      <description>If the log4j2 configuration is not overriden by the user, the Slider pkg creation fails since the config is generated from a URL.Allow for the .properties file to be created from default JAR resources if user provides no overrides.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13086" opendate="2016-2-18 00:00:00" fixdate="2016-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Programmatically initialize log4j2 to print out the properties location</summary>
      <description>In some cases, llap daemon gets initialized with different log4j2.properties than the expected llap-daemon-log4j2.properties. It will be easier if programmatically configure log4j2 so that we can print out the location of properties file that is used for initialization.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13087" opendate="2016-2-18 00:00:00" fixdate="2016-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Print STW pause time and useful application time</summary>
      <description>The daemons currently prints GC details. It will be useful to print the total useful time application spent and the total time for which application threads are stopped.Need to add-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTimeto get something likeApplication time: 0.3440086 secondsTotal time for which application threads were stopped: 0.0620105 secondsApplication time: 0.2100691 secondsTotal time for which application threads were stopped: 0.0890223 secondsReference: https://plumbr.eu/blog/performance-blog/logging-stop-the-world-pauses-in-jvmNO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13095" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support view column authorization</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13096" opendate="2016-2-19 00:00:00" fixdate="2016-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cost to choose side table in MapJoin conversion based on cumulative cardinality</summary>
      <description>HIVE-11954 changed the logic to choose the side table in the MapJoin conversion algorithm. Initial heuristic for the cost was based on number of heavyweight operators.This extends that work so the heuristic is based on accumulate cardinality. In the future, we should choose the side based on total latency for the input.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="13097" opendate="2016-2-19 00:00:00" fixdate="2016-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Umbrella] Changes dependent on Tez 0.8.3</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13099" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-SQLOperations lead to Web UI NPE</summary>
      <description>To support display of live operations in the WebUI, we record SQLOperations (in liveSqlOperations). However, to support historic operations, we save all operations in historicSqlOperations, including non-SQLOperations which do not have display entries in liveSqlOperations.This leads to a race condition depending on whether sessions use non-sql operations. Reproduce-able by issuing a 'set' operation.java.lang.NullPointerException at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:131) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)We should save only SQLOperations in historicSqlOperations.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="131" opendate="2008-12-7 00:00:00" fixdate="2008-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert overwrite directory leaves behind uncommitted/tmp files from failed tasks</summary>
      <description>_tmp files are getting left behind on insert overwrite directory:/user/jssarma/ctst1/40422_m_000195_0.deflate &lt;r 3&gt; 13285 2008-12-07 01:47 rw-r-r- jssarma supergroup/user/jssarma/ctst1/40422_m_000196_0.deflate &lt;r 3&gt; 3055 2008-12-07 01:46 rw-r-r- jssarma supergroup/user/jssarma/ctst1/_tmp.40422_m_000033_0 &lt;r 3&gt; 0 2008-12-07 01:53 rw-r-r- jssarma supergroup/user/jssarma/ctst1/_tmp.40422_m_000037_1 &lt;r 3&gt; 0 2008-12-07 01:53 rw-r-r- jssarma supergroupthis happened with speculative execution. the code looks good (in fact in this case many speculative tasks were launched - and only a couple caused problems). Almost seems like these files did not appear in the namespace until after the map-reduce job finished and the movetask did a listing of the output dir ..</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1310" opendate="2010-4-14 00:00:00" fixdate="2010-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioning columns should be of primitive types only</summary>
      <description>If the user specify the partitioning column as complex type (e.g., array, map) we should throw an error in semantic analyzer.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="13100" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-13015 that bundles log4j2 jars in hive-exec.jar</summary>
      <description>HIVE-13015 bundles log4j2 jars in hive-exec. This is causing other systems like tez and pig that uses log4j1.x to fail initialization due to the incompatibility in properties based configuration.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13102" opendate="2016-2-19 00:00:00" fixdate="2016-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Reduce operations in Calcite do not fold as tight as rule-based folding</summary>
      <description>With CBOcreate temporary table table1(id int, val int, val1 int, dimid int);create temporary table table3(id int, val int, val1 int);hive&gt; explain select table1.id, table1.val, table1.val1 from table1 inner join table3 on table1.dimid = table3.id and table3.id = 1 where table1.dimid &lt;&gt;1 ;Warning: Map Join MAPJOIN[14][bigTable=?] in task 'Map 1' is a cross productOKPlan optimized by CBO.Vertex dependency in root stageMap 1 &lt;- Map 2 (BROADCAST_EDGE)Stage-0 Fetch Operator limit:-1 Stage-1 Map 1 llap File Output Operator [FS_11] Map Join Operator [MAPJOIN_14] (rows=1 width=0) Conds:(Inner),Output:["_col0","_col1","_col2"] &lt;-Map 2 [BROADCAST_EDGE] llap BROADCAST [RS_8] Select Operator [SEL_5] (rows=1 width=0) Filter Operator [FIL_13] (rows=1 width=0) predicate:(id = 1) TableScan [TS_3] (rows=1 width=0) default@table3,table3,Tbl:PARTIAL,Col:NONE,Output:["id"] &lt;-Select Operator [SEL_2] (rows=1 width=0) Output:["_col0","_col1","_col2"] Filter Operator [FIL_12] (rows=1 width=0) predicate:((dimid = 1) and (dimid &lt;&gt; 1)) TableScan [TS_0] (rows=1 width=0) default@table1,table1,Tbl:PARTIAL,Col:NONE,Output:["id","val","val1","dimid"]without CBOhive&gt; explain select table1.id, table1.val, table1.val1 from table1 inner join table3 on table1.dimid = table3.id and table3.id = 1 where table1.dimid &lt;&gt;1 ;OKVertex dependency in root stageMap 1 &lt;- Map 2 (BROADCAST_EDGE)Stage-0 Fetch Operator limit:-1 Stage-1 Map 1 llap File Output Operator [FS_9] Map Join Operator [MAPJOIN_14] (rows=1 width=0) Conds:FIL_12.1=RS_17.1(Inner),Output:["_col0","_col1","_col2"] &lt;-Map 2 [BROADCAST_EDGE] vectorized, llap BROADCAST [RS_17] PartitionCols:1 Filter Operator [FIL_16] (rows=1 width=0) predicate:false TableScan [TS_1] (rows=1 width=0) default@table3,table3,Tbl:PARTIAL,Col:COMPLETE &lt;-Filter Operator [FIL_12] (rows=1 width=0) predicate:false TableScan [TS_0] (rows=1 width=0) default@table1,table1,Tbl:PARTIAL,Col:NONE,Output:["id","val","val1"]Time taken: 0.044 seconds, Fetched: 23 row(s)</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13107" opendate="2016-2-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Rotate GC logs periodically to prevent full disks</summary>
      <description>STDOUT cannot be rotated easily, so log GC logs to a different file and rotate periodically with -XX:+UseGCLogFileRotationNO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13110" opendate="2016-2-20 00:00:00" fixdate="2016-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Package log4j2 jars into Slider pkg</summary>
      <description>This forms the alternative path for HIVE-13015 (reverted), so that HIVE-13027 can pick up the right logger impl always.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13111" opendate="2016-2-22 00:00:00" fixdate="2016-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix timestamp / interval_day_time wrong results with HIVE-9862</summary>
      <description>Fix timestamp / interval_day_time issues discovered when testing the Vectorized Text patch.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestPisaTimestamp.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.RandomTypeUtil.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.PisaTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.DateTimeMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateScalarSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalDayTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastMillisecondsLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarSampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarPopTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdSampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdPopTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgTimestamp.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticDateColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.DateUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterIntervalDayTimeColumnCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterIntervalDayTimeColumnCompareIntervalDayTimeScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterIntervalDayTimeScalarCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalDayTimeColumnCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalDayTimeColumnCompareIntervalDayTimeScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalDayTimeScalarCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampScalarBase.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13122" opendate="2016-2-23 00:00:00" fixdate="2016-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: simple Model/View separation for UI</summary>
      <description>The current LLAP UI in master uses a fixed loop to both extract data and to display it in the same loop.Split this up into a model-view, for modularity.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.metrics.js</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.index.html</file>
    </fixedFiles>
  </bug>
  <bug id="13130" opendate="2016-2-24 00:00:00" fixdate="2016-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 changes : API calls for retrieving primary keys and foreign keys information</summary>
      <description>ODBC exposes the SQLPrimaryKeys and SQLForeignKeys API calls and JDBC exposes getPrimaryKeys and getCrossReference API calls. We need to provide these interfaces as part of PK/FK implementation in Hive.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ICLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.EmbeddedCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.TCLIService.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.TCLIService-remote</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.TCLIService.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TCLIService.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.server.skeleton.cpp</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13135" opendate="2016-2-24 00:00:00" fixdate="2016-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: HTTPS Webservices needs trusted keystore configs</summary>
      <description>ssl-server.xml is not picked up internally by the hive-common HttpServer impl, unlike the default configs.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebApp.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13153" opendate="2016-2-25 00:00:00" fixdate="2016-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SessionID is appended to thread name twice</summary>
      <description>HIVE-12249 added sessionId to thread name. In some cases the sessionId could be appended twice. Example log lineDEBUG [6432ec22-9f66-4fa5-8770-488a9d3f0b61 6432ec22-9f66-4fa5-8770-488a9d3f0b61 main]</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13156" opendate="2016-2-25 00:00:00" fixdate="2016-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow specifying the name of the queue in which llap will run</summary>
      <description>llap service driver should accept a parameter for the llap queue name.cc gopalv</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13178" opendate="2016-2-28 00:00:00" fixdate="2016-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance ORC Schema Evolution to handle more standard data type conversions</summary>
      <description>Currently, SHORT -&gt; INT -&gt; BIGINT is supported.Handle ORC data type conversions permitted by Implicit conversion allowed by TypeIntoUtils.implicitConvertible method. STRING_GROUP -&gt; DOUBLE STRING_GROUP -&gt; DECIMAL DATE_GROUP -&gt; STRING NUMERIC_GROUP -&gt; STRING STRING_GROUP -&gt; STRING_GROUP * // Upward from "lower" type to "higher" numeric type: BYTE -&gt; SHORT -&gt; INT -&gt; BIGINT -&gt; FLOAT -&gt; DOUBLE -&gt; DECIMAL</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.orc.type.promotion3.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns2.acid.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns3.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.replace.columns3.acid.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion1.acid.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion2.acid.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion3.q</file>
      <file type="M">ql.src.test.queries.clientnegative.orc.type.promotion3.acid.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.int.type.promotion.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.schema.evolution.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.results.clientnegative.orc.replace.columns2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.replace.columns2.acid.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.replace.columns3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.replace.columns3.acid.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.type.promotion1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.type.promotion1.acid.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.type.promotion2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.type.promotion2.acid.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orc.type.promotion3.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13185" opendate="2016-2-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>orc.ReaderImp.ensureOrcFooter() method fails on small text files with IndexOutOfBoundsException</summary>
      <description>Steps to reproduce:1. Create a Text source table with one line of data:create table src (id int);insert overwrite table src values (1);2. Create a target table:create table trg (id int);3. Try to load small text file to the target table:load data inpath 'user/hive/warehouse/src/000000_0' into table trg;Error message:FAILED: SemanticException Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsExceptionStack trace:org.apache.hadoop.hive.ql.parse.SemanticException: Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.ensureFileFormatsMatch(LoadSemanticAnalyzer.java:340) at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:224) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:242) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:481) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:317) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1190) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1285) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1104)...</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13191" opendate="2016-3-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DummyTable map joins mix up columns between tables</summary>
      <description>SELECT a.key, a.a_one, b.b_one, a.a_zero, b.b_zeroFROM( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero) aLEFT JOIN( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero) bON a.key = b.key;11 1 0 0 1This should be 11, 1, 1, 0, 0 instead. Disabling map-joins &amp; using shuffle-joins returns the right result.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mapjoin2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13199" opendate="2016-3-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NDC stopped working in LLAP logging</summary>
      <description>NDC context were missing from the log lines. Reason for it is NDC class is part of log4j-1.2-api (bridge jar). This is added as compile time dependency. Due to the absence of this jar in llap daemons, the NDC context failed to initialize. Log4j2 replaced NDC with ThreadContext. Hence we need the bridge jar.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13204" opendate="2016-3-3 00:00:00" fixdate="2016-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Add ChainedCheckerFactory for LIKE</summary>
      <description>Currently, Vectorization runs through a UTF-8 decode to produce a String &amp; then check for mildly complex patterns like "http://%.exe" using a Regex.Since this pattern doesn't need any backtracking patterns, using a full fledged Regex is too expensive.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
    </fixedFiles>
  </bug>
  <bug id="13226" opendate="2016-3-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve tez print summary to print query execution breakdown</summary>
      <description>When tez print summary is enabled, methods summary is printed which are difficult to correlate with the actual execution time. We can improve that to print the execution times in the sequence of operations that happens behind the scenes.Instead of printing the methods name it will be useful to print something like below1) Query Compilation time2) Query Submit to DAG Submit time3) DAG Submit to DAG Accept time4) DAG Accept to DAG Start time5) DAG Start to DAG End timeWith this it will be easier to find out where the actual time is spent.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="13227" opendate="2016-3-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Change daemon initialization logs from INFO to WARN</summary>
      <description>In production LLAP is typically run with WARN log level. It will be useful to print the llap daemon initialization configs at WARN level instead of INFO level so that we can verify if daemon configs are propagated properly. NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="13233" opendate="2016-3-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use min and max values to estimate better stats for comparison operators</summary>
      <description>We should benefit from the min/max values for each column to calculate more precisely the number of rows produced by expressions with comparison operators</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.filter.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13242" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DISTINCT keyword is dropped by the parser for windowing</summary>
      <description>To reproduce, the following query can be used:select distinct first_value(t) over ( partition by si order by i, b ) from over10k limit 100;The distinct keyword is ignored and duplicates are produced.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
    </fixedFiles>
  </bug>
  <bug id="13246" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add log line to ORC writer to print out the file path</summary>
      <description>Currently ORC writer does not log anything making it difficult find where the destination path is. NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13248" opendate="2016-3-9 00:00:00" fixdate="2016-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change date_add/date_sub/to_date functions to return Date type rather than String</summary>
      <description>Some of the original "date" related functions return string values rather than Date values, because they were created before the Date type existed in Hive. We can try to change these to return Date in the 2.x line.Date values should be implicitly convertible to String.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.offcbo.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
    </fixedFiles>
  </bug>
  <bug id="13251" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive can&amp;#39;t read the decimal in AVRO file generated from previous version</summary>
      <description>HIVE-7174 makes the avro schema change to match avro definition, while it breaks the compatibility if the file is generated from the previous Hive although the file schema from the file for such decimal is not correct based on avro definition. We should allow to read old file format "precision" : "4", "scale": "8", but when we write, we should write in the new format.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13255" opendate="2016-3-10 00:00:00" fixdate="2016-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FloatTreeReader.nextVector is expensive</summary>
      <description>Some TPCDS queries on 1TB scale shows FloatTreeReader on profile samples. It is most likely because of multiple branching and polymorphic dispatch in FloatTreeReader.nextVector() implementation. See attached image for sampling profile output.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestSerializationUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.SerializationUtils.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerReaderV2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.RunLengthIntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.IntegerReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="13258" opendate="2016-3-10 00:00:00" fixdate="2016-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add hdfs bytes read and spilled bytes to tez print summary</summary>
      <description>When printing counters to console it will be useful to print hdfs bytes read and spilled bytes which will help with debugging issues faster.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.counters.LlapIOCounters.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13263" opendate="2016-3-11 00:00:00" fixdate="2016-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Unable to vectorize regexp_extract/regexp_replace " Udf: GenericUDFBridge, is not supported"</summary>
      <description>Add regexp_extract to the UDFs we bridge to.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13267" opendate="2016-3-11 00:00:00" fixdate="2016-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Add SelectLikeStringColScalar for non-filter operations</summary>
      <description>FilterStringColLikeStringScalar only applies to the values within filter clauses.Borrow the Checker impls and extend to the value generation - for cases likeselect col is null or col like '%(null)%'</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
    </fixedFiles>
  </bug>
  <bug id="13269" opendate="2016-3-11 00:00:00" fixdate="2016-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simplify comparison expressions using column stats</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">data.conf.perf-reg.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13283" opendate="2016-3-14 00:00:00" fixdate="2016-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: make sure IO elevator is enabled by default in the daemons</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13285" opendate="2016-3-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc concatenation may drop old files from moving to final path</summary>
      <description>ORC concatenation uses combine hive input format for merging files. Under specific case where all files within a combine split are incompatible for merge (old files without stripe statistics) then these files are added to incompatible file set. But this file set is not processed as closeOp() will not be called (no output file writer will exist which will skip super.closeOp()). As a result, these incompatible files are not moved to final path.</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13287" opendate="2016-3-15 00:00:00" fixdate="2016-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logic to estimate stats for IN operator</summary>
      <description>Currently, IN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13291" opendate="2016-3-16 00:00:00" fixdate="2016-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC BI Split strategy should consider block size instead of file size</summary>
      <description>When we force split strategy to use "BI" (using hive.exec.orc.split.strategy), entire file is considered as single split. This might be inefficient when the files are large. Instead, BI should consider splitting at block boundary.</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="13320" opendate="2016-3-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
      <description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
      <version>1.2.1,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
    </fixedFiles>
  </bug>
  <bug id="13324" opendate="2016-3-21 00:00:00" fixdate="2016-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: history log for FRAGMENT_START doesn&amp;#39;t log DagId correctly</summary>
      <description>$ grep -B 1 "TaskId=212" history.log Event=FRAGMENT_START, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000213, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=0, VertexName=Map 1, TaskId=212, TaskAttemptId=0, SubmitTime=1457493007357--Event=FRAGMENT_END, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000213, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=2, VertexName=Map 1, TaskId=212, TaskAttemptId=0, ThreadName=Task-Executor-1, Succeeded=true, StartTime=1457493007358, EndTime=1457493011916--Event=FRAGMENT_START, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000434, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=0, VertexName=Map 1, TaskId=212, TaskAttemptId=0, SubmitTime=1457493023131--Event=FRAGMENT_END, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000434, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=3, VertexName=Map 1, TaskId=212, TaskAttemptId=0, ThreadName=Task-Executor-2, Succeeded=true, StartTime=1457493023132, EndTime=1457493024695etc. It's always 0.</description>
      <version>None</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
    </fixedFiles>
  </bug>
  <bug id="13327" opendate="2016-3-22 00:00:00" fixdate="2016-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SessionID added to HS2 threadname does not trim spaces</summary>
      <description>HIVE-13153 introduced off-by-one in appending spaces to thread names. NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="13330" opendate="2016-3-22 00:00:00" fixdate="2016-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC vectorized string dictionary reader does not differentiate null vs empty string dictionary</summary>
      <description>Vectorized string dictionary reader cannot differentiate between the case where all dictionary entries are null vs single entry with empty string. This causes wrong results when reading data out of such files. Vectorization OnSET hive.vectorized.execution.enabled=true;SET hive.fetch.task.conversion=none;select vcol from testnullorc3 limit 1;OKNULLVectorization OffSET hive.vectorized.execution.enabled=false;SET hive.fetch.task.conversion=none;select vcol from testnullorc3 limit 1;OKThe input table testnullorc3 contains a varchar column vcol with few empty strings and few nulls. For this table, non vectorized reader returns empty as first row but vectorized reader returns NULL.</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13340" opendate="2016-3-23 00:00:00" fixdate="2016-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: from_unixtime UDF shim</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13342" opendate="2016-3-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging in llap decider and throw exception in case llap mode is all but we cannot run in llap.</summary>
      <description>Currently we do not log our decisions with respect to llap. Are we running everything in llap mode or only parts of the plan. We need more logging. Also, if llap mode is all but for some reason, we cannot run the work in llap mode, fail and throw an exception advise the user to change the mode to auto.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.udf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.llap.udf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="13343" opendate="2016-3-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash join</summary>
      <description>Due to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.hybridgrace.hashjoin.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13361" opendate="2016-3-24 00:00:00" fixdate="2016-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc concatenation should enforce the compression buffer size</summary>
      <description>With HIVE-11807 buffer size estimation happens by default. This can have undesired effect wrt file concatenation. Consider the following table with filestesttable -- 000000_0 (created before HIVE-11807 which has buffer size 256KB) -- 000001_0 (created before HIVE-11807 which has buffer size 256KB) -- 000002_0 (created after HIVE-11807 with buffer size chosen as 128KB) -- 000003_0 (created after HIVE-11807 with buffer size chosen as 128KB)If we perform ALTER TABLE .. CONCATENATE on the above table with HIVE-11807, then depending on the split arrangement 000000_0 and 000001_0 will be concatenated together to new merged file. But this new merged file will have 128KB buffer size (estimated buffer size and not requested buffer size). Since new ORC writer size does not honor the requested buffer size the new merged files will have smaller buffers than the required 256KB making the file unreadable. Following exception will be thrown when reading the table after concatenation2016-03-24T16:26:33,974 ERROR [a9e27a9a-37cb-411d-9708-6c58a4ce34f2 main]: CliDriver (SessionState.java:printError(1049)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187java.io.IOException: java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187 at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:513) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:420) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:145) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1848) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:256) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13364" opendate="2016-3-26 00:00:00" fixdate="2016-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow llap to work with dynamic ports for rpc, shuffle, ui</summary>
      <description>At the moment - the ports specified in the configuration are the ones which are used to register with the Zookeeper service. Setting the ports to 0 effectively means that the services cannot be discovered.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13365" opendate="2016-3-26 00:00:00" fixdate="2016-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the MiniLLAPCluster to work with a MiniZKCluster</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.llap.LlapItUtils.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13367" opendate="2016-3-28 00:00:00" fixdate="2016-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extending HPLSQL parser</summary>
      <description>Extending HPL/SQL parser to support more procedural constructs.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.interval.out.txt</file>
      <file type="M">hplsql.src.test.results.local.if.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.procedure.no.params.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.package.out.txt</file>
      <file type="M">hplsql.src.test.results.db.rowtype.attribute.out.txt</file>
      <file type="M">hplsql.src.test.results.db.create.procedure.return.cursor2.out.txt</file>
      <file type="M">hplsql.src.test.results.db.create.procedure.return.cursor.out.txt</file>
      <file type="M">hplsql.src.test.results.db.create.procedure.mssql.out.txt</file>
      <file type="M">hplsql.src.test.queries.local.interval.sql</file>
      <file type="M">hplsql.src.test.queries.local.if.sql</file>
      <file type="M">hplsql.src.test.queries.db.schema.sql</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Row.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Package.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionString.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.File.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Converter.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conf.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="13389" opendate="2016-3-30 00:00:00" fixdate="2016-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external submission client ends up attempting to find an LLAP instance based on the submitting user instead of the hive user</summary>
      <description>In the LLAP external client, the LLAP Zookeeper registry creates the Zk path based on the current user, but the user specified here should really be the user running LLAP.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.LlapInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestLlapInputSplit.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13398" opendate="2016-3-31 00:00:00" fixdate="2016-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Simple /status and /peers web services</summary>
      <description>MiniLLAP doesn't have a UI service, so this has no easy tests.curl localhost:15002/status{ "status" : "STARTED", "uptime" : 139093, "build" : "2.1.0-SNAPSHOT from 77474581df4016e3899a986e079513087a945674 by gopal source checksum a9caa5faad5906d5139c33619f1368bb"}curl localhost:15002/peers{ "dynamic" : true, "identity" : "718264f1-722e-40f1-8265-ac25587bf336", "peers" : [ { "identity" : "940d6838-4dd7-4e85-95cc-5a6a2c537c04", "host" : "sandbox121.hortonworks.com", "management-port" : 15004, "rpc-port" : 15001, "shuffle-port" : 15551, "resource" : { "vcores" : 24, "memory" : 128000 }, "host" : "sandbox121.hortonworks.com" }, ]}</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13400" opendate="2016-3-31 00:00:00" fixdate="2016-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Following up HIVE-12481, add retry for Zookeeper service discovery</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="13401" opendate="2016-3-31 00:00:00" fixdate="2016-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Kerberized HS2 with LDAP auth enabled fails kerberos/delegation token authentication</summary>
      <description>When HS2 is running in kerberos cluster but with other Sasl authentication (e.g. LDAP) enabled, it fails in kerberos/delegation token authentication. It is because the HS2 server uses the TSetIpAddressProcess when other authentication is enabled.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.MiniHiveKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="13402" opendate="2016-3-31 00:00:00" fixdate="2016-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily disable failing spark tests</summary>
      <description>There's a bunch of tests which end up hanging causing the precommit builds to run forever.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13436" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow the package directory to be specified for the llap setup script</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13437" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>httpserver getPort does not return the actual port when attempting to use a dynamic port</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13438" opendate="2016-4-6 00:00:00" fixdate="2016-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a service check script for llap</summary>
      <description>We want to have a test script that can be run by an installer such as ambari that makes sure that the service is up and running.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13467" opendate="2016-4-9 00:00:00" fixdate="2016-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show llap info on hs2 ui when available</summary>
      <description>When llap is on and hs2 is configured with access to an llap cluster, HS2 UI should show some status of the daemons and provide a mechanism to click through to their respective UIs.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">LICENSE</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13469" opendate="2016-4-9 00:00:00" fixdate="2016-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Support delayed scheduling for locality</summary>
      <description>LLAP currently supports forcing locality. Change this to support a time based delay for locality as well.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.ContainerFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13472" opendate="2016-4-10 00:00:00" fixdate="2016-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace primitive wrapper&amp;#39;s valueOf method with parse* method to avoid unnecessary boxing/unboxing</summary>
      <description>There are lots of primitive wrapper's valueOf method which should be replaced with parseXX method.For example, Integer.valueOf(String) returns Integer type but Integer.parseInt(String) returns primitive int type so we can avoid unnecessary boxing/unboxing by replacing some of them.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.src.java.org.apache.hive.testutils.junit.runners.ConcurrentTestRunner.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.HiveSQLException.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.thrift.grammar.java</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part.max.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseImport.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Validator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="13475" opendate="2016-4-11 00:00:00" fixdate="2016-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow aggregate functions in over clause</summary>
      <description>Support to reference aggregate functions within the over clause needs to be added. For instance, currently the following query will fail:select rank() over (order by sum(ws.c_int)) as return_rankfrom cbo_t3 wsgroup by ws.key;</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13485" opendate="2016-4-11 00:00:00" fixdate="2016-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Session id appended to thread name multiple times.</summary>
      <description>HIVE-13153 addressed a portion of this issue. Follow up from there.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13487" opendate="2016-4-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Finish time is wrong when perflog is missing SUBMIT_TO_RUNNING</summary>
      <description>Sometimes PerfLog misses SUBMIT_TO_RUNNING end time which makes finish time to be wrong. Like belowCompile Query 0.60sPrepare Plan 0.44sSubmit Plan 0.83sStart 0.00sFinish 1460423234.44sNO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13488" opendate="2016-4-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore dag summary when tez exec print summary enabled and in-place updates disabled</summary>
      <description>Restore the old way of printing methods summary when file redirection is enabled. This may be used by some tools which will break because of the change introduced by HIVE-13226NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13511" opendate="2016-4-14 00:00:00" fixdate="2016-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run clidriver tests from within the qtest dir for the precommit tests</summary>
      <description>The tests are currently run from the itests directory - which means there's additional overhead of having to at least check whether files have changed. Will attach a sample output - this adds up to 40+ seconds per batch. Getting rid of this should be a reasonable saving overall.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestQFileTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.UnitTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.QFileTestBatch.java</file>
    </fixedFiles>
  </bug>
  <bug id="13516" opendate="2016-4-14 00:00:00" fixdate="2016-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding BTEQ .IF, .QUIT, ERRORCODE to HPL/SQL</summary>
      <description>Adding Teradata BTEQ features to HPL/SQL such as .IF, .QUIT, ERRORCODE.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.db2.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.pg.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.ora2.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.ora.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.mysql.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.mssql2.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.mssql.out.txt</file>
      <file type="M">hplsql.src.test.results.local.lang.out.txt</file>
      <file type="M">hplsql.src.test.results.db.select.into2.out.txt</file>
      <file type="M">hplsql.src.test.results.db.select.into.out.txt</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.resources.hplsql-site.xml</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Signal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="13536" opendate="2016-4-18 00:00:00" fixdate="2016-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add metrics for task scheduler</summary>
      <description>Currently there are no metrics for task scheduler. It will be useful to provide one.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonIOInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13537" opendate="2016-4-18 00:00:00" fixdate="2016-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update slf4j version to 1.7.10</summary>
      <description>Both Hadoop and Tez are on 1.7.10, and have been for a while. We should update hive to use this version as well. Should get rid of some of the noise in the logs related to multiple version.s</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13554" opendate="2016-4-20 00:00:00" fixdate="2016-1-20 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>[Umbrella] SQL:2011 compliance</summary>
      <description>There are various gaps in language which needs to be addressed to bring Hive under SQL:2011 compliance</description>
      <version>2.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.subquery.subquery.chain.exists.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.in.lhs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeSubQueryDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13555" opendate="2016-4-20 00:00:00" fixdate="2016-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add nullif udf</summary>
      <description>nullif(exp1, exp2) is shorthand for: case when exp1 = exp2 then null else exp1</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="13556" opendate="2016-4-20 00:00:00" fixdate="2016-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for double precision data type</summary>
      <description>Add support for DOUBLE PRECISION data type as an alias for DOUBLE datatype</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="13557" opendate="2016-4-20 00:00:00" fixdate="2016-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make interval keyword optional while specifying DAY in interval arithmetic</summary>
      <description>Currently we support expressions like: WHERE SOLD_DATE BETWEEN ((DATE('2000-01-31')) - INTERVAL '30' DAY) AND DATE('2000-01-31')We should support:WHERE SOLD_DATE BETWEEN ((DATE('2000-01-31')) + (-30) DAY) AND DATE('2000-01-31')</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="13559" opendate="2016-4-20 00:00:00" fixdate="2016-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass exception to failure hooks</summary>
      <description>Pass exception to failure hooks so that they know more about the failure.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="13563" opendate="2016-4-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Streaming does not honor orc.compress.size and orc.stripe.size table properties</summary>
      <description>According to the doc:https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-HiveQLSyntaxOne should be able to specify tblproperties for many ORC options.But the settings for orc.compress.size and orc.stripe.size don't take effect.</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.globallimit.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1357" opendate="2010-5-21 00:00:00" fixdate="2010-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineHiveInputSplit should initialize the inputFileFormat once for a single split</summary>
      <description>If a split consists of multiple files, the FileFormat should always be the same, whether RCFile or SequenceFile. Currently the CombineHiveInputSplit tries to get the inputFileFormat for each new file in the split, which is O where n is the number of files in the split. This is an O(n^2) operation and degrade the performance badly for combining large number of small files.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13591" opendate="2016-4-22 00:00:00" fixdate="2016-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSchemaTool is failing on master</summary>
      <description>Not sure at what point this started to fail.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.1.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="13592" opendate="2016-4-22 00:00:00" fixdate="2016-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore calls map is not thread safe</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="13596" opendate="2016-4-23 00:00:00" fixdate="2016-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 should be able to get UDFs on demand from metastore</summary>
      <description>When multiple HS2s are run, creating a permanent fn is only executed on one of them, and the other HS2s don't get the new function. Unlike say with tables, where we always get stuff from db on demand, fns are registered at certain points in the code and if the new one is not registered, it will not be available. We should restore the pre-HIVE-2573 behavior of being able to refresh the UDFs on demand.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13603" opendate="2016-4-25 00:00:00" fixdate="2016-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ptest unit tests broken by HIVE13505</summary>
      <description>HIVE-13505 broke some unit tests in the ptest2 framework, which need to be fixed.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13616" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate renaming a table without invalidating the column stats</summary>
      <description>Right now when we rename a table, we clear the column stats rather than updating it (HIVE-9720) since ObjectStore uses DN to talk to DB. Investigate the possibility that if we can achieve updating the stats without rescanning the whole table.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="13619" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket map join plan is incorrect</summary>
      <description>Same as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1362" opendate="2010-5-21 00:00:00" fixdate="2010-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer statistics on columns in tables and partitions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-php.queryplan.queryplan.types.php</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="13620" opendate="2016-4-27 00:00:00" fixdate="2016-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge llap branch work to master</summary>
      <description>Would like to try to merge the llap branch work for HIVE-12991 into the master branch.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.tez.dag.api.TaskSpecBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapDataOutputBuffer.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapRowInputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.SubmitWorkInfo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="13621" opendate="2016-4-27 00:00:00" fixdate="2016-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>compute stats in certain cases fails with NPE</summary>
      <description>FAILED: NullPointerException nulljava.lang.NullPointerException at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:693) at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:739) at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:728) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:183) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:136) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:124)</description>
      <version>2.0.1,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13628" opendate="2016-4-27 00:00:00" fixdate="2016-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for permanent functions - error handling if no restart</summary>
      <description>Support for permanent functions - error handling if no restart</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13637" opendate="2016-4-28 00:00:00" fixdate="2016-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fold CASE into NVL when CBO optimized the plan</summary>
      <description>After HIVE-13068 goes in, folding CASE into NVL got disabled when CBO has optimized the plan, as it was done by ConstantPropagate in Hive. We need to enable it back.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.constantPropWhen.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  <bug id="13638" opendate="2016-4-28 00:00:00" fixdate="2016-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO rule to pull up constants through Sort/Limit</summary>
      <description>After HIVE-13068 goes in, we need to pull up constants through Sort/Limit operator, as it was done previously by ConstantPropagate in Hive.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="13639" opendate="2016-4-28 00:00:00" fixdate="2016-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO rule to pull up constants through Union</summary>
      <description>After HIVE-13068 goes in, we need to pull up constants through Union operator, as it was done previously by ConstantPropagate in Hive.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.input26.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13645" opendate="2016-4-28 00:00:00" fixdate="2016-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline needs null-guard around hiveVars and hiveConfVars read</summary>
      <description>Beeline has a bug wherein if a user does a !save ever, then on next load, if beeline.hiveVariables or beeline.hiveconfvariables are empty, i.e. {} or unspecified, then it loads it as null, and then, on next connect, there is no null-check on these variables leading to an NPE.</description>
      <version>2.1.0</version>
      <fixedVersion>1.2.2,1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="13670" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Beeline connect/reconnect semantics</summary>
      <description>For most users of beeline, chances are that they will be using it with a single HS2 instance most of the time. In this scenario, having them type out a jdbc uri for HS2 every single time to !connect can get tiresome. Thus, we should improve semantics so that if a user does a successful !connect, then we must store the last-connected-to-url, so that if they do a !close, and then a !reconnect, then !reconnect should attempt to connect to the last successfully used url.Also, if they then do a !save, then that last-successfully-used url must be saved, so that in subsequent sessions, they can simply do !reconnect rather than specifying a url for !connect.In addition, it would be useful to introduce a new way of doing !connect that does involve typing out a jdbc url every time (since this is highly likely to be error-prone)</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="13671" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add PerfLogger to log4j2.properties logger</summary>
      <description>To enable perflogging, root logging has to be set to DEBUG. Provide a way to to independently configure perflogger and root logger levels.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13672" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use loginUser from UGI to get llap user when generating LLAP splits.</summary>
      <description>HIVE-13389 used RegistryUtils.currentUser() to get the llap user name when generating LLAP splits. However it looks like this will return the client username, while we really want to get the hive/llap user.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13674" opendate="2016-5-3 00:00:00" fixdate="2016-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>usingTezAm field not required in LLAP SubmitWorkRequestProto</summary>
      <description>From sseth during review of HIVE-13620, the usingTezAm field is not needed in SubmitWorkRequestProto.Also, SendEventsRequestProto/SendEventsResponseProto are not needed.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="13675" opendate="2016-5-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add HMAC signatures to LLAPIF splits</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSignerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SigningSecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapSigner.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.SubmitWorkInfo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapProxy.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13683" opendate="2016-5-3 00:00:00" fixdate="2016-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove erroneously included patch file</summary>
      <description>The commit for HIVE-13509 accidentally included a patch file in the source tree.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">HIVE-13509.2.patch</file>
    </fixedFiles>
  </bug>
  <bug id="13701" opendate="2016-5-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use different prefix for llap task scheduler metrics</summary>
      <description>LLAP task scheduler runs inside AM and typically runs on different hosts than llap daemons. Using the same prefix "llapdaemon" for daemon metrics and task scheduler metrics will cause conflicts when these metrics are published with hadoop-metrics2.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.main.resources.hadoop-metrics2.properties.template</file>
    </fixedFiles>
  </bug>
  <bug id="13705" opendate="2016-5-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into table removes existing data</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.META-INF.services.org.apache.hadoop.fs.FileSystem</file>
    </fixedFiles>
  </bug>
  <bug id="13722" opendate="2016-5-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add flag to detect constants to CBO pull up rules</summary>
      <description>Add flag to avoid firing CBO pull up constants rules indefinitely. This issue can be reproduced using e.g. union27.q, union_remove_19.q, unionDistinct_1.q.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="13730" opendate="2016-5-10 00:00:00" fixdate="2016-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid double spilling the same partition when memory threshold is set very low</summary>
      <description>I am seeing hybridgrace_hashjoin_1.q getting stuck on master.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13731" opendate="2016-5-10 00:00:00" fixdate="2016-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: return LLAP token with the splits</summary>
      <description>Need to return the token with the splits, then take it in LLAPIF and make sure it's used when talking to LLAP</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="13743" opendate="2016-5-12 00:00:00" fixdate="2016-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data move codepath is broken with hive (2.1.0-SNAPSHOT)</summary>
      <description>Data move codepath is broken with hive 2.1.0-SNAPSHOT with hadoop 2.8.0-snapshot.Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path not found: /apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1 at org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.getEZForPath(FSDirEncryptionZoneOp.java:178) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZForPath(FSNamesystem.java:7336) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEZForPath(NameNodeRpcServer.java:1973) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEZForPath(ClientNamenodeProtocolServerSideTranslatorPB.java:1376) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:645) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2339) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2335) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2333) at org.apache.hadoop.ipc.Client.call(Client.java:1448) at org.apache.hadoop.ipc.Client.call(Client.java:1385) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) at com.sun.proxy.$Proxy30.getEZForPath(Unknown Source)/apps/hive/warehouse/tpcds_bin_partitioned_orc_200.db/.........2016-05-11T09:40:43,760 ERROR [main]: ql.Driver (:()) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to move source hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/.hive-staging_hive_2016-05-11_09-40-42_489_5056654133706433454-1/-ext-10002 to destination hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1https://github.com/apache/hive/blob/26b5c7b56a4f28ce3eabc0207566cce46b29b558/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2836hdfsEncryptionShim.isPathEncrypted(destf) in Hive could end up throwing FileNotFoundException as the destf is not present yet. This causes moveFile to fail.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="13758" opendate="2016-5-13 00:00:00" fixdate="2016-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Create table like" command should initialize the basic stats for the table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13767" opendate="2016-5-16 00:00:00" fixdate="2016-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong type inferred in Semijoin condition leads to AssertionError</summary>
      <description>Following query fails to run:SELECT COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50), FLOOR(t1.double_col_16) DESC), 524) AS int_col, (t2.int_col_10) + (t1.smallint_col_50) AS int_col_1, FLOOR(t1.double_col_16) AS float_col, COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50) DESC, FLOOR(t1.double_col_16) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2FROM table_1 t1INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND ((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND ((t2.tinyint_col_20) = (t1.tinyint_col_3))WHERE (t2.smallint_col_19) IN (SELECT COALESCE(-92, -994) AS int_col FROM table_1 tt1 INNER JOIN table_18 tt2 ON (tt2.decimal1911_col_16) = (tt1.decimal2612_col_77) WHERE (t1.timestamp_col_9) = (tt2.timestamp_col_18));Following error is seen in the logs:2016-04-27T04:32:09,605 WARN [...2a24 HiveServer2-Handler-Pool: Thread-211]: thrift.ThriftCLIService (ThriftCLIService.java:ExecuteStatement(501)) - Error executing statement:org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.AssertionError: mismatched type $8 TIMESTAMP(9) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:178) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:216) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.cli.operation.Operation.run(Operation.java:327) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:458) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:435) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:272) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:492) [hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317) [hive-service-rpc-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302) [hive-service-rpc-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) [hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]Caused by: java.lang.AssertionError: mismatched type $8 TIMESTAMP(9) at org.apache.calcite.rex.RexUtil$FixNullabilityShuttle.visitInputRef(RexUtil.java:2042) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexUtil$FixNullabilityShuttle.visitInputRef(RexUtil.java:2020) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexShuttle.visitList(RexShuttle.java:144) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:93) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:36) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexCall.accept(RexCall.java:108) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:275) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexShuttle.mutate(RexShuttle.java:234) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:252) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rex.RexUtil.fixUp(RexUtil.java:1239) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.rel.rules.FilterJoinRule.perform(FilterJoinRule.java:232) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule$HiveFilterJoinMergeRule.onMatch(HiveFilterJoinRule.java:78) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule$HiveFilterJoinMergeRule.onMatch(HiveFilterJoinRule.java:78) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:514) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:285) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:72) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan(CalcitePlanner.java:1293) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:1166) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:956) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:887) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:113) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:969) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:149) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:106) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:706) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:274) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10642) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:233) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:476) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1191) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248] ... 15 moreHive DDL for supporting tables are:CREATE TABLE table_1 (timestamp_col_1 TIMESTAMP, decimal3003_col_2 DECIMAL(30, 3), tinyint_col_3 TINYINT, decimal0101_col_4 DECIMAL(1, 1), boolean_col_5 BOOLEAN, float_col_6 FLOAT, bigint_col_7 BIGINT, varchar0098_col_8 VARCHAR(98), timestamp_col_9 TIMESTAMP, bigint_col_10 BIGINT, decimal0903_col_11 DECIMAL(9, 3), timestamp_col_12 TIMESTAMP, timestamp_col_13 TIMESTAMP, float_col_14 FLOAT, char0254_col_15 CHAR(254), double_col_16 DOUBLE, timestamp_col_17 TIMESTAMP, boolean_col_18 BOOLEAN, decimal2608_col_19 DECIMAL(26, 8), varchar0216_col_20 VARCHAR(216), string_col_21 STRING, bigint_col_22 BIGINT, boolean_col_23 BOOLEAN, timestamp_col_24 TIMESTAMP, boolean_col_25 BOOLEAN, decimal2016_col_26 DECIMAL(20, 16), string_col_27 STRING, decimal0202_col_28 DECIMAL(2, 2), float_col_29 FLOAT, decimal2020_col_30 DECIMAL(20, 20), boolean_col_31 BOOLEAN, double_col_32 DOUBLE, varchar0148_col_33 VARCHAR(148), decimal2121_col_34 DECIMAL(21, 21), tinyint_col_35 TINYINT, boolean_col_36 BOOLEAN, boolean_col_37 BOOLEAN, string_col_38 STRING, decimal3420_col_39 DECIMAL(34, 20), timestamp_col_40 TIMESTAMP, decimal1408_col_41 DECIMAL(14, 8), string_col_42 STRING, decimal0902_col_43 DECIMAL(9, 2), varchar0204_col_44 VARCHAR(204), boolean_col_45 BOOLEAN, timestamp_col_46 TIMESTAMP, boolean_col_47 BOOLEAN, bigint_col_48 BIGINT, boolean_col_49 BOOLEAN, smallint_col_50 SMALLINT, decimal0704_col_51 DECIMAL(7, 4), timestamp_col_52 TIMESTAMP, boolean_col_53 BOOLEAN, timestamp_col_54 TIMESTAMP, int_col_55 INT, decimal0505_col_56 DECIMAL(5, 5), char0155_col_57 CHAR(155), boolean_col_58 BOOLEAN, bigint_col_59 BIGINT, boolean_col_60 BOOLEAN, boolean_col_61 BOOLEAN, char0249_col_62 CHAR(249), boolean_col_63 BOOLEAN, timestamp_col_64 TIMESTAMP, decimal1309_col_65 DECIMAL(13, 9), int_col_66 INT, float_col_67 FLOAT, timestamp_col_68 TIMESTAMP, timestamp_col_69 TIMESTAMP, boolean_col_70 BOOLEAN, timestamp_col_71 TIMESTAMP, double_col_72 DOUBLE, boolean_col_73 BOOLEAN, char0222_col_74 CHAR(222), float_col_75 FLOAT, string_col_76 STRING, decimal2612_col_77 DECIMAL(26, 12), timestamp_col_78 TIMESTAMP, char0128_col_79 CHAR(128), timestamp_col_80 TIMESTAMP, double_col_81 DOUBLE, timestamp_col_82 TIMESTAMP, float_col_83 FLOAT, decimal2622_col_84 DECIMAL(26, 22), double_col_85 DOUBLE, float_col_86 FLOAT, decimal0907_col_87 DECIMAL(9, 7)) STORED AS orc;CREATE TABLE table_18 (boolean_col_1 BOOLEAN, boolean_col_2 BOOLEAN, decimal2518_col_3 DECIMAL(25, 18), float_col_4 FLOAT, timestamp_col_5 TIMESTAMP, double_col_6 DOUBLE, double_col_7 DOUBLE, char0035_col_8 CHAR(35), decimal2709_col_9 DECIMAL(27, 9), int_col_10 INT, timestamp_col_11 TIMESTAMP, decimal3604_col_12 DECIMAL(36, 4), string_col_13 STRING, int_col_14 INT, tinyint_col_15 TINYINT, decimal1911_col_16 DECIMAL(19, 11), float_col_17 FLOAT, timestamp_col_18 TIMESTAMP, smallint_col_19 SMALLINT, tinyint_col_20 TINYINT, timestamp_col_21 TIMESTAMP, boolean_col_22 BOOLEAN, int_col_23 INT) STORED AS orc;The problem is that the reference indices in the condition (and thus, their type) are inferred incorrectly.</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="1378" opendate="2010-6-1 00:00:00" fixdate="2010-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return value for map, array, and struct needs to return a string</summary>
      <description>In order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">data.scripts.input20.script</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13787" opendate="2016-5-19 00:00:00" fixdate="2016-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: bug in recent security patches (wrong argument order; using full user name in id)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
    </fixedFiles>
  </bug>
  <bug id="13788" opendate="2016-5-19 00:00:00" fixdate="2016-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive msck listpartitions need to make use of directSQL instead of datanucleus</summary>
      <description>Currently, for tables having 1000s of partitions too many DB calls are made via datanucleus.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="13809" opendate="2016-5-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hybrid Grace Hash Join memory usage estimation didn&amp;#39;t take into account the bloom filter size</summary>
      <description>Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there's enough memory but it turns out not the case, we will run into OOM problem.Currently hybrid grace hash join memory usage estimation didn't take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.The solution is to count in the bloom filter size into memory estimation.Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13818" opendate="2016-5-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fast Vector MapJoin Long hashtable has to handle all integral types</summary>
      <description>Changes for HIVE-13682 did fix a bug in Fast Hash Tables, but evidently not this issue according to Gopal/Rajesh/Nita.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="13821" opendate="2016-5-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OrcSplit groups all delta files together into a single split</summary>
      <description>HIVE-7428 had fix for worst case column projection size estimate. It was removed in HIVE-10397 to return file length but for ACID strategy file length is passed as 0. In worst case, this always return 0 and all files ends up in single split.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13823" opendate="2016-5-23 00:00:00" fixdate="2016-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary log line in common join operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13826" opendate="2016-5-24 00:00:00" fixdate="2016-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make VectorUDFAdaptor work for GenericUDFBetween when used as FILTER</summary>
      <description>GenericUDFBetween doesn't vectorize with VectorUDFAdaptor when used as FILTER (i.e. as single item for WHERE).</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.between.columns.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="13827" opendate="2016-5-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAPIF: authentication on the output channel</summary>
      <description>The current thinking is that we'd send the token. There's no protocol on the channel right now.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormat.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13832" opendate="2016-5-24 00:00:00" fixdate="2016-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing license header to files</summary>
      <description>Preparing to cut the branch for 2.1.0.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.util.JavaDataModelTest.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ThriftFormatter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestTezWorkConcurrency.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.AcidWriteSetService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SparkJobUtils.java</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestDataReaderProperties.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DataReaderProperties.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MConstraint.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.IExtrapolatePartStatus.java</file>
      <file type="M">llap-server.sql.serviceCheckScript.sql</file>
    </fixedFiles>
  </bug>
  <bug id="13833" opendate="2016-5-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an initial delay when starting the heartbeat</summary>
      <description>Since the scheduling of heartbeat happens immediately after lock acquisition, it's unnecessary to send heartbeat at the time when locks is acquired. Add an initial delay to skip this.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13839" opendate="2016-5-25 00:00:00" fixdate="2016-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Refactor] Remove SHIMS.listLocatedStatus</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13840" opendate="2016-5-25 00:00:00" fixdate="2016-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc split generation is reading file footers twice</summary>
      <description>Recent refactorings to move orc out introduced a regression in split generation. This leads to reading the orc file footers twice during split generation.</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13857" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert overwrite select from some table fails throwing org.apache.hadoop.security.AccessControlException - II</summary>
      <description>HIVE-13810 missed a fix, tracking it here.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13860" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix more json related JDK8 test failures</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>java8,2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.json.serde1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13861" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix up nullability issue that might be created by pull up constants rules</summary>
      <description>When we pull up constants through Union or Sort operators, we might end up rewriting the original expression into an expression whose schema has different nullability properties for some of its columns.This results in AssertionError of the following kind:...org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.AssertionError: Internal error: Cannot add expression of different type to set:...</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="13863" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve AnnotateWithStatistics with support for cartesian product</summary>
      <description>Currently cartesian product stats based on cardinality of inputs are not inferred correctly.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqual.corr.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="13864" opendate="2016-5-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline ignores the command that follows a semicolon and comment</summary>
      <description>Beeline ignores the next line/command that follows a command with semicolon and comments.Example 1:select *from table1; &amp;#8211; commentsselect * from table2;In this case, only the first command is executed.. second command "select * from table2" is not executed.------Example 2:select *from table1; &amp;#8211; commentsselect * from table2;select * from table3;In this case, first command and third command is executed. second command "select * from table2" is not executed.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="13868" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include derby.log file in the Hive ptest logs</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug id="13873" opendate="2016-5-27 00:00:00" fixdate="2016-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support column pruning for struct fields in select statement</summary>
      <description>This is the grounding work for the nested column pruning in Hive, for Parquet format. In this patch, we address the case for struct type in select statements. In particular, for queries such as:select s.a from tblwhere tbl has schema:s:struct&lt;a:int, b:boolean, c:array&lt;int&gt;&gt;then only the field a should have been scanned in the Parquet reader, while field b and c can be ignored.Future work includes support other types of statements, as well as more combinations of types (e.g., selecting fields of array type inside a struct type).</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13878" opendate="2016-5-27 00:00:00" fixdate="2016-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Column pruning for Text vectorization</summary>
      <description>Column pruning in TextFile vectorization does not work with Vector SerDe settings due to LazySimple deser codepath issues.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.VerifyFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinaryFast.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.fast.DeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VerifyFastRow.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CheckFastRowHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="1388" opendate="2010-6-3 00:00:00" fixdate="2010-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>combinehiveinputformat does not work if files are of different types</summary>
      <description>rop table t1;drop table t2;create table t1 (key string, value string) partitioned by (ds string, hr string);create table t2 (key string, value string) partitioned by (ds string);insert overwrite table t1 partition (ds='1', hr='1') select key, value from src cluster by key;insert overwrite table t1 partition (ds='1', hr='2') select key, value from src cluster by key;insert overwrite table t1 partition (ds='1', hr='2') select key, value from t1 where ds = '1' and hr = '2';desc extended t1;desc extended t1 partition (ds='1', hr='1');desc extended t1 partition (ds='1', hr='2');alter table t2 add partition (ds='1') location '/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/t1/ds=1';select count(1) from t2 where ds='1';set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;select count(1) from t2 where ds='1';drop table t1;drop table t2;Consider the above testcase, some files are generated by mappers, whereas some others are generated by reducers. It is therefore possible that some files contain Text in their key, whereas others contain BytesWritable.Due to that, combinehiveInputFormat record reader may get an error.Note that, this works in hiveinputformat because different files are not combined in the same mapper - it even works ifwe query 't1' because different partitions are not combined in the same mapper</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13882" opendate="2016-5-27 00:00:00" fixdate="2016-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When hive.server2.async.exec.async.compile is turned on, from JDBC we will get "The query did not generate a result set"</summary>
      <description>The following would fail with "The query did not generate a result set" stmt.execute("SET hive.driver.parallel.compilation=true"); stmt.execute("SET hive.server2.async.exec.async.compile=true"); ResultSet res = stmt.executeQuery("SELECT * FROM " + tableName); res.next();</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.OperationStatus.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TGetOperationStatusResp.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13887" opendate="2016-5-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LazySimpleSerDe should parse "NULL" dates faster</summary>
      <description>Date string which contain "NULL" or "(null)" are being parsed through a very slow codepath involving exception handling as a normal codepath.These are currently ~4x slower than parsing an actual date field.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyDate.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.serde.LazySimpleSerDeBench.java</file>
    </fixedFiles>
  </bug>
  <bug id="13902" opendate="2016-6-1 00:00:00" fixdate="2016-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Refactor] Minimize metastore jar dependencies on task nodes</summary>
      <description>Unnecessary dependency on metastore jar can be eliminated.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13911" opendate="2016-6-1 00:00:00" fixdate="2016-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>load inpath fails throwing org.apache.hadoop.security.AccessControlException</summary>
      <description>Similar to HIVE-13857</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="13916" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the max runtime of a test further</summary>
      <description>HIVE-13520 reduced this from 2h to 1h.We should bring it down further - 40minutes.From logs added in HIVE-13507 - the following are the longest runtimes from run 476. ElapsedTime(seconds)=979 ElapsedTime(seconds)=1011 ElapsedTime(seconds)=1014 ElapsedTime(seconds)=1060 ElapsedTime(seconds)=1114 ElapsedTime(seconds)=1157 ElapsedTime(seconds)=1169 ElapsedTime(seconds)=1240 ElapsedTime(seconds)=1307 ElapsedTime(seconds)=1325 ElapsedTime(seconds)=1326 ElapsedTime(seconds)=1334 ElapsedTime(seconds)=1368 ElapsedTime(seconds)=1372 ElapsedTime(seconds)=1395 ElapsedTime(seconds)=1434 ElapsedTime(seconds)=1459 ElapsedTime(seconds)=1492 ElapsedTime(seconds)=1499 ElapsedTime(seconds)=1604 ElapsedTime(seconds)=3601 ElapsedTime(seconds)=3601 ElapsedTime(seconds)=3601The last 3 are tests that timed out. The longest valid test takes 1604 seconds or about 26 minutes. 1.5 times that is 40, which seems like a reasonable number.With the current state of the builds - if the last 3 tests were not timing out - the build/test-run would complete 20minutes faster. There's two more stragglers before that - which add another 15 minutes to the test run. i.e. if we can fix some of these longer running tests, or have them trigger early - we can likely reduce the jenkins runtime further - TestParseNegative (1368s runtime) and TestHBaseMinimrCliDriver (1060seconds runtime)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.LocalCommand.java</file>
    </fixedFiles>
  </bug>
  <bug id="13917" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate and fix tests which are timing out in the precommit build</summary>
      <description>Three tests seem to timeout consistently.TestJdbcWithMiniHATestJdbcWithMiniMrTestOperationLoggingAPIWithTez</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13927" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding missing header to Java files</summary>
      <description></description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.ThriftCliServiceMessageSizeTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="13933" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to turn off parallel file moves</summary>
      <description>Since this is a new feature, it make sense to have an ability to turn it off.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13934" opendate="2016-6-2 00:00:00" fixdate="2016-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure Tez to make nocondiional task size memory available for the Processor</summary>
      <description>Currently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.Check this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="13956" opendate="2016-6-6 00:00:00" fixdate="2016-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: external client output is writing to channel before it is writable again</summary>
      <description>Rows are being written/flushed on the output channel without checking if the channel is writable. Introduce a writability check/wait.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.ChannelOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="13961" opendate="2016-6-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Major compaction fails to include the original bucket files if there&amp;#39;s no delta directory</summary>
      <description>The issue can be reproduced by steps below:1. Insert a row to Non-ACID table2. Convert Non-ACID to ACID table (i.e. set transactional=true table property)3. Perform Major compaction</description>
      <version>1.3.0,2.1.0,2.2.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
    </fixedFiles>
  </bug>
  <bug id="13997" opendate="2016-6-12 00:00:00" fixdate="2016-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite directory doesn&amp;#39;t overwrite existing files</summary>
      <description>Can be easily reproduced by running INSERT OVERWRITE DIRECTORY to the same dir twice.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14003" opendate="2016-6-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>queries running against llap hang at times - preemption issues</summary>
      <description>The preemption logic in the Hive processor needs some more work. There are definitely windows where the abort flag is completely dropped within the Hive processor.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14012" opendate="2016-6-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some ColumnVector-s are missing ensureSize</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug id="14018" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make IN clause row selectivity estimation customizable</summary>
      <description>After HIVE-13287 went in, we calculate IN clause estimates natively (instead of just dividing incoming number of rows by 2). However, as the distribution of values of the columns is considered uniform, we might end up heavily underestimating/overestimating the resulting number of rows.This issue is to add a factor that multiplies the IN clause estimation so we can alleviate this problem. The solution is not very elegant, but it is the best we can do until we have histograms to improve our estimate.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14021" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When converting to CNF, fail if the expression exceeds a threshold</summary>
      <description>When converting to conjunctive normal form (CNF), fail if the expression exceeds a threshold. CNF can explode exponentially in the size of the input expression, but rarely does so in practice. Add a maxNodeCount parameter to RexUtil.toCnf and throw or return null if it is exceeded.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14053" opendate="2016-6-17 00:00:00" fixdate="2016-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should report that primary keys can&amp;#39;t be null.</summary>
      <description>HIVE-13076 introduces "rely novalidate" primary and foreign keys to Hive. With the right driver in place, tools like Tableau can do join elimination and queries can run much faster.Some gaps remain, currently getAttributes() in HiveDatabaseMetaData doesn't work quite right for keys. In particular, primary keys by definition are not null and the metadata should reflect this for improved join elimination.In this example that uses the TPC-H schema and its constraints, we sum l_extendedprice and group by l_shipmode. This query should not use more than just the lineitem table.With all the constraints in place, Tableau generates this query:SELECT `lineitem`.`l_shipmode` AS `l_shipmode`, SUM(`lineitem`.`l_extendedprice`) AS `sum_l_extendedprice_ok`FROM `tpch_bin_flat_orc_2`.`lineitem` `lineitem` JOIN `tpch_bin_flat_orc_2`.`orders` `orders` ON (`lineitem`.`l_orderkey` = `orders`.`o_orderkey`) JOIN `tpch_bin_flat_orc_2`.`customer` `customer` ON (`orders`.`o_custkey` = `customer`.`c_custkey`) JOIN `tpch_bin_flat_orc_2`.`nation` `nation` ON (`customer`.`c_nationkey` = `nation`.`n_nationkey`)WHERE ((((NOT (`lineitem`.`l_partkey` IS NULL)) AND (NOT (`lineitem`.`l_suppkey` IS NULL))) AND ((NOT (`lineitem`.`l_partkey` IS NULL)) AND (NOT (`lineitem`.`l_suppkey` IS NULL)))) AND (NOT (`nation`.`n_regionkey` IS NULL)))Since these are the primary keys the denormalization and the where condition is unnecessary and this sort of query can be a lot faster by just accessing the lineitem table.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="14060" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: Remove bogus "localhost" from Hive splits</summary>
      <description>On remote filesystems like Azure, GCP and S3, the splits contain a filler location of "localhost".This is worse than having no location information at all - on large clusters yarn waits upto 200&amp;#91;1&amp;#93; seconds for heartbeat from "localhost" before allocating a container.To speed up this process, the split affinity provider should scrub the bogus "localhost" from the locations and allow for the allocation of "*" containers instead on each heartbeat.&amp;#91;1&amp;#93; - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14062" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changes from HIVE-13502 overwritten by HIVE-13566</summary>
      <description>Appears that changes from HIVE-13566 overwrote the changes from HIVE-13502. I will confirm with the author that it was inadvertent before I re-add it. Thanks</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="14063" opendate="2016-6-20 00:00:00" fixdate="2016-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline to auto connect to the HiveServer2</summary>
      <description>Currently one has to give an jdbc:hive2 url in order for Beeline to connect a hiveserver2 instance. It would be great if Beeline can get the info somehow (from a properties file at a well-known location?) and connect automatically if user doesn't specify such a url. If the properties file is not present, then beeline would expect user to provide the url and credentials using !connect or ./beeline -u .. commandsWhile Beeline is flexible (being a mere JDBC client), most environments would have just a single HS2. Having users to manually connect into this via either "beeline ~/.propsfile" or -u or !connect statements is lowering the experience part.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.resources.hive-site.xml</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeeLineHistory.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="14073" opendate="2016-6-21 00:00:00" fixdate="2016-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update config whiltelist for sql std authorization</summary>
      <description>New configs that should go in security whitelist have been added. Whitelist needs updating.</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14091" opendate="2016-6-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some errors are not propagated to LLAP external clients</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="14115" opendate="2016-6-28 00:00:00" fixdate="2016-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom FetchFormatter is not supported</summary>
      <description>The following code is supported only FetchFormatter of ThriftFormatter and DefaultFetchFormatter. It can not be used Custom FetchFormatter. if (SessionState.get().isHiveServerQuery()) { conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName()); } else { conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName()); }</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="14126" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>With ranger enabled, partitioned columns is returned first when you execute select star</summary>
      <description>Click to add description</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
    </fixedFiles>
  </bug>
  <bug id="1413" opendate="2010-6-17 00:00:00" fixdate="2010-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bring a table/partition offline</summary>
      <description>There should be a way to bring a table/partition offline.At that time, no read/write operations should be supported on that table.It would be very useful for housekeeping operations</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14132" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t fail config validation for removed configs</summary>
      <description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.set.metaconf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14141" opendate="2016-6-30 00:00:00" fixdate="2016-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix for HIVE-14062 breaks indirect urls in beeline</summary>
      <description>Looks like the patch for HIVE-14062 breaks indirect urls which uses environment variables to get the url in beelineIn order to reproduce this issue:$ export BEELINE_URL_DEFAULT="jdbc:hive2://localhost:10000"$ beeline -u default</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="14144" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permanent functions are showing up in show functions, but describe says it doesn&amp;#39;t exist</summary>
      <description>Click to add description</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
    </fixedFiles>
  </bug>
  <bug id="14149" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Joda Time causes an AmazonS3Exception on Hadoop3.0.0</summary>
      <description>Java1.8u60 and higher cause Joda Time 2.5 to incorrectly format timezones, which leads to the aws server rejecting requests with the aws sdk hadoop3.0 uses. This means any queries involving the s3a connector will return the following AmazonS3Exception:com.amazonaws.services.s3.model.AmazonS3Exception: AWS authentication requires a valid Date or x-amz-date headerThe fix for this is to update Joda Time from 2.5 to 2.8.1. See here for details:https://github.com/aws/aws-sdk-java/issues/444</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14151" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use of USE_DEPRECATED_CLI environment variable does not work</summary>
      <description>According to https://cwiki.apache.org/confluence/display/Hive/Replacing+the+Implementation+of+Hive+CLI+Using+Beeline if we set USE_DEPRECATED_CLI=false it should use beeline for hiveCli. But it doesn't seem to work.In order to reproduce this issue:$ echo $USE_DEPRECATED_CLI$ ./hiveHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt;$$ export USE_DEPRECATED_CLI=false$ echo $USE_DEPRECATED_CLIfalse$ ./hiveHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14153" opendate="2016-7-1 00:00:00" fixdate="2016-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: beeline history doesn&amp;#39;t work on Hive2</summary>
      <description>The up arrow on console is supposed to display history, which is broken currently. Changes in HIVE-6758 broke it.</description>
      <version>1.2.1,2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  <bug id="14167" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use work directories provided by Tez instead of directly using YARN local dirs</summary>
      <description>HIVE-13303 fixed things to use multiple directories instead of a single tmp directory. However it's using yarn-local-dirs directly.I'm not sure how well using the yarn-local-dir will work on a secure cluster.Would be better to use Tez*Context.getWorkDirs. This provides an app specific directory - writable by the user.cc sershe</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14175" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix creating buckets without scheme information</summary>
      <description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="14176" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO nesting windowing function within each other when merging Project operators</summary>
      <description>The translation into a physical plan does not support this way of expressing windowing functions. Instead, we will not merge the Project operators when we find this pattern.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
    </fixedFiles>
  </bug>
  <bug id="14177" opendate="2016-7-6 00:00:00" fixdate="2016-2-6 01:00:00" resolution="Not A Bug">
    <buginformation>
      <summary>AddPartitionEvent contains the table location, but not the partition location</summary>
      <description>AddPartitionEvent contains the table location, but not the partition location</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug id="14191" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bump a new api version for ThriftJDBCBinarySerde changes</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TProtocolVersion.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="14196" opendate="2016-7-8 00:00:00" fixdate="2016-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable LLAP IO when complex types are involved</summary>
      <description>Let's exclude vector_complex_* tests added for llap which is currently broken and fails in all test runs. We can re-enable it with HIVE-14089 patch.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="14197" opendate="2016-7-8 00:00:00" fixdate="2016-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP service driver precondition failure should include the values</summary>
      <description>LLAP service driver's precondition failure message are like belowWorking memory + cache has to be smaller than the container sizingIt will be better to include the actual values for the sizes in the precondition failure message.NO PRECOMMIT TESTS</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14204" opendate="2016-7-11 00:00:00" fixdate="2016-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize loading dynamic partitions</summary>
      <description>Lots of time is spent in sequential fashion to load dynamic partitioned dataset in driver side. E.g simple dynamic partitioned load as follows takes 300+ secondsINSERT INTO web_sales_test partition(ws_sold_date_sk) select * from tpcds_bin_partitioned_orc_200.web_sales;Time taken to load dynamic partitions: 309.22 seconds</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14205" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive doesn&amp;#39;t support union type with AVRO file format</summary>
      <description>Reproduce steps:hive&gt; CREATE TABLE avro_union_test &gt; PARTITIONED BY (p int) &gt; ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' &gt; STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' &gt; OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' &gt; TBLPROPERTIES ('avro.schema.literal'='{ &gt; "type":"record", &gt; "name":"nullUnionTest", &gt; "fields":[ &gt; { &gt; "name":"value", &gt; "type":[ &gt; "null", &gt; "int", &gt; "long" &gt; ], &gt; "default":null &gt; } &gt; ] &gt; }');OKTime taken: 0.105 secondshive&gt; alter table avro_union_test add partition (p=1);OKTime taken: 0.093 secondshive&gt; select * from avro_union_test;FAILED: RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException: Failed with exception Hive internal error inside isAssignableFromSettablePrimitiveOI void not supported yet.java.lang.RuntimeException: Hive internal error inside isAssignableFromSettablePrimitiveOI void not supported yet. at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettablePrimitiveOI(ObjectInspectorUtils.java:1140) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1149) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1187) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1220) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1200) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219) at org.apache.hadoop.hive.ql.exec.FetchOperator.setupOutputObjectInspector(FetchOperator.java:581) at org.apache.hadoop.hive.ql.exec.FetchOperator.initialize(FetchOperator.java:172) at org.apache.hadoop.hive.ql.exec.FetchOperator.&lt;init&gt;(FetchOperator.java:140) at org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:79) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:482) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:311) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1194) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1289) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1120) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1108) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:218) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:170) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:381) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:773) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:691) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Another test case to show this problem is:hive&gt; create table avro_union_test2 (value uniontype&lt;int,bigint&gt;) stored as avro;OKTime taken: 0.053 secondshive&gt; show create table avro_union_test2;OKCREATE TABLE `avro_union_test2`( `value` uniontype&lt;void,int,bigint&gt; COMMENT '')ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'LOCATION 'hdfs://localhost/user/hive/warehouse/avro_union_test2'TBLPROPERTIES ( 'transient_lastDdlTime'='1468173589')Time taken: 0.051 seconds, Fetched: 12 row(s)Although column value is defined as uniontype&lt;int,bigint&gt; in create table command, its type becomes uniontype&lt;void,int,bigint&gt; after table is defined. Hive accidentally make the nullable definition in avro schema (["null", "int", "long"]) into union definition.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14207" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Strip HiveConf hidden params in webui conf</summary>
      <description>HIVE-12338 introduced a new web ui, which has a page that displays the current HiveConf being used by HS2. However, before it displays that config, it does not strip entries from it which are considered "hidden" conf parameters, thus exposing those values from a web-ui for HS2. We need to add stripping to this.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14236" opendate="2016-7-14 00:00:00" fixdate="2016-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS with UNION ALL puts the wrong stats in Tez</summary>
      <description>to repo. in Tez, create table t as select * from src union all select * from src;</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14244" opendate="2016-7-14 00:00:00" fixdate="2016-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketmap right outer join query throws ArrayIndexOutOfBoundsException</summary>
      <description>bucketmap right outer join on partitioned bucketed table throws this error:Vertex failed, vertexName=Map 1, vertexId=vertex_1466710232033_0539_6_00, diagnostics=[Task failed, taskId=task_1466710232033_0539_6_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1466710232033_0539_6_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1466710232033_0539_6_00, diagnostics=[Task failed, taskId=task_1466710232033_0539_6_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1466710232033_0539_6_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 14 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:850) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 17 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:416) at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:104) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762) ... 18 moreCaused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:314) at org.apache.tez.runtime.library.common.writers.UnorderedPartitionedKVWriter.write(UnorderedPartitionedKVWriter.java:257) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor$TezKVOutputCollector.collect(TezProcessor.java:253) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(ReduceSinkOperator.java:552) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.process(ReduceSinkOperator.java:398) ... 22 more]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1466710232033_0539_6_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0Steps to reproduce the issue:set hive.execution.engine=tez;drop table if exists src_nonemptybucket_partitioned_1; create table src_nonemptybucket_partitioned_1 (name string, age int, gpa double) partitioned by (year int) clustered by (age) into 120 buckets stored as orc;insert into table src_nonemptybucket_partitioned_1 partition(year=2015) select * from studenttab10k;drop table if exists src_nonemptybucket_partitioned_2; create table src_nonemptybucket_partitioned_2 (name varchar(50), age bigint, gpa decimal(38,18)) partitioned by (year int) clustered by (age) into 240 buckets stored as orc;insert into table src_nonemptybucket_partitioned_2 partition(year=2015) select * from studenttab10k ;set hive.optimize.bucketmapjoin=true;set hive.convert.join.bucket.mapjoin.tez=true;select /*+ MAPJOIN(e2) */ e1.name as e1_name, e1.age as e1_age, e1.gpa as e1_gpa, e2.name as e2_name, e2.age as e2_age, e2.gpa as e2_gpa from src_nonemptybucket_partitioned_1 e1 right outer join src_nonemptybucket_partitioned_2 e2 on e1.age = e2.age where e1.year = 2015 and e2.year = 2016;</description>
      <version>1.3.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="14245" opendate="2016-7-15 00:00:00" fixdate="2016-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NoClassDefFoundError when starting LLAP daemon</summary>
      <description>Env: hive master branch2016-07-14T20:40:00,646WARN [main[]] conf.Configuration: hive-site.xml:an attempt to override final parameter: hive.server2.tez.sessions.per.default.queue;Ignoring.2016-07-14T20:40:00,652WARN [main[]] impl.LlapDaemon: Failed to start LLAP Daemon with exceptionjava.lang.NoClassDefFoundError: org/apache/hadoop/registry/client/binding/RegistryUtils$ServiceRecordMarshalat org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.&lt;init&gt;(LlapZookeeperRegistryImpl.java:134) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]at org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.serviceInit(LlapRegistryService.java:84) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) ~[hadoop-common-2.7.1.jar:?]at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.serviceStart(LlapDaemon.java:369) ~[hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193) ~[hadoop-common-2.7.1.jar:?]at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.main(LlapDaemon.java:460) [hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshalat java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_65]at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_65]at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_65]at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_65]... 6 more</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14254" opendate="2016-7-15 00:00:00" fixdate="2016-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the hive version by changing "svn" to "git"</summary>
      <description>When running "hive --version", "subversion" is displayed below, which should be "git".$ hive --versionHive 2.1.0-SNAPSHOTSubversion git://</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveVersionInfo.java</file>
      <file type="M">common.src.java.org.apache.hive.common.HiveVersionAnnotation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14265" opendate="2016-7-18 00:00:00" fixdate="2016-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partial stats in Join operator may lead to data size estimate of 0</summary>
      <description>For some tables, we might not have the column stats available. However, if the table is partitioned, we will have the stats for partition columns.When we estimate the size of the data produced by a join operator, we end up using only the columns that are available for the calculation e.g. partition columns in this case.However, even in these cases, we should add the data size for those columns for which we do not have stats (default size for the column type x estimated number of rows).To reproduce, the following example can be used:create table sample_partitioned (x int) partitioned by (y int);insert into sample_partitioned partition(y=1) values (1),(2);create temporary table sample as select * from sample_partitioned;analyze table sample compute statistics for columns;explain select sample_partitioned.x from sample_partitioned, sample where sample.y = sample_partitioned.y;</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="1427" opendate="2010-6-22 00:00:00" fixdate="2010-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide metastore schema migration scripts (0.5 -&gt; 0.6)</summary>
      <description>At a minimum this ticket covers packaging up example MySQL migration scripts (cumulative across all schema changes from 0.5 to 0.6) and explaining what to do with them in the release notes.This is also probably a good point at which to decide and clearly state which Metastore DBs we officially support in production, e.g. do we need to provide migration scripts for Derby?</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14270" opendate="2016-7-18 00:00:00" fixdate="2016-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write temporary data to HDFS when doing inserts on tables located on S3</summary>
      <description>Currently, when doing INSERT statements on tables located at S3, Hive writes and reads temporary (or intermediate) files to S3 as well. If HDFS is still the default filesystem on Hive, then we can keep such temporary files on HDFS to keep things run faster.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14291" opendate="2016-7-19 00:00:00" fixdate="2016-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>count(*) on a table written by hcatstorer returns incorrect result</summary>
      <description>count(*) on a table written by hcatstorer returns wrong result. steps to repro the issue:1) create hive tablecreate table ${DEST_TABLE}(name string, age int, gpa float) row format delimited fields terminated by '\t' stored as textfile;2) load data into table using hcatstorerA = LOAD '$DATA_1' USING PigStorage() AS (name:chararray, age:int, gpa:float);B = LOAD '$DATA_2' USING PigStorage() AS (name:chararray, age:int, gpa:float);C = UNION A, B;STORE C INTO '$HIVE_TABLE' USING org.apache.hive.hcatalog.pig.HCatStorer();</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14308" opendate="2016-7-21 00:00:00" fixdate="2016-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>While using column stats estimated data size may become 0</summary>
      <description>Found during a run of HIVE-12181</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats.ppr.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partial.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="14322" opendate="2016-7-24 00:00:00" fixdate="2016-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgres db issues after Datanucleus 4.x upgrade</summary>
      <description>With the upgrade to datanucleus 4.x versions in HIVE-6113, hive does not work properly with postgres.The nullable fields in the database have string "NULL::character varying" instead of real NULL values. This causes various issues.One example is -hive&gt; create table t(i int);OKTime taken: 1.9 secondshive&gt; create view v as select * from t;OKTime taken: 0.542 secondshive&gt; select * from v;FAILED: SemanticException Unable to fetch table v. java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying</description>
      <version>2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14324" opendate="2016-7-25 00:00:00" fixdate="2016-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC PPD for floats is broken</summary>
      <description>ORC stores min/max stats, bloom filters by passing floats as doubles using java's widening conversion. So if we write a float value of 0.22 to ORC file, the min/max stats and bloom filter will use 0.2199999988079071 double value.But when we do PPD, SARG creates literals by converting float to string and then to double which compares 0.22 to 0.2199999988079071 and fails PPD evaluation. hive&gt; create table orc_float (f float) stored as orc;hive&gt; insert into table orc_float values(0.22);hive&gt; set hive.optimize.index.filter=true;hive&gt; select * from orc_float where f=0.22;OKhive&gt; set hive.optimize.index.filter=false;hive&gt; select * from orc_float where f=0.22;OK0.22This is not a problem for doubles and decimals.This issue was introduced in HIVE-8460 but back then there was no strict type check when SARGs are created and also PPD evaluation does not convert to column type. But now predicate leaf creation in SARG enforces strict type check for boxed literals and predicate type and PPD evaluation converts stats and constants to column type (predicate).</description>
      <version>1.3.0,2.0.0,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.basic.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
    </fixedFiles>
  </bug>
  <bug id="14331" opendate="2016-7-25 00:00:00" fixdate="2016-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task should set exception for failed map reduce job.</summary>
      <description>Task should set exception for failed map reduce job. So the exception can be seen in HookContext.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="14333" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC schema evolution from float to double changes precision and breaks filters</summary>
      <description>ORC vs text schema evolution from float to double changes precisionText Schema Evolutionhive&gt; create table float_text(f float);hive&gt; insert into float_text values(74.72);hive&gt; select f from float_text;OK74.72hive&gt; alter table float_text change column f f double;hive&gt; select f from float_text;OK74.72Orc Schema Evolutionhive&gt; create table float_orc(f float) stored as orc;hive&gt; insert into float_orc values(74.72);hive&gt; select f from float_orc;OK74.72hive&gt; alter table float_orc change column f f double;hive&gt; select f from float_orc;OK74.72000122070312This will break all filters on the evolved column "f"Filter returning no resultshive&gt; set hive.optimize.index.filter=false;hive&gt; select f from float_orc where f=74.72;OK</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acid.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.acidvec.mapwork.part.q.out</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestSchemaEvolution.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14335" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskDisplay&amp;#39;s return value is not getting deserialized properly</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug id="14336" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make usage of VectorUDFAdaptor configurable</summary>
      <description>Add a Hive configuration variable:hive.vectorized.adaptor.usage.mode = {none, chosen, all}for configuring whether to attempt vectorization using the VectorUDFAdaptor.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14338" opendate="2016-7-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete/Alter table calls failing with HiveAccessControlException</summary>
      <description>Many Hcatalog/Webhcat tests are failing with below error, when tests try to alter/delete/describe tables. Error is thrown when the same user or a different user (same group) who created the table is trying to run the delete/alter table call.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
    </fixedFiles>
  </bug>
  <bug id="14355" opendate="2016-7-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema evolution for ORC in llap is broken for int to string conversion</summary>
      <description>When schema is evolved from any integer type to string then following exceptions are thrown in LLAP (Works fine in Tez). I guess this should happen even for other conversions.hive&gt; create table orc_integer(b bigint) stored as orc;hive&gt; insert into orc_integer values(100);hive&gt; select count(*) from orc_integer where b=100;OK1hive&gt; alter table orc_integer change column b b string;hive&gt; select count(*) from orc_integer where b=100;// FAIL with following exceptionWhen vectorization is enabled2016-07-27T01:48:05,611 INFO [TezTaskRunner ()] vector.VectorReduceSinkOperator: RECORDS_OUT_INTERMEDIATE_Map_1:0,2016-07-27T01:48:05,611 ERROR [TezTaskRunner ()] tez.TezProcessor: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:866) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 18 moreCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringGroupColEqualStringGroupScalarBase.evaluate(FilterStringGroupColEqualStringGroupScalarBase.java:42) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:110) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:774) ... 19 moreWhen vectorization is disabled2016-07-27T01:52:43,328 INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.ReduceSinkOperator: Using tag = -12016-07-27T01:52:43,328 INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.OperatorUtils: Setting output collector: RS[4] --&gt; Reducer 22016-07-27T01:52:43,329 ERROR [TezTaskRunner (1469608604787_0002_26_00_000000_0)] io.BatchToRowReader: Error at row 0/1, column 0/1 org.apache.hadoop.hive.ql.exec.vector.LongColumnVector@7630e56ajava.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextString(BatchToRowReader.java:334) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextValue(BatchToRowReader.java:602) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:149) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:78) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151) ~[tez-mapreduce-0.8.4.jar:0.8.4] at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) ~[tez-mapreduce-0.8.4.jar:0.8.4] at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_91] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_91] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) ~[hadoop-common-2.6.0.jar:?] at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) ~[tez-runtime-internals-0.8.4.jar:0.8.4] at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.jar:0.8.4] at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) ~[hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_91] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14358" opendate="2016-7-27 00:00:00" fixdate="2016-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics for number of queries executed for each execution engine (mr, spark, tez)</summary>
      <description>HiveServer2 currently has a metric for the total number of queries ran since last restart, but it would be useful to also have metrics for number of queries ran for each execution engine. This would improve supportability by allowing users to get a high-level understanding of what workloads had been running on the server.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  <bug id="14359" opendate="2016-7-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark might fail in HS2 with LDAP authentication in a kerberized cluster</summary>
      <description>When HS2 is used as a gateway for the LDAP users to access and run the queries in kerberized cluster, it's authentication mode is configured as LDAP and at this time, HoS might fail by the same reason as HIVE-10594. hive.server2.authentication is not a proper property to determine if a cluster is kerberized, instead hadoop.security.authentication should be used.The failure is in spark client communicating with rest of hadoop as it assumes kerberos does not need to be used.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14367" opendate="2016-7-28 00:00:00" fixdate="2016-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Estimated size for constant nulls is 0</summary>
      <description>since type is incorrectly assumed as void.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.deep.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantfolding.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="14382" opendate="2016-7-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the Functionality of Reverse FOR Statement</summary>
      <description>According to SQL Standards, Reverse FOR Statement should be like this:-FOR index IN Optional&amp;#91;Reverse&amp;#93; Lower_Bound Upper_Boundbut in hive it is like this :- FOR index IN Optional&amp;#91;Reverse&amp;#93; Upper_Bound Lower_Boundso i m just trying to improve the functionality for Reverse FOR StatementREFERNCES :- https://docs.oracle.com/cloud/latest/db112/LNPLS/for_loop_statement.htm#LNPLS1536</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.queries.local.for.range.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
    </fixedFiles>
  </bug>
  <bug id="14394" opendate="2016-7-31 00:00:00" fixdate="2016-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce excessive INFO level logging</summary>
      <description>We need to cull down on the number of logs we generate in HMS and HS2 that are not needed.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14397" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries ran after reopening of tez session launches additional sessions</summary>
      <description>Say we have configured hive.server2.tez.default.queues with 2 queues q1 and q2 with default expiry interval of 5 mins.After 5 mins of non-usage the sessions corresponding to queues q1 and q2 will be expired. When new set of queries are issue after this expiry, the default sessions backed by q1 and q2 and reopened again. Now when we run more queries the reopened sessions are not used instead new session is opened. At this point there will be 4 sessions running (2 abandoned sessions and 2 current sessions).</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14403" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP node specific preemption will only preempt once on a node per AM</summary>
      <description>Query hang reported by cartershanklinTurns out that once an AM has preempted a task on a node for locality, it will not be able to preempt another task on the same node (specifically for local requests)Manifests as a query hanging. It's possible for a previous query to interfere with a subsequent query since the AM is shared.</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14405" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Have tests log to the console along with hive.log</summary>
      <description>When running tests from the IDE (not itests), logs end up going to hive.log - making it difficult to debug tests.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14408" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>thread safety issue in fast hashtable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="14511" opendate="2016-8-10 00:00:00" fixdate="2016-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve MSCK for partitioned table to deal with special cases</summary>
      <description>Some users will have a folder rather than a file under the last partition folder. However, msck is going to search for the leaf folder rather than the last partition folder. We need to improve that.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="14545" opendate="2016-8-16 00:00:00" fixdate="2016-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 with http transport mode spends too much time just creating configs</summary>
      <description>Env: Hive master with LLAP with Q-55 running via beeline mode continuously. Query itself completes in 0.6 - 0.7 seconds. HiveServer2 is configured with hive.server2.transport.mode=http and hive.server2.long.polling.timeout=1000In such cases, HiverServer2 consistently runs with high CPU usage and profiling shows spending too much time just for creating hiveConf.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
    </fixedFiles>
  </bug>
  <bug id="14552" opendate="2016-8-16 00:00:00" fixdate="2016-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestParseNegative fix</summary>
      <description>1300s runtime.Straggler towards the end of the build.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
    </fixedFiles>
  </bug>
  <bug id="14566" opendate="2016-8-18 00:00:00" fixdate="2016-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO reads timestamp wrongly</summary>
      <description>HIVE-10127 is causing incorrect results when orc_merge12.q is run in llap.It reads timestamp wrongly.LLAP IO Enabledhive&gt; select atimestamp1 from alltypesorc3xcols limit 10;OK1969-12-31 15:59:46.674NULL1969-12-31 15:59:55.7871969-12-31 15:59:44.1871969-12-31 15:59:50.4341969-12-31 16:00:15.0071969-12-31 16:00:07.0211969-12-31 16:00:04.9631969-12-31 15:59:52.1761969-12-31 15:59:44.569LLAP IO Disabledhive&gt; select atimestamp1 from alltypesorc3xcols limit 10;OK1969-12-31 15:59:46.674NULL1969-12-31 15:59:55.7871969-12-31 15:59:44.1871969-12-31 15:59:50.4341969-12-31 16:00:14.0071969-12-31 16:00:06.0211969-12-31 16:00:03.9631969-12-31 15:59:52.1761969-12-31 15:59:44.569</description>
      <version>2.0.1,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.TreeReaderFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14576" opendate="2016-8-18 00:00:00" fixdate="2016-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Testing: Fixes to TestHBaseMinimrCliDriver</summary>
      <description>1. Runtime over 1000s.2. Runs as an isolated test.Need to fix both.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="14580" opendate="2016-8-19 00:00:00" fixdate="2016-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce || operator</summary>
      <description>Functionally equivalent to concat() udf. But standard allows usage of || for string concatenations.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="14582" opendate="2016-8-19 00:00:00" fixdate="2016-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add trunc(numeric) udf</summary>
      <description>https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions200.htm</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.trunc.error2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.trunc.error1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrunc.java</file>
    </fixedFiles>
  </bug>
  <bug id="14590" opendate="2016-8-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path) Incorrect result set when limit is present in one of the union branches</summary>
      <description>Limit gets propagated outside union.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.null.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.null.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14591" opendate="2016-8-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 is shut down unexpectedly during the startup time</summary>
      <description>If there is issue with Zookeeper (e.g. connection issues), then it takes HS2 some time to connect. During this time, Ambari could issue health checks against HS2 and the CloseSession call will trigger the shutdown of HS2, which is not expected. That triggering should happen only when the HS2 has been deregistered with Zookeeper, not during the startup time when HS2 is not registered with ZK yet.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14618" opendate="2016-8-24 00:00:00" fixdate="2016-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline fetch logging delays before query completion</summary>
      <description>Beeline has a thread that fetches logs from HS2. However, it uses the same HiveStatement object to also wait for query completion using a long-poll (with default interval of 5 seconds).The jdbc client has a lock around the thrift api calls, resulting in the getLogs api blocking on the query completion check. ie the logs would get shown only every 5 seconds by default.cc vgumashta gopalv thejas</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="14652" opendate="2016-8-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect results for not in on partition columns</summary>
      <description>create table foo (i int) partitioned by (s string);insert overwrite table foo partition(s='foo') select cint from alltypesorc limit 10;insert overwrite table foo partition(s='bar') select cint from alltypesorc limit 10;select * from foo where s not in ('bar');No results. IN ... works correctly</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="14655" opendate="2016-8-26 00:00:00" fixdate="2016-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP input format should escape the query string being passed to getSplits()</summary>
      <description>Query may not be parsed correctly by get_splits() otherwise.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug id="14656" opendate="2016-8-26 00:00:00" fixdate="2016-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up driver instance in get_splits</summary>
      <description>get_splits() creates a Driver instance that needs to be closed/cleaned up after use.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug id="14678" opendate="2016-8-31 00:00:00" fixdate="2016-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-on-MR deprecation warning is not diplayed when engine is set to capital letter &amp;#39;MR&amp;#39;</summary>
      <description>hive&gt; SET hive.execution.engine=mr;Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt; hive&gt; SET hive.execution.engine=MR;hive&gt;This warning seems to be generated only with small letter 'mr' wand not with capital letter "MR".generateMrDeprecationWarning() seem to be displaying this warning and is being called in different classes. Ex: https://github.com/apache/hive/blob/master/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java#L754https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/Driver.java#L1900</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14697" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not access kerberized HS2 Web UI</summary>
      <description>Failed to access kerberized HS2 WebUI with following error msg:curl -v -u : --negotiate http://util185.phx2.cbsig.net:10002/ &gt; GET / HTTP/1.1 &gt; Host: util185.phx2.cbsig.net:10002 &gt; Authorization: Negotiate YIIU7...[redacted]... &gt; User-Agent: curl/7.42.1 &gt; Accept: */* &gt; &lt; HTTP/1.1 413 FULL head &lt; Content-Length: 0 &lt; Connection: close &lt; Server: Jetty(7.6.0.v20120127) It is because the Jetty default request header (4K) is too small in some kerberos case.So this patch is to increase the request header to 64K.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="147" opendate="2008-12-10 00:00:00" fixdate="2008-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need a tool for extracting lineage info from hive sql</summary>
      <description>Need a tool to extract the line information from hive query. This tool should take hive query as input and it should output, input and output tables used by the query.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DefaultASTEventDispatcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14706" opendate="2016-9-6 00:00:00" fixdate="2016-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage information not set properly</summary>
      <description>I am trying to fetch column level lineage after a CTAS query in a Post Execution hook in Hive. Below are the queries:create table t1(id int, name string);create table t2 as select * from t1;The lineage information is retrieved using the following sample piece of code:lInfo = hookContext.getLinfo()for(Map.Entry&lt;LineageInfo.DependencyKey, LineageInfo.Dependency&gt; e : lInfo.entrySet()) { System.out.println("Col Lineage Key : " + e.getKey()); System.out.println("Col Lineage Value: " + e.getValue());}The Dependency field(i.e Col Lineage Value) is coming in as null.</description>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14726" opendate="2016-9-8 00:00:00" fixdate="2016-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>delete statement fails when spdo is on</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.optimization.acid.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14762" opendate="2016-9-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging while removing scratch space</summary>
      <description>Useful for debugging</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="14764" opendate="2016-9-15 00:00:00" fixdate="2016-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enabling "hive.metastore.metrics.enabled" throws OOM in HiveMetastore</summary>
      <description>After running some queries with metrics enabled, metastore starts throwing the following messages.Caused by: java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1075) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:989) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:929) at com.mysql.jdbc.Util.handleNewInstance(Util.java:433) at com.mysql.jdbc.PreparedStatement.getInstance(PreparedStatement.java:877) at com.mysql.jdbc.ConnectionImpl.clientPrepareStatement(ConnectionImpl.java:1489) at com.mysql.jdbc.ConnectionImpl.prepareStatement(ConnectionImpl.java:4343) at com.mysql.jdbc.ConnectionImpl.prepareStatement(ConnectionImpl.java:4242) at com.jolbox.bonecp.ConnectionHandle.prepareStatement(ConnectionHandle.java:1024) at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:350) at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:295) at org.datanucleus.store.rdbms.scostore.JoinListStore.listIterator(JoinListStore.java:761) ... 36 moreNested Throwables StackTrace:java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1075) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:989) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:929) at com.mysql.jdbc.Util.handleNewInstance(Util.java:433) at com.mysql.jdbc.PreparedStatement.getInstance(PreparedStatement.java:877) at com.mysql.jdbc.ConnectionImpl.clientPrepareStatement(ConnectionImpl.java:1489) at com.mysql.jdbc.ConnectionImpl.prepareStatement(ConnectionImpl.java:4343) at com.mysql.jdbc.ConnectionImpl.prepareStatement(ConnectionImpl.java:4242) at com.jolbox.bonecp.ConnectionHandle.prepareStatement(ConnectionHandle.java:1024) at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:350) at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:295) at org.datanucleus.store.rdbms.scostore.JoinListStore.listIterator(JoinListStore.java:761) at org.datanucleus.store.rdbms.scostore.AbstractListStore.listIterator(AbstractListStore.java:93) at org.datanucleus.store.rdbms.scostore.AbstractListStore.iterator(AbstractListStore.java:83) at org.datanucleus.store.types.wrappers.backed.List.loadFromStore(List.java:264) at org.datanucleus.store.types.wrappers.backed.List.iterator(List.java:492) at org.apache.hadoop.hive.metastore.ObjectStore.convertToFieldSchemas(ObjectStore.java:1199) at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1266) at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1281) at org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(ObjectStore.java:1138) at org.apache.hadoop.hive.metastore.ObjectStore.ensureGetTable(ObjectStore.java:2651) at org.apache.hadoop.hive.metastore.ObjectStore.updatePartitionColumnStatistics(ObjectStore.java:6141)HiveMetastore uses start/end functions for starting/ending the scope in MetricsFactory. In some places in HiveMetastore the function names are not matching causing gradual memory leak in metastore with metrics enabled.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="14768" opendate="2016-9-15 00:00:00" fixdate="2016-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new UDTF Replicate_Rows</summary>
      <description>For intersect all and except all implementation purpose.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="14773" opendate="2016-9-16 00:00:00" fixdate="2016-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE aggregating column statistics for date column in partitioned table</summary>
      <description>Hive runs into a NPE when the query has a filter on a date column and the partitioned column eg: create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;set hive.exec.dynamic.partition.mode=nonstrict;insert into date_dim partition(d_date_sk=2416945) values('1905-04-09');insert into date_dim partition(d_date_sk=2416946) values('1905-04-10');insert into date_dim partition(d_date_sk=2416947) values('1905-04-11');analyze table date_dim partition(d_date_sk) compute statistics for columns;explain select count(*) from date_dim where d_date &gt; date "1900-01-02" and d_date_sk= 2416945;Here d_date_sk is a partition column and d_date is of type date.2016-09-16T08:27:06,510 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.AggregateStatsCache: No aggregate stats cached for database:default, table:date_dim, column:d_date2016-09-16T08:27:06,512 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: Direct SQL query in 1.302231ms + 0.00653ms, the query is [select "COLUMN_NAME", "COLUMN_TYPE", min("LONG_LOW_VALUE"), max("LONG_HIGH_VALUE"), min("DOUBLE_LOW_VALUE"), max("DOUBLE_HIGH_VALUE"), min(cast("BIG_DECIMAL_LOW_VALUE" as decimal)), max(cast("BIG_DECIMAL_HIGH_VALUE" as decimal)), sum("NUM_NULLS"), max("NUM_DISTINCTS"), max("AVG_COL_LEN"), max("MAX_COL_LEN"), sum("NUM_TRUES"), sum("NUM_FALSES"), avg(("LONG_HIGH_VALUE"-"LONG_LOW_VALUE")/cast("NUM_DISTINCTS" as decimal)),avg(("DOUBLE_HIGH_VALUE"-"DOUBLE_LOW_VALUE")/"NUM_DISTINCTS"),avg((cast("BIG_DECIMAL_HIGH_VALUE" as decimal)-cast("BIG_DECIMAL_LOW_VALUE" as decimal))/"NUM_DISTINCTS"),sum("NUM_DISTINCTS") from "PART_COL_STATS" where "DB_NAME" = ? and "TABLE_NAME" = ? and "COLUMN_NAME" in (?) and "PARTITION_NAME" in (?) group by "COLUMN_NAME", "COLUMN_TYPE"]2016-09-16T08:27:06,526 INFO [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: useDensityFunctionForNDVEstimation = falsepartsFound = 1ColumnStatisticsObj = [ColumnStatisticsObj(colName:d_date, colType:date, statsData:&lt;ColumnStatisticsData &gt;)]2016-09-16T08:27:06,526 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: Commit transaction: count = 0, isactive true at: org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2827)2016-09-16T08:27:06,531 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: null retrieved using SQL in 43.425925ms2016-09-16T08:27:06,545 ERROR [90d4780f-77e4-4704-9907-4860ce11a206 main] ql.Driver: FAILED: NullPointerException nulljava.lang.NullPointerException at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:451) at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDateStats(ColumnStatisticsData.java:574) at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759) at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:304) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152) at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140) at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:126) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143) at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122) at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78) at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260) at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129) at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10928) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:255) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:467) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1235) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1355) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148)</description>
      <version>1.2.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
    </fixedFiles>
  </bug>
  <bug id="14774" opendate="2016-9-16 00:00:00" fixdate="2016-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Canceling query using Ctrl-C in beeline might lead to stale locks</summary>
      <description>Terminating a running query using Ctrl-C in Beeline might lead to stale locks since the process running the query might still be able to acquire the locks but fail to release them after the query terminate abnormally.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14793" opendate="2016-9-19 00:00:00" fixdate="2016-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ptest branch to be specified, PROFILE override</summary>
      <description>Post HIVE-14734 - the profile is automatically determined. Add an option to override this via Jenkins. Also add an option to specify the branch from which ptest is built (This is hardcoded to github.com/apache/hive)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="148" opendate="2008-12-10 00:00:00" fixdate="2008-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>extend bin/hive to include the lineage tool</summary>
      <description>biin/hive currently used only to execute the query. Add options to bin/hive to output the lineage info given the query as the input.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14806" opendate="2016-9-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support UDTF in CBO (AST return path)</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udtf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14841" opendate="2016-9-26 00:00:00" fixdate="2016-1-26 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Replication - Phase 2</summary>
      <description>Per email sent out to the dev list, the current implementation of replication in hive has certain drawbacks, for instance : Replication follows a rubberbanding pattern, wherein different tables/ptns can be in a different/mixed state on the destination, so that unless all events are caught up on, we do not have an equivalent warehouse. Thus, this only satisfies DR cases, not load balancing usecases, and the secondary warehouse is really only seen as a backup, rather than as a live warehouse that trails the primary. The base implementation is a naive implementation, and has several performance problems, including a large amount of duplication of data for subsequent events, as mentioned in HIVE-13348, having to copy out entire partitions/tables when just a delta of files might be sufficient/etc. Also, using EXPORT/IMPORT allows us a simple implementation, but at the cost of tons of temporary space, much of which is not actually applied at the destination.Thus, to track this, we now create a new branch (repl2) and a uber-jira(this one) to track experimental development towards improvement of this situation.</description>
      <version>2.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="14861" opendate="2016-9-29 00:00:00" fixdate="2016-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support precedence for set operator using parentheses</summary>
      <description>We should support precedence for set operator by using parentheses. For exampleselect * from src union all (select * from src union select * from src);</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionSortBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionOrderBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionLimit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionDistributeBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionClusterBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.ptf.negative.NoWindowDefn.q</file>
      <file type="M">ql.src.test.queries.clientnegative.ptf.negative.AmbiguousWindowDefn.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="14876" opendate="2016-10-3 00:00:00" fixdate="2016-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make the number of rows to fetch from various HS2 clients/servers configurable</summary>
      <description>Right now, it's hardcoded to a variety of values</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.RetryingThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.EmbeddedCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14887" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the memory used by MiniMr, MiniTez, MiniLlap tests</summary>
      <description>The clusters that we spin up end up requiring 16GB at times. Also the maven arguments seem a little heavy weight.Reducing this will allow for additional ptest drones per box, which should bring down the runtime.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">data.conf.tez.tez-site.xml</file>
      <file type="M">data.conf.perf-reg.tez-site.xml</file>
      <file type="M">data.conf.llap.tez-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14889" opendate="2016-10-4 00:00:00" fixdate="2016-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline leaks sensitive environment variables of HiveServer2 when you type set;</summary>
      <description>When you type set; beeline prints all the environment variables including passwords which could be major security risk. Eg: HADOOP_CREDENTIAL_PASSWORD below is leaked.| env:HADOOP_CREDSTORE_PASSWORD=password || env:HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS || env:HADOOP_HOME_WARN_SUPPRESS=true || env:HADOOP_IDENT_STRING=vihang || env:HADOOP_PID_DIR= |</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestSetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14958" opendate="2016-10-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the &amp;#39;TestClass&amp;#39; did not produce a TEST-*.xml file message to include list of all qfiles in a batch, batch id</summary>
      <description>Should make it easier to hunt down the logs.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.QFileTestBatch.java</file>
    </fixedFiles>
  </bug>
  <bug id="14966" opendate="2016-10-14 00:00:00" fixdate="2016-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Make cookie-auth work in HTTP mode</summary>
      <description>HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.The repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.The HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.This due to the fact that the cookies are not sent out with matching flags for SSL usage.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14982" opendate="2016-10-16 00:00:00" fixdate="2016-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove some reserved keywords in Hive 2.2</summary>
      <description>It seems that CACHE, DAYOFWEEK, VIEWS are reserved keywords in master. This conflicts with SQL2011 standard.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="15018" opendate="2016-10-19 00:00:00" fixdate="2016-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER rewriting flag in materialized view</summary>
      <description>We should extend the ALTER statement in case we want to change the rewriting behavior of the materialized view after we have created it.ALTER MATERIALIZED VIEW [db_name.]materialized_view_name DISABLE REWRITE;ALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE REWRITE;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15019" opendate="2016-10-19 00:00:00" fixdate="2016-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle import for MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  <bug id="15029" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logic to estimate stats for BETWEEN operator</summary>
      <description>Currently, BETWEEN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15030" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes in inference of collation for Tez cost model</summary>
      <description>Tez cost model might get NPE if collation returned by join algorithm is null.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15031" opendate="2016-10-21 00:00:00" fixdate="2016-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix flaky LLAP unit test (TestSlotZNode)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.llap.registry.impl.TestSlotZnode.java</file>
    </fixedFiles>
  </bug>
  <bug id="15042" opendate="2016-10-24 00:00:00" fixdate="2016-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support intersect/except without distinct keyword</summary>
      <description>basically, intersect = intersect distinct.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="15072" opendate="2016-10-26 00:00:00" fixdate="2016-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should recognize missing tables in metastore</summary>
      <description>When Install a new database failed half way(for some other reasons), not all of the metastore tables are installed. This caused HMS server failed to start up due to missing tables. Re-run the Schematool, It ran successfully, and in the stdout log said: "Database already has tables. Skipping table creation".However, restarting HMS getting the same error reporting missing tables.Schematool should detect missing tables and provide options to go ahead and recreate missing tables in the case of new installation</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15073" opendate="2016-10-26 00:00:00" fixdate="2016-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should detect malformed URIs</summary>
      <description>For some causes(most unknown), HMS DB tables sometimes has invalid entries, for example URI missing scheme for SDS table's LOCATION column or DBS's DB_LOCATION_URI column. These malformed URIs lead to hard to analyze errors in HIVE and SENTRY. Schematool need to provide a command to detect these malformed URI, give a warning and provide an option to fix the URIs</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15074" opendate="2016-10-26 00:00:00" fixdate="2016-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool provides a way to detect invalid entries in VERSION table</summary>
      <description>For some unknown reason, we see customer's HMS can not start because there are multiple entries in their HMS VERSION table. Schematool should provide a way to validate the HMS db and provide warning and fix options for this kind of issues.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="15076" opendate="2016-10-26 00:00:00" fixdate="2016-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve scalability of LDAP authentication provider group filter</summary>
      <description>Current implementation uses following algorithm: For a given user find all groups that user is a member of. (A list of LDAP groups is constructed as a result of that request) Match this list of groups with provided group filter.Time/Memory complexity of this approach is O(N) on client side, where N  is a number of groups the user has membership in. On a large directory (800+ groups per user) we can observe up to 2x performance degradation and failures because of size of LDAP response (LDAP: error code 4 - Sizelimit Exceeded).Some Directory Services (Microsoft Active Directory for instance) provide a virtual attribute for User Object that contains a list of groups that user belongs to. This attribute can be used to quickly determine whether this user passes or fails the group filter.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAtnProviderWithMiniDS.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestQueryFactory.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestLdapSearch.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.TestGroupFilter.java</file>
      <file type="M">service.src.test.org.apache.hive.service.auth.ldap.LdapAuthenticationTestCase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.QueryFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.Query.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapSearch.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.DirSearch.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15085" opendate="2016-10-27 00:00:00" fixdate="2016-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the memory used by unit tests, MiniCliDriver, MiniLlapLocal, MiniSpark</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15090" opendate="2016-10-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporary DB failure can stop ExpiredTokenRemover thread</summary>
      <description>In HIVE-13090 we decided that we should not close the metastore if there is an unexpected exception during the expired token removal process, but that fix leaves a running metastore without ExpiredTokenRemover thread.To fix this I will move the catch inside the running loop, and hope the thread could recover from the exception</description>
      <version>1.3.0,2.0.1,2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="15096" opendate="2016-10-29 00:00:00" fixdate="2016-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hplsql registerUDF conflicts with pom.xml</summary>
      <description>in hplsql code, registerUDF code is sql.add("ADD JAR " + dir + "hplsql.jar"); sql.add("ADD JAR " + dir + "antlr-runtime-4.5.jar"); sql.add("ADD FILE " + dir + Conf.SITE_XML);but pom configufation is &lt;parent&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive&lt;/artifactId&gt; &lt;version&gt;2.2.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;hive-hplsql&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Hive HPL/SQL&lt;/name&gt; &lt;dependency&gt; &lt;groupId&gt;org.antlr&lt;/groupId&gt; &lt;artifactId&gt;antlr4-runtime&lt;/artifactId&gt; &lt;version&gt;4.5&lt;/version&gt; &lt;/dependency&gt;when run hplsql , errors occur as below Error while processing statement: /opt/apps/apache-hive-2.0.0-bin/lib/hplsql.jar does not exist</description>
      <version>2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
    </fixedFiles>
  </bug>
  <bug id="151" opendate="2008-12-10 00:00:00" fixdate="2008-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveQL Query execution bug: java.lang.NullPointerException</summary>
      <description>Executing a query ------------------------------------- query start ----------------------------------------------------SELECT t11.subject, t22.object , t33.subject , t55.object, t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66ON (t66.subject=t55.object)------------------------------------- query end ----------------------------------------------------on table ------------------------------------- table start ----------------------------------------------------CREATE TABLE triples (foo string,subject string, predicate string, object string, foo2 string)------------------------------------- table end -----------------------------------------------------gives the foolowing output ------------------------------------ console output ---------------------------------------------- INFO &amp;#91;main&amp;#93; (Driver.java:156) - Starting command: SELECT t11.subject, t22.object , t33.subject , t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33 ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44 ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object as obh FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55 ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66 ON (t66.subject=t55.obh) INFO &amp;#91;main&amp;#93; (ParseDriver.java:249) - Parsing command: SELECT t11.subject, t22.object , t33.subject , t66.object FROM ( SELECT t1.subject FROM triples t1 WHERE t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation' ) t11 JOIN ( SELECT t2.subject , t2.object FROM triples t2 WHERE t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t22 ON (t11.subject=t22.subject) JOIN ( SELECT t3.subject , t3.object FROM triples t3 WHERE t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from' ) t33 ON (t11.subject=t33.object) JOIN ( SELECT t4.subject FROM triples t4 WHERE t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL' AND t4.object='http://ontos/OntosMiner/Common.English/ontology#Author' ) t44 ON (t44.subject=t33.subject) JOIN ( SELECT t5.subject, t5.object as obh FROM triples t5 WHERE t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to' ) t55 ON (t55.subject=t44.subject) JOIN ( SELECT t6.subject, t6.object FROM triples t6 WHERE t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL' ) t66 ON (t66.subject=t55.obh) INFO &amp;#91;main&amp;#93; (ParseDriver.java:263) - Parse Completed INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:126) - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore INFO &amp;#91;main&amp;#93; (ObjectStore.java:124) - ObjectStore, initialize called INFO &amp;#91;main&amp;#93; (ObjectStore.java:146) - found resource jpox.properties at file:/home/vseledkin/workspace/HiveDrv/bin/jpox.properties WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - Bundle "org.jpox" has an optional dependency to "org.eclipse.equinox.registry" but it cannot be resolved WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - Bundle "org.jpox" has an optional dependency to "org.eclipse.core.runtime" but it cannot be resolved INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - ================= Persistence Configuration =============== INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - JPOX Persistence Factory - Vendor: "JPOX" Version: "1.2.2" INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - JPOX Persistence Factory initialised for datastore URL="jdbc:derby:;databaseName=metastore_db;create=true" driver="org.apache.derby.jdbc.EmbeddedDriver" userName="APP" INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - =========================================================== INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Initialising Catalog "", Schema "APP" using "SchemaTable" auto-start option INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MDatabase since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - No manager for annotations was found in the CLASSPATH so all annotations are ignored. WARN &amp;#91;main&amp;#93; (Log4JLogger.java:98) - MetaData Parser encountered an error in file "jar:file:/home/vseledkin/workspace/hive/build/hive_metastore.jar!/package.jdo" at line 282, column 13 : The content of element type "class" must match "(extension*,implements*,datastore-identity?,primary-key?,inheritance?,version?,join*,foreign-key*,index*,unique*,column*,field*,property*,query*,fetch-group*,extension*)". - Please check your specification of DTD and the validity of the MetaData XML that you have specified. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MStorageDescriptor since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MSerDeInfo since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MTable since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of org.apache.hadoop.hive.metastore.model.MPartition since it was managed previously INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MDatabase &amp;#91;Table : DBS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MSerDeInfo &amp;#91;Table : SERDES, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStorageDescriptor &amp;#91;Table : SDS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTable &amp;#91;Table : TBLS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table. INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartition &amp;#91;Table : PARTITIONS, InheritanceStrategy : new-table&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters &amp;#91;Table : SERDE_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.parameters &amp;#91;Table : PARTITION_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.values &amp;#91;Table : PARTITION_KEY_VALS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.parameters &amp;#91;Table : TABLE_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.partitionKeys &amp;#91;Table : PARTITION_KEYS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols &amp;#91;Table : BUCKETING_COLS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cols &amp;#91;Table : COLUMNS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters &amp;#91;Table : SD_PARAMS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols &amp;#91;Table : SORT_COLS&amp;#93; INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 0 foreign key(s) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 index(es) for table SERDES INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 foreign key(s) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 4 index(es) for table PARTITIONS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 foreign key(s) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 4 index(es) for table TBLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SDS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 unique key(s) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 0 foreign key(s) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table DBS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SORT_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table TABLE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table COLUMNS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_KEYS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SD_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table PARTITION_KEY_VALS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table SERDE_PARAMS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 unique key(s) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 1 foreign key(s) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Validating 2 index(es) for table BUCKETING_COLS INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - Catalog "", Schema "APP" initialised - managing 14 classes INFO &amp;#91;main&amp;#93; (Log4JLogger.java:79) - &gt;&gt; Found StoreManager org.jpox.store.rdbms.RDBMSManager INFO &amp;#91;main&amp;#93; (ObjectStore.java:110) - Initialized ObjectStore INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3086) - Starting Semantic Analysis INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3088) - Completed phase 1 of Semantic Analysis INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3091) - Completed getting MetaData in Semantic Analysis INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4, string reducesinkvalue5, string reducesinkvalue6, string reducesinkvalue7} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:579) - Get metadata for source tables INFO &amp;#91;main&amp;#93; (HiveMetaStore.java:164) - 0: get_table : db=default tbl=triples INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct triples { string tid, string subject, string predicate, string object, string type} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:595) - Get metadata for subqueries INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:602) - Get metadata for destination tables INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1, string reducesinkvalue2, string reducesinkvalue3, string reducesinkvalue4, string reducesinkvalue5, string reducesinkvalue6, string reducesinkvalue7} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string reducesinkkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string reducesinkvalue0, string reducesinkvalue1} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_sortable_table { string joinkey0} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3107) - Completed partition pruning INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3111) - Completed sample pruning INFO &amp;#91;main&amp;#93; (MetaStoreUtils.java:461) - DDL: struct binary_table { string temporarycol0, string temporarycol1, string temporarycol2, string temporarycol3, string temporarycol4} INFO &amp;#91;main&amp;#93; (SemanticAnalyzer.java:3120) - Completed plan generation INFO &amp;#91;main&amp;#93; (Driver.java:173) - Semantic Analysis CompletedTotal MapReduce jobs = 3 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Total MapReduce jobs = 3Number of reducers = 1 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Number of reducers = 1In order to change numer of reducers use: INFO &amp;#91;main&amp;#93; (SessionState.java:254) - In order to change numer of reducers use: set mapred.reduce.tasks = &lt;number&gt; INFO &amp;#91;main&amp;#93; (SessionState.java:254) - set mapred.reduce.tasks = &lt;number&gt; WARN &amp;#91;main&amp;#93; (ExecDriver.java:109) - Number of reduce tasks not specified. Defaulting to jobconf value of: 1 INFO &amp;#91;main&amp;#93; (ExecDriver.java:238) - Adding input file /user/hive/warehouse/triples WARN &amp;#91;main&amp;#93; (JobClient.java:547) - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. INFO &amp;#91;main&amp;#93; (FileInputFormat.java:181) - Total input paths to process : 1Starting Job = job_200812091129_0144, Tracking URL = http://ubunder.avicomp.com:50030/jobdetails.jsp?jobid=job_200812091129_0144 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Starting Job = job_200812091129_0144, Tracking URL = http://ubunder.avicomp.com:50030/jobdetails.jsp?jobid=job_200812091129_0144Kill Command = /home/vseledkin/workspace/HiveDrv/programs/hadoop-0.19.0 job -Dmapred.job.tracker=ubunder.avicomp.com:9001 -kill job_200812091129_0144 INFO &amp;#91;main&amp;#93; (SessionState.java:254) - Kill Command = /home/vseledkin/workspace/HiveDrv/programs/hadoop-0.19.0 job -Dmapred.job.tracker=ubunder.avicomp.com:9001 -kill job_200812091129_0144 map = 0%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 0%, reduce =0% map = 50%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 50%, reduce =0% map = 100%, reduce =0% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 100%, reduce =0% map = 100%, reduce =100% INFO &amp;#91;main&amp;#93; (SessionState.java:254) - map = 100%, reduce =100%ERROR &amp;#91;main&amp;#93; (SessionState.java:263) - Ended Job = job_200812091129_0144 with errorsEnded Job = job_200812091129_0144 with errorsFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.ExecDriverERROR &amp;#91;main&amp;#93; (SessionState.java:263) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.ExecDriver------------------------------------ console output end ----------------------------------------and the stack trace in hadoop logs ------------------------------------ stack trace ---------------------------------------------------java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:81) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:83) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:337) at org.apache.hadoop.mapred.Child.main(Child.java:155)------------------------------------ stack trace end ---------------------------------------------attached file contains table data to test problematic query</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15112" opendate="2016-11-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Parquet vectorization reader for Struct type</summary>
      <description>Like HIVE-14815, we need support Parquet vectorized reader for struct type.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedDictionaryEncodingColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedColumnReaderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedStructColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedColumnReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15125" opendate="2016-11-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Parallelize slider package generator</summary>
      <description>The metastore init + download of functions takes approx 4 seconds.This is enough time to complete all the other operations in parallel.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="15151" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap support for replv2</summary>
      <description>We need to support the ability to bootstrap an initial state, dumping out currently existing dbs/tables, etc, so that incremental replication can take over from that point. To this end, we should implement commands such as REPL DUMP, REPL LOAD, REPL STATUS, as described over at https://cwiki.apache.org/confluence/display/Hive/HiveReplicationv2Development</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MetaDataExportListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15155" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Hive version shortname to 2.2.0</summary>
      <description>Pointing to 2.1.0.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15159" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP ContainerRunner should not reduce the available heap while distributing to individual executors</summary>
      <description>hive.llap.daemon.memory.per.instance.mb is set to the heapsize of the process when llap service driver is used.ContainerRunner currently cuts this down to 80% of the available. This reduction is not necessary if the heapsize is used, instead of a container size.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15160" opendate="2016-11-8 00:00:00" fixdate="2016-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t order by an unselected column</summary>
      <description>If a grouping key hasn't been selected, Hive complains. For comparison, Postgres does not.Example. Notice i_item_id is not selected:select i_item_desc ,i_category ,i_class ,i_current_price ,sum(cs_ext_sales_price) as itemrevenue ,sum(cs_ext_sales_price)*100/sum(sum(cs_ext_sales_price)) over (partition by i_class) as revenueratio from catalog_sales ,item ,date_dim where cs_item_sk = i_item_sk and i_category in ('Jewelry', 'Sports', 'Books') and cs_sold_date_sk = d_date_sk and d_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30 days) group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratiolimit 100;</description>
      <version>2.0.0,2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15161" opendate="2016-11-8 00:00:00" fixdate="2016-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate ColumnStats to use jackson</summary>
      <description>json.org has license issues jackson can provide a fully compatible alternative to it there are a few flakiness issues caused by the order of the map entries of the columns...this cat be addressed, org.json api was unfriendly in this manner</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestStatsSetupConst.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug id="15164" opendate="2016-11-8 00:00:00" fixdate="2016-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default RPC port for llap to be a dynamic port</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15166" opendate="2016-11-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide beeline option to set the jline history max size</summary>
      <description>Currently Beeline does not provide an option to limit the max size for beeline history file, in the case that each query is very big, it will flood the history file and slow down beeline on start up and shutdown.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="15178" opendate="2016-11-10 00:00:00" fixdate="2016-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC stripe merge may produce many MR jobs and no merge if split size is small</summary>
      <description>orc_createas1logs the following:2016-11-10T13:38:54,366 INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Paths:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/-ext-10004/000001_0:2400+100InputFormatClass: org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeInputFormat2016-11-10T13:38:54,373 INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Paths:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/-ext-10004/000001_0:2500+100InputFormatClass: org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeInputFormat2016-11-10T13:38:54,380 INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Paths:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/-ext-10004/000001_0:2600+100InputFormatClass: org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeInputFormat2016-11-10T13:38:54,387 INFO [LocalJobRunner Map Task Executor #0] mapred.MapTask: Processing split: Paths:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/-ext-10004/000001_0:2700+100InputFormatClass: org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeInputFormat...It tries to merge 2 files, but instead ends up running tons of MR tasks for every 100 bytes and produces 2 files again (I assume most tasks don't produce the files because the split at a random 100-byte offset is invalid).2016-11-10T13:38:53,985 INFO [LocalJobRunner Map Task Executor #0] OrcFileMergeOperator: Merged stripe from file pfile:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/-ext-10004/000000_0 [ offset : 3 length: 2770 row: 500 ]2016-11-10T13:38:53,995 INFO [LocalJobRunner Map Task Executor #0] exec.AbstractFileMergeOperator: renamed path pfile:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/_task_tmp.-ext-10002/_tmp.000002_0 to pfile:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/_tmp.-ext-10002/000002_0 . File size is 29862016-11-10T13:38:54,206 INFO [LocalJobRunner Map Task Executor #0] OrcFileMergeOperator: Merged stripe from file pfile:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/-ext-10004/000001_0 [ offset : 3 length: 2770 row: 500 ]2016-11-10T13:38:54,215 INFO [LocalJobRunner Map Task Executor #0] exec.AbstractFileMergeOperator: renamed path pfile:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/_task_tmp.-ext-10002/_tmp.000030_0 to pfile:/Users/sergey/git/hivegit2/itests/qtest/target/warehouse/.hive-staging_hive_2016-11-10_13-38-52_334_1323113125332102866-1/_tmp.-ext-10002/000030_0 . File size is 2986This is because the test sets the max split size to 100. Merge jobs is supposed to override that, but that doesn't happen somehow.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="15181" opendate="2016-11-10 00:00:00" fixdate="2016-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>buildQueryWithINClause didn&amp;#39;t properly handle multiples of ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_IN_CLAUSE</summary>
      <description>As there is a bug, we can still work around the issue by using the settings below (making sure the second setting is always at least 1000 times of the first setting):set hive.direct.sql.max.query.length=1;set hive.direct.sql.max.elements.in.clause=1000;</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15190" opendate="2016-11-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Field names are not preserved in ORC files written with ACID</summary>
      <description>To repro:drop table if exists orc_nonacid;drop table if exists orc_acid;create table orc_nonacid (a int) clustered by (a) into 2 buckets stored as orc;create table orc_acid (a int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');insert into table orc_nonacid values(1), (2);insert into table orc_acid values(1), (2);Running hive --service orcfiledump &lt;file&gt; on the files created by the insert statements above, you'll see that for orc_nonacid, the files have schema struct&lt;a:int&gt; whereas for orc_acid, the files have schema struct&lt;operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct&lt;_col0:int&gt;&gt;. The last field row should have schema struct&lt;a:int&gt;.</description>
      <version>2.1.0,2.2.0,3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15227" opendate="2016-11-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize join + gby into semijoin</summary>
      <description>Calcite has a rule which can do this transformation. Lets take advantage of this since Hive has native Left semi join operator.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15234" opendate="2016-11-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin cardinality estimation can be improved</summary>
      <description>Currently calcite optimization rules rely on (Hive)SemiJoin to represent semi join node, whereas Stats estimate use leftSemiJoin field of Join to estimate stats. As a result semi-join specific stats calculation logic is never hit since at plan generation time HiveSemiJoin is created and leftSemiJoin field of Join is never set.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
    </fixedFiles>
  </bug>
  <bug id="15236" opendate="2016-11-17 00:00:00" fixdate="2016-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>timestamp and date comparison should happen in timestamp</summary>
      <description>Currently it happens in string, which results in incorrect result.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cast.on.constant.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cast.on.constant.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="15237" opendate="2016-11-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate Spark job failure to Hive</summary>
      <description>If a Spark job failed for some reason, Hive doesn't get any additional error message, which makes it very hard for user to figure out why. Here is an example:Status: Running (Hive on Spark job[0])Job Progress FormatCurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]2016-11-17 21:32:53,134 Stage-0_0: 0/23 Stage-1_0: 0/28 2016-11-17 21:32:55,156 Stage-0_0: 0(+1)/23 Stage-1_0: 0/28 2016-11-17 21:32:57,167 Stage-0_0: 0(+3)/23 Stage-1_0: 0/28 2016-11-17 21:33:00,216 Stage-0_0: 0(+3)/23 Stage-1_0: 0/28 2016-11-17 21:33:03,251 Stage-0_0: 0(+3)/23 Stage-1_0: 0/28 2016-11-17 21:33:06,286 Stage-0_0: 0(+4)/23 Stage-1_0: 0/28 2016-11-17 21:33:09,308 Stage-0_0: 0(+2,-3)/23 Stage-1_0: 0/28 2016-11-17 21:33:12,332 Stage-0_0: 0(+2,-3)/23 Stage-1_0: 0/28 2016-11-17 21:33:13,338 Stage-0_0: 0(+21,-3)/23 Stage-1_0: 0/28 2016-11-17 21:33:15,349 Stage-0_0: 0(+21,-5)/23 Stage-1_0: 0/28 2016-11-17 21:33:16,358 Stage-0_0: 0(+18,-8)/23 Stage-1_0: 0/28 2016-11-17 21:33:19,373 Stage-0_0: 0(+21,-8)/23 Stage-1_0: 0/28 2016-11-17 21:33:22,400 Stage-0_0: 0(+18,-14)/23 Stage-1_0: 0/28 2016-11-17 21:33:23,404 Stage-0_0: 0(+15,-20)/23 Stage-1_0: 0/28 2016-11-17 21:33:24,408 Stage-0_0: 0(+12,-23)/23 Stage-1_0: 0/28 2016-11-17 21:33:25,417 Stage-0_0: 0(+9,-26)/23 Stage-1_0: 0/28 2016-11-17 21:33:26,420 Stage-0_0: 0(+12,-26)/23 Stage-1_0: 0/28 2016-11-17 21:33:28,427 Stage-0_0: 0(+9,-29)/23 Stage-1_0: 0/28 2016-11-17 21:33:29,432 Stage-0_0: 0(+12,-29)/23 Stage-1_0: 0/28 2016-11-17 21:33:31,444 Stage-0_0: 0(+18,-29)/23 Stage-1_0: 0/28 2016-11-17 21:33:34,464 Stage-0_0: 0(+18,-29)/23 Stage-1_0: 0/28 Status: FailedFAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTaskIt would be better if we can propagate Spark error to Hive.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.JobHandleImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.JobHandle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.java</file>
    </fixedFiles>
  </bug>
  <bug id="15239" opendate="2016-11-18 00:00:00" fixdate="2016-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive on spark combine equivalent work get wrong result because of TS operation compare</summary>
      <description>env: hive on spark enginereproduce step:create table a1(KEHHAO string, START_DT string) partitioned by (END_DT string);create table a2(KEHHAO string, START_DT string) partitioned by (END_DT string);alter table a1 add partition(END_DT='20161020');alter table a1 add partition(END_DT='20161021');insert into table a1 partition(END_DT='20161020') values('2000721360','20161001');SELECT T1.KEHHAO,COUNT(1) FROM ( SELECT KEHHAO FROM a1 T WHERE T.KEHHAO = '2000721360' AND '20161018' BETWEEN T.START_DT AND T.END_DT-1 UNION ALL SELECT KEHHAO FROM a2 TWHERE T.KEHHAO = '2000721360' AND '20161018' BETWEEN T.START_DT AND T.END_DT-1 ) T1 GROUP BY T1.KEHHAO HAVING COUNT(1)&gt;1; +-------------+------+--+| t1.kehhao | _c1 |+-------------+------+--+| 2000721360 | 2 |+-------------+------+--+the result should be none record</description>
      <version>1.2.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.CombineEquivalentWorkResolver.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1524" opendate="2010-8-10 00:00:00" fixdate="2010-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parallel execution failed if mapred.job.name is set</summary>
      <description>The plan file name was generated based on mapred.job.name. If the user specify mapred.job.name before the query, two parallel queries will have conflict plan file name.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15247" opendate="2016-11-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass the purge option for drop table to storage handlers</summary>
      <description>This gives storage handler more control on how to handle drop table.</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15291" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparison of timestamp fails if only date part is provided.</summary>
      <description>Summary : If a query needs to compare two timestamp with one timestamp provided in "YYYY-MM-DD" format, skipping the time part, it returns incorrect result. Steps to reproduce : 1. Start a hive-cli. 2. Fire up the query -&gt; select cast("2016-12-31 12:00:00" as timestamp) &gt; "2016-12-30";3. Expected result : true4. Actual result : NULLDetailed description : If two primitives of different type needs to compared, a common comparator type is chosen. Prior to 2.1, Common type Text was chosen to compare Timestamp type and Text type. In version 2.1, Common type Timestamp is chosen to compare Timestamp type and Text type. This leads to converting Text type (YYYY-MM-DD) into java.sql.Timestamp which throws exception saying the input is not in proper format. The exception is suppressed and a null is returned. Code below from org.apache.hadoop.hive.ql.exec.FunctionRegistryif (pgA == PrimitiveGrouping.STRING_GROUP &amp;&amp; pgB == PrimitiveGrouping.DATE_GROUP) { return b; } // date/timestamp is higher precedence than String_GROUP if (pgB == PrimitiveGrouping.STRING_GROUP &amp;&amp; pgA == PrimitiveGrouping.DATE_GROUP) { return a; }The bug was introduced in HIVE-13381</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15294" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Capture additional metadata to replicate a simple insert at destination</summary>
      <description>For replicating inserts like INSERT INTO ... SELECT ... FROM, we will need to capture the newly added files in the notification message to be able to replicate the event at destination.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15295" opendate="2016-11-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HCatalog javadoc generation with Java 8</summary>
      <description>Realized while generating artifacts for Hive 2.1.1 release.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug id="15298" opendate="2016-11-28 00:00:00" fixdate="2016-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test failures in TestCliDriver sample[2,4,6,7,9]</summary>
      <description>Failing for the past 5 builds:https://builds.apache.org/job/PreCommit-HIVE-Build/2301/testReport/</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample2.q</file>
    </fixedFiles>
  </bug>
  <bug id="15299" opendate="2016-11-29 00:00:00" fixdate="2016-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yarn-cluster and yarn-client deprecated in Spark 2.0</summary>
      <description>Need to use master "yarn" with specified deploy mode instead.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1531" opendate="2010-8-12 00:00:00" fixdate="2010-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive build work with Ivy versions &lt; 2.1.0</summary>
      <description>Many projects in the Hadoop ecosystem still use Ivy 2.0.0 (including Hadoop and Pig),yet Hive requires version 2.1.0. Ordinarily this would not be a problem, but many usershave a copy of an older version of Ivy in their $ANT_HOME directory, and this copy willalways get picked up in preference to what the Hive build downloads for itself.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15311" opendate="2016-11-29 00:00:00" fixdate="2016-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze column stats should skip non-primitive column types</summary>
      <description>after this patch, when you compute column stats, it will skip the non-primitive column types and give you warning on the console.</description>
      <version>1.2.0,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.columnstats.tbllvl.complex.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15312" opendate="2016-11-30 00:00:00" fixdate="2016-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce logging in certain places</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15355" opendate="2016-12-4 00:00:00" fixdate="2016-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues during parallel moveFile due to HDFSUtils.setFullFileStatus</summary>
      <description>It is possible to run into concurrency issues during multi-threaded moveFile issued when processing queries like INSERT OVERWRITE TABLE ... SELECT .. when there are multiple files in the staging directory which is a subdirectory of the target directory. The issue is hard to reproduce but following stacktrace is one such example:INFO : Loading data to table functional_text_gzip.alltypesaggmultifilesnopart from hdfs://localhost:20500/test-warehouse/alltypesaggmultifilesnopart_text_gzip/.hive-staging_hive_2016-12-01_19-58-21_712_8968735301422943318-1/-ext-10000ERROR : Failed with exception java.lang.ArrayIndexOutOfBoundsExceptionorg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2858) at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:3124) at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1701) at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:313) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1200) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)Getting log thread is interrupted, since query is done! at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.ArrayIndexOutOfBoundsException at java.lang.System.arraycopy(Native Method) at java.util.ArrayList.removeRange(ArrayList.java:616) at java.util.ArrayList$SubList.removeRange(ArrayList.java:1021) at java.util.AbstractList.clear(AbstractList.java:234) at com.google.common.collect.Iterables.removeIfFromRandomAccessList(Iterables.java:213) at com.google.common.collect.Iterables.removeIf(Iterables.java:184) at org.apache.hadoop.hive.shims.Hadoop23Shims.removeBaseAclEntries(Hadoop23Shims.java:865) at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:757) at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2835) at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2828) ... 4 moreERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTaskQuick online search also shows some other instances like the one mentioned in http://stackoverflow.com/questions/38900333/get-concurrentmodificationexception-in-step-2-create-intermediate-flat-hive-tabThe issue seems to be coming from the below code :if (aclEnabled) { aclStatus = sourceStatus.getAclStatus(); if (aclStatus != null) { LOG.trace(aclStatus.toString()); aclEntries = aclStatus.getEntries(); removeBaseAclEntries(aclEntries); //the ACL api's also expect the tradition user/group/other permission in the form of ACL aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.USER, sourcePerm.getUserAction())); aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.GROUP, sourcePerm.getGroupAction())); aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.OTHER, sourcePerm.getOtherAction())); } }removeBaseAclEntries removes objects from List&lt;AclEntry&gt; aclEntries When HDFSUtils.setFullFileStatus() method is called from multiple threads like from https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2835 it is possible that multiple threads try to modify the List&lt;AclEntry&gt; aclEntries leading to concurrency issues. We should either move that block into a thread-safe region or call setFullFileStatus when all the threads converge.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15515" opendate="2016-12-27 00:00:00" fixdate="2016-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the docs directory</summary>
      <description>Hive xdocs have not been used since 2012. The docs directory only holds six xml documents, and their contents are in the wiki.It's past time to remove the docs directory from the Hive code.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">docs.xdocs.udf.reflect.xml</file>
      <file type="M">docs.xdocs.language.manual.working.with.bucketed.tables.xml</file>
      <file type="M">docs.xdocs.language.manual.var.substitution.xml</file>
      <file type="M">docs.xdocs.language.manual.joins.xml</file>
      <file type="M">docs.xdocs.language.manual.data-manipulation-statements.xml</file>
      <file type="M">docs.xdocs.language.manual.cli.xml</file>
      <file type="M">docs.xdocs.index.xml</file>
      <file type="M">docs.velocity.properties</file>
      <file type="M">docs.stylesheets.site.vsl</file>
      <file type="M">docs.stylesheets.project.xml</file>
      <file type="M">docs.site.css</file>
      <file type="M">docs.changes.ChangesSimpleStyle.css</file>
      <file type="M">docs.changes.ChangesFancyStyle.css</file>
      <file type="M">docs.changes.changes2html.pl</file>
    </fixedFiles>
  </bug>
  <bug id="15517" opendate="2016-12-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NOT (x &lt;=&gt; y) returns NULL if x or y is NULL</summary>
      <description>I created a table as following:create table test(x string, y string);insert into test values ('q', 'q'), ('q', 'w'), (NULL, 'q'), ('q', NULL), (NULL, NULL);Then I try to compare values taking NULLs into account:select *, x&lt;=&gt;y, not (x&lt;=&gt; y), (x &lt;=&gt; y) = false from test;OKq q true false falseq w false true trueq NULL false NULL trueNULL q false NULL trueNULL NULL true NULL falseI expected that 4th column will be the same as 5th one but actually got NULL as result of "not false" and "not true" expressions.Hive 1.2.1000.2.5.0.0-1245Subversion git://c66-slave-20176e25-3/grid/0/jenkins/workspace/HDP-parallel-centos6/SOURCES/hive -r da6c690d384d1666f5a5f450be5cbc54e2fe4bd6Compiled by jenkins on Fri Aug 26 01:39:52 UTC 2016From source with checksum c30648316a632f7a753f4359e5c8f4d6</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
    </fixedFiles>
  </bug>
  <bug id="15518" opendate="2016-12-27 00:00:00" fixdate="2016-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring rows and range related classes to put the window type on Window</summary>
      <description>/* * - A Window Frame that has only the /start/boundary, then it is interpreted as: BETWEEN &lt;start boundary&gt; AND CURRENT ROW * - A Window Specification with an Order Specification and no Window * Frame is interpreted as: ROW BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW * - A Window Specification with no Order and no Window Frame is interpreted as: ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING */The comments in WindowSpec above doesn't really match what it's claimed to do. Correct the comment to reduce the confusion.Also currently the window type is specified on each BoundarySpec but makes sense to put the type (rows or range) for each window.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15522" opendate="2016-12-28 00:00:00" fixdate="2016-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental ALTER_TABLE/ALTER_PTN including renames</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="15534" opendate="2017-1-3 00:00:00" fixdate="2017-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update db/table repl.last.id at the end of REPL LOAD of a batch of events</summary>
      <description>Tracking TODO task in ReplSemanticAnalyzer : // TODO : Over here, we need to track a Map&lt;dbName:String,evLast:Long&gt; for every db updated // and update repl.last.id for each, if this is a wh-level load, and if it is a db-level load, // then a single repl.last.id update, and if this is a tbl-lvl load which does not alter the // table itself, we'll need to update repl.last.id for that as well.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="15552" opendate="2017-1-6 00:00:00" fixdate="2017-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to coalesce DATE and TIMESTAMP types</summary>
      <description>COALESCE expression does not expect DATE and TIMESTAMP types select tdt.rnum, coalesce(tdt.cdt, cast(tdt.cdt as timestamp)) from certtext.tdtError: Error while compiling statement: FAILED: SemanticException Line 0:-1 Argument type mismatch 'cdt': The expressions after COALESCE should all have the same type: "date" is expected but "timestamp" is foundSQLState: 42000ErrorCode: 40000</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="15554" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add task information to LLAP AM heartbeat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15556" opendate="2017-1-6 00:00:00" fixdate="2017-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replicate views</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="15558" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix char whitespace handling for vectorization</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
    </fixedFiles>
  </bug>
  <bug id="15646" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column level lineage is not available for table Views</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.cli.createtab.noauthzapi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.inputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.authorization.sqlstd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionall.unbalancedppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionall.join.nullconstant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.struct.in.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.field.garbage.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.special.character.in.tabnames.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.column.in.single.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.column.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.subq.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.windowing.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.ddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cteViews.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.defaultformats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.subq.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.const.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.owner.actions.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.join.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.viewjoins.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.LineageState.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.view.delete.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.view.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.cli.createtab.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="15647" opendate="2017-1-17 00:00:00" fixdate="2017-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Combination of a boolean condition and null-safe comparison leads to NPE</summary>
      <description>Here's a simple example with the foodmart database:hive&gt; explain select count(*) from &gt; sales_fact_1997 join store on sales_fact_1997.store_id = store.store_id &gt; where ((store.salad_bar)) and ((store_number) &lt;=&gt; (customer_id));FAILED: NullPointerException nullThis happens on trunk and on HDP 2.5.3 / Hive 2. If you use = the NPE doesn't happen. If you remove the boolean condition the NPE doesn't happen.FAILED: NullPointerException null2016-12-13T18:23:33,604 ERROR [c4b7242e-1252-4709-8adf-22f631af75e8 main] ql.Driver: FAILED: NullPointerException nulljava.lang.NullPointerException at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateFilterProc.process(ConstantPropagateProcFactory.java:1047) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate$ConstantPropagateWalker.walk(ConstantPropagate.java:151) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform(ConstantPropagate.java:120) at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:242) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10913) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:246) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:75) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:435) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:326) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1169) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148)</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15769" opendate="2017-1-31 00:00:00" fixdate="2017-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support view creation in CBO</summary>
      <description>Right now, set operator needs to run in CBO. If a view contains a set op, it will throw exception. We need to support view creation in CBO.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.selectDistinctStarNeg.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15884" opendate="2017-2-11 00:00:00" fixdate="2017-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize not between for vectorization</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="15902" opendate="2017-2-14 00:00:00" fixdate="2017-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select query involving date throwing Hive 2 Internal error: unsupported conversion from type: date</summary>
      <description>The following query is throwing Hive 2 Internal error: unsupported conversion from type: dateQuery:create table table_one (ts timestamp, dt date) stored as orc;insert into table_one values ('2034-08-04 17:42:59','2038-07-01');insert into table_one values ('2031-02-07 13:02:38','2072-10-19');create table table_two (ts timestamp, dt date) stored as orc;insert into table_two values ('2069-04-01 09:05:54','1990-10-12');insert into table_two values ('2031-02-07 13:02:38','2072-10-19');create table table_three asselect count from table_onegroup by ts,dthaving dt in (select dt from table_two);Error while running task ( failure ) : attempt_1486991777989_0184_18_02_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1833) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:420) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ... 15 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:883) at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86) ... 18 moreCaused by: java.lang.RuntimeException: Hive 2 Internal error: unsupported conversion from type: date at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getLong(PrimitiveObjectInspectorUtils.java:770) at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColumnBetweenDynamicValue.evaluate(FilterLongColumnBetweenDynamicValue.java:82) at org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.evaluate(FilterExprAndExpr.java:39) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:112) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:883) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:783) ... 19 more</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15903" opendate="2017-2-14 00:00:00" fixdate="2017-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute table stats when user computes column stats</summary>
      <description></description>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.remove.26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.special.character.in.tabnames.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.drop.partition.with.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.table.invalidate.column.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15955" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make explain formatted to include opId and etc</summary>
      <description></description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15964" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15983" opendate="2017-2-20 00:00:00" fixdate="2017-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the named columns join</summary>
      <description>The named columns join is a common shortcut allowing joins on identically named keys. Example: select * from t1 join t2 using c1 is equivalent to select * from t1 join t2 on t1.c1 = t2.c1. SQL standard reference: Section 7.7</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15986" opendate="2017-2-20 00:00:00" fixdate="2017-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "is [not] distinct from"</summary>
      <description>Support standard "is &amp;#91;not&amp;#93; distinct from" syntax. For example this gives a standard way to do a comparison to null safe join: select * from t1 join t2 on t1.x is not distinct from t2.y. SQL standard reference Section 8.15</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="15995" opendate="2017-2-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Syncing metastore table with serde schema</summary>
      <description>Hive enables table schema evolution via properties. For avro e.g. we can alter the 'avro.schema.url' property to update table schema to the next version. Updating properties however doesn't affect column list stored in metastore DB so the table is not in the newest version when returned from metastore API. This is problem for tools working with metastore (e.g. Presto).To solve this issue I suggest to introduce new DDL statement syncing metastore columns with those from serde:ALTER TABLE user_test1 UPDATE COLUMNSNote that this is format independent solution. To reproduce, follow the instructions below: Create table based on avro schema version 1 (cxv1.avsc)CREATE EXTERNAL TABLE user_test1 PARTITIONED BY (dt string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' LOCATION '/tmp/schema-evolution/user_test1' TBLPROPERTIES ('avro.schema.url'='/tmp/schema-evolution/cx1.avsc'); Update schema to version 2 (cx2.avsc)ALTER TABLE user_test1 SET TBLPROPERTIES ('avro.schema.url' = '/tmp/schema-evolution/cx2.avsc'); Print serde columns (top info) and metastore columns (Detailed Table Information):DESCRIBE EXTENDED user_test1</description>
      <version>1.2.1,2.1.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16019" opendate="2017-2-23 00:00:00" fixdate="2017-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query fails when group by/order by on same column with uppercase name</summary>
      <description>Query with group by/order by on same column KEY failed:SELECT T1.KEY AS MYKEY FROM SRC T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="16186" opendate="2017-3-13 00:00:00" fixdate="2017-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP shows last event ID of the database even if we use LIMIT option.</summary>
      <description>Looks like LIMIT option doesn't work well with REPL DUMP.0: jdbc:hive2://localhost:10001/default&gt; REPL DUMP default FROM 170 LIMIT 1;---------------------------------------+ dump_dir last_repl_id ---------------------------------------+ /tmp/dump/1489395053411 195 ---------------------------------------+</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16197" opendate="2017-3-14 00:00:00" fixdate="2017-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental insert into a partitioned table doesn&amp;#39;t get replicated.</summary>
      <description>Insert to a partitioned table doesn't replicate properly in case of incremental dump/load. Few key points to be noted.1. If insert command itself created the new partition, then the inserted row is replicated. But the subsequent inserts into the same table doesn't get replicated.2. If the partition is created using ALTER TABLE command, then none of the inserted rows to this partition is getting replicated. However, the partition metadata is getting replicated.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="16230" opendate="2017-3-16 00:00:00" fixdate="2017-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable CBO in presence of hints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder.q</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.on.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.comments.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.bucketmapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.merge.join.desc.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pcs.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.distinct.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.reorder2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.queries.clientnegative.bucket.mapjoin.mismatch1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.bucket.mapjoin.wrong.table.metadata.1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.bucket.mapjoin.wrong.table.metadata.2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.mapjoin1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join28.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join29.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join32.q</file>
      <file type="M">ql.src.test.queries.clientnegative.join35.q</file>
      <file type="M">ql.src.test.queries.clientnegative.smb.bucketmapjoin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.smb.mapjoin.14.q</file>
      <file type="M">ql.src.test.queries.clientnegative.sortmerge.mapjoin.mismatch.1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.union22.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.comments.q</file>
      <file type="M">ql.src.test.queries.clientpositive.infer.bucket.sort.map.operators.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join25.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join26.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join27.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join30.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join36.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join37.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join39.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.map.ppr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.on.varchar.q</file>
    </fixedFiles>
  </bug>
  <bug id="16232" opendate="2017-3-16 00:00:00" fixdate="2017-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support stats computation for column in QuotedIdentifier</summary>
      <description>right now if a column contains double quotes ``, we can not compute its stats.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16249" opendate="2017-3-17 00:00:00" fixdate="2017-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>With column stats, mergejoin.q throws NPE</summary>
      <description>stack trace:2017-03-17T16:00:26,356 ERROR [3d512d4d-72b5-48fc-92cb-0c72f7c876e5 main] parse.CalcitePlanner: CBO failed, skipping CBO.java.lang.NullPointerException at org.apache.calcite.rel.metadata.RelMdUtil.estimateFilteredRows(RelMdUtil.java:719) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:123) ~[calcite-core-1.10.0.jar:1.10.0] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:201) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:132) ~[calcite-core-1.10.0.jar:1.10.0] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?] at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?] at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:201) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.swapInputs(LoptOptimizeJoinRule.java:1866) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.createJoinSubtree(LoptOptimizeJoinRule.java:1739) ~[calcite-core-1.10.0.jar:1.10.0] at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.addToTop(LoptOptimizeJoinRule.java:1216) ~[calcite-core-1.10.0.jar:1.10.0]</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.FilterSelectivityEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16274" opendate="2017-3-21 00:00:00" fixdate="2017-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support tuning of NDV of columns using lower/upper bounds</summary>
      <description>For partitioned tables, the distinct value (nDV) estimate for a column is by default set to the largest nDV value in any of the partitions being considered, which is a lower bound on the nDV estimate.This provides a config setting to allow the estimate to a specified fraction (0.0 - 1.0) of the higher bound on the nDV estimate (the sum of all the nDVs in all partitions).</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16275" opendate="2017-3-22 00:00:00" fixdate="2017-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Add ReduceSink support for TopN (in specialized native classes)</summary>
      <description>Currently, we don't specialize vectorization of ReduceSink when Top N is planned.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.include.no.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.number.compare.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nullsafe.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.non.constant.in.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.include.no.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.limit.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.vector.nohybridgrace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.adaptor.usage.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id3.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="16276" opendate="2017-3-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NoSuchMethodError: com.amazonaws.services.s3.transfer.TransferManagerConfiguration.setMultipartUploadThreshold(I)V</summary>
      <description>The druid-handler is pulling in some com.amazonaws dependencies that conflict with the version that Hadoop is using. This causes the above exception to be thrown when running Hive against S3. This patch fixes the dependency issue by shading the aws dependencies in the druid artifacts. Unfortunately, I can't find a great way to add a test for this so it doesn't happen in the future. We will need some more robust S3-integration tests for that.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16278" opendate="2017-3-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: metadata cache may incorrectly decrease memory usage in mem manager</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16386" opendate="2017-4-5 00:00:00" fixdate="2017-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add debug logging to describe why runtime filtering semijoins are removed</summary>
      <description>Add a few logging statements to detail the reason why semijoin optimizations are being removed, which can help during debugging.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="16422" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should kill running Spark Jobs when a query is cancelled.</summary>
      <description>Should kill running Spark Jobs when a query is cancelled. When a query is cancelled, Driver.releaseDriverContext will be called by Driver.close. releaseDriverContext will call DriverContext.shutdown which will call all the running tasks' shutdown. public synchronized void shutdown() { LOG.debug("Shutting down query " + ctx.getCmd()); shutdown = true; for (TaskRunner runner : running) { if (runner.isRunning()) { Task&lt;?&gt; task = runner.getTask(); LOG.warn("Shutting down task : " + task); try { task.shutdown(); } catch (Exception e) { console.printError("Exception on shutting down task " + task.getId() + ": " + e); } Thread thread = runner.getRunner(); if (thread != null) { thread.interrupt(); } } } running.clear(); }since SparkTask didn't implement shutdown method to kill the running spark job, the spark job may be still running after the query is cancelled. So it will be good to kill the spark job in SparkTask.shutdown to save cluster resource.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16423" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hint to enforce semi join optimization</summary>
      <description>Add hints in semijoin to enforce particular semi join optimization.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDynamicListDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HintParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16440" opendate="2017-4-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failing test columnstats_partlvl_invalid_values when autogather column stats is on</summary>
      <description></description>
      <version>2.1.0,2.3.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16441" opendate="2017-4-13 00:00:00" fixdate="2017-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>De-duplicate semijoin branches in n-way joins</summary>
      <description>Currently in n-way joins, semi join optimization creates n branches on same key. Instead it should reuse one branch for all the joins.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug id="16488" opendate="2017-4-20 00:00:00" fixdate="2017-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replicating into existing db if the db is empty</summary>
      <description>This is a potential usecase where a user may want to manually create a db on destination to make sure it goes to a certain dir root, or they may have cases where the db (default, for instance) was automatically created. We should still allow replicating into this without failing if the db is empty.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16493" opendate="2017-4-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip column stats when colStats is empty</summary>
      <description>Otherwise it will throw NPE</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16494" opendate="2017-4-21 00:00:00" fixdate="2017-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>udaf percentile_approx() may fail on CBO</summary>
      <description>select percentile_approx(key, array(0.50, 0.70, 0.90, 0.95, 0.99)) from t; fails with error : The second argument must be a constant.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.approx.23.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16628" opendate="2017-5-9 00:00:00" fixdate="2017-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix query25 when it uses a mix of MergeJoin and MapJoin</summary>
      <description></description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
    </fixedFiles>
  </bug>
  <bug id="16633" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>username for ATS data shall always be the uid who submit the job</summary>
      <description>When submitting query via HS2, username for ATS data becomes HS2 process uid in case of hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator. This should always be the real user id to make ATS data more secure and useful.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="16634" opendate="2017-5-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Use a pool of connections to a single AM from a daemon</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="16684" opendate="2017-5-16 00:00:00" fixdate="2017-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap REPL DUMP shouldn&amp;#39;t fail when table is dropped after fetching the table names.</summary>
      <description>Currently, bootstrap dump will fail if the table does't exist when try to dump. This shall occur when table is dropped after REPL DUMP fetched the table names.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16686" opendate="2017-5-16 00:00:00" fixdate="2017-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>repl invocations of distcp needs additional handling</summary>
      <description>When REPL LOAD invokes distcp, there needs to be a way for the user invoking REPL LOAD to pass on arguments to distcp. In addition, there is sometimes a need for distcp to be invoked from within an impersonated context, such as running as user "hdfs", asking distcp to preserve ownerships of individual files.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestFileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16706" opendate="2017-5-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap REPL DUMP shouldn&amp;#39;t fail when a partition is dropped/renamed when dump in progress.</summary>
      <description>Currently, bootstrap REPL DUMP gets the partitions in a batch and then iterate through it. If any partition is dropped/renamed during iteration, it may lead to failure/exception. In this case, the partition should be skipped from dump and also need to ensure no failure of REPL DUMP and the subsequent incremental dump should ensure the consistent state of the table.This bug is related to HIVE-16684.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16727" opendate="2017-5-22 00:00:00" fixdate="2017-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL DUMP for insert event should&amp;#39;t fail if the table is already dropped.</summary>
      <description>Currently, insert event doesn't log the table object as part of event notification message. During dump, the table object is obtained from metastore which can be null if the table is already dropped and hence REPL DUMP fails.Steps:1. Bootstrap dump/load with a table.2. Insert into the table.3. Drop the table.4. REPL DUMP (incremental).</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="16742" opendate="2017-5-23 00:00:00" fixdate="2017-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cap the number of reducers for LLAP at the configured value</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="16785" opendate="2017-5-30 00:00:00" fixdate="2017-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure replication actions are idempotent if any series of events are applied again.</summary>
      <description>Some of the events(ALTER, RENAME, TRUNCATE) are not idempotent and hence leads to failure of REPL LOAD if applied twice or applied on an object which is latest than current event. For example, if TRUNCATE is applied on a table which is already dropped will fail instead of noop.Also, need to consider the scenario where the object is missing while applying an event. For example, if RENAME_TABLE event is applied on target where the old table is missing should validate if table should be recreated or should treat the event as noop. This can be done by verifying the DB level last repl ID against the current event ID.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropFunctionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbstractMessageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16813" opendate="2017-6-2 00:00:00" fixdate="2017-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL LOAD should load the events in the same sequence as it is dumped.</summary>
      <description>Currently, incremental REPL DUMP use $dumpdir/&lt;eventID&gt; to dump the metadata and data files corresponding to the event. The event is dumped in the same sequence in which it was generated.Now, REPL LOAD, lists the directories inside $dumpdir using listStatus and sort it using compareTo algorithm of FileStatus class which doesn't check the length before sorting it alphabetically.Due to this, the event-100 is processed before event-99 and hence making the replica database non-sync with source.Need to use a customized compareTo algorithm to sort the FileStatus.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16990" opendate="2017-6-29 00:00:00" fixdate="2017-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD should update last repl ID only after successful copy of data files.</summary>
      <description>For REPL LOAD operations that includes both metadata and data changes should follow the below rule.1. Copy the metadata excluding the last repl ID.2. Copy the data files3. If Step 1 and 2 are successful, then update the last repl ID of the object.This rule will allow the the failed events to be re-applied by REPL LOAD and ensures no data loss due to failures.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.AbstractMessageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.PartitionSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.BootStrapReplicationSpecFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="16992" opendate="2017-6-29 00:00:00" fixdate="2017-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: monitoring and better default lambda for LRFU policy</summary>
      <description>LRFU is currently skewed heavily towards LRU; there are 10k-s or 100k-s of buffers tracked during a typical workload, but the heap size is around 700. We should see if making it closer to LFU (by tweaking the lambda) will improve hit rate with small queries infrequently interleaved with large scans; and whether it will have negative effects due to perf overhead.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DataCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileEstimateErrors.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.MetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SimpleBufferManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapDataBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="16996" opendate="2017-6-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HLL as an alternative to FM sketch to compute stats</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tunable.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.table.update.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.multi.output.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.varchar.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.vector.nohybridgrace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.identity.project.remove.skip.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.dist.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.parse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exec.parallel.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlated.join.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.long.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.quoting.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">common.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.StatsCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.ColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.ColumnStatsMergerFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.DateColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.LongColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.merge.StringColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.NumDistinctValueEstimator.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.DecimalNumDistinctValueEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.DoubleNumDistinctValueEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.LongNumDistinctValueEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.StringNumDistinctValueEstimator.java</file>
      <file type="M">ql.src.test.queries.clientpositive.char.udf1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.compute.stats.date.q</file>
      <file type="M">ql.src.test.queries.clientpositive.compute.stats.decimal.q</file>
      <file type="M">ql.src.test.queries.clientpositive.compute.stats.double.q</file>
      <file type="M">ql.src.test.queries.clientpositive.compute.stats.long.q</file>
      <file type="M">ql.src.test.queries.clientpositive.compute.stats.string.q</file>
      <file type="M">ql.src.test.queries.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q</file>
      <file type="M">ql.src.test.queries.clientpositive.varchar.udf1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.udf1.q</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.deep.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17021" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replication of concatenate operation.</summary>
      <description>We need to handle cases like ALTER TABLE ... CONCATENATE that also change the files on disk, and potentially treat them similar to INSERT OVERWRITE, as it does something equivalent to a compaction.Note that a ConditionalTask might also be fired at the end of inserts at the end of a tez task (or other exec engine) if appropriate HiveConf settings are set, to automatically do this operation - these also need to be taken care of for replication.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17022" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add mode in lock debug statements</summary>
      <description>Currently, lock debug statements print IMPLICIT/EXPLICIT as lock mode,whereas SHARED/EXCLUSIVE/SEMI_SHARED are more usefulwhen debugging.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17084" opendate="2017-7-13 00:00:00" fixdate="2017-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on hive.stats.fetch.column.stats configuration flag</summary>
      <description>This flag is off by default and could result in bad plans due to missing column statistics.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.udtf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf.octet.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf.character.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.struct.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.duplicate.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.constant.in.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.map.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.include.no.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.gather.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.trailing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.custom.udf.configure.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.string.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.join46.mr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.context.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.parquet.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.numeric.overflows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.nested.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.decimal.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateBasicStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.plan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.paren.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionall.unbalancedppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.data.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.number.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.octet.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.notequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isops.simplify.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.in.file.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.find.in.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.character.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.binarysetfunctions.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.binarysetfunctions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqual.corr.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.structin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.special.character.in.tabnames.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.onesideskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.variable.sub.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.processor.namespaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.setop.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.runtime.skewjoin.mapjoin.spark.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reloadJar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.result.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptfgroupbyjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.windowing1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udtf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.deterministic.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.position.alias.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.condition.remover.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.decimal.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.types.non.dictionary.encoding.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.no.row.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.by.expr.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.str.conversion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.threshold.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.named.column.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.with.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.union.src.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.manyViewJoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.keyword.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.parse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.on.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.grp.diff.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.emit.interval.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.pointlook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insertoverwrite.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.columnarserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast.during.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.identity.project.remove.skip.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multialias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.join.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.duplicate.key.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fp.literal.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.to.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.folder.predicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.floor.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.flatten.and.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.numeric.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.in.or.dup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.HIVE.15647.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.outputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exec.parallel.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.except.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.sortby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.orderby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.clusterby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.erasurecoding.erasure.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.with.different.encryption.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.empty.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.dist.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.when.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropWhen.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruning.partitioned.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.quoting.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.SortUnionTransposeRule.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.gby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.ppd.non.deterministic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.const.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast.on.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.explain.outputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.inclause.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.array.map.access.nonconstant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ansi.sql.arithmetic.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes4.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.arraymapstruct.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.vector.udf.example.arraymapstruct.q.out</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.spark.local.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-cluster.hive-site.xml</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key2.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.directory.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.spark.task.failure.q</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.masking.mv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sortmerge.mapjoin.mismatch.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alias.casted.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.date.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17183" opendate="2017-7-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable rename operations during bootstrap dump</summary>
      <description>Currently, bootstrap dump shall lead to data loss when any rename happens while dump in progress. Scenario: Fetch table names (T1 and T2) Dump table T1 Rename table T2 to T3 generates RENAME event Dump table T2 is noop as table doesnt exist. In target after load, it only have T1. Apply RENAME event will fail as T2 doesnt exist in target.This feature can be supported in next phase development as it need proper design to keep track of renamed tables/partitions. So, for time being, we shall disable rename operations when bootstrap dump in progress to avoid any inconsistent state.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17195" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Long chain of tasks created by REPL LOAD shouldn&amp;#39;t cause stack corruption.</summary>
      <description>Currently, long chain REPL LOAD tasks lead to huge recursive calls when try to traverse the DAG.For example, getMRTasks, getTezTasks, getSparkTasks and iterateTasks methods run recursively to traverse the DAG.Need to modify this traversal logic to reduce stack usage.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NodeUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17196" opendate="2017-7-28 00:00:00" fixdate="2017-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CM: ReplCopyTask should retain the original file names even if copied from CM path.</summary>
      <description>Consider the below scenario,1. Insert into table T1 with value(X).2. Insert into table T1 with value(X).3. Truncate the table T1.  This step backs up 2 files with same content to cmroot which ends up with one file in cmroot as checksum matches.4. Incremental repl with above 3 operations. In this step, both the insert event files will be read from cmroot where copy of one leads to overwrite the other one as the file name is same in cm path (checksum as file name).So, this leads to data loss and hence it is necessary to retain the original file names even if we copy from cm path.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17212" opendate="2017-7-31 00:00:00" fixdate="2017-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic add partition by insert shouldn&amp;#39;t generate INSERT event.</summary>
      <description>A partition is dynamically added if INSERT INTO is invoked on a non-existing partition.Generally, insert operation generated INSERT event to notify the operation with new data files.In this case, Hive should generate only ADD_PARTITION events with the new files added. It shouldn't create INSERT event.Need to test and verify this behaviour.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17213" opendate="2017-7-31 00:00:00" fixdate="2017-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: file merging doesn&amp;#39;t work for union all</summary>
      <description>HoS file merging doesn't work properly since it doesn't set linked file sinks properly which is used to generate move tasks.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="17214" opendate="2017-7-31 00:00:00" fixdate="2017-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>check/fix conversion of unbucketed non-acid to acid</summary>
      <description>bucketed tables have stricter rules for file layout on disk - bucket files are direct children of a partition directory.for un-bucketed tables I'm not sure there are any rulesfor example, CTAS with Tez + Union operator creates 1 directory for each leg of the unionSupposedly Hive can read table by picking all files recursively. Can it also write (other than CTAS example above) arbitrarily?Does it mean Acid write can also write anywhere?Figure out what can be supported and how can existing layout can be checked? Examining a full "ls -l -R" for a large table could be expensive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
    </fixedFiles>
  </bug>
  <bug id="17327" opendate="2017-8-15 00:00:00" fixdate="2017-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: restrict native file ID usage to default FS to avoid hypothetical collisions when HDFS federation is used</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ExternalCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.FileMetadataManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="17428" opendate="2017-9-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD of ALTER_PARTITION event doesn&amp;#39;t create import tasks if the partition doesn&amp;#39;t exist during analyze phase.</summary>
      <description>If the incremental dump event sequence have ADD_PARTITION followed by ALTER_PARTITION doesn't create any task for ALTER_PARTITION event as the partition doesn't exist during analyze phase. Due to this REPL STATUS returns wrong last repl ID.Scenario:1. Create DB2. Create partitioned table.3. Bootstrap dump and load4. Insert into table to a dynamically created partition. - This insert generate ADD_PARTITION and ALTER_PARTITION events.5. Incremental dump and load. Load will be successful. But the last repl ID set was incorrect as ALTER_PARTITION event was never applied.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17429" opendate="2017-9-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC doesn&amp;#39;t return rows when querying Impala</summary>
      <description>The Hive JDBC driver used to return a result set when querying Impala. Now, instead, it gets data back but interprets the data as query logs instead of a resultSet. This causes many issues (we see complaints about beeline as well as test failures).This appears to be a regression introduced with asynchronous operation against Hive.Ideally, we could make both behaviors work. I have a simple patch that should fix the problem.</description>
      <version>2.1.0,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>2.1.0,2.1.1,2.2.1,2.3.4,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="1743" opendate="2010-10-22 00:00:00" fixdate="2010-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group-by to determine equals of Keys in reverse order</summary>
      <description>When processing group-by, in reduce side, keys are ordered. Comparing equality of two keys can be more efficient in reverse order.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17527" opendate="2017-9-13 00:00:00" fixdate="2017-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replication for rename/move table across database</summary>
      <description>Rename/move table across database should be supported for replication. The scenario is as follows.1. Create 2 databases (db1 and db2) in source cluster.2. Create the table db1.tbl1.3. Run bootstrap replication for db1 and db2 to target cluster.4. Rename db1.tbl1 to db2.tbl1 in source.5. Run incremental replication for both db1 and db2. db1 dump missed the rename table operation as no event is generated for db1. So, table exist after load. db2 load skips the rename event as the source table is missing in target.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="17529" opendate="2017-9-14 00:00:00" fixdate="2017-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket Map Join : Sets incorrect edge type causing execution failure</summary>
      <description>If while traversing the tree to generate tasks, a bucket mapjoin may set its edge as CUSTOM_SIMPLE_EDGE against CUSTOM_EDGE if the bigtable is already not traversed causing Tez to assert and fail the vertex.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="17661" opendate="2017-9-30 00:00:00" fixdate="2017-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBTxnManager.acquireLocks() - MM tables should use shared lock for Insert</summary>
      <description>case INSERT: assert t != null; if(AcidUtils.isFullAcidTable(t)) { compBuilder.setShared(); } else { if (conf.getBoolVar(HiveConf.ConfVars.HIVE_TXN_STRICT_LOCKING_MODE)) {if(AcidUtils.isFullAcidTable(t)) { should probably be if(AcidUtils.isAcidTable(t)) {</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.mm.truncate.cols.q</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.convert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="17664" opendate="2017-10-2 00:00:00" fixdate="2017-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description></description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="17830" opendate="2017-10-18 00:00:00" fixdate="2017-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>dbnotification fails to work with rdbms other than postgres</summary>
      <description>as part of HIVE-17721 we had changed the direct sql to acquire the lock for postgres asselect "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" for update;however this breaks other databases and we have to use different sql statements for different databases for postgres useselect "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" for update;for SQLServer select "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" with (updlock);for other databases select NEXT_EVENT_ID from NOTIFICATION_SEQUENCE for update;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17976" opendate="2017-11-3 00:00:00" fixdate="2017-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: don&amp;#39;t set output collector if there&amp;#39;s no data to process</summary>
      <description>MR doesn't set an output collector if no row is processed, i.e. ExecMapper::map is never called. Let's investigate whether Spark should do the same.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="18023" opendate="2017-11-8 00:00:00" fixdate="2017-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redact the expression in lineage info</summary>
      <description>The query redactor is redacting the query itself while the expression shown in lineage info is not, which may still expose sensitive info. The following queryselect customers.id, customers.name from customers where customers.addresses&amp;#91;&amp;#39;shipping&amp;#39;&amp;#93;.zip_code ='1234-5678-1234-5678'; will have a log entry in lineage. The expression should also be redacted.[HiveServer2-Background-Pool: Thread-43]: {"version":"1.0","user":"hive","timestamp":1510179280,"duration":40747,"jobIds":["job_1510150684172_0006"],"engine":"mr","database":"default","hash":"a2b4721a0935e3770d81649d24ab1cd4","queryText":"select customers.id, customers.name from customers where customers.addresses['shipping'].zip_code ='XXXX-XXXX-XXXX-XXXX'","edges":[{"sources":[2],"targets":[0],"edgeType":"PROJECTION"},{"sources":[3],"targets":[1],"edgeType":"PROJECTION"},{"sources":[],"targets":[0,1],"expression":"(addresses['shipping'].zip_code = '1234-5678-1234-5678')","edgeType":"PREDICATE"}],"vertices":[{"id":0,"vertexType":"COLUMN","vertexId":"customers.id"},{"id":1,"vertexType":"COLUMN","vertexId":"customers.name"},{"id":2,"vertexType":"COLUMN","vertexId":"default.customers.id"},{"id":3,"vertexType":"COLUMN","vertexId":"default.customers.name"}]}</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="18251" opendate="2017-12-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loosen restriction for some checks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="18414" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18459" opendate="2018-1-16 00:00:00" fixdate="2018-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec.jar leaks contents fb303.jar into classpath</summary>
      <description>thrift classes are now in the hive classpath in the hive-exec.jar (HIVE-11553). This makes it hard to test with other versions of this library. This library is already a declared dependency and is not required to be included in the hive-exec.jar.I am proposing that we not include these classes like we have done in the past releases.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1846" opendate="2010-12-9 00:00:00" fixdate="2010-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change hive assumption that local mode mappers/reducers always run in same jvm</summary>
      <description>we are trying out a version of hadoop local mode that runs multiple mappers/reducers by spawning jvm's for them. In this mode hive mappers fail in reading the plan file. it seems that we assume (in the setMapredWork call) that local mode mappers/reducers will run in the same jvm (we can cache the current plan in a global var and don't serialize to a path). this needs to get fixed.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java.orig</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java.orig</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java.orig</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18955" opendate="2018-3-14 00:00:00" fixdate="2018-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: Unable to create Channel from class NioServerSocketChannel</summary>
      <description>Hit the issue when trying launch spark job. Stack trace:Caused by: java.lang.NoSuchMethodError: io.netty.channel.DefaultChannelId.newInstance()Lio/netty/channel/DefaultChannelId; at io.netty.channel.AbstractChannel.newId(AbstractChannel.java:111) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.AbstractChannel.&lt;init&gt;(AbstractChannel.java:83) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.nio.AbstractNioChannel.&lt;init&gt;(AbstractNioChannel.java:84) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.nio.AbstractNioMessageChannel.&lt;init&gt;(AbstractNioMessageChannel.java:42) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.socket.nio.NioServerSocketChannel.&lt;init&gt;(NioServerSocketChannel.java:86) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.socket.nio.NioServerSocketChannel.&lt;init&gt;(NioServerSocketChannel.java:72) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_151] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_151] at io.netty.channel.ReflectiveChannelFactory.newChannel(ReflectiveChannelFactory.java:38) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] ... 32 moreIt seems we have conflicts versions of class io.netty.channel.DefaultChannelId from async-http-client.jar and netty-all.jar</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19083" opendate="2018-3-30 00:00:00" fixdate="2018-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19231" opendate="2018-4-17 00:00:00" fixdate="2018-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline generates garbled output when using UnsupportedTerminal</summary>
      <description>We had a customer that was using some sort of front end that would invoke beeline commands with some query files on a node that that remote to the HS2 node.So beeline runs locally on this edge but connects to a remote HS2. Since the fix made in HIVE-14342, the beeline started producing garbled line in the output. Something like^Mnull ^Mnull^Mnull ^Mnull00-0000 All Occupations 135185230 4227011-0000 Management occupations 6152650 100310I havent been able to reproduce the issue locally as I do not have their system, but with some additional instrumentation I have been able to get some info regarding the beeline process.Essentially, such invocation causes beeline process to run with -Djline.terminal=jline.UnsupportedTerminal all the time and thus causes the issue. They can run the same beeline command directly in the shell on the same host and it does not cause this issue.PID S TTY TIME COMMAND44107 S S ? 00:00:00 bash beeline -u ...PID S TTY TIME COMMAND48453 S+ S pts/4 00:00:00 bash beeline -u ...Somehow that process wasnt attached to any local terminals. So the check made for /dev/stdin wouldnt work.Instead an additional check to check the TTY session of the process before using the UnsupportedTerminal (which really should only be used for backgrounded beeline sessions) seems to resolve the issue.</description>
      <version>2.1.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="19250" opendate="2018-4-19 00:00:00" fixdate="2018-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema column definitions inconsistencies in MySQL</summary>
      <description>There are some inconsistencies in column definitions in MySQL between a schema that was upgraded to 2.1 (from an older release) vs installing the 2.1.0 schema directly.&gt; `CQ_TBLPROPERTIES` varchar(2048) DEFAULT NULL,117d117&lt; `CQ_TBLPROPERTIES` varchar(2048) DEFAULT NULL,135a136&gt; `CC_TBLPROPERTIES` varchar(2048) DEFAULT NULL,143d143&lt; `CC_TBLPROPERTIES` varchar(2048) DEFAULT NULL,156c156&lt; `CTC_TXNID` bigint(20) DEFAULT NULL,&amp;#8212;&gt; `CTC_TXNID` bigint(20) NOT NULL,158c158&lt; `CTC_TABLE` varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,&amp;#8212;&gt; `CTC_TABLE` varchar(256) DEFAULT NULL,476c476&lt; `TBL_NAME` varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,&amp;#8212;&gt; `TBL_NAME` varchar(256) DEFAULT NULL,664c664&lt; KEY `PCS_STATS_IDX` (`DB_NAME`,`TABLE_NAME`,`COLUMN_NAME`,`PARTITION_NAME`),&amp;#8212;&gt; KEY `PCS_STATS_IDX` (`DB_NAME`,`TABLE_NAME`,`COLUMN_NAME`,`PARTITION_NAME`) USING BTREE,768c768&lt; `PARAM_VALUE` mediumtext,&amp;#8212;&gt; `PARAM_VALUE` mediumtext CHARACTER SET latin1 COLLATE latin1_bin,814c814&lt; `PARAM_VALUE` mediumtext,&amp;#8212;&gt; `PARAM_VALUE` mediumtext CHARACTER SET latin1 COLLATE latin1_bin,934c934&lt; `PARAM_VALUE` mediumtext,&amp;#8212;&gt; `PARAM_VALUE` mediumtext CHARACTER SET latin1 COLLATE latin1_bin,1066d1065&lt; `TXN_HEARTBEAT_COUNT` int(11) DEFAULT NULL,1067a1067&gt; `TXN_HEARTBEAT_COUNT` int(11) DEFAULT NULL,1080c1080&lt; `TC_TXNID` bigint(20) DEFAULT NULL,&amp;#8212;&gt; `TC_TXNID` bigint(20) NOT NULL,1082c1082&lt; `TC_TABLE` varchar(128) DEFAULT NULL,&amp;#8212;&gt; `TC_TABLE` varchar(128) NOT NULL,1084c1084&lt; `TC_OPERATION_TYPE` char(1) DEFAULT NULL,&amp;#8212;&gt; `TC_OPERATION_TYPE` char(1) NOT NULL,</description>
      <version>2.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.039-HIVE-12274.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="19471" opendate="2018-5-9 00:00:00" fixdate="2018-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucket_map_join_tez1 and bucket_map_join_tez2 are failing</summary>
      <description>https://builds.apache.org/job/PreCommit-HIVE-Build/10766/testReport/TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;bucket_map_join_tez1&amp;#93;TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;bucket_map_join_tez2&amp;#93;Both are failing. Probably need golden file update.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19744" opendate="2018-5-31 00:00:00" fixdate="2018-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In Beeline if -u is specified the default connection should not be tried at all</summary>
      <description>I wanted to explicitly connect to a hiveserver by specifying -u but that didn't work because it was not running/etc...The strange thing is that somehow the default connection is activated...and triedThe possible "hazard" here is that if someone specifies -u $MY_DEV_HS2 -f recreate_db.sql to run some sql script...beeline may connect somewhere else and run the commands there - which might have serious consequences.... (in the above case having default as production might be interesting)beeline -u jdbc:hive2://localhost:10502/;transportMode=binary -n hrt_qaSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/hdp/3.0.0.0-1406/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/hdp/3.0.0.0-1406/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Connecting to jdbc:hive2://localhost:10502/18/05/31 07:51:20 [main]: WARN jdbc.HiveConnection: Failed to connect to localhost:10502Unknown HS2 problem when communicating with Thrift server.Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10502/: Invalid status 72 (state=08S01,code=0)Connecting to jdbc:hive2://ctr-e138-1518143905142-336795-01-000016.hwx.site:2181,ctr-e138-1518143905142-336795-01-000008.hwx.site:2181,ctr-e138-1518143905142-336795-01-000014.hwx.site:2181,ctr-e138-1518143905142-336795-01-000009.hwx.site:2181,ctr-e138-1518143905142-336795-01-000015.hwx.site:2181/default;httpPath=cliservice;principal=hive/_HOST@EXAMPLE.COM;serviceDiscoveryMode=zooKeeper;ssl=true;transportMode=http;zooKeeperNamespace=hiveserver218/05/31 07:51:21 [main]: INFO jdbc.HiveConnection: Connected to ctr-e138-1518143905142-336795-01-000003.hwx.site:1000118/05/31 07:51:21 [main]: ERROR jdbc.HiveConnection: Error opening sessionorg.apache.thrift.transport.TTransportException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: signature check failed</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.hs2connection.BeelineWithHS2ConnectionFileTestBase.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="19831" opendate="2018-6-8 00:00:00" fixdate="2018-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiveserver2 should skip doAuth checks for CREATE DATABASE/TABLE if database/table already exists</summary>
      <description>with sqlstdauthon,Create database if exists takeTOOLONG if there are too many objects inside the database directory. Hive should not run the doAuth checks for all the objects within databaseif the database already exists.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19944" opendate="2018-6-19 00:00:00" fixdate="2018-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate and fix version mismatch of GCP</summary>
      <description>We've observed that adding a new image to the ptest GCP project breaks our currently working infrastructure when we try to restart the hive ptest server.This is because upon initialization the project's images are queried and we immediately get an exception for newly added images - they don't have a field that our client thinks should be mandatory to have. I believe there's an upgrade needed on our side for the GCP libs we depend on.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20082" opendate="2018-7-4 00:00:00" fixdate="2018-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveDecimal to string conversion doesn&amp;#39;t format the decimal correctly - master</summary>
      <description>Example: LPAD on a decimal(7,1) values of 0 returns "0" (plus padding) but it should be "0.0" (plus padding)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.complex.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIColumnArithmeticDTIColumnNoConvert.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIScalarArithmeticDTIColumnNoConvert.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToString.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.pad.convert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20100" opendate="2018-7-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OpTraits : Select Optraits should stop when a mismatch is detected</summary>
      <description>The select operator's optraits logic as stated in the comment is,// For bucket columns// If all the columns match to the parent, put them in the bucket cols// else, add empty list.// For sort columns// Keep the subset of all the columns as long as order is maintained.However, this is not happening due to a bug. The bool found is never reset, so if a single match is found, the value remains true and allows the optraits get populated with partial list of columns for bucket col which is incorrect.This may lead to creation of SMB join which should not happen.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="20201" opendate="2018-7-18 00:00:00" fixdate="2018-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive shouldn&amp;#39;t use HBase&amp;#39;s Base64 implementation</summary>
      <description>HBase is removing their Base64 implementation because it never should have been public, so Hive should switch to a different provider. Hive already uses Commons-Codec Base64 in other places, so that would be a natural replacement.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="2052" opendate="2011-3-12 00:00:00" fixdate="2011-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostHook and PreHook API to add flag to indicate it is pre or post hook plus cache for content summary</summary>
      <description>This will allow hooks to share some information better and reduce their latency</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="20978" opendate="2018-11-28 00:00:00" fixdate="2018-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"hive.jdbc.*" should add to sqlStdAuthSafeVarNameRegexes</summary>
      <description>User should be able to change hive.jdbc settings, include "hive.jdbc.pushdown.enable".</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21009" opendate="2018-12-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP - Specify binddn for ldap-search</summary>
      <description>When user accounts cannot do an LDAP search, there is currently no way of specifying a custom binddn to use for the ldap-search.So I'm missing something like that:hive.server2.authentication.ldap.bindn=cn=ldapuser,ou=user,dc=examplehive.server2.authentication.ldap.bindnpw=password</description>
      <version>2.1.0,2.1.1,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22055" opendate="2019-7-26 00:00:00" fixdate="2019-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select count gives incorrect result after loading data from text file</summary>
      <description>Add one more load to mm_loaddata.q:Load data 3 times (both kv1.txt and kv2.txt contains 500 records)create table load0_mm (key string, value string) stored as textfile tblproperties("transactional"="true", "transactional_properties"="insert_only");load data local inpath '../../data/files/kv1.txt' into table load0_mm;select count(1) from load0_mm;load data local inpath '../../data/files/kv2.txt' into table load0_mm;select count(1) from load0_mm;load data local inpath '../../data/files/kv2.txt' into table load0_mm;select count(1) from load0_mm;Expected outputPREHOOK: query: load data local inpath '../../data/files/kv2.txt' into table load0_mmPREHOOK: type: LOAD#### A masked pattern was here ####PREHOOK: Output: default@load0_mmPOSTHOOK: query: load data local inpath '../../data/files/kv2.txt' into table load0_mmPOSTHOOK: type: LOAD#### A masked pattern was here ####POSTHOOK: Output: default@load0_mmPREHOOK: query: select count(1) from load0_mmPREHOOK: type: QUERYPREHOOK: Input: default@load0_mm#### A masked pattern was here ####POSTHOOK: query: select count(1) from load0_mmPOSTHOOK: type: QUERYPOSTHOOK: Input: default@load0_mm#### A masked pattern was here ####1500Got:&amp;#91;ERROR&amp;#93;  TestMiniLlapLocalCliDriver.testCliDriver:59 Client Execution succeeded but contained differences (error code = 1) after executing mm_loaddata.q63c63&lt; 1480&gt; 1500</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="724" opendate="2009-8-5 00:00:00" fixdate="2009-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Server needs a getHiveSchema() thrift function</summary>
      <description>Currently, the Hive Server only has a getSchema() thrift function that will return a schema with fields listed as thrift DDL types. We should also have another function, getHiveSchema() (or some other function name), that will instead return a schema with fields listed as native Hive types. This will make the returned type names consistent with the HiveMetaStoreUtils get_fields() thrift function.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">service.src.gen-py.hive.service.ThriftHive.py</file>
      <file type="M">service.src.gen-py.hive.service.ThriftHive-remote</file>
      <file type="M">service.src.gen-php.ThriftHive.php</file>
      <file type="M">service.src.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">service.if.hive.service.thrift</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7240" opendate="2014-6-16 00:00:00" fixdate="2014-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add classifier for avro-mapred jar</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
