<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10006" opendate="2015-3-18 00:00:00" fixdate="2015-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RSC has memory leak while execute multi queries.[Spark Branch]</summary>
      <description>While execute query with RSC, MapWork/ReduceWork number is increased all the time, and lead to OOM at the end.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="10099" opendate="2015-3-26 00:00:00" fixdate="2015-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant folding for Decimal</summary>
      <description></description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="101" opendate="2008-12-2 00:00:00" fixdate="2008-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add various files and directories to svn:ignore</summary>
      <description>When creating patches or committing code it's nice to know that certain directories and files will never be included.I suggest we add the following to the svn:ignore variable (for more information see: http://svnbook.red-bean.com/en/1.1/ch07s02.html):build.classpath.project.settings</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1010" opendate="2009-12-23 00:00:00" fixdate="2009-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement INFORMATION_SCHEMA in Hive</summary>
      <description>INFORMATION_SCHEMA is part of the SQL92 standard and would be useful to implement using our metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.jdbc.handler.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.InputEstimatorTestClass.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">jdbc-handler.src.test.java.org.apache.hive.config.JdbcStorageConfigManagerTest.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcStorageHandler.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcSerDe.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcRecordReader.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputFormat.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.MySqlDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.JdbcRecordIterator.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.conf.JdbcStorageConfigManager.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.conf.DatabaseType.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="10115" opendate="2015-3-27 00:00:00" fixdate="2015-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 running on a Kerberized cluster should offer Kerberos(GSSAPI) and Delegation token(DIGEST) when alternate authentication is enabled</summary>
      <description>In a Kerberized cluster when alternate authentication is enabled on HS2, it should also accept Kerberos Authentication. The reason this is important is because when we enable LDAP authentication HS2 stops accepting delegation token authentication. So we are forced to enter username passwords in the oozie configuration.The whole idea of SASL is that multiple authentication mechanism can be offered. If we disable Kerberos(GSSAPI) and delegation token (DIGEST) authentication when we enable LDAP authentication, this defeats SASL purpose.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10122" opendate="2015-3-27 00:00:00" fixdate="2015-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore filter-by-expression is broken for non-partition expressions</summary>
      <description>See https://issues.apache.org/jira/browse/HIVE-10091?focusedCommentId=14382413&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14382413These two lines of code // Replace virtual columns with nulls. See javadoc for details. prunerExpr = removeNonPartCols(prunerExpr, extractPartColNames(tab), partColsUsedInFilter); // Remove all parts that are not partition columns. See javadoc for details. ExprNodeDesc compactExpr = compactExpr(prunerExpr.clone());are supposed to take care of this; I see there were bunch of changes to this code over some time, and now it appears to be broken.Thanks to thejas for info.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
    </fixedFiles>
  </bug>
  <bug id="10140" opendate="2015-3-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window boundary is not compared correctly</summary>
      <description>“ROWS between 10 preceding and 2 preceding” is not handled correctly.Underlying error: Window range invalid, start boundary is greater than end boundary: window(start=range(10 PRECEDING), end=range(2 PRECEDING))If I change it to “2 preceding and 10 preceding”, the syntax works but the results are 0 of course.Reason for the function: during analysis, it is sometimes desired to design the window to filter the most recent events, in the case of the events' responses are not available yet. There is a workaround for this, but it is better/more proper to fix the bug.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.windowspec.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
    </fixedFiles>
  </bug>
  <bug id="1015" opendate="2009-12-28 00:00:00" fixdate="2009-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java MapReduce wrapper for TRANSFORM/MAP/REDUCE scripts</summary>
      <description>Larry Ogrodnek has written a set of wrapper classes that make it possibleto write Hive TRANSFORM/MAP/REDUCE scripts in Java in a style thatmore closely resembles conventional Hadoop MR programs.A blog post describing this library can be found here: http://dev.bizo.com/2009/10/hive-map-reduce-in-java.htmlThe source code (with Apache license) is available here: http://github.com/ogrodnek/shmrjWe should add this to contrib.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10150" opendate="2015-3-30 00:00:00" fixdate="2015-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>delete from acidTbl where a in(select a from nonAcidOrcTbl) fails</summary>
      <description>this query raises an error "10297,FAILED: SemanticException &amp;#91;Error 10297&amp;#93;: Attempt to do update or delete on table nonAcidOrcTbl that does not use an AcidOutputFormat or is not bucketed"even though nonAcidOrcTbl is only being read, not written.select b from " + Table.ACIDTBL + " where a in (select b from " + Table.NONACIDORCTBL + ")runs fine.There doesn't seem to be any logical reason why we should rise the error here.Same for 'update' statement.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10167" opendate="2015-3-31 00:00:00" fixdate="2015-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 logs the server started only before the server is shut down</summary>
      <description>TThreadPoolServer#serve() blocks till the server is down. We should log before that.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="10177" opendate="2015-4-1 00:00:00" fixdate="2015-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant folding for char &amp; varchar</summary>
      <description></description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10225" opendate="2015-4-6 00:00:00" fixdate="2015-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI JLine does not flush history on quit/Ctrl-C</summary>
      <description>Hive CLI is not saving history, if hive cli is terminated using a Ctrl-C or "quit;".HIVE-9310 fixed it for the case where one exits with Ctrl-D (EOF), but not for the above ways of exiting.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="10231" opendate="2015-4-6 00:00:00" fixdate="2015-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute partition column stats fails if partition col type is date</summary>
      <description>Currently the command "analyze table .. partition .. compute statistics for columns" may only work for partition column type of string, numeric types, but not others like date. See following case using date as partition coltype:create table colstatspartdate (key int, value string) partitioned by (ds date, hr int);insert into colstatspartdate partition (ds=date '2015-04-02', hr=2) select key, value from src limit 20;analyze table colstatspartdate partition (ds=date '2015-04-02', hr=2) compute statistics for columns;you will get RuntimeException:FAILED: RuntimeException Cannot convert to Date from: int15/04/06 17:30:01 ERROR ql.Driver: FAILED: RuntimeException Cannot convert to Date from: intjava.lang.RuntimeException: Cannot convert to Date from: int at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDate(PrimitiveObjectInspectorUtils.java:1048) at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$DateConverter.convert(PrimitiveObjectInspectorConverter.java:264) at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.typeCast(ConstantPropagateProcFactory.java:163) at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.propagate(ConstantPropagateProcFactory.java:333) at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.foldExpr(ConstantPropagateProcFactory.java:242)....</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10263" opendate="2015-4-8 00:00:00" fixdate="2015-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Aggregate checking input for bucketing should be conditional</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
    </fixedFiles>
  </bug>
  <bug id="10291" opendate="2015-4-10 00:00:00" fixdate="2015-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark job configuration needs to be logged [Spark Branch]</summary>
      <description>In a Hive on MR job, all the job properties are put into the JobConf, which can then be viewed via the MR2 HistoryServer's Job UI.However, in Hive on Spark we are submitting an application that is long-lived. Hence, we only put properties into the SparkConf relevant to application submission (spark and yarn properties). Only these are viewable through the Spark HistoryServer Application UI.It is the Hive application code (RemoteDriver, aka RemoteSparkContext) that is responsible for serializing and deserializing the job.xml per job (ie, query) within the application. Thus, for supportability we also need to give an equivalent mechanism to print the job.xml per job.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="10304" opendate="2015-4-10 00:00:00" fixdate="2015-1-10 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add deprecation message to HiveCLI</summary>
      <description>As Beeline is now the recommended command line tool to Hive, we should add a message to HiveCLI to indicate that it is deprecated and redirect them to Beeline. This is not suggesting to remove HiveCLI for now, but just a helpful direction for user to know the direction to focus attention in Beeline.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="10306" opendate="2015-4-10 00:00:00" fixdate="2015-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We need to print tez summary when hive.server2.logging.level &gt;= PERFORMANCE.</summary>
      <description>We need to print tez summary when hive.server2.logging.level &gt;= PERFORMANCE. We introduced this parameter via HIVE-10119.The logging param for levels is only relevant to HS2, so for hive-cli users the hive.tez.exec.print.summary still makes sense. We can check for log-level param as well, in places we are checking value of hive.tez.exec.print.summary. Ie, consider hive.tez.exec.print.summary=true if log.level &gt;= PERFORMANCE.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingAPI.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hive.jdbc.TestSchedulerQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="10324" opendate="2015-4-13 00:00:00" fixdate="2015-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metatool should take table_param_key to allow for changes to avro serde&amp;#39;s schema url key</summary>
      <description>HIVE-3443 added support to change the serdeParams from 'metatool updateLocation' command.However, in avro it is possible to specify the schema via the tableParams:CREATE TABLE `testavro`( `test` string COMMENT 'from deserializer')ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'TBLPROPERTIES ( 'avro.schema.url'='hdfs://namenode:8020/tmp/test.avsc', 'kite.compression.type'='snappy', 'transient_lastDdlTime'='1427996456')Hence for those tables the 'metatool updateLocation' will not help.This is necessary in case like upgrade the namenode to HA where the absolute paths have changed.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="10326" opendate="2015-4-13 00:00:00" fixdate="2015-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Invoke Hive&amp;#39;s Cumulative Cost</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="10328" opendate="2015-4-14 00:00:00" fixdate="2015-1-14 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Enable new return path for cbo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnColumn.txt</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizationBench.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="10329" opendate="2015-4-14 00:00:00" fixdate="2015-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop reflectionutils has issues</summary>
      <description>1) Constructor cache leaks classes and their attendant static overhead forever.2) Class cache inside conf used when getting JobConfigurable classes has an epic lock.Both bugs are files in Hadoop but will hardly ever be fixed at this rate. This version avoids both problems</description>
      <version>None</version>
      <fixedVersion>llap,1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10331" opendate="2015-4-14 00:00:00" fixdate="2015-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC : Is null SARG filters out all row groups written in old ORC format</summary>
      <description>Queries are returning wrong results as all row groups gets filtered out and no rows get scanned.SELECT count(*) FROM store_sales WHERE ss_addr_sk IS NULLWith hive.optimize.index.filter disabled we get the correct resultsIn pickRowGroups stats show that hasNull_ is fales, while the rowgroup actually has null.Same query runs fine for newly loaded ORC tables.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="10451" opendate="2015-4-23 00:00:00" fixdate="2015-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTF deserializer fails if values are not used in reducer</summary>
      <description>In this particular case no values are needed from reducer to complete processing.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
    </fixedFiles>
  </bug>
  <bug id="10452" opendate="2015-4-23 00:00:00" fixdate="2015-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup fix for HIVE-10202 to restrict it it for script mode.</summary>
      <description>The fix made in HIVE-10202 needs to be limited to when beeline is running in a script mode aka -f option. Otherwise, if --silent=true is set in interactive mode, the prompt disappears and so does what you type in.say beeline -u jdbc:hive2://localhost:10000 --silent=trueIt appears to hang but in reality it doesnt display any prompt. The workaround is to not use the --silent=true option with non-interactive mode.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="10453" opendate="2015-4-23 00:00:00" fixdate="2015-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 leaking open file descriptors when using UDFs</summary>
      <description>1. create a custom function byCREATE FUNCTION myfunc AS 'someudfclass' using jar 'hdfs:///tmp/myudf.jar';2. Create a simple jdbc client, just do connect, run simple query which using the function such as:select myfunc(col1) from sometable3. Disconnect.Check open file for HiveServer2 by:lsof -p HSProcID | grep myudf.jarYou will see the leak as:java 28718 ychen txt REG 1,4 741 212977666 /private/var/folders/6p/7_njf13d6h144wldzbbsfpz80000gp/T/1bfe3de0-ac63-4eba-a725-6a9840f1f8d5_resources/myudf.jarjava 28718 ychen 330r REG 1,4 741 212977666 /private/var/folders/6p/7_njf13d6h144wldzbbsfpz80000gp/T/1bfe3de0-ac63-4eba-a725-6a9840f1f8d5_resources/myudf.jar</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
    </fixedFiles>
  </bug>
  <bug id="10455" opendate="2015-4-23 00:00:00" fixdate="2015-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Different data types at Reducer before JoinOp</summary>
      <description>The following error occured for cbo_subq_not_in.q java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error: Unable to deserialize reduce input key from x1x128x0x0x1 with properties {columns=reducesinkkey0, serialization.lib=org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe, serialization.sort.order=+, columns.types=double} at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)A more easier way to reproduce is set hive.cbo.enable=true;set hive.exec.check.crossproducts=false;set hive.stats.fetch.column.stats=true;set hive.auto.convert.join=false;select p_size, src.keyfrom part join srcon p_size=key;As you can see, p_size is integer while src.key is string. Both of them should be cast to double when they join. When return path is off, this will happen before Join, at RS. However, when return path is on, this will be considered as an expression in Join. Thus, when reducer is collecting different types of keys from different join branches, it throws exception.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortExchange.java</file>
    </fixedFiles>
  </bug>
  <bug id="10468" opendate="2015-4-23 00:00:00" fixdate="2015-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create scripts to do metastore upgrade tests on jenkins for Oracle DB.</summary>
      <description>This JIRA is to isolate the work specific to Oracle DB in HIVE-10239. Because of absence of 64 bit debian packages for oracle-xe, the apt-get install fails on the AWS systems.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.dbs.oracle.prepare.sh</file>
      <file type="M">metastore.dbs.oracle.execute.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10533" opendate="2015-4-29 00:00:00" fixdate="2015-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Join to MultiJoin support for outer joins</summary>
      <description>CBO return path: auto_join7.q can be used to reproduce the problem.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="10535" opendate="2015-4-29 00:00:00" fixdate="2015-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Cleanup map join cache when a query completes</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedTreeReaderFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10542" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Full outer joins in tez produce incorrect results in certain cases</summary>
      <description>If there is no records for one of the tables in the full outer join, we do not read the other input and end up not producing rows which we should be.</description>
      <version>1.0.0,1.1.0,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mergejoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="10546" opendate="2015-4-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>genFileSinkPlan should use the generated SEL&amp;#39;s RR for the partition col of FS</summary>
      <description>Right now, when Hive writes data into a bucketed table, it will use the last OP to generate RS-SEL and then FS. However, the context rsCtx carries partition column from the last OP rather than from the SEL, which makes FS use the wrong partition column.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10576" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add jar command does not work with Windows OS</summary>
      <description>Steps to reproduce this issue in Windows OS:hadoop.cmd fs -mkdir -p /tmp/testjarshadoop.cmd fs copyFromLocal &lt;hive-hcatalog-core*.jar&gt; /tmp/testjarsfrom hive cli:add jar hdfs:///tmp/testjars/hive-hcatalog-core-*.jar;add jar D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar;hive&gt; add jar hdfs:///tmp/testjars/hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar;converting to local hdfs:///tmp/testjars/hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarIllegal character in opaque part at index 2: C:\Users\hadoopqa\AppData\Local\Temp\cf0c70a4-f8e5-43ae-8c94-aa528f90887d_resources\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarQuery returned non-zero code: 1, cause: java.net.URISyntaxException: Illegal character in opaque part at index 2: C:\Users\hadoopqa\AppData\Local\Temp\cf0c70a4-f8e5-43ae-8c94-aa528f90887d_resources\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarhive&gt; add jar D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar;Illegal character in opaque part at index 2: D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarQuery returned non-zero code: 1, cause: java.net.URISyntaxException: Illegal character in opaque part at index 2: D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="10595" opendate="2015-5-4 00:00:00" fixdate="2015-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dropping a table can cause NPEs in the compactor</summary>
      <description>Reproduction: start metastore with compactor off insert enough entries in a table to trigger a compaction drop the table stop metastore restart metastore with compactor onResult: NPE in the compactor threads. I suspect this would also happen if the inserts and drops were done in between a run of the compactor, but I haven't proven it.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="10636" opendate="2015-5-6 00:00:00" fixdate="2015-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CASE comparison operator rotation optimization</summary>
      <description>Step 1 as outlined in description of HIVE-9644</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFGreatest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10639" opendate="2015-5-7 00:00:00" fixdate="2015-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create SHA1 UDF</summary>
      <description>Calculates an SHA-1 160-bit checksum for the string and binary, as described in RFC 3174 (Secure Hash Algorithm). The value is returned as a string of 40 hex digits, or NULL if the argument was NULL.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="10659" opendate="2015-5-8 00:00:00" fixdate="2015-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline command which contains semi-colon as a non-command terminator will fail</summary>
      <description>Consider a scenario where beeline is used to connect to a mysql server. The commands executed via beeline can include stored procedures. For e.g. the following command used to create a stored procedure is a valid command :CREATE PROCEDURE RM_TLBS_LINKID() BEGIN IF EXISTS (SELECT * FROM `INFORMATION_SCHEMA`.`COLUMNS` WHERE `TABLE_NAME` = 'TBLS' AND `COLUMN_NAME` = 'LINK_TARGET_ID') THEN ALTER TABLE `TBLS` DROP FOREIGN KEY `TBLS_FK3` ; ALTER TABLE `TBLS` DROP KEY `TBLS_N51` ; ALTER TABLE `TBLS` DROP COLUMN `LINK_TARGET_ID` ; END IF; ENDMySQL stored procedures have semi-colon ( ; ) as the statement terminator. Since this coincides with beeline's only available command terminator, semi-colon, beeline will not able to execute the above command successfully . i.e, beeline tries to execute the below partial command instead of the complete command shown above.CREATE PROCEDURE RM_TLBS_LINKID() BEGIN IF EXISTS (SELECT * FROM `INFORMATION_SCHEMA`.`COLUMNS` WHERE `TABLE_NAME` = 'TBLS' AND `COLUMN_NAME` = 'LINK_TARGET_ID') THEN ALTER TABLE `TBLS` DROP FOREIGN KEY `TBLS_FK3` ; The above situation can actually happen within Hive when Hive SchemaTool is used to upgrade a mysql metastore db and the scripts used for the upgrade process contain stored procedures(as the one introduced initially by HIVE-7018). As of now, we cannot have any stored procedure as part of MySQL metastore db upgrade scripts because schemaTool uses beeline to connect to MySQL. As of now, beeline fails to execute any "create procedure" command or similar command containing ; . This is a serious limitation; it needs to be fixed by allowing the end user to provide an option to beeline to not use semi-colon as the command delimiter and instead use new line character as the command delimiter.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="10678" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update sql standard authorization configuration whitelist - more optimization flags</summary>
      <description>hive.exec.parallel and hive.groupby.orderby.position.alias are optimization config parameters that should be settable when sql standard authorization is enabled.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="10679" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JsonSerde ignores varchar and char size limit specified during table creation</summary>
      <description>JsonSerde ignores varchar and char size limit specified during table creation and always creates varchar or char column with max length.steps to reproduce the issue:create table jsonserde_1 (v varchar(50), c char(50)) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe';desc jsonserde_1;OKv varchar(65535) from deserializer c char(255) from deserializer Time taken: 0.468 seconds, Fetched: 2 row(s)</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="10682" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug id="10683" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add a mechanism for daemons to inform the AM about killed tasks</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug id="10685" opendate="2015-5-12 00:00:00" fixdate="2015-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table concatenate oparetor will cause duplicate data</summary>
      <description>"Orders" table has 1500000000 rows and stored as ORC. hive&gt; select count(*) from orders;OK1500000000Time taken: 37.692 seconds, Fetched: 1 row(s)The table contain 14 files,the size of each file is about 2.1 ~ 3.2 GB.After executing command : ALTER TABLE orders CONCATENATE;The table is already 1530115000 rows.My hive version is 1.1.0.</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.2.1,1.3.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="10686" opendate="2015-5-12 00:00:00" fixdate="2015-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.lang.IndexOutOfBoundsException for query with rank() over(partition ...)</summary>
      <description>CBO throws Index out of bound exception for TPC-DS Q70.Query explainselect sum(ss_net_profit) as total_sum ,s_state ,s_county ,grouping__id as lochierarchy , rank() over(partition by grouping__id, case when grouping__id == 2 then s_state end order by sum(ss_net_profit)) as rank_within_parentfrom store_sales ss join date_dim d1 on d1.d_date_sk = ss.ss_sold_date_sk join store s on s.s_store_sk = ss.ss_store_sk where d1.d_month_seq between 1193 and 1193+11 and s.s_state in ( select s_state from (select s_state as s_state, sum(ss_net_profit), rank() over ( partition by s_state order by sum(ss_net_profit) desc) as ranking from store_sales, store, date_dim where d_month_seq between 1193 and 1193+11 and date_dim.d_date_sk = store_sales.ss_sold_date_sk and store.s_store_sk = store_sales.ss_store_sk group by s_state ) tmp1 where ranking &lt;= 5 ) group by s_state,s_county with rolluporder by lochierarchy desc ,case when lochierarchy = 0 then s_state end ,rank_within_parent limit 100Original plan (correct) HiveSort(fetch=[100]) HiveSort(sort0=[$3], sort1=[$5], sort2=[$4], dir0=[DESC], dir1=[ASC], dir2=[ASC]) HiveProject(total_sum=[$4], s_state=[$0], s_county=[$1], lochierarchy=[$5], rank_within_parent=[rank() OVER (PARTITION BY $5, when(==($5, 2), $0) ORDER BY $4 ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)], (tok_function when (= (tok_table_or_col lochierarchy) 0) (tok_table_or_col s_state))=[when(=($5, 0), $0)]) HiveAggregate(group=[{0, 1}], groups=[[{0, 1}, {0}, {}]], indicator=[true], agg#0=[sum($2)], GROUPING__ID=[GROUPING__ID()]) HiveProject($f0=[$7], $f1=[$6], $f2=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_county=[$1], s_state=[$2]) SemiJoin(condition=[=($2, $3)], joinType=[inner]) HiveProject(s_store_sk=[$0], s_county=[$23], s_state=[$24]) HiveTableScan(table=[[tpcds.store]]) HiveProject(s_state=[$0]) HiveFilter(condition=[&lt;=($1, 5)]) HiveProject((tok_table_or_col s_state)=[$0], rank_window_0=[rank() OVER (PARTITION BY $0 ORDER BY $1 DESC ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)]) HiveAggregate(group=[{0}], agg#0=[sum($1)]) HiveProject($f0=[$6], $f1=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_state=[$24]) HiveTableScan(table=[[tpcds.store]])Plan after fixTopOBSchema (incorrect) HiveSort(fetch=[100]) HiveSort(sort0=[$3], sort1=[$5], sort2=[$4], dir0=[DESC], dir1=[ASC], dir2=[ASC]) HiveProject(total_sum=[$4], s_state=[$0], s_county=[$1], lochierarchy=[$5], rank_within_parent=[rank() OVER (PARTITION BY $5, when(==($5, 2), $0) ORDER BY $4 ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)]) HiveAggregate(group=[{0, 1}], groups=[[{0, 1}, {0}, {}]], indicator=[true], agg#0=[sum($2)], GROUPING__ID=[GROUPING__ID()]) HiveProject($f0=[$7], $f1=[$6], $f2=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_county=[$1], s_state=[$2]) SemiJoin(condition=[=($2, $3)], joinType=[inner]) HiveProject(s_store_sk=[$0], s_county=[$23], s_state=[$24]) HiveTableScan(table=[[tpcds.store]]) HiveProject(s_state=[$0]) HiveFilter(condition=[&lt;=($1, 5)]) HiveProject((tok_table_or_col s_state)=[$0], rank_window_0=[rank() OVER (PARTITION BY $0 ORDER BY $1 DESC ROWS BETWEEN 2147483647 FOLLOWING AND 2147483647 PRECEDING)]) HiveAggregate(group=[{0}], agg#0=[sum($1)]) HiveProject($f0=[$6], $f1=[$1]) HiveJoin(condition=[=($5, $2)], joinType=[inner], algorithm=[none], cost=[{1177.2086187101072 rows, 0.0 cpu, 0.0 io}]) HiveJoin(condition=[=($3, $0)], joinType=[inner], algorithm=[none], cost=[{2880430.428726483 rows, 0.0 cpu, 0.0 io}]) HiveProject(ss_sold_date_sk=[$0], ss_net_profit=[$21], ss_store_sk=[$22]) HiveTableScan(table=[[tpcds.store_sales]]) HiveProject(d_date_sk=[$0], d_month_seq=[$3]) HiveFilter(condition=[between(false, $3, 1193, +(1193, 11))]) HiveTableScan(table=[[tpcds.date_dim]]) HiveProject(s_store_sk=[$0], s_state=[$24]) HiveTableScan(table=[[tpcds.store]])Exception 15/04/14 02:42:52 [main]: ERROR parse.CalcitePlanner: CBO failed, skipping CBO.java.lang.IndexOutOfBoundsException: Index: 5, Size: 5 at java.util.ArrayList.rangeCheck(ArrayList.java:635) at java.util.ArrayList.get(ArrayList.java:411) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitInputRef(ASTConverter.java:395) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitInputRef(ASTConverter.java:372) at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:543) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:372) at org.apache.calcite.rex.RexCall.accept(RexCall.java:107) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:543) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter$RexVisitor.visitCall(ASTConverter.java:372) at org.apache.calcite.rex.RexCall.accept(RexCall.java:107) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convertOBToASTNode(ASTConverter.java:252) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:208) at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:98) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:607) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:239) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:202) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311) at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409) at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="10698" opendate="2015-5-13 00:00:00" fixdate="2015-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>query on view results fails with table not found error if view is created with subquery alias (CTE).</summary>
      <description>To reproduce it, use bugtest;create table basetb(id int, name string);create view testv1 aswith subtb as (select id, name from bugtest.basetb)select id from subtb;use castest;explain select * from bugtest.testv1;hive&gt; explain select * from bugtest.testv1;FAILED: SemanticException Line 2:15 Table not found 'subtb' in definition of VIEW testv1 [with subtb as (select id, name from bugtest.basetb)select id from `bugtest`.`subtb`] used as testv1 at Line 1:22Note that there is a database prefix `bugtest`.`subtb`</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="10705" opendate="2015-5-14 00:00:00" fixdate="2015-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update tests for HIVE-9302 after removing binaries</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveTestUtils.java</file>
      <file type="M">beeline.src.test.resources.postgresql-9.3.jdbc3.jar</file>
      <file type="M">beeline.src.test.resources.DummyDriver-1.0-SNAPSHOT.jar</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10719" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore failure when alter table rename is attempted.</summary>
      <description>create database newDB location "/tmp/";describe database extended newDB;use newDB;create table tab (name string);alter table tab rename to newName;Fails:InvalidOperationException(message:Unable to access old location hdfs://localhost:8020/tmp/tab for table x.tab)</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="10746" opendate="2015-5-19 00:00:00" fixdate="2015-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 1.2.0+Tez produces 1-byte FileSplits from mapred.TextInputFormat</summary>
      <description>The following query: SELECT appl_user_id, arsn_cd, COUNT(*) as RecordCount FROM adw.crc_arsn GROUP BY appl_user_id,arsn_cd ORDER BY appl_user_id; runs consistently fast in Spark and Mapreduce on Hive 1.2.0. When attempting to run this same query against Tez as the execution engine it consistently runs for over 300-500 seconds this seems extremely long. This is a basic external table delimited by tabs and is a single file in a folder. In Hive 0.13 this query with Tez runs fast and I tested with Hive 0.14, 0.14.1/1.0.0 and now Hive 1.2.0 and there clearly is something going awry with Hive w/Tez as an execution engine with Single or small file tables. I can attach further logs if someone needs them for deeper analysis.HDFS Output:hadoop fs -ls /example_dw/crc/arsnFound 2 items-rwxr-x--- 6 loaduser hadoopusers 0 2015-05-17 20:03 /example_dw/crc/arsn/_SUCCESS-rwxr-x--- 6 loaduser hadoopusers 3883880 2015-05-17 20:03 /example_dw/crc/arsn/part-m-00000Hive Table Describe:hive&gt; describe formatted crc_arsn;OK# col_name data_type comment arsn_cd string clmlvl_cd string arclss_cd string arclssg_cd string arsn_prcsr_rmk_ind string arsn_mbr_rspns_ind string savtyp_cd string arsn_eff_dt string arsn_exp_dt string arsn_pstd_dts string arsn_lstupd_dts string arsn_updrsn_txt string appl_user_id string arsntyp_cd string pre_d_indicator string arsn_display_txt string arstat_cd string arsn_tracking_no string arsn_cstspcfc_ind string arsn_mstr_rcrd_ind string state_specific_ind string region_specific_in string arsn_dpndnt_cd string unit_adjustment_in string arsn_mbr_only_ind string arsn_qrmb_ind string # Detailed Table Information Database: adw Owner: LOADUSER@EXA.EXAMPLE.COM CreateTime: Mon Apr 28 13:28:05 EDT 2014 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn Table Type: EXTERNAL_TABLE Table Parameters: EXTERNAL TRUE transient_lastDdlTime 1398706085 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim \t line.delim \n serialization.format \t Time taken: 1.245 seconds, Fetched: 54 row(s)Explain Hive 1.2.0 w/Tez:STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1STAGE PLANS: Stage: Stage-1 Tez Edges: Reducer 2 &lt;- Map 1 (SIMPLE_EDGE) Reducer 3 &lt;- Reducer 2 (SIMPLE_EDGE)Explain Hive 0.13 w/Tez:STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stageSTAGE PLANS: Stage: Stage-1 Tez Edges: Reducer 2 &lt;- Map 1 (SIMPLE_EDGE) Reducer 3 &lt;- Reducer 2 (SIMPLE_EDGE) Results: Hive 1.2.0 w/Spark 1.3.1: Finished successfully in 7.09 seconds Hive 1.2.0 w/Mapreduce: Stage 1: 32 Seconds Stage 2: 35 Seconds Hive 1.2.0 w/Tez 0.5.3: Time taken: 565.025 seconds, Fetched: 11516 row(s) Hive 0.13 w/Tez 0.4.0: Time taken: 13.552 seconds, Fetched: 11516 row(s)And finally looking at the Dag Attempt that is stuck for 500 seconds or so in Tez it looks to be stuck running the same method over and over again:8 duration=2561 from=org.apache.hadoop.hive.ql.exec.tez.RecordProcessor&gt;2015-05-18 19:58:41,719 INFO [TezChild] exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/dbc4b0b5-7859-4487-a56d-969440bc5e90/hive_2015-05-18_19-58-25_951_5497535752804149087-1/gss2002/_tez_scratch_dir/4e635121-c4cd-4e3f-b96b-9f08a6a7bf5d/map.xml2015-05-18 19:58:41,822 INFO [TezChild] exec.MapOperator: MAP[4]: records read - 12015-05-18 19:58:41,835 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-000002015-05-18 19:58:41,848 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000......2015-05-18 20:07:46,560 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-000002015-05-18 20:07:46,574 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-000002015-05-18 20:07:46,587 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-000002015-05-18 20:07:46,603 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-000002015-05-18 20:07:46,603 INFO [TezChild] log.PerfLogger: &lt;/PERFLOG method=TezRunProcessor start=1431993518764 end=1431994066603 duration=547839 from=org.apache.hadoop.hive.ql.exec.tez.TezProcessor&gt;2015-05-18 20:07:46,603 INFO [TezChild] exec.MapOperator: 4 finished. closing... 2015-05-18 20:07:46,603 INFO [TezChild] exec.MapOperator: RECORDS_IN_Map_1:13440</description>
      <version>0.14.0,0.14.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.2.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="10747" opendate="2015-5-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable the cleanup of side effect for the Encryption related qfile test</summary>
      <description>The hive conf is not reset in the clearTestSideEffects method which is involved from HIVE-8900. This will have pollute other qfile's settings running by TestEncryptedHDFSCliDriver</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="10748" opendate="2015-5-19 00:00:00" fixdate="2015-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace StringBuffer with StringBuilder where possible</summary>
      <description>I found 40 places in Hive where "new StringBuffer(" is used."Where possible, it is recommended that StringBuilder be used in preference to StringBuffer as it will be faster under most implementations"https://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatException.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveVarchar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.tez.TezJsonParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="1075" opendate="2010-1-20 00:00:00" fixdate="2010-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible for users to recover data when moveTask fails</summary>
      <description>If a "INSERT OVERWRITE" query fails after deleting the output table content but before moving the new content into that output table, we should allow user to recover the data manually.In order to do that, we should expose the temp location of the data and we should not remove the temp data.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10835" opendate="2015-5-27 00:00:00" fixdate="2015-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="10868" opendate="2015-5-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release note for 1.2.0 and 1.1.0</summary>
      <description>It's recently found that Hive's release notes don't contain all JIRAs fixed. This happened due to a lack of correct or missing fix version in a JIRA. A large chunk of such JIRAs are due to the fact that their fix versions didn't get updated when a merge from feature branch to trunk (master). This JIRA is to fix such JIRAs related to Hive on Spark work.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10896" opendate="2015-6-2 00:00:00" fixdate="2015-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: the return of the stuck DAG</summary>
      <description>Mapjoin issue again - preempted task that is loading the hashtable</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="10925" opendate="2015-6-4 00:00:00" fixdate="2015-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-static threadlocals in metastore code can potentially cause memory leak</summary>
      <description>There are many places where non-static threadlocals are used. I can't seem to find a good logic for using them. However, they can potentially result in leaking objects if for example they are created in a long running thread every time the thread handles a new session.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="10927" opendate="2015-6-4 00:00:00" fixdate="2015-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add number of HMS/HS2 connection metrics</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.LegacyMetrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.Metrics.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JvmPauseMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="10999" opendate="2015-6-13 00:00:00" fixdate="2015-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spark dependency to 1.4 [Spark Branch]</summary>
      <description>Spark 1.4.0 is release. Let's update the dependency version from 1.3.1 to 1.4.0.</description>
      <version>None</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11031" opendate="2015-6-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC concatenation of old files can fail while merging column statistics</summary>
      <description>Column statistics in ORC are optional protobuf fields. Old ORC files might not have statistics for newly added types like decimal, date, timestamp etc. But column statistics merging assumes column statistics exists for these types and invokes merge. For example, merging of TimestampColumnStatistics directly casts the received ColumnStatistics object without doing instanceof check. If the ORC file contains time stamp column statistics then this will work else it will throw ClassCastException.Also, the file merge operator swallows the exception.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="11035" opendate="2015-6-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD: Orc Split elimination fails because filterColumns=[-1]</summary>
      <description>create temporary table xx (x int) stored as orc ;insert into xx values (20),(200);set hive.fetch.task.conversion=none;select * from xx where x is null;This should generate zero tasks after optional split elimination in the app master, instead of generating the 1 task which for sure hits the row-index filters and removes all rows anyway.Right now, this runs 1 task for the stripe containing (min=20, max=200, has_null=false), which is broken.Instead, it returns YES_NO_NULL from the following default casehttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java#L976</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11100" opendate="2015-6-24 00:00:00" fixdate="2015-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should escape semi-colon in queries</summary>
      <description>Beeline should escape the semicolon in queries. for example, the query like followings:CREATE TABLE beeline_tb (c1 int, c2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ';' LINES TERMINATED BY '\n';or CREATE TABLE beeline_tb (c1 int, c2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\;' LINES TERMINATED BY '\n';both failed.But the 2nd query with semicolon escaped with "\" works in CLI.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="11104" opendate="2015-6-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select operator doesn&amp;#39;t propagate constants appearing in expressions</summary>
      <description></description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="11132" opendate="2015-6-26 00:00:00" fixdate="2015-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries using join and group by produce incorrect output when hive.auto.convert.join=false and hive.optimize.reducededuplication=true</summary>
      <description>Queries using join and group by produce multiple output rows with the same key when hive.auto.convert.join=false and hive.optimize.reducededuplication=true. This interaction between configuration parameters is unexpected and should be well documented at the very least and should likely be considered a bug.e.g. hive&gt; set hive.auto.convert.join = false;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 79XYZ 74XYZ 297XYZ 66hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 516</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  <bug id="11134" opendate="2015-6-27 00:00:00" fixdate="2015-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 should log open session failure</summary>
      <description>HiveServer2 should log OpenSession failure. If beeline is not running with "--verbose=true" all stack trace information is not available for later debugging, as it is not currently logged in server side.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="11208" opendate="2015-7-8 00:00:00" fixdate="2015-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not drop a default partition __HIVE_DEFAULT_PARTITION__ which is not a "string" type</summary>
      <description>When partition is not a string type, for example, if it is a int type, when drop the default partition _HIVE_DEFAULT_PARTITION_, you will get:SemanticException Unexpected unknown partitionsReproduce:SET hive.exec.dynamic.partition=true;SET hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.max.dynamic.partitions.pernode=10000;DROP TABLE IF EXISTS test;CREATE TABLE test (col1 string) PARTITIONED BY (p1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' STORED AS TEXTFILE;INSERT OVERWRITE TABLE test PARTITION (p1) SELECT code, IF(salary &gt; 600, 100, null) as p1 FROM jsmall;hive&gt; SHOW PARTITIONS test;OKp1=100p1=__HIVE_DEFAULT_PARTITION__Time taken: 0.124 seconds, Fetched: 2 row(s)hive&gt; ALTER TABLE test DROP partition (p1 = '__HIVE_DEFAULT_PARTITION__');FAILED: SemanticException Unexpected unknown partitions for (p1 = null)</description>
      <version>1.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="1121" opendate="2010-2-2 00:00:00" fixdate="2010-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombinedHiveInputFormat for hadoop 19</summary>
      <description>Creating the jira from the mail from Roberto Congiu &amp;#91;roberto.congiu@openx.org&amp;#93;</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.19.java.org.apache.hadoop.hive.shims.Hadoop19Shims.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11210" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on HiveConf from Orc reader &amp; writer</summary>
      <description>Currently the ORC reader and writer get their default values from HiveConf. I propose that we make the reader and writer have their own programatic defaults and the OrcInputFormat and OrcOutputFormat can use the version in HiveConf.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MemoryManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="11212" opendate="2015-7-8 00:00:00" fixdate="2015-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create vectorized types for complex types</summary>
      <description>We need vectorized types for structs, maps, lists, and unions.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="11213" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: more out file changes compared to master</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="11214" opendate="2015-7-9 00:00:00" fixdate="2015-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into ACID table switches vectorization off</summary>
      <description>PROBLEM:vectorization is switched off automatically after run insert into ACID table.STEPS TO REPRODUCE:set hive.vectorized.execution.enabled=true;create table testv (id int, name string) clustered by (id) into 2 buckets stored as orc tblproperties("transactional"="true");insert into testv values(1,'a');set hive.vectorized.execution.enabled;false</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="11250" opendate="2015-7-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change in spark.executor.instances (and others) doesn&amp;#39;t take effect after RSC is launched for HS2 [Spark Brnach]</summary>
      <description>Hive CLI works as expected.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11252" opendate="2015-7-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): DUMMY project in plan</summary>
      <description>When the return path is on, we might end up with a Project with DUMMY column in the plan; thus, we need to run the ProjectMergeRule after the column trimmer runs for the second time. To reproduce it, we can run interval_udf.q with the return path on.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="11254" opendate="2015-7-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Process result sets returned by a stored procedure</summary>
      <description>Stored procedure can return one or more result sets. A caller should be able to process them.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.queries.local.exception5.sql</file>
      <file type="M">hplsql.src.test.queries.local.exception4.sql</file>
      <file type="M">hplsql.src.test.queries.local.exception3.sql</file>
      <file type="M">hplsql.src.test.queries.local.exception2.sql</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Query.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="11282" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Inferring Hive type char/varchar of length zero which is not allowed</summary>
      <description>When RT is on, we try to infer the Hive type from the Calcite type for the value '’ e.g. in udf3.q, and we end up with char (length=0) as a result. The min length of char/varchar in Hive is 1, thus an Exception is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11314" opendate="2015-7-20 00:00:00" fixdate="2015-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print "Execution completed successfully" as part of spark job info [Spark Branch]</summary>
      <description>Like Hive on MR, Hive on Spark should print "Execution completed successfully" as part of the spark job info.</description>
      <version>1.1.0</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11363" opendate="2015-7-23 00:00:00" fixdate="2015-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prewarm Hive on Spark containers [Spark Branch]</summary>
      <description>When Hive job is launched by Oozie, a Hive session is created and job script is executed. Session is closed when Hive job is completed. Thus, Hive session is not shared among Hive jobs either in an Oozie workflow or across workflows. Since the parallelism of a Hive job executed on Spark is impacted by the available executors, such Hive jobs will suffer the executor ramp-up overhead. The idea here is to wait a bit so that enough executors can come up before a job can be executed.</description>
      <version>1.1.0</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="114" opendate="2008-12-4 00:00:00" fixdate="2008-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop partition should not delete data for external tables</summary>
      <description>As a continuation of HIVE-86, dropping partitions in an external table shouldn't delete the underlying data</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1140" opendate="2010-2-8 00:00:00" fixdate="2010-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect ambiguous column reference error message</summary>
      <description>Whenever there is an ambiguous column name reference, the error message does not reference the proper column.hive&gt; FROM (SELECT key, concat(value) AS key FROM src) a SELECT a.key;FAILED: Error in semantic analysis: line 1:25 Ambiguous Column Reference value</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11424" opendate="2015-7-31 00:00:00" fixdate="2015-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rule to transform OR clauses into IN clauses in CBO</summary>
      <description>We create a rule that will transform OR clauses into IN clauses (when possible).</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.semijoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="11482" opendate="2015-8-6 00:00:00" fixdate="2015-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add retrying thrift client for HiveServer2</summary>
      <description>Similar to https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java, this improvement request is to add a retrying thrift client for HiveServer2 to do retries upon thrift exceptions.Here are few commits done on a forked branch that can be picked - https://github.com/InMobi/hive/commit/7fb957fb9c2b6000d37c53294e256460010cb6b7https://github.com/InMobi/hive/commit/11e4b330f051c3f58927a276d562446761c9cd6dhttps://github.com/InMobi/hive/commit/241386fd870373a9253dca0bcbdd4ea7e665406c</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11483" opendate="2015-8-6 00:00:00" fixdate="2015-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add encoding and decoding for query string config</summary>
      <description>We have seen some queries in production where some of the literals passed in the query have control characters, which result in exception when query string is set in the job xml.Proposing a solution to encode the query string in configuration and provide getters decoded string.Here is a commit in a forked repo : https://github.com/InMobi/hive/commit/2faf5761191fa3103a0d779fde584d494ed75bf5Suggestions are welcome on the solution.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.hooks.TestHooks.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11484" opendate="2015-8-6 00:00:00" fixdate="2015-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ObjectInspector for Char and VarChar</summary>
      <description>The creation of HiveChar and Varchar is not happening through ObjectInspector.Here is fix we pushed internally : https://github.com/InMobi/hive/commit/fe95c7850e7130448209141155f28b25d3504216</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  <bug id="11499" opendate="2015-8-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Datanucleus leaks classloaders when used using embedded metastore with HiveServer2 with UDFs</summary>
      <description>When UDFs are used, we create a new classloader to add the UDF jar. Similar to what hadoop's reflection utils does(HIVE-11408), datanucleus caches the classloaders (https://github.com/datanucleus/datanucleus-core/blob/3.2/src/java/org/datanucleus/NucleusContext.java#L161). JDOPersistanceManager factory (1 per JVM) holds on to a NucleusContext reference (https://github.com/datanucleus/datanucleus-api-jdo/blob/3.2/src/java/org/datanucleus/api/jdo/JDOPersistenceManagerFactory.java#L115). Until we call NucleusContext#close, the classloader cache is not cleared. In case of UDFs this can lead to permgen leak, as shown in the attached screenshot, where NucleusContext holds on to several URLClassloader objects.</description>
      <version>0.14.0,1.0.0,1.1.0,1.1.1,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="1150" opendate="2010-2-10 00:00:00" fixdate="2010-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add comment to explain why we check for dir first in add_partitions().</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11512" opendate="2015-8-10 00:00:00" fixdate="2015-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive LDAP Authenticator should also support full DN in Authenticate()</summary>
      <description>In certain LDAP implementation, LDAP Binding can occur using the full DN for the user. Currently, LDAPAuthentication Provider assumes that the username passed into Authenticate() is a short username &amp; not a full DN. While the initial bind works fine either way, the filter code is reliant on it being a shortname.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="11517" opendate="2015-8-11 00:00:00" fixdate="2015-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized auto_smb_mapjoin_14.q produces different results</summary>
      <description>Converted Q file to use ORC and turned on vectorization.The query:select count(*) from ( select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key) subq1produces 10 instead of 22.The query:select src1.key, src1.cnt1, src2.cnt1 from( select key, count(*) as cnt1 from ( select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key ) subq1 group by key) src1join( select key, count(*) as cnt1 from ( select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key ) subq2 group by key) src2produces:0 3 32 1 14 1 15 3 38 1 19 1 1instead of:0 9 92 1 14 1 15 9 98 1 19 1 1</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="11525" opendate="2015-8-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning</summary>
      <description>Logically and functionally bucketing and partitioning are quite similar - both provide mechanism to segregate and separate the table's data based on its content. Thanks to that significant further optimisations like &amp;#91;partition&amp;#93; PRUNING or &amp;#91;bucket&amp;#93; MAP JOIN are possible.The difference seems to be imposed by design where the PARTITIONing is open/explicit while BUCKETing is discrete/implicit.Partitioning seems to be very common if not a standard feature in all current RDBMS while BUCKETING seems to be HIVE specific only.In a way BUCKETING could be also called by "hashing" or simply "IMPLICIT PARTITIONING".Regardless of the fact that these two are recognised as two separate features available in Hive there should be nothing to prevent leveraging same existing query/join optimisations across the two.BUCKET pruningEnable partition PRUNING equivalent optimisation for queries on BUCKETED tablesSimplest example is for queries like:"SELECT … FROM x WHERE colA=123123"to read only the relevant bucket file rather than all file-buckets that belong to a table.</description>
      <version>0.13.0,0.13.1,0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11526" opendate="2015-8-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  <bug id="11607" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export tables broken for data &gt; 32 MB</summary>
      <description>Broken for both hadoop-1 as well as hadoop-2 line</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.0.20S.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11623" opendate="2015-8-23 00:00:00" fixdate="2015-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): fix the tableAlias for ReduceSink operator</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="11624" opendate="2015-8-24 00:00:00" fixdate="2015-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline-cli: support hive.cli.print.header in new CLI[beeline-cli branch]</summary>
      <description>In the old CLI, it uses "hive.cli.print.header" from the hive configuration to force execution a script . We need to support the previous configuration using beeline functionality.</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="11694" opendate="2015-8-31 00:00:00" fixdate="2015-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude hbase-metastore for hadoop-1</summary>
      <description>hbase-metastore doesn't compile for hadoop-1 and we don't have development plan to make it work with hadoop-1. Exclude hbase-metastore related file so hadoop-1 still compiles.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11695" opendate="2015-8-31 00:00:00" fixdate="2015-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If user have no permission to create LOCAL DIRECTORY ，the Hql does not throw any exception and fail silently.</summary>
      <description>If user have no permission to create LOCAL DIRECTORY such as "/data/wangmeng/hiveserver2" ,the query does not throw any exception and fail silently.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="11745" opendate="2015-9-4 00:00:00" fixdate="2015-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table Exchange partition with multiple partition_spec is not working</summary>
      <description>Single partition works, but multiple partitions will not work.Reproduce steps:DROP TABLE IF EXISTS t1;DROP TABLE IF EXISTS t2;DROP TABLE IF EXISTS t3;DROP TABLE IF EXISTS t4;CREATE TABLE t1 (a int) PARTITIONED BY (d1 int);CREATE TABLE t2 (a int) PARTITIONED BY (d1 int);CREATE TABLE t3 (a int) PARTITIONED BY (d1 int, d2 int);CREATE TABLE t4 (a int) PARTITIONED BY (d1 int, d2 int);INSERT OVERWRITE TABLE t1 PARTITION (d1 = 1) SELECT salary FROM jsmall LIMIT 10;INSERT OVERWRITE TABLE t3 PARTITION (d1 = 1, d2 = 1) SELECT salary FROM jsmall LIMIT 10;SELECT * FROM t1;SELECT * FROM t3;ALTER TABLE t2 EXCHANGE PARTITION (d1 = 1) WITH TABLE t1;SELECT * FROM t1;SELECT * FROM t2;ALTER TABLE t4 EXCHANGE PARTITION (d1 = 1, d2 = 1) WITH TABLE t3;SELECT * FROM t3;SELECT * FROM t4;The output:0: jdbc:hive2://10.17.74.148:10000/default&gt; SELECT * FROM t3;+-------+--------+--------+--+| t3.a | t3.d1 | t3.d2 |+-------+--------+--------+--++-------+--------+--------+--+No rows selected (0.227 seconds)0: jdbc:hive2://10.17.74.148:10000/default&gt; SELECT * FROM t4;+-------+--------+--------+--+| t4.a | t4.d1 | t4.d2 |+-------+--------+--------+--++-------+--------+--------+--+No rows selected (0.266 seconds)</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="11746" opendate="2015-9-6 00:00:00" fixdate="2015-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connect command should not to be allowed from user[beeline-cli branch]</summary>
      <description>For new cli, user should not be allowed to connect a server or database.</description>
      <version>None</version>
      <fixedVersion>beeline-cli-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="11810" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Exception is ignored if MiniLlap cluster fails to start</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="11835" opendate="2015-9-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type decimal(1,1) reads 0.0, 0.00, etc from text file as NULL</summary>
      <description>Steps to reproduce:1. create a text file with values like 0.0, 0.00, etc.2. create table in hive with type decimal(1,1).3. run "load data local inpath ..." to load data into the table.4. run select * on the table.You will see that NULL is displayed for 0.0, 0.00, .0, etc. Instead, these should be read as 0.0.</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="11891" opendate="2015-9-18 00:00:00" fixdate="2015-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add basic performance logging to metastore calls</summary>
      <description>At present it's extremely difficult to debug slow calls to the metastore. Ideally there would be some basic means of doing so.</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="11892" opendate="2015-9-18 00:00:00" fixdate="2015-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDTF run in local fetch task does not return rows forwarded during GenericUDTF.close()</summary>
      <description>Using the example UDTF GenericUDTFCount2, which is part of hive-contrib:create temporary function udtfCount2 as 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2';set hive.fetch.task.conversion=minimal;-- Task created, correct output (2 rows)select udtfCount2() from src;set hive.fetch.task.conversion=more;-- Runs in local task, incorrect output (0 rows)select udtfCount2() from src;</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="119" opendate="2008-12-4 00:00:00" fixdate="2008-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add more informative error messages to grammar parsing</summary>
      <description>Some error messages give the user no context or help really to know what part of their stmt is wrong.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.create.tbl2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1194" opendate="2010-2-24 00:00:00" fixdate="2010-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sorted merge join</summary>
      <description>If the input tables are sorted on the join key, and a mapjoin is being performed, it is useful to exploit the sorted properties of the table.This can lead to substantial cpu savings - this needs to work across bucketed map joins also.Since, sorted properties of a table are not enforced currently, a new parameter can be added to specify to use the sort-merge join.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="11978" opendate="2015-9-28 00:00:00" fixdate="2015-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: NPE in Expr toString</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="12007" opendate="2015-10-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive LDAP Authenticator should allow just Domain without baseDN (for AD)</summary>
      <description>When the baseDN is not configured but only the Domain has been set in hive-site.xml, LDAP Atn provider cannot locate the user in the directory. Authentication fails in such cases. This is a change from the prior implementation where the auth request succeeds based on being able to bind to the directory. This has been called out in the design doc in HIVE-7193. But we should allow this for now for backward compatibility.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12042" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update some out files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="12043" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UGI instances being used in IO elevator threads are incorrect</summary>
      <description>... which leads to FileSystem closed exceptions.I'm not sure yet if this is a result of the threadpool being used, and UGI not working well with threadpools, or something else.The UGI instance which was setup - at what looks to be thread creation time - ends up being used for several different reads, ignoring the actual UGI passed in. At some point this changes to a new incorrect UGI.A simple fix is to propagate the correct UGI all the way to the reader, and that fixes the FileSystem Closed exception. Figuring out the precise reason would be good though.Related to HIVE-9898.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="12058" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change hive script to record errors when calling hbase fails</summary>
      <description>By default hive will try to find out which jars need to be added to the classpath in order to run MR jobs against an HBase cluster, however if hbase can't be found or if hbase mapredcp fails, the hive script will fail silently and ignore some of the jars to be included into the. That makes very difficult to analyze the real problem.Hive script should record the error not just simply redirect two hbase failures:HBASE_BIN=${HBASE_BIN:-"$(which hbase 2&gt;/dev/null)"}$HBASE_BIN mapredcp 2&gt;/dev/null</description>
      <version>0.14.0,1.1.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="12059" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up reference to deprecated constants in AvroSerdeUtils</summary>
      <description>AvroSerdeUtils contains several deprecated String constants that are used by other Hive modules. Those should be cleaned up.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="12060" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: create separate variable for llap tests</summary>
      <description>No real reason to just reuse tez one; also needed to parallelize the tests</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12062" opendate="2015-10-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable HBase metastore file metadata cache for tez tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12250" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zookeeper connection leaks in Hive&amp;#39;s HBaseHandler.</summary>
      <description>HiveServer2 performance regresses severely due to what appears to be a leak in the ZooKeeper connections. lsof output on the HS2 process shows about 8000 TCP connections to the ZK ensemble nodes.grep TCP lsof-hive-node11 | grep node11 | grep -E "node03|node04|node05" | wc -l 7866 grep TCP lsof-hive-node11 | grep node11 | grep -E "node03" | wc -l 2615grep TCP lsof-hive-node11 | grep node11 | grep -E "node04" | wc -l 2622grep TCP lsof-hive-node11 | grep node11 | grep -E "node05" | wc -l 2629node11 - HMS nodenode03, node04 and node05 are the hosts for zookeeper ensemble.</description>
      <version>1.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="12325" opendate="2015-11-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn hive.map.groupby.sorted on by default</summary>
      <description>When applicable it can avoid shuffle phase altogether for group by, which will be a performance win.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.test.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.8.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12327" opendate="2015-11-3 00:00:00" fixdate="2015-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e tests TestJob_1 and TestJob_2 fail</summary>
      <description>The tests are added in HIVE-7035. Both are negative tests and check if the http status code is 400. The original patch capture the exception containing specific message. However, in latter version of Hadoop, the message change so the exception is not contained.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
    </fixedFiles>
  </bug>
  <bug id="12345" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup for HIVE-9013 : Hidden conf vars still visible through beeline</summary>
      <description>HIVE-9013 introduced the ability to hide certain conf variables when output through the "set" command. However, there still exists one further bug in it that causes these variables to still be visible through beeline connecting to HS2, wherein HS2 exposes hidden variables such as the HS2's metastore password when "set" is run.</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12372" opendate="2015-11-9 00:00:00" fixdate="2015-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve to support the multibyte character at lpad and rpad</summary>
      <description>The current lpad and rpad don't support the multibyte character at "str" and "pad".For example, we can see the following result.hive&gt; select name from sample1;OKtokyoＴＯＫＹＯhive&gt; select lpad(name, 20, '*') from sample1;OK***************tokyo*****ＴＯＫＹＯThis is improved as follows.hive&gt; select lpad(name, 20, '*') from sample1;***************tokyo***************ＴＯＫＹＯ</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.java</file>
    </fixedFiles>
  </bug>
  <bug id="12568" opendate="2015-12-2 00:00:00" fixdate="2015-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to specify network interface used by Spark remote client [Spark Branch]</summary>
      <description>Spark client sends a pair of host name and port number to the remote driver so that the driver can connects back to HS2 where the user session is. Spark client has its own way determining the host name, and pick one network interface if the host happens to have multiple network interfaces. This can be problematic. For that, there is parameter, hive.spark.client.server.address, which user can pick an interface. Unfortunately, this interface isn't exposed.Instead of exposing this parameter, we can use the same logic as Hive in determining the host name. Therefore, the remote driver connecting to HS2 using the same network interface as a HS2 client would do.There might be a case where user may want the remote driver to use a different network. This is rare if at all. Thus, for now it should be sufficient to use the same network interface.</description>
      <version>1.1.0</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12612" opendate="2015-12-8 00:00:00" fixdate="2015-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline always exits with 0 status when reading query from standard input</summary>
      <description>Similar to what was reported on HIVE-6978, but now it only happens when the query is read from the standard input. For example, the following fails as expected:bash$ if beeline -u "jdbc:hive2://..." -e "boo;" ; then echo "Ok?!" ; else echo "Failed!" ; fiConnecting to jdbc:hive2://...Connected to: Apache Hive (version 1.1.0-cdh5.5.0)Driver: Hive JDBC (version 1.1.0-cdh5.5.0)Transaction isolation: TRANSACTION_REPEATABLE_READError: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'boo' '&lt;EOF&gt;' '&lt;EOF&gt;' (state=42000,code=40000)Closing: 0: jdbc:hive2://...Failed!But the following does not:bash$ if echo "boo;"|beeline -u "jdbc:hive2://..." ; then echo "Ok?!" ; else echo "Failed!" ; fiConnecting to jdbc:hive2://...Connected to: Apache Hive (version 1.1.0-cdh5.5.0)Driver: Hive JDBC (version 1.1.0-cdh5.5.0)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.1.0-cdh5.5.0 by Apache Hive0: jdbc:hive2://...:8&gt; Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'boo' '&lt;EOF&gt;' '&lt;EOF&gt;' (state=42000,code=40000)0: jdbc:hive2://...:8&gt; Closing: 0: jdbc:hive2://...Ok?!This was misleading our batch scripts to always believe that the execution of the queries succeded, when sometimes that was not the case. WorkaroundWe found we can work around the issue by always using the -e or the -f parameters, and even reading the standard input through the /dev/stdin device (this was useful because a lot of the scripts fed the queries from here documents), like this:some-script.sh#!/bin/shset -o nounset -o errexit -o pipefail# As beeline is failing to report an error status if reading the query# to be executed from STDIN, check whether no -f or -e option is used# and, in that case, pretend it has to read the query from a regular# file using -f to read from /dev/stdinfunction beeline_workaround_exit_status () { for arg in "$@" do if [ "$arg" = "-f" -o "$arg" = "-e" ] then beeline -u "..." "$@" return fi done beeline -u "..." "$@" -f /dev/stdin}beeline_workaround_exit_status &lt;&lt;EOFboo;EOF</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="12708" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark doesn&amp;#39;t work with Kerboresed HBase [Spark Branch]</summary>
      <description>Spark application launcher (spark-submit) acquires HBase delegation token on Hive user's behalf when the application is launched. This mechanism, which doesn't work for long-running sessions, is not in line with what Hive is doing. Hive actually acquires the token automatically whenever a job needs it. The right approach for Spark should be allowing applications to dynamically add whatever tokens they need to the spark context. While this needs work on Spark side, we provide a workaround solution in Hive.</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="12721" opendate="2015-12-21 00:00:00" fixdate="2015-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UUID built in function</summary>
      <description>A UUID function would be very useful for ETL jobs that need to generate surrogate keys.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="12723" opendate="2015-12-21 00:00:00" fixdate="2015-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats_filemetadata test was added to the wrong driver</summary>
      <description>HBase metastore is only used in MiniTez, but the test somehow got added to MiniLlap</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="12850" opendate="2016-1-12 00:00:00" fixdate="2016-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes after changes made in TEZ-2669 and TEZ-3024</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="12853" opendate="2016-1-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: localize permanent UDF jars to daemon and add them to classloader</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestAddResource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.DosToUnix.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.DependencyResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12855" opendate="2016-1-13 00:00:00" fixdate="2016-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add checks when resolving UDFs to enforce whitelist</summary>
      <description>Currently, adding a temporary UDF and calling LLAP with it (bypassing the LlapDecider check, I did it by just modifying the source) only fails because the class could not be found. If the UDF was accessible to LLAP, it would execute. Inside the daemon, UDF instantiation should fail for custom UDFs (and only succeed for whitelisted custom UDFs, once that is implemented).</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.FunctionLocalizer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12857" opendate="2016-1-13 00:00:00" fixdate="2016-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: modify the decider to allow using LLAP with whitelisted UDFs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1286" opendate="2010-3-28 00:00:00" fixdate="2010-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove debug message from stdout in ColumnarSerDe</summary>
      <description>'Found class for org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'should go to stderr where other informational messages are sent.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12875" opendate="2016-1-14 00:00:00" fixdate="2016-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify sem.getInputs() and sem.getOutputs()</summary>
      <description>For every partition entity object present in sem.getInputs() and sem.getOutputs(), we must verify the appropriate Table in the list of Entities.</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.1.2,1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="12885" opendate="2016-1-18 00:00:00" fixdate="2016-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP Authenticator improvements</summary>
      <description>Currently Hive's LDAP Atn provider assumes certain defaults to keep its configuration simple. 1) One of the assumptions is the presence of an attribute "distinguishedName". In certain non-standard LDAP implementations, this attribute may not be available. So instead of basing all ldap searches on this attribute, getNameInNamespace() returns the same value. So this API is to be used instead.2) It also assumes that the "user" value being passed in, will be able to bind to LDAP. However, certain LDAP implementations, by default, only allow the full DN to be used, just short user names are not permitted. We will need to be able to support short names too when hive configuration only has "BaseDN" specified (not userDNPatterns). So instead of hard-coding "uid" or "CN" as keys for the short usernames, it probably better to make this a configurable parameter.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12897" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading</summary>
      <description>There are many redundant calls to metastore which is not needed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dynamic.partitions.with.whitelist.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="12907" opendate="2016-1-22 00:00:00" fixdate="2016-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading - II</summary>
      <description>Remove unnecessary calls to metastore.</description>
      <version>0.14.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug id="12908" opendate="2016-1-22 00:00:00" fixdate="2016-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading III</summary>
      <description>Remove unnecessary Namenode calls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="12941" opendate="2016-1-27 00:00:00" fixdate="2016-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unexpected result when using MIN() on struct with NULL in first field</summary>
      <description>Using MIN() on struct with NULL in first field of a row yields NULL as result.Example:select min(a) FROM (select 1 as a union all select 2 as a union all select cast(null as int) as a) tmp;OK_c01As expected. But if we wrap it in a struct:select min(a) FROM (select named_struct("field",1) as a union all select named_struct("field",2) as a union all select named_struct("field",cast(null as int)) as a) tmp;OK_c0NULLUsing MAX() works as expected for structs.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
    </fixedFiles>
  </bug>
  <bug id="13040" opendate="2016-2-10 00:00:00" fixdate="2016-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle empty bucket creations more efficiently</summary>
      <description></description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.test.results.clientpositive.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="13118" opendate="2016-2-22 00:00:00" fixdate="2016-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add some logging to LLAP token related paths</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="13183" opendate="2016-2-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>More logs in operation logs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.StreamPrinter.java</file>
    </fixedFiles>
  </bug>
  <bug id="13184" opendate="2016-2-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: DAG credentials (e.g. HBase tokens) are not passed to the tasks in Tez plugin</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13285" opendate="2016-3-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Orc concatenation may drop old files from moving to final path</summary>
      <description>ORC concatenation uses combine hive input format for merging files. Under specific case where all files within a combine split are incompatible for merge (old files without stripe statistics) then these files are added to incompatible file set. But this file set is not processed as closeOp() will not be called (no output file writer will exist which will skip super.closeOp()). As a result, these incompatible files are not moved to final path.</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1331" opendate="2010-4-29 00:00:00" fixdate="2010-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select * does not work if different partitions contain different formats</summary>
      <description>Will try to come up with a concrete test - but looks like we are using the table's input format</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.wise.fileformat.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13373" opendate="2016-3-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use most specific type for numerical constants</summary>
      <description>tinyint &amp; shortint are currently inferred as ints, if they are without postfix.</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.type.widening.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13376" opendate="2016-3-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS emits too many logs with application state</summary>
      <description>The logs get flooded with something like:&gt; Mar 28, 3:12:21.851 PM INFO org.apache.hive.spark.client.SparkClientImpl&gt; &amp;#91;stderr-redir-1&amp;#93;: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)&gt; Mar 28, 3:12:21.912 PM INFO org.apache.hive.spark.client.SparkClientImpl&gt; &amp;#91;stderr-redir-1&amp;#93;: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)&gt; Mar 28, 3:12:22.853 PM INFO org.apache.hive.spark.client.SparkClientImpl&gt; &amp;#91;stderr-redir-1&amp;#93;: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)&gt; Mar 28, 3:12:22.913 PM INFO org.apache.hive.spark.client.SparkClientImpl&gt; &amp;#91;stderr-redir-1&amp;#93;: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)&gt; Mar 28, 3:12:23.855 PM INFO org.apache.hive.spark.client.SparkClientImpl&gt; &amp;#91;stderr-redir-1&amp;#93;: 16/03/28 15:12:23 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)While this is good information, it is a bit much.Seems like SparkJobMonitor hard-codes its interval to 1 second. It should be higher and perhaps made configurable.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13378" opendate="2016-3-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP help formatter is too narrow</summary>
      <description>usage: llap -a,--args &lt;args&gt; java arguments to the llap instance -c,--cache &lt;cache&gt; cache size per instance -d,--directory &lt;directory&gt; Temp directory for jars etc. -e,--executors &lt;executors&gt; executor per instance</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13379" opendate="2016-3-29 00:00:00" fixdate="2016-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-12851 args do not work (slider-keytab-dir, etc.)</summary>
      <description>I've no idea how they ever worked. But I'm pretty sure they did... go figure.</description>
      <version>None</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13380" opendate="2016-3-30 00:00:00" fixdate="2016-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal should have lower precedence than double in type hierachy</summary>
      <description>Currently its other way round. Also, decimal should be lower than float.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.table.cascade.q</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.partition.change.col.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
    </fixedFiles>
  </bug>
  <bug id="13381" opendate="2016-3-30 00:00:00" fixdate="2016-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timestamp &amp; date should have precedence in type hierarchy than string group</summary>
      <description>Both sql server &amp; oracle treats date/timestamp higher in hierarchy than varchars</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="13502" opendate="2016-4-13 00:00:00" fixdate="2016-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesnt support session parameters in JDBC URL as documentation states.</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLsdocuments that sessions variables like credentials etc are accepted as part of the URL. However, Beeline does not support such URLs today.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="13505" opendate="2016-4-13 00:00:00" fixdate="2016-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip running TestDummy where possibe during precommit builds</summary>
      <description>On the main Hive build - this does nothing. There are some tests named TestDummy under qtests - I'm not sure they do anything useful though.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug id="13507" opendate="2016-4-13 00:00:00" fixdate="2016-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improved logging for ptest</summary>
      <description>NO PRECOMMIT TESTSInclude information about batch runtimes, outlier lists, host completion times, etc. Try identifying tests which cause the build to take a long time while holding onto resources.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PrepPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.LocalCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.Host.java</file>
    </fixedFiles>
  </bug>
  <bug id="13525" opendate="2016-4-15 00:00:00" fixdate="2016-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS hangs when job is empty</summary>
      <description>Observed in local tests. This should be the cause of HIVE-13402.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1354" opendate="2010-5-19 00:00:00" fixdate="2010-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition level properties honored if it exists</summary>
      <description>drop table partition_test_partitioned;create table partition_test_partitioned(key string, value string) partitioned by (dt string);alter table partition_test_partitioned set fileformat rcfile;insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;show table extended like partition_test_partitioned partition(dt=101);alter table partition_test_partitioned set fileformat Sequencefile;insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;show table extended like partition_test_partitioned partition(dt=102);insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;show table extended like partition_test_partitioned partition(dt=101);drop table partition_test_partitioned;Partition (dt=101) still points to RCFile, since it was created as a RCFile</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13614" opendate="2016-4-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implementation for PreparedStatement#setBigDecimal</summary>
      <description>Hi,I am a committer of MyBatis project (https://github.com/mybatis) and some of our users have difficulty with using MyBatis-Migrations (a db migration tool) with Hive.https://github.com/mybatis/migrations/issues/25Basically, Migrations uses BigDecimal as the ID type and the lack of support in Hive's JDBC driver prevents our users from using Migrations with Hive.So, it would be beneficial to both of Hive and MyBatis users if you could apply the following change to mitigate the situation.https://github.com/harawata/hive/commit/1910632442ad9b70b41b28e37596843d1b9f7d3fPlease let me know if I should send the modification as a GitHub pull request.Thanks in advance!Iwao</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="13625" opendate="2016-4-27 00:00:00" fixdate="2016-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Prepared Statement when executed with escape characters in parameter fails</summary>
      <description>When setting parameters to a Hive Prepared Statement, if the parameter has an odd number of escape characters, then the Statement fails.For example, I set one of the parameters to "/somepath/\044{yyyy}/\044{MM}/\044{dd}/". Here, I have escaped the dollar character with \044 because Hive gives an Atlas exception with "$" character. Now, when the parameters are getting set inside Hive, getCharIndexFromSqlByParamLocation throws an Exception.Hive records something called signal count. if (c == '\'' || c == '')// record the count of char "'" and char "\" { signalCount++; } And the parameter is set only if the signalCount %2 is 0.else if (c == cchar &amp;&amp; signalCount % 2 == 0) {// check if the ? is really the parameter num++; if (num == paramLoc) { charIndex = i; break; }Since my parameter has three "\" characters, the signal Count modulo is not 0 and the parameter is not set at all throwing an exception.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13666" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Provide the log url for a task attempt to display on the UI</summary>
      <description>The log url needs to be provided for task attempts, to display on the Tez UI associated with a query.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug id="13667" opendate="2016-5-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve performance for ServiceInstanceSet.getByHost</summary>
      <description>ServiceInstanceSet.getByHost is used for scheduling local tasks as well as constructing the log URL.It ends up traversing all hosts on each lookup. This should be avoided.cc prasanth_j</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13669" opendate="2016-5-2 00:00:00" fixdate="2016-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: io.enabled config is ignored on the server side</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug id="1367" opendate="2010-5-25 00:00:00" fixdate="2010-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cluster by multiple columns does not work if parenthesis is present</summary>
      <description>The following query:select ... from src cluster by (key, value)throws a compile error:whereas the queryselect ... from src cluster by key, valueworks fine</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
    </fixedFiles>
  </bug>
  <bug id="13710" opendate="2016-5-6 00:00:00" fixdate="2016-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP registry ACL check causes error due to namespacing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13749" opendate="2016-5-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in Hive Metastore</summary>
      <description>Looking a heap dump of 10GB, a large number of Configuration objects(&gt; 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.I will attach an exported snapshot from the eclipse MAT.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="14091" opendate="2016-6-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some errors are not propagated to LLAP external clients</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="14162" opendate="2016-7-5 00:00:00" fixdate="2016-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow disabling of long running job on Hive On Spark On YARN</summary>
      <description>Hive On Spark launches a long running process on the first query to handle all queries for that user session. In some use cases this is not desired, for instance when using Hue with large intervals between query executions.Could we have a property that would cause long running spark jobs to be terminated after each query execution and started again for the next one?</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14163" opendate="2016-7-5 00:00:00" fixdate="2016-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: use different kerberized/unkerberized zk paths for registry</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14218" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: ACL validation fails if the user name is different from principal user name</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14219" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external client on secure cluster: Protocol interface org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol is not known</summary>
      <description>2016-07-07T23:10:35,249 INFO [TaskHeartbeatThread[]]: task.TezTaskRunner2 (:()) - TaskReporter reporter error which will cause the task to failorg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): Protocol interface org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol is not known. at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1551) at org.apache.hadoop.ipc.Client.call(Client.java:1495) at org.apache.hadoop.ipc.Client.call(Client.java:1395) at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:241) at com.sun.proxy.$Proxy39.heartbeat(Unknown Source) at org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable.heartbeat(LlapTaskReporter.java:280) at org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable.call(LlapTaskReporter.java:202) at org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable.call(LlapTaskReporter.java:139) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14345" opendate="2016-7-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline result table has erroneous characters</summary>
      <description>Beeline returns query results with erroneous characters. For example:0: jdbc:hive2://xxxx:10000/def&gt; select 10;+------+--+| _c0 |+------+--+| 10 |+------+--+1 row selected (3.207 seconds)</description>
      <version>1.1.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="1454" opendate="2010-7-8 00:00:00" fixdate="2010-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert overwrite and CTAS fail in hive local mode</summary>
      <description>this is because of the changes in HIVE-543. We switched to using local storage for intermediate data for local mode queries. However there are code paths that are incorrectly allocating intermediate storage where they should be allocating external file system storage (based on table/directory uri). This is causing regressions in running queries in local mode.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14540" opendate="2016-8-15 00:00:00" fixdate="2016-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support in ptest to create batches for non qfile tests</summary>
      <description>From run 790:Reported runtime by junit: 17 hoursReported runtime by ptest: 34 hoursA lot of time is wasted spinning up mvn test for each individual test, which otherwise takes less than 1 second. These tests could end up taking 20-30 seconds. Combined with HIVE-14539 - 60-70s.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.test.resources.test-configuration2.properties</file>
      <file type="M">testutils.ptest2.src.test.resources.log4j2.properties</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.testParallelFailsOnRsync.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.testParallelFailsOnExec.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.testIsolatedFailsOnRsyncUnknown.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.testIsolatedFailsOnRsyncOne.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.testIsolatedFailsOnExec.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.testBasic.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testPassingUnitTest.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.testFailingUnitTest.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.MockSSHCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.MockRSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestUnitTestPropertiesParser.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestTestParser.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.conf.TestQFileTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.UnitTestPropertiesParser.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.UnitTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.QFileTestBatch.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.FileListProvider.java</file>
    </fixedFiles>
  </bug>
  <bug id="1466" opendate="2010-7-15 00:00:00" fixdate="2010-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add NULL DEFINED AS to ROW FORMAT specification</summary>
      <description>NULL values are passed to transformers as a literal backslash and a literal N. NULL values are saved when INSERT OVERWRITing LOCAL DIRECTORies as "NULL". This is inconsistent.The ROW FORMAT specification of tables should be able to specify the manner in which a null character is represented. ROW FORMAT NULL DEFINED AS '\N' or '\003' or whatever should apply to all instances of table export and saving.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="14715" opendate="2016-9-7 00:00:00" fixdate="2016-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive throws NumberFormatException with query with Null value</summary>
      <description>The java.lang.NumberFormatException will throw with following reproduce:set hive.cbo.enable=false;CREATE TABLE `paqtest`(`c1` int,`s1` string,`s2` string,`bn1` bigint)ROW FORMAT SERDE'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'STORED AS INPUTFORMAT'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'OUTPUTFORMAT'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';insert into paqtest values (58, '', 'ABC', 0);SELECT'PM' AS cy,c1,NULL AS iused,NULL AS itp,s2,NULL AS cvg,NULL AS acavg,sum(bn1) AS ccaFROM paqtestWHERE (s1 IS NULL OR length(s1) = 0)GROUP BY 'Pricing mismatch', c1, NULL, NULL, s2, NULL, NULL;The stack like following:java.lang.NumberFormatException: ABCGroupByOperator.process(Object, int) line: 773 ExecReducer.reduce(Object, Iterator, OutputCollector, Reporter) line: 236 ReduceTask.runOldReducer(JobConf, TaskUmbilicalProtocol, TaskReporter, RawKeyValueIterator, RawComparator&lt;INKEY&gt;, Class&lt;INKEY&gt;, Class&lt;INVALUE&gt;) line: 444 ReduceTask.run(JobConf, TaskUmbilicalProtocol) line: 392 LocalJobRunner$Job$ReduceTaskRunnable.run() line: 319 Executors$RunnableAdapter&lt;T&gt;.call() line: 471 It works fine when hive.cbo.enable = true</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14737" opendate="2016-9-12 00:00:00" fixdate="2016-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem accessing /logs in a Kerberized Hive Server 2 Web UI</summary>
      <description>The /logs menu fails with error &amp;#91;1&amp;#93; when the cluster is Kerberized. Other menu items are working properly.&amp;#91;1&amp;#93; HTTP ERROR: 401Problem accessing /logs/. Reason: Unauthenticated users are not authorized to access this page.Powered by Jetty://</description>
      <version>1.1.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15061" opendate="2016-10-25 00:00:00" fixdate="2016-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore types are sometimes case sensitive</summary>
      <description>Impala recently encountered an issue with the metastore (IMPALA-4260 ) where column stats would get dropped when adding a column to a table.The reason seems to be that Hive does a case sensitive check on the column stats types during an "alter table" and expects the types to be all lower case. This case sensitive check doesn't appear to happen when the stats are set in the first place.We're solving this on the Impala end by storing types in the metastore as all lower case, but Hive's behavior here is very confusing. It should either always be case sensitive, so that you can't create column stats with types that Hive considers invalid, or it should never be case sensitive.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15064" opendate="2016-10-26 00:00:00" fixdate="2016-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix explain for MM tables - don&amp;#39;t output for non-MM tables</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="15322" opendate="2016-12-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skipping "hbase mapredcp" in hive script for certain services</summary>
      <description>"hbase mapredcp" is intended to append hbase classpath to hive. However, the command can take some time when the system is heavy loaded. In some extreme cases, we saw ~20s delay due to it. For certain commands, such as "schemaTool", hbase classpath is certainly useless, and we can safely skip invoking it.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="15323" opendate="2016-12-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow the user to turn off reduce-side SMB join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15359" opendate="2016-12-5 00:00:00" fixdate="2016-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>skip.footer.line.count doesnt work properly for certain situations</summary>
      <description>This issue's reproduce is very like HIVE-12718 , but the data file is larger than 128M . In this case, even make sure only one mapper is used, the footer is still wrongly skipped. This issue happens when hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="15395" opendate="2016-12-8 00:00:00" fixdate="2016-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t try to intern strings from empty map</summary>
      <description>Otherwise it unnecessarily create another map object.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="15672" opendate="2017-1-20 00:00:00" fixdate="2017-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP text cache: improve first query perf II</summary>
      <description>4) Send VRB to the pipeline and write ORC in parallel (in background).</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.StructColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.MultiValuedColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.VertorDeserializeOrcWriter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15674" opendate="2017-1-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more setOp tests to HivePerfCliDriver</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query87.q</file>
    </fixedFiles>
  </bug>
  <bug id="15680" opendate="2017-1-20 00:00:00" fixdate="2017-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results when hive.optimize.index.filter=true and same ORC table is referenced twice in query</summary>
      <description>To repro:set hive.optimize.index.filter=true;create table test_table(number int) stored as ORC;-- Two insertions will create two files, with one stripe eachinsert into table test_table VALUES (1);insert into table test_table VALUES (2);-- This should and does return 2 recordsselect * from test_table;-- These should and do each return 1 recordselect * from test_table where number = 1;select * from test_table where number = 2;-- This should return 2 records but only returns 1 recordselect * from test_table where number = 1union allselect * from test_table where number = 2;What's happening is only the last predicate is being pushed down.</description>
      <version>1.1.0,2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="15795" opendate="2017-2-2 00:00:00" fixdate="2017-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Accumulo Index Tables in Hive Accumulo Connector</summary>
      <description>Ability to specify an accumulo index table for an accumulo-hive table.This would greatly improve performance for non-rowid query predicates</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloIndexLexicoder.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloIndexLexicoder.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15964" opendate="2017-2-17 00:00:00" fixdate="2017-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="16090" opendate="2017-3-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addendum to HIVE-16014</summary>
      <description>HIVE-16014 changed the HiveMetastoreChecker to use METASTORE_FS_HANDLER_THREADS_COUNT for pool size. Some of the tests in TestHiveMetastoreChecker still use HIVE_MOVE_FILES_THREAD_COUNT which leads to incorrect test behavior.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="16471" opendate="2017-4-18 00:00:00" fixdate="2017-4-18 01:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add metrics for "waiting compilation time"</summary>
      <description>When parallel compilation is off, there could be multiple queries waiting on the compilation lock. Currently we have metrics for the # of waiting queries, but it's good to also track the average/min/max waiting time, etc.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="16473" opendate="2017-4-18 00:00:00" fixdate="2017-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-on-Tez may fail to write to an HBase table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.3.0,2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16474" opendate="2017-4-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Druid version to 0.10</summary>
      <description>Druid 0.10 is out. We shall upgrade to it to take advantage of improvements it brings.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.DerbyConnectorTestUtility.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16721" opendate="2017-5-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent behavior in dealing with Timestamp stats</summary>
      <description>HIVE-15003 added support for additional types for col stats. However, it treats timestamp as DateColumnStatsData whereas when we read the timestamp stats, we read as LongColumnStatsData (https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java#L229). We should make it consistent with original hive behavior</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="16907" opendate="2017-6-15 00:00:00" fixdate="2017-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"INSERT INTO" overwrite old data when destination table encapsulated by backquote</summary>
      <description>A way to reproduce:create database tdb;use tdb;create table t1(id int);create table t2(id int);explain insert into `tdb.t1` select * from t2;+---------------------------------------------------------------------------------------------------------------------------------------------------+| Explain |+---------------------------------------------------------------------------------------------------------------------------------------------------+| STAGE DEPENDENCIES: || Stage-1 is a root stage || Stage-6 depends on stages: Stage-1 , consists of Stage-3, Stage-2, Stage-4 || Stage-3 || Stage-0 depends on stages: Stage-3, Stage-2, Stage-5 || Stage-2 || Stage-4 || Stage-5 depends on stages: Stage-4 || || STAGE PLANS: || Stage: Stage-1 || Map Reduce || Map Operator Tree: || TableScan || alias: t2 || Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE || Select Operator || expressions: id (type: int) || outputColumnNames: _col0 || Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE || File Output Operator || compressed: false || Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE || table: || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat || serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe || name: tdb.t1 || || Stage: Stage-6 || Conditional Operator || || Stage: Stage-3 || Move Operator || files: || hdfs directory: true || destination: hdfs://hacluster/user/hive/warehouse/tdb.db/t1/.hive-staging_hive_2017-06-15_15-52-34_017_849305017872068583-1/-ext-10000 || || Stage: Stage-0 || Move Operator || tables: || replace: true || table: || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat || serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe || name: tdb.t1 || || Stage: Stage-2 || Merge File Operator || Map Operator Tree: || RCFile Merge Operator || merge level: block || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || || Stage: Stage-4 || Merge File Operator || Map Operator Tree: || RCFile Merge Operator || merge level: block || input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat || || Stage: Stage-5 || Move Operator || files: || hdfs directory: true || destination: hdfs://hacluster/user/hive/warehouse/tdb.db/t1/.hive-staging_hive_2017-06-15_15-52-34_017_849305017872068583-1/-ext-10000 || |+---------------------------------------------------------------------------------------------------------------------------------------------------+Note that 'replace: true' in move operator</description>
      <version>1.1.0,2.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure4.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="1691" opendate="2010-10-5 00:00:00" fixdate="2010-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ANALYZE TABLE command should check columns in partition spec</summary>
      <description>ANALYZE TABEL PARTITION (col1, col2,...) should check whether col1, col2 etc are partition columns.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16961" opendate="2017-6-26 00:00:00" fixdate="2017-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark leaks spark application in case user cancels query and closes session</summary>
      <description>It's found that a Spark application is leaked when user cancels query and closes the session while Hive is waiting for remote driver to connect back. This is found for asynchronous query execution, but seemingly equally applicable for synchronous submission when session is abruptly closed. The leaked Spark application that runs Spark driver connects back to Hive successfully and run for ever (until HS2 restarts), but receives no job submission because the session is already closed. Ideally, Hive should rejects the connection from the driver so the driver will exist.</description>
      <version>1.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16962" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better error msg for Hive on Spark in case user cancels query and closes session</summary>
      <description>In case user cancels a query and closes the session, Hive marks the query as failed. However, the error message is a little confusing. It still says:org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create spark client. This is likely because the queue you assigned to does not have free resource at the moment to start the job. Please check your queue usage and try the query again later.followed by some InterruptedException.Ideally, the error should clearly indicates the fact that user cancels the execution.</description>
      <version>1.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16963" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>rely on AcidUtils.getAcidState() for read path</summary>
      <description>This is to make MM table more consistent to full ACID table. Also it's a prerequisite for Insert Overwrite support for MM table (refer to HIVE-14988).</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="17253" opendate="2017-8-4 00:00:00" fixdate="2017-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding SUMMARY statement to HPL/SQL</summary>
      <description>Adding SUMMARY statement to HPL/SQL to describe a data set (table, query result) similar to Python and R.For each column output the data type, number of distinct values, non-NULL rows, mean, std, percentiles, min, max. Output additional stats for categorical columns. This helps perform quick and easy exploratory data analysis for SQL devs and business users. http://hplsql.org/summary</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.main.resources.hplsql-site.xml</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Row.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="17256" opendate="2017-8-5 00:00:00" fixdate="2017-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a notion of a guaranteed task to LLAP</summary>
      <description>Tasks are basically on two levels, guaranteed and speculative, with speculative being the default. As long as noone uses the new flag, the tasks behave the same.All the tasks that do have the flag also behave the same with regard to each other.The difference is that a guaranteed task is always higher priority, and preempts, a speculative task.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.FirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="17257" opendate="2017-8-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should merge empty files</summary>
      <description>Currently if merging file option is turned on and the dest dir contains large number of empty files, Hive will not trigger merge task: private long getMergeSize(FileSystem inpFs, Path dirPath, long avgSize) { AverageSize averageSize = getAverageSize(inpFs, dirPath); if (averageSize.getTotalSize() &lt;= 0) { return -1; } if (averageSize.getNumFiles() &lt;= 1) { return -1; } if (averageSize.getTotalSize()/averageSize.getNumFiles() &lt; avgSize) { return averageSize.getTotalSize(); } return -1; }This logic doesn't seem right as the it seems better to combine these empty files into one.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
    </fixedFiles>
  </bug>
  <bug id="17259" opendate="2017-8-6 00:00:00" fixdate="2017-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC does not recognize UNIONTYPE columns</summary>
      <description>Hive JDBC does not recognize UNIONTYPE columns.I've an external table backed by an avro schema containing a union type field."name" : "value","type" : [ "int", "string", "null" ]When describing the table I've:describe test_table;+-------------------+---------------------------------------------------------------+----------+--+| col_name | data_type | comment |+-------------------+---------------------------------------------------------------+----------+--+| description | string | || name | string | || value | uniontype&lt;int,string,void&gt; | |+-------------------+---------------------------------------------------------------+----------+--+When doing a select query over the data using the Hive CLI, it works:hive&gt; select value from test_table;OK{0:10}{0:10}{0:9}{0:9}...But when using beeline, it fails:0: jdbc:hive2://....&gt; select * from test_table;Error: Unrecognized column type: UNIONTYPE (state=,code=0)By applying the patch provided with this JIRA, the command succeeds and return the expected output.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
    </fixedFiles>
  </bug>
  <bug id="17401" opendate="2017-8-28 00:00:00" fixdate="2017-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive session idle timeout doesn&amp;#39;t function properly</summary>
      <description>It's apparent in our production environment that HS2 leaks sessions, which at least contributed to memory leaks in HS2. We further found that idle HS2 sessions rarely get timed out and the number of live session keeps increasing as time goes on. Eventually, HS2 becomes irresponsive and demands a restart.Investigation shows that session idle timeout doesn't work appropriately.</description>
      <version>1.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17452" opendate="2017-9-5 00:00:00" fixdate="2017-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL function variable block is not initialized</summary>
      <description>Variable inside declaration block are not initialized:CREATE FUNCTION test1() RETURNS STRINGAS ret string DEFAULT 'Initial value';BEGIN print(ret); ret := 'VALUE IS SET'; print(ret);END;test1();Output:ret VALUE IS SETShould be:Initial value VALUE IS SET</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.create.function4.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.function3.out.txt</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.test.queries.local.create.procedure4.sql</file>
      <file type="M">hplsql.src.test.queries.local.create.function5.sql</file>
      <file type="M">hplsql.src.test.queries.db.summary.sql</file>
    </fixedFiles>
  </bug>
  <bug id="17587" opendate="2017-9-22 00:00:00" fixdate="2017-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary filter from getPartitionsFromPartitionIds call</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="17590" opendate="2017-9-23 00:00:00" fixdate="2017-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade hadoop to 2.8.1</summary>
      <description>seems like hadoop 2.8.0 has no source attachment:http://central.maven.org/maven2/org/apache/hadoop/hadoop-common/2.8.0/howeverhttp://central.maven.org/maven2/org/apache/hadoop/hadoop-common/2.8.1/has source.jar-s</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18745" opendate="2018-2-19 00:00:00" fixdate="2018-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix MetaStore creation in tests, so multiple MetaStores can be started on the same machine</summary>
      <description>janulatha fixed the problem, when multiple MetaStore tests are started on the same machine, then they tried to reserve the same port. This caused flakiness in the MetaStore tests run with the ptest framework. See: HIVE-18147I reviewed the HIVE-17980, and tried to make sure, that the fix remains in every codepath. I was unsuccessful in it. This Jira aims to go through the MetaStore tests, and make sure all of them is using the  startMetaStoreWithRetry method so the different tests will not cause each other to fail. Also there were clashes not only in port numbers, but warehouse directories as well, so this Jira should fix that also.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerWithOldConf.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.RemoteMetaStoreForTests.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.StorageBasedMetastoreTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.metastore.security.TestHadoopAuthBridge23.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="18814" opendate="2018-2-27 00:00:00" fixdate="2018-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Add Partition For Acid tables</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/LanguageManual%2BDDL#LanguageManualDDL-AddPartitionsAdd Partition command creates a Partition metadata object and sets the location to the directory containing data files.In current master (Hive 3.0), Add partition on an acid table doesn't fail and at read time the data is decorated with row__id but the original transaction is 0.  I suspect in earlier Hive versions this will throw or return no data.Since this new partition didn't have data before, assigning txnid:0 isn't going to generate duplicate IDs but it could violate Snapshot Isolation in multi stmt txns. Suppose txnid:7 runs select * from T. Then txnid:8 adds a partition to T. Now if txnid:7 runs the same query again, it will see the data in the new partition.This can't be release like this since a delete on this data (added via Add partition) will use row_ids with txnid:0 so a later upgrade that sees un-compacted may generate row_ids with different txnid (assuming this is fixed by then) One option is follow Load Data approach and create a new delta_x_x/ and move/copy the data there. Another is to allocate a new writeid and save it in Partition metadata.  This could then be used to decorate data with ROW__IDs.  This avoids move/copy but retains data "outside" of the table tree which make it more likely that this data will be modified in some way which can really break things if done after and SQL update/delete on this data have happened.  It performs no validations on add (except for partition spec) so any file with any format can be added. It allows add to bucketed tables as well.Seems like a very dangerous command. Maybe a better option is to block it and advise using Load Data. Alternatively, make this do Add partition metadata op followed by Load Data.   </description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18815" opendate="2018-2-27 00:00:00" fixdate="2018-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused feature in HPL/SQL</summary>
      <description>Remove FTP feature in HPL/SQL.</description>
      <version>None</version>
      <fixedVersion>2.3.3,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.db.copy.from.ftp.out.txt</file>
      <file type="M">hplsql.src.test.queries.db.copy.from.ftp.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Ftp.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug id="18857" opendate="2018-3-5 00:00:00" fixdate="2018-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store default value text instead of default value expression in metastore</summary>
      <description>Currently for default value an expression is generated and serialized to store in metastore. It should be improved to rather serialize the default value itself instead of expression and store that in metastore. This will have the following benefits: It will make metastore schema upgrade safe. e.g. if a UDF function name is changed hive wouldn't be able to parse back the expression for this UDF which was serialized in earlier version. It will make metastore schema for default constraint hive agnostic. Other databases would be able to use the value as it is.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19298" opendate="2018-4-25 00:00:00" fixdate="2018-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix operator tree of CTAS for Druid Storage Handler</summary>
      <description>Current operator plan of CTAS for Druid storage handler is broken when used enables the property {code} hive.exec.parallel{code} as {code} true{code}</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.DefaultHiveMetaHook.java</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.dynamic.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.InsertTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="19446" opendate="2018-5-7 00:00:00" fixdate="2018-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryCache: Transaction lists needed for pending cache entries</summary>
      <description>Hive query-cache needs a transactional list, even when the entry is pending state so that other identical queries with the same transactional state can wait for the first query to complete, instead of triggering their own instance.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.cache.results.QueryResultsCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="19833" opendate="2018-6-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce LLAP IO min allocation to match ORC variable CB size</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="19993" opendate="2018-6-26 00:00:00" fixdate="2018-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using a table alias which also appears as a column name is not possible</summary>
      <description>drop table if exists tableA;drop table if exists tableB;create table tableA (a integer,z integer);create table tableB (a integer,b integer,z integer);select a.z, b.b from tableB as b JOIN tableA as aon a.a=b.b;Error: Error while compiling statement: FAILED: SemanticException Column a Found in more than One Tables/Subqueries (state=42000,code=40000)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="19994" opendate="2018-6-26 00:00:00" fixdate="2018-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Impala "drop table" fails with Hive Metastore exception</summary>
      <description>"drop table" statement in Impala shell fails with the following exception:ImpalaRuntimeException: Error making 'dropTable' RPC to Hive Metastore: CAUSED BY: MetaException: One or more instances could not be deleted Metastore log file shows that "DELETE FROM `PARTITION_KEYS` WHERE `TBL_ID`=?" statement fails because of foreign key violation (full stacktrace will be added):Caused by: java.sql.BatchUpdateException: Cannot delete or update a parent row: a foreign key constraint fails ("hivemetastore_emtig3vtq7qp1tiooo07sb70ud"."COLUMNS_V2", CONSTRAINT "COLUMNS_V2_FK1" FOREIGN KEY ("CD_ID") REFERENCES "CDS" ("CD_ID")) The table is created and then dropped as a part of ETL process executed every hour. Most of the time it works fine, the issue is not reproducible at will.Table creation script is:CREATE TABLE IF NOT EXISTS price_advisor_ouput.t_switching_coef_source{{( }}...fields here...PRIMARY KEY (...PK field here...))PARTITION BY HASH(matrix_pcd) PARTITIONS 3STORED AS KUDU; Not sure how to approach diagnostics and fix, so any input will be really appreciated. Thanks in advance, Rodion Myronov</description>
      <version>1.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
    </fixedFiles>
  </bug>
  <bug id="19995" opendate="2018-6-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate row traffic for acid tables</summary>
      <description>for transactional tables we store basic stats in case of explicit analyze/rewrite; but doesn't do anything in other cases....which may even lead to plans which oom...It would be better to aggregate the total row traffic...because that is already available; so that operator tree estimations could work with a real upper bound of the row numbers.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.transactional.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="20093" opendate="2018-7-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LlapOutputFomatService: Use ArrowBuf with Netty for Accounting</summary>
      <description>Combining Unpooled.wrappedBuffer with Arrow buffers can create corrupted buffers from buffer reuse race-condition.This change ensures Arrow memory to be accounted by the same BufferAllocator.RootAllocator will return an ArrowBuf which cooperates with Arrow memory arrow accounting after Netty release(1) the buffer.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.WritableByteChannelAdapter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
    </fixedFiles>
  </bug>
  <bug id="20094" opendate="2018-7-5 00:00:00" fixdate="2018-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Druid to 0.12.1 version</summary>
      <description>As per Jira title.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20174" opendate="2018-7-13 00:00:00" fixdate="2018-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix NULL / Wrong Results issues in GROUP BY Aggregation Functions</summary>
      <description>Write new UT tests that use random data and intentional isRepeating batches to checks for NULL and Wrong Results for vectorized aggregation functions. BUGs found:1) AVG/VARIANCE (family) in PARTIAL1 mode was returning NULL instead of count = 0, sum = 0 (All data types).  For AVG DECIMAL, only return NULL if there was an overflow.2) AVG/MIN/MAX was not detecting repeated NULL correctly for the TIMESTAMP, INTERVAL_DAY_TIME, and String Family.  Eliminated redundant code.3) Fix incorrect calculation  for VARIANCE (family) in PARTIAL2 and FINAL modes (HIVE-18758).4) Fix row-mode AVG DECIMAL to enforce output type precision and scale in COMPLETE and FINAL modes. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomBatchSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorSubStr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringUnary.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal64ToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal64.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountMerge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxIntervalDayTime.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimalMerge.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimal64ToDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
    </fixedFiles>
  </bug>
  <bug id="20890" opendate="2018-11-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Allow whole table ReadLocks to skip all partition locks</summary>
      <description>HIVE-19369 proposes adding a EXCL_WRITE lock which does not wait for any SHARED_READ locks for read operations - in the presence of that lock, the insert overwrite no longer takes an exclusive lock.The only exclusive operation will be a schema change or drop table, which should take an exclusive lock on the entire table directly.explain locks select * from tpcds_bin_partitioned_orc_1000.store_sales where ss_sold_date_sk=2452626 +----------------------------------------------------+| Explain |+----------------------------------------------------+| LOCK INFORMATION: || tpcds_bin_partitioned_orc_1000.store_sales -&gt; SHARED_READ || tpcds_bin_partitioned_orc_1000.store_sales.ss_sold_date_sk=2452626 -&gt; SHARED_READ |+----------------------------------------------------+So the per-partition SHARED_READ locks are no longer necessary, if the lock builder already includes the table-wide SHARED_READ locks.The removal of entire partitions is the only part which needs to be taken care of within this semantics as row-removal instead of directory removal (i.e "drop partition" -&gt; "truncate partition" and have the truncation trigger a whole directory cleaner, so that the partition disappears when there are 0 rows left).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.explain.locks.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20985" opendate="2018-11-29 00:00:00" fixdate="2018-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If select operator inputs are temporary columns vectorization may reuse some of them as output</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.numeric.overflows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf.adaptor.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orc.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="20987" opendate="2018-11-30 00:00:00" fixdate="2018-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Druid Tests to avoid Timeouts</summary>
      <description>Currently Druid Tests fail with Timeout issue.I am plaining on splitting the test into 2 batches at least to avoid timeouts.I will tweak the test code to pick random Druid nodes ports like that minimize the collision issue that we saw before.  </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.SingleNodeKafkaCluster.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.MiniDruidCluster.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.ForkingDruidNode.java</file>
    </fixedFiles>
  </bug>
  <bug id="21746" opendate="2019-5-16 00:00:00" fixdate="2019-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayIndexOutOfBoundsException during dynamically partitioned hash join, with CBO disabled</summary>
      <description>ArrayIndexOutOfBounds exception during query execution with dynamically partitioned hash join.Found on Hive 2.x. Seems to occur with CBO disabled/failed.Disabling constant propagation seems to allow the query to succeed.java.lang.ArrayIndexOutOfBoundsException: 203 at org.apache.hadoop.hive.serde2.io.TimestampWritable.getTotalLength(TimestampWritable.java:217) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.checkObjectByteInfo(LazyBinaryUtils.java:205) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.parse(LazyBinaryStruct.java:142) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getFieldsAsList(LazyBinaryStruct.java:281) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$ReusableRowContainer.unpack(MapJoinBytesTableContainer.java:744) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$ReusableRowContainer.next(MapJoinBytesTableContainer.java:730) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer$ReusableRowContainer.next(MapJoinBytesTableContainer.java:605) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.next(UnwrapRowContainer.java:70) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.next(UnwrapRowContainer.java:34) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:819) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:924) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:456) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:359) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:290) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:319) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:189) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:172) ~[hive-exec-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:377) ~[tez-runtime-internals-0.8.4.2.6.4.119-3.jar:0.8.4.2.6.4.119-3] at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) ~[tez-runtime-internals-0.8.4.2.6.4.119-3.jar:0.8.4.2.6.4.119-3] at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.2.6.4.119-3.jar:0.8.4.2.6.4.119-3] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_112] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_112] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869) ~[hadoop-common-2.7.3.2.6.4.119-3.jar:?] at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.2.6.4.119-3.jar:0.8.4.2.6.4.119-3] at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) ~[tez-runtime-internals-0.8.4.2.6.4.119-3.jar:0.8.4.2.6.4.119-3] at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.2.6.4.119-3.jar:0.8.4.2.6.4.119-3] at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118) ~[hive-llap-server-2.1.0.2.6.4.119-3.jar:2.1.0.2.6.4.119-3] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="7224" opendate="2014-6-12 00:00:00" fixdate="2014-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set incremental printing to true by default in Beeline</summary>
      <description>See HIVE-7221.By default beeline tries to buffer the entire output relation before printing it on stdout. This can cause OOM when the output relation is large. However, beeline has the option of incremental prints. We should keep that as the default.</description>
      <version>0.13.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug id="7226" opendate="2014-6-12 00:00:00" fixdate="2014-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windowing Streaming mode causes NPE for empty partitions</summary>
      <description>Change in HIVE-7062 doesn't handle empty partitions properly. StreamingState is not correctly initialized for empty partition</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="7476" opendate="2014-7-22 00:00:00" fixdate="2014-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS does not work properly for s3</summary>
      <description>When we use CTAS to create a new table in s3, the table location is not set correctly. As a result, the data from the existing table cannot be inserted into the new created table.We can use the following example to reproduce this issue.set hive.metastore.warehouse.dir=OUTPUT_PATH;drop table s3_dir_test;drop table s3_1;drop table s3_2;create external table s3_dir_test(strct struct&lt;a:int, b:string, c:string&gt;)row format delimitedfields terminated by '\t'collection items terminated by ' 'location 'INPUT_PATH';create table s3_1(strct struct&lt;a:int, b:string, c:string&gt;)row format delimitedfields terminated by '\t'collection items terminated by ' ';insert overwrite table s3_1 select * from s3_dir_test;select * from s3_1;create table s3_2 as select * from s3_1;select * from s3_1;select * from s3_2;The data could be as follows.1 abc 10.52 def 11.53 ajss 90.232324 djns 89.020025 random 2.996 data 3.0027 ne 71.9084The root cause is that the SemanticAnalyzer class did not handle s3 location properly for CTAS.A patch will be provided shortly.</description>
      <version>0.13.1,1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="8344" opendate="2014-10-3 00:00:00" fixdate="2014-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Tez sets mapreduce.framework.name to yarn-tez</summary>
      <description>This was done to run MR jobs when in Tez mode (emulate MR on Tez). However, we don't switch back when the user specifies MR as exec engine.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="853" opendate="2009-9-23 00:00:00" fixdate="2009-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a hint to select which tables to stream in a join</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.uniquejoin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.rc.seq.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.uniquejoin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.union2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.load.wrong.fileformat.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8530" opendate="2014-10-21 00:00:00" fixdate="2014-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Preserve types of literals</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="8746" opendate="2014-11-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC timestamp columns are sensitive to daylight savings time</summary>
      <description>Hive uses Java's Timestamp class to manipulate timestamp columns. Unfortunately the textual parsing in Timestamp is done in local time and the internal storage is in UTC.ORC mostly side steps this issue by storing the difference between the time and a base time also in local and storing that difference in the file. Reading the file between timezones will mostly work correctly "2014-01-01 12:34:56" will read correctly in every timezone.However, when moving between timezones with different daylight saving it creates trouble. In particular, moving from a computer in PST to UTC will read "2014-06-06 12:34:56" as "2014-06-06 11:34:56".</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.static.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="877" opendate="2009-10-15 00:00:00" fixdate="2009-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>error with cast(null as short)</summary>
      <description>also cast(null as long)</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.negative.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8891" opendate="2014-11-16 00:00:00" fixdate="2014-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Another possible cause to NucleusObjectNotFoundException from drops/rollback</summary>
      <description>It might be another possible cause to org.datanucleus.exceptions.NucleusObjectNotFoundException: No such database row in drops and retries. The rollback might also fail for some reason (e.g. same as that for commit), and the detached objects should also be evicted for same reason as HIVE-3826. The evictAll needs to be put in the finally block.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="8892" opendate="2014-11-16 00:00:00" fixdate="2014-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use MEMORY_AND_DISK for RDD caching [Spark Branch]</summary>
      <description>In HIVE-8844, we made the persistent policy for RDD caching configurable. We should do something simpler and don't add additional configurations.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
    </fixedFiles>
  </bug>
  <bug id="9019" opendate="2014-12-4 00:00:00" fixdate="2014-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using SPARK_JAVA_OPTS [Spark Branch]</summary>
      <description>SPARK_JAVA_OPTS has been deprecated, see SparkConf.validateSettings.Using it together with spark.driver.extraJavaOptions will cause SparkContext fail to start.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">data.conf.spark.log4j.properties</file>
      <file type="M">data.conf.spark.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9118" opendate="2014-12-16 00:00:00" fixdate="2014-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support auto-purge for tables, when dropping tables/partitions.</summary>
      <description>HIVE-7100 introduced a way to skip the trash directory, when deleting table-data, while dropping tables.In HIVE-9083/HIVE-9086, I extended this to work when partitions are dropped.Here, I propose a table-parameter ("auto.purge") to set up tables to skip-trash when table/partition data is deleted, without needing to say "PURGE" on the Hive CLI. Apropos, on dropTable() and dropPartition(), table data is deleted directly (and not moved to trash) if the following hold true: The table is MANAGED. The deleteData parameter to the HMSC.drop*() methods is true. Either PURGE is explicitly specified on the command-line (or rather, "ifPurge" is set in the environment context, OR TBLPROPERTIES contains "auto.purge"="true"</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="9447" opendate="2015-1-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore: inefficient Oracle query for removing unused column descriptors when add/drop table/partition</summary>
      <description>Metastore needs removing unused column descriptors when drop/add partitions or tables. For query the unused column descriptor, the current implementation utilizes datanuleus' range function, which basically equals LIMIT syntax. However, Oracle does not support LIMIT, the query is converted as SQL&gt; SELECT * FROM (SELECT subq.*,ROWNUM rn FROM (SELECT'org.apache.hadoop.hive.metastore.model.MStorageDescriptor' ASNUCLEUS_TYPE,A0.INPUT_FORMAT,A0.IS_COMPRESSED,A0.IS_STOREDASSUBDIRECTORIES,A0.LOCATION,A0.NUM_BUCKETS,A0.OUTPUT_FORMAT,A0.SD_ID FROM drhcat.SDS A0 WHERE A0.CD_ID = ? ) subq ) WHERE rn &lt;= 1;Given that CD_ID is not very selective, this query may have to access large amount of rows (depends how many partitions the table has, millions of rows in our case). Metastore may become unresponsive because of this. Since Metastore only needs to know if the specific CD_ID is referenced in SDS table and does not need access the whole row. We can use select count(1) from SDS where SDS.CD_ID=?CD_ID is index column, the above query will do range scan for index, which is faster. For other DBs support LIMIT syntax such as MySQL, this problem does not exist. However, the new query does not hurt.</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="9499" opendate="2015-1-28 00:00:00" fixdate="2015-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.limit.query.max.table.partition makes queries fail on non-partitioned tables</summary>
      <description>If you use hive.limit.query.max.table.partition to limit the amount of partitions that can be queried it makes queries on non-partitioned tables fail.Example:CREATE TABLE tmp(test INT);SELECT COUNT(*) FROM TMP; -- works fineSET hive.limit.query.max.table.partition=20;SELECT COUNT(*) FROM TMP; -- generates NPE (FAILED: NullPointerException null)SET hive.limit.query.max.table.partition=-1;SELECT COUNT(*) FROM TMP; -- works fine again</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="95" opendate="2008-12-2 00:00:00" fixdate="2008-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve cli error messages by lowering backtracking to 1</summary>
      <description>Stop antlr from backtracking so much should (and does) improve error messages since antlr will report the error closer to where it happened.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="9513" opendate="2015-1-29 00:00:00" fixdate="2015-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="9514" opendate="2015-1-29 00:00:00" fixdate="2015-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool is broken in hive 1.0.0</summary>
      <description>Schematool gives following error - bin/schematool -dbType derby -initSchemaStarting metastore schema initialization to 1.0org.apache.hadoop.hive.metastore.HiveMetaException: Unknown version specified for initialization: 1.0Metastore schema hasn't changed from 0.14.0 to 1.0.0. So there is no need for new .sql files for 1.0.0. However, schematool needs to be made aware of the metastore schema equivalence.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="9527" opendate="2015-1-31 00:00:00" fixdate="2015-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include dot files in tarball</summary>
      <description>Ideally the source tarball exactly matches the svn tag. On item that is missing is the dot files.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9529" opendate="2015-1-31 00:00:00" fixdate="2015-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"alter table .. concatenate" under Tez mode should create TezTask</summary>
      <description>"alter table .. concatenate" DDL command creates MR task by default. When hive cli is launched with execution engine as tez, the scheduling of the MR task for file merging could be delayed until tez session expiration. This happens because YARN will not have capacity to launch another AppMaster for MR task. We should create tez task to overcome this. When the execution engine is tez TezTask will be created else MRTask will be created.</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="9538" opendate="2015-2-1 00:00:00" fixdate="2015-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude thirdparty directory from tarballs</summary>
      <description></description>
      <version>spark-branch,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9549" opendate="2015-2-2 00:00:00" fixdate="2015-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include missing directories in source tarball</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9552" opendate="2015-2-2 00:00:00" fixdate="2015-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge trunk to Spark branch 2/2/2015 [Spark Branch]</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ql.rewrite.gbtoidx.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9554" opendate="2015-2-2 00:00:00" fixdate="2015-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename 0.15 upgrade scripts to 1.1</summary>
      <description></description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.14.0-to-0.15.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.15.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.14.0-to-0.15.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.15.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.14.0-to-0.15.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.15.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-0.14.0-to-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.14.0-to-0.15.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.15.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="9555" opendate="2015-2-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>assorted ORC refactorings for LLAP on trunk</summary>
      <description>To minimize conflicts and given that ORC is being developed rapidly on trunk, I would like to refactor some parts of ORC "in advance" based on the changes in LLAP branch. Mostly it concerns making parts of ORC code (esp. SARG, but also some internal methods) more modular and easier to use from alternative codepaths. There's also significant change to how data reading is handled - BufferChunk inherits from DiskRange; the reader receives a list of DiskRange-s (as before), but instead of making a list of buffer chunks it replaces ranges with buffer chunks in the original (linked) list.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInStream.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.PositionProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.CompressionCodec.java</file>
    </fixedFiles>
  </bug>
  <bug id="956" opendate="2009-11-27 00:00:00" fixdate="2009-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support of columnar binary serde</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryObject.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9593" opendate="2015-2-5 00:00:00" fixdate="2015-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC Reader should ignore unknown metadata streams</summary>
      <description>ORC readers should ignore metadata streams which are non-essential additions to the main data streams.This will include additional indices, histograms or anything we add as an optional stream.</description>
      <version>0.11.0,0.12.0,0.13.1,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="9610" opendate="2015-2-8 00:00:00" fixdate="2015-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Continuation of HIVE-9438 - The standalone-jdbc jar missing some classes</summary>
      <description>We've not had success only including specific shim classes as part of the standalone jdbc jar. Since all shim classes shouldn't be too large we'll include them all.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9621" opendate="2015-2-9 00:00:00" fixdate="2015-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 http mode - embedded jetty should use SynchronousQueue</summary>
      <description>Noticed an unreproducible bug (customer reported), where HiveServer2 in http mode would accept the incoming tcp connection but would not allocate a new thread to process the request. Switching from LinkedBlockingQueue to SynchronousQueue fixes the issue and also simplifies the threading model.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="9646" opendate="2015-2-10 00:00:00" fixdate="2015-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesn&amp;#39;t show Spark job progress info [Spark Branch]</summary>
      <description>Beeline can show MR job progress info, but can't show that of Spark job. CLI doesn't have this problem.</description>
      <version>spark-branch,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug id="9647" opendate="2015-2-10 00:00:00" fixdate="2015-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Discrepancy in cardinality estimates between partitioned and un-partitioned tables</summary>
      <description>High-level summaryHiveRelMdSelectivity.computeInnerJoinSelectivity relies on per column number of distinct value to estimate join selectivity.The way statistics are aggregated for partitioned tables results in discrepancy in number of distinct values which results in different plans between partitioned and un-partitioned schemas.The table below summarizes the NDVs in computeInnerJoinSelectivity which are used to estimate selectivity of joins.Column Partitioned count distincts Un-Partitioned count distinctssr_customer_sk 71,245 1,415,625sr_item_sk 38,846 62,562sr_ticket_number 71,245 34,931,085ss_customer_sk 88,476 1,415,625ss_item_sk 38,846 62,562ss_ticket_number 100,756 56,256,175The discrepancy is because NDV calculation for a partitioned table assumes that the NDV range is contained within each partition and is calculates as "select max(NUM_DISTINCTS) from PART_COL_STATS” .This is problematic for columns like ticket number which are naturally increasing with the partitioned date column ss_sold_date_sk.SuggestionsUse Hyper Log Log as suggested by Gopal, there is an HLL implementation for HBASE co-porccessors which we can use as a reference here Using the global stats from TAB_COL_STATS and the per partition stats from PART_COL_STATS extrapolate the NDV for the qualified partitions as in :Max ( (NUM_DISTINCTS from TAB_COL_STATS) x (Number of qualified partitions) / (Number of Partitions), max(NUM_DISTINCTS) from PART_COL_STATS))More detailsWhile doing TPC-DS Partitioned vs. Un-Partitioned runs I noticed that many of the plans are different, then I dumped the CBO logical plan and I found that join estimates are drastically differentUnpartitioned schema :2015-02-10 11:33:27,624 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12624)) - Plan After Join Reordering:HiveProjectRel(store_sales_quantitycount=[$0], store_sales_quantityave=[$1], store_sales_quantitystdev=[$2], store_sales_quantitycov=[/($2, $1)], as_store_returns_quantitycount=[$3], as_store_returns_quantityave=[$4], as_store_returns_quantitystdev=[$5], store_returns_quantitycov=[/($5, $4)]): rowcount = 1.0, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2956 HiveAggregateRel(group=[{}], agg#0=[count($0)], agg#1=[avg($0)], agg#2=[stddev_samp($0)], agg#3=[count($1)], agg#4=[avg($1)], agg#5=[stddev_samp($1)]): rowcount = 1.0, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2954 HiveProjectRel($f0=[$4], $f1=[$8]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2952 HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$1], ss_customer_sk=[$2], ss_ticket_number=[$3], ss_quantity=[$4], sr_item_sk=[$5], sr_customer_sk=[$6], sr_ticket_number=[$7], sr_return_quantity=[$8], d_date_sk=[$9], d_quarter_name=[$10]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2982 HiveJoinRel(condition=[=($9, $0)], joinType=[inner]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2980 HiveJoinRel(condition=[AND(AND(=($2, $6), =($1, $5)), =($3, $7))], joinType=[inner]): rowcount = 28880.460910696, cumulative cost = {6.05654559E8 rows, 0.0 cpu, 0.0 io}, id = 2964 HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_customer_sk=[$3], ss_ticket_number=[$9], ss_quantity=[$10]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2920 HiveTableScanRel(table=[[tpcds_bin_orc_200.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 2822 HiveProjectRel(sr_item_sk=[$2], sr_customer_sk=[$3], sr_ticket_number=[$9], sr_return_quantity=[$10]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2923 HiveTableScanRel(table=[[tpcds_bin_orc_200.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 2823 HiveProjectRel(d_date_sk=[$0], d_quarter_name=[$15]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2948 HiveFilterRel(condition=[=($15, '2000Q1')]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2946 HiveTableScanRel(table=[[tpcds_bin_orc_200.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 2821Partitioned schema :2015-02-10 11:32:16,880 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12624)) - Plan After Join Reordering:HiveProjectRel(store_sales_quantitycount=[$0], store_sales_quantityave=[$1], store_sales_quantitystdev=[$2], store_sales_quantitycov=[/($2, $1)], as_store_returns_quantitycount=[$3], as_store_returns_quantityave=[$4], as_store_returns_quantitystdev=[$5], store_returns_quantitycov=[/($5, $4)]): rowcount = 1.0, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2791 HiveAggregateRel(group=[{}], agg#0=[count($0)], agg#1=[avg($0)], agg#2=[stddev_samp($0)], agg#3=[count($1)], agg#4=[avg($1)], agg#5=[stddev_samp($1)]): rowcount = 1.0, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2789 HiveProjectRel($f0=[$3], $f1=[$8]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2787 HiveProjectRel(ss_item_sk=[$4], ss_customer_sk=[$5], ss_ticket_number=[$6], ss_quantity=[$7], ss_sold_date_sk=[$8], sr_item_sk=[$0], sr_customer_sk=[$1], sr_ticket_number=[$2], sr_return_quantity=[$3], d_date_sk=[$9], d_quarter_name=[$10]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2817 HiveJoinRel(condition=[AND(AND(=($5, $1), =($4, $0)), =($6, $2))], joinType=[inner]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2815 HiveProjectRel(sr_item_sk=[$1], sr_customer_sk=[$2], sr_ticket_number=[$8], sr_return_quantity=[$9]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2758 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 2658 HiveJoinRel(condition=[=($5, $4)], joinType=[inner]): rowcount = 762935.5811373093, cumulative cost = {5.500766553162274E8 rows, 0.0 cpu, 0.0 io}, id = 2801 HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_ticket_number=[$8], ss_quantity=[$9], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2755 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 2657 HiveProjectRel(d_date_sk=[$0], d_quarter_name=[$15]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2783 HiveFilterRel(condition=[=($15, '2000Q1')]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2781 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 2656This was puzzling knowing that the stats for both tables are “identical” in TAB_COL_STATS.Column statistics from TAB_COL_STATS, notice how the column statistics are identical in both cases.DB_NAME COLUMN_NAME COLUMN_TYPE NUM_NULLS LONG_HIGH_VALUE LONG_LOW_VALUE MAX_COL_LEN NUM_DISTINCTStpcds_bin_orc_200 d_date_sk int 0 2,488,070 2,415,022 NULL 65,332tpcds_bin_partitioned_orc_200 d_date_sk int 0 2,488,070 2,415,022 NULL 65,332tpcds_bin_orc_200 d_quarter_name string 0 NULL NULL 6 721tpcds_bin_partitioned_orc_200 d_quarter_name string 0 NULL NULL 6 721tpcds_bin_orc_200 sr_customer_sk int 1,009,571 1,600,000 1 NULL 1,415,625tpcds_bin_partitioned_orc_200 sr_customer_sk int 1,009,571 1,600,000 1 NULL 1,415,625tpcds_bin_orc_200 sr_item_sk int 0 48,000 1 NULL 62,562tpcds_bin_partitioned_orc_200 sr_item_sk int 0 48,000 1 NULL 62,562tpcds_bin_orc_200 sr_ticket_number int 0 48,000,000 1 NULL 34,931,085tpcds_bin_partitioned_orc_200 sr_ticket_number int 0 48,000,000 1 NULL 34,931,085tpcds_bin_orc_200 ss_customer_sk int 12,960,424 1,600,000 1 NULL 1,415,625tpcds_bin_partitioned_orc_200 ss_customer_sk int 12,960,424 1,600,000 1 NULL 1,415,625tpcds_bin_orc_200 ss_item_sk int 0 48,000 1 NULL 62,562tpcds_bin_partitioned_orc_200 ss_item_sk int 0 48,000 1 NULL 62,562tpcds_bin_orc_200 ss_sold_date_sk int 0 2,452,642 2,450,816 NULL 2,226tpcds_bin_partitioned_orc_200 ss_sold_date_sk int 0 2,452,642 2,450,816 NULL 2,226tpcds_bin_orc_200 ss_ticket_number int 0 48,000,000 1 NULL 56,256,175tpcds_bin_partitioned_orc_200 ss_ticket_number int 0 48,000,000 1 NULL 56,256,175For partitioned tables we get the statistics using get_aggr_stats_for which eventually issues the query belowselect COLUMN_NAME, COLUMN_TYPE, … max(NUM_DISTINCTS), …from PART_COL_STATSWherewhere DB_NAME = and TABLE_NAME = and COLUMN_NAME in and PARTITION_NAME in (1 … N)group by COLUMN_NAME , COLUMN_TYPE; …</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="9652" opendate="2015-2-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez in place updates should detect redirection of STDERR</summary>
      <description>Tez in place updates detects STDOUT redirection and logs using old logging method. Similarly it should detect STDERR redirection as well. This will make sure following will log using old methodhive -e '&lt;some_query&gt;' 2&gt; err.log</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9655" opendate="2015-2-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition table insertion error</summary>
      <description>We have these two tables:create table t1 (c1 bigint, c2 string);CREATE TABLE t2 (c1 int, c2 string)PARTITIONED BY (p1 string);load data local inpath 'data' into table t1;load data local inpath 'data' into table t1;load data local inpath 'data' into table t1;load data local inpath 'data' into table t1;load data local inpath 'data' into table t1;But, when try to insert into table t2 from t1:SET hive.exec.dynamic.partition.mode=nonstrict;insert overwrite table t2 partition(p1) select *,c1 as p1 from t1 distribute by p1;The query failed with the following exception:2015-02-11 12:50:52,756 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(178)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"c1":1,"c2":"one"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1] at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493) ... 10 moreCaused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1] at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410) at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147) at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954) at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325) ... 16 more</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="9684" opendate="2015-2-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect disk range computation in ORC because of optional stream kind</summary>
      <description>HIVE-9593 changed all required fields in ORC protobuf message to optional field. But DiskRange computation and stream creation code assumes existence of stream kind everywhere. This leads to incorrect calculation of diskranges resulting in out of range exceptions. The proper fix is to check if stream kind exists using stream.hasKind() before adding the stream to disk range computation.</description>
      <version>1.0.0,1.0.1,1.1.0</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
    </fixedFiles>
  </bug>
  <bug id="9701" opendate="2015-2-16 00:00:00" fixdate="2015-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JMH module does not compile under hadoop-1 profile</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9708" opendate="2015-2-17 00:00:00" fixdate="2015-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove testlibs directory</summary>
      <description>The testlibs directory is left over from the old ant build. We can delete it as it's downloaded by maven now:https://github.com/apache/hive/blob/trunk/pom.xml#L610</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testlibs.ant-contrib.LICENSE.txt</file>
      <file type="M">testlibs.ant-contrib-1.0b3.jar</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9727" opendate="2015-2-19 00:00:00" fixdate="2015-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GroupingID translation from Calcite</summary>
      <description>The translation from Calcite back to Hive might produce wrong results while interacting with other Calcite optimization rules.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveGroupingID.java</file>
    </fixedFiles>
  </bug>
  <bug id="9728" opendate="2015-2-19 00:00:00" fixdate="2015-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add heap mode to allocator (for q files, YARN w/o direct buffer accounting support)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.llap.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.Allocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="9729" opendate="2015-2-19 00:00:00" fixdate="2015-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: design and implement proper metadata cache</summary>
      <description>Simple approach: add external priorities to data cache, read metadata parts of orc file into it. Advantage: simple; consistent management (no need to coordinate sizes and eviction between data and metadata caches, etc); disadvantage - have to decode every time.Maybe add decoded metadata cache on top - fixed size, small and opportunistic? Or some other approach.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestIncrementalObjectSizeEstimator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Metadata.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileMetadata.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.EvictionListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetadataReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.orc.RecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.orc.Reader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.orc.OrcFile.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.orc.LLAPRecordReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.orc.LLAPReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataLoader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.orc.stream.StreamUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LlapMemoryBuffer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9730" opendate="2015-2-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>make sure logging is never called when not needed in perf-sensitive places</summary>
      <description>log4j logging has really inefficient serialization</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.LlapUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="9750" opendate="2015-2-23 00:00:00" fixdate="2015-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid log locks in operators</summary>
      <description>Basically wrap all LOG.xx calls in isLogXXXEnabled to avoid unnecessary locks on these calls.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9828" opendate="2015-3-2 00:00:00" fixdate="2015-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semantic analyzer does not capture view parent entity for tables referred in view with union all</summary>
      <description>Hive compiler adds tables used in a view definition in the input entity list, with the view as parent entity for the table.In case of a view with union all query, this is not being done property. For example,create view view1 as select t.id from (select tab1.id from db.tab1 union all select tab2.id from db.tab2 ) t;This query will capture tab1 and tab2 as read entity without view1 as parent.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9829" opendate="2015-3-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: fix unit tests</summary>
      <description>Unit tests are broken.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="9830" opendate="2015-3-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map join could dump a small table multiple times [Spark Branch]</summary>
      <description>We found auto_sortmerge_join_8 is flaky is flaky for Spark. Sometimes, the output could be wrong.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="9831" opendate="2015-3-2 00:00:00" fixdate="2015-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 should use ConcurrentHashMap in ThreadFactory</summary>
      <description></description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
    </fixedFiles>
  </bug>
  <bug id="9836" opendate="2015-3-3 00:00:00" fixdate="2015-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: fails when virtual columns are present in the join conditions (for e.g. partition columns)</summary>
      <description>explainselect a.key, a.value, b.valuefrom tab a join tab_part b on a.key = b.key and a.ds = b.ds;fails.</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug id="9839" opendate="2015-3-3 00:00:00" fixdate="2015-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks OperationHandle on async queries which fail at compile phase</summary>
      <description>Using beeline to connect to HiveServer2.And type the following:drop table if exists table_not_exists;select * from table_not_exists;There will be an OperationHandle object staying in HiveServer2's memory for ever even after quit from beeline .</description>
      <version>0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="9867" opendate="2015-3-5 00:00:00" fixdate="2015-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate usage of deprecated Calcite methods</summary>
      <description>use recommended methods instead.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.cbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="9870" opendate="2015-3-5 00:00:00" fixdate="2015-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add JvmPauseMonitor threads to HMS and HS2 daemons</summary>
      <description>The hadoop-common carries in it a nifty thread that prints GC or non-GC pauses within the JVM if it exceeds a specific threshold.This has been immeasurably useful in supporting several clusters, in identifying GC or other form of process pauses to be the root cause of some event being investigated.The HMS and HS2 daemons are good targets for running similar threads within it. It can be loaded in an if-available style.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="9871" opendate="2015-3-5 00:00:00" fixdate="2015-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print spark job id in history file [spark branch]</summary>
      <description>Maintain the spark job id in history file for the corresponding queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="9886" opendate="2015-3-6 00:00:00" fixdate="2015-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: NPE when converting join to SMB in sub-query</summary>
      <description>set hive.auto.convert.sortmerge.join = true;create table t1(id string,od string);create table t2(id string,od string);select vt1.id from(select rt1.id from(select t1.id, row_number() over (partition by id order by od desc) as row_no from t1) rt1where rt1.row_no=1) vt1join(select rt2.id from(select t2.id, row_number() over (partition by id order by od desc) as row_no from t2) rt2where rt2.row_no=1) vt2where vt1.id=vt2.id;throws NPE: at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.getValueObjectInspectors(AbstractMapJoinOperator.java:96) at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:167) at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:310) at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:72) at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:89) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425) at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:66) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425) at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:410) at org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:89) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425) at org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116) ... 14 more</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.0,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="9977" opendate="2015-3-16 00:00:00" fixdate="2015-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compactor not running on partitions after dynamic partitioned insert</summary>
      <description>When an insert, update, or delete is done using dynamic partitioning the lock is obtained on the table instead of on the individual partitions, since the partitions are not known at lock acquisition time. The compactor is using the locks to determine which partitions to check to see if they need compacted. Since the individual partitions aren't locked they aren't checked.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
