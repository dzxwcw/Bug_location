<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11417" opendate="2015-7-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create shims for the row by row read path that is backed by VectorizedRowBatch</summary>
      <description>I'd like to make the default path for reading and writing ORC files to be vectorized. To ensure that Hive can still read row by row, we'll need shims to support the old API.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestStringDictionary.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.ZeroCopyShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestTimestampWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.json</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestUnrolledBitPack.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestTypeDescription.java</file>
      <file type="M">bin.ext.orcfiledump.cmd</file>
      <file type="M">bin.ext.orcfiledump.sh</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">orc.pom.xml</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShims.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShimsCurrent.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.HadoopShims.2.2.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.IntegerReader.java</file>
      <file type="M">orc.src.java.org.apache.orc.OrcFile.java</file>
      <file type="M">orc.src.java.org.apache.orc.Reader.java</file>
      <file type="M">orc.src.java.org.apache.orc.TypeDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastMillisecondsLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.FileFormatException.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ConvertTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.JsonFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SchemaEvolution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestTimestampWritableAndColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestFileDump.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcTimezone1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcTimezone2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRLEv2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestStreamName.java</file>
    </fixedFiles>
  </bug>
  <bug id="12897" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading</summary>
      <description>There are many redundant calls to metastore which is not needed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dynamic.partitions.with.whitelist.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1511" opendate="2010-8-4 00:00:00" fixdate="2010-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive plan serialization is slow</summary>
      <description>As reported by Edward Capriolo:For reference I did this as a test case....SELECT * FROM src wherekey=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0OR key=0 OR key=0 OR key=0 ORkey=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0 OR key=0OR key=0 OR key=0 OR key=0 OR...(100 more of these)No OOM but I gave up after the test case did not go anywhere for about2 minutes.</description>
      <version>0.7.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.java</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.RowSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20314" opendate="2018-8-4 00:00:00" fixdate="2018-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include partition pruning in materialized view rewriting</summary>
      <description>To be able to reduce the cost of the expression using the materialized view when some of its partitions are pruned.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePartitionPruneRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="23020" opendate="2020-3-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using _files for replication data copy during incremental run</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestMetaStoreEventListenerInRepl.java</file>
    </fixedFiles>
  </bug>
  <bug id="23023" opendate="2020-3-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR compaction ignores column schema evolution</summary>
      <description>Repro:create table compaction_error(i int) partitioned by (`part1` string) stored as orc TBLPROPERTIES ('transactional'='true');insert into table compaction_error values (1, 'aa');ALTER TABLE compaction_error ADD COLUMNS (newcol string);insert into table compaction_error values (2, 2000, 'aa');alter table compaction_error partition (part1='aa') compact 'minor'; --or majordata row will look like:1, NULL, 'aa'2, NULL, 'aa'</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
    </fixedFiles>
  </bug>
  <bug id="2519" opendate="2011-10-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema</summary>
      <description>Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:insert overwrite ... partition (p2="...", p1);which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="313" opendate="2009-2-27 00:00:00" fixdate="2009-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add UDF date_add, date_sub, datediff</summary>
      <description>See http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-addSee http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-subSee http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_datediff</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3405" opendate="2012-8-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="3527" opendate="2012-10-3 00:00:00" fixdate="2012-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow CREATE TABLE LIKE command to take TBLPROPERTIES</summary>
      <description>CREATE TABLE ... LIKE ... commands currently don't take TBLPROPERTIES. I think it would be a useful feature.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3528" opendate="2012-10-3 00:00:00" fixdate="2012-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro SerDe doesn&amp;#39;t handle serializing Nullable types that require access to a Schema</summary>
      <description>Deserialization properly handles hiding Nullable Avro types, including complex types like record, map, array, etc. However, when Serialization attempts to write out these types it erroneously makes use of the UNION schema that contains NULL and the other type.This results in Schema mis-match errors for Record, Array, Enum, Fixed, and Bytes.Here's a review board of unit tests that express the problem, as well as one that supports the case that it's only when the schema is needed.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3631" opendate="2012-10-29 00:00:00" fixdate="2012-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>script_pipe.q fails when using JDK7</summary>
      <description>Hive Runtime Error while closing operators: Hit error while closing ..The MR job fails on this test. Unfortunately, the exception is not all that helpful.I tracked this down to a class which attempts to close a stream that is already closed. Broken pipe exceptions are caught and not propagated further, but stream closed exception are not caught.</description>
      <version>0.9.1,0.10.0,0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3632" opendate="2012-10-29 00:00:00" fixdate="2012-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade datanucleus to support JDK7</summary>
      <description>I found serious problems with datanucleus code when using JDK7, resulting in some sort of exception being thrown when datanucleus code is entered.I tried source=1.7, target=1.7 with JDK7 as well as source=1.6, target=1.6 with JDK7 and there was no visible difference in that the same unit tests failed.I tried upgrading datanucleus to 3.0.1, as per HIVE-2084.patch, which did not fix the failing tests.I tried upgrading datanucleus to 3.1-release, as per the advise of http://www.datanucleus.org/servlet/jira/browse/NUCENHANCER-86, which suggests using ASMv4 will allow datanucleus to work with JDK7. I was not successful with this either.I tried upgrading datanucleus to 3.1.2. I was not successful with this either.Regarding datanucleus support for JDK7+, there is the following JIRAhttp://www.datanucleus.org/servlet/jira/browse/NUCENHANCER-81which suggests that they don't plan to actively support JDK7+ bytecode any time soon.I also tested the following JVM parameters found onhttp://veerasundar.com/blog/2012/01/java-lang-verifyerror-expecting-a-stackmap-frame-at-branch-target-jdk-7/with no success either.This will become a more serious problem as people move to newer JVMs. If there are other who have solved this issue, please post how this was done. Otherwise, it is a topic that I would like to raise for discussion.Test Properties:CLEAR LIBRARY CACHE</description>
      <version>0.9.1,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3633" opendate="2012-10-29 00:00:00" fixdate="2012-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sort-merge join does not work with sub-queries</summary>
      <description>Consider the following query:create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 6 BUCKETS STORED AS TEXTFILE;create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 6 BUCKETS STORED AS TEXTFILE;&amp;#8211; load the above tablesset hive.optimize.bucketmapjoin = true;set hive.optimize.bucketmapjoin.sortedmerge = true;set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;explainselect count from(select /+mapjoin(a)/ a.key as key1, b.key as key2, a.value as value1, b.value as value2from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key)subq;The above query does not use sort-merge join. This would be very useful as we automatically convert the queries to use sorting and bucketing properties for join.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SMBJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3721" opendate="2012-11-19 00:00:00" fixdate="2012-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER TABLE ADD PARTS should check for valid partition spec and throw a SemanticException if part spec is not valid</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3750" opendate="2012-11-28 00:00:00" fixdate="2012-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBCStatsPublisher fails when ID length exceeds length of ID column</summary>
      <description>When the length of the ID field passed to JDBCStatsPublisher exceeds the length of the column in the table (currently 255 characters) stats collection fails. This causes the entire query to fail when hive.stats.reliable is set to true.One way to prevent this would be to calculate a deterministic, very low collision hash of the ID prefix used for aggregation and use that when the length of the ID is too long.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3764" opendate="2012-12-3 00:00:00" fixdate="2012-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support metastore version consistency check</summary>
      <description>Today there's no version/compatibility information stored in hive metastore. Also the datanucleus configuration property to automatically create missing tables is enabled by default. If you happen to start an older or newer hive or don't run the correct upgrade scripts during migration, the metastore would end up corrupted. The autoCreate schema is not always sufficient to upgrade metastore when migrating to newer release. It's not supported with all databases. Besides the migration often involves altering existing table, changing or moving data etc.Hence it's very useful to have some consistency check to make sure that hive is using correct metastore and for production systems the schema is not automatically by running hive.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.11.0-to-0.12.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.12.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.11.0-to-0.12.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.12.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.11.0-to-0.12.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.12.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.11.0-to-0.12.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.10.0-to-0.11.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.12.0.derby.sql</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3766" opendate="2012-12-3 00:00:00" fixdate="2012-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable adding hooks to hive meta store init</summary>
      <description>We will enable hooks to be added to init HMSHandler</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3771" opendate="2012-12-4 00:00:00" fixdate="2012-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-3750 broke TestParse</summary>
      <description>see title</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3783" opendate="2012-12-8 00:00:00" fixdate="2012-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats19.q is failing on trunk</summary>
      <description>This test-case was introduced in HIVE-3750 and is failing since as soon as it was introduced.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.KeyVerifyingStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3806" opendate="2012-12-15 00:00:00" fixdate="2012-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ptest failing due to "Argument list too long" errors</summary>
      <description>ptest creates a really huge shell command to delete from each test host those .q files that it should not be running. For TestCliDriver, the command has become long enough that it is over the threshold allowed by the shell. We should rewrite it so that the same semantics is captured in a shorter command.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="3846" opendate="2012-12-29 00:00:00" fixdate="2012-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter view rename NPEs with authorization on.</summary>
      <description>Click to add description</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3852" opendate="2013-1-3 00:00:00" fixdate="2013-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-groupby optimization fails when same distinct column is used twice or more</summary>
      <description>FROM INPUTINSERT OVERWRITE TABLE dest1 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.keyINSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key;fails with exception FAILED: IndexOutOfBoundsException Index: 0,Size: 0</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby10.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3888" opendate="2013-1-11 00:00:00" fixdate="2013-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong mapside groupby if no partition is being selected</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="389" opendate="2009-4-6 00:00:00" fixdate="2009-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow option to build without invoking ivy</summary>
      <description>have had trouble getting ivy and hadoop tar downloads working in coLinux (because the checksums didn't match up for some reason). also disconnected operations are a pain in general. need ability to compile against custom hadoop tree. the easiest way to do this is to point to an existing hadoop tree and skip the whole ivy thing.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3891" opendate="2013-1-14 00:00:00" fixdate="2013-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>physical optimizer changes for auto sort-merge join</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3903" opendate="2013-1-15 00:00:00" fixdate="2013-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow updating bucketing/sorting metadata of a partition through the CLI</summary>
      <description>Right now users can update the bucketing/sorting metadata of a table through the CLI, but not a partition. Use case:Need to merge a partition's files, but it's bucketed/sorted, so want to mark the partition as unbucketed/unsorted.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3915" opendate="2013-1-18 00:00:00" fixdate="2013-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Union with map-only query on one side and two MR job query on the other produces wrong results</summary>
      <description>When a query contains a union with a map only subquery on one side and a subquery involving two sequential map reduce jobs on the other, it can produce wrong results. It appears that if the map only queries table scan operator is processed first the task involving a union is made a root task. Then when the other subquery is processed, the second map reduce job gains the task involving the union as a child and it is made a root task. This means that both the first and second map reduce jobs are root tasks, so the dependency between the two is ignored. If they are run in parallel (i.e. the cluster has more than one node) no results will be produced for the side of the union with the two map reduce jobs and only the results of the other side of the union will be returned.The order TableScan operators are processed is crucial to reproducing this bug, and it is determined by the order values are retrieved from a map, and hence hard to predict, so it doesn't always reproduce.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3926" opendate="2013-1-22 00:00:00" fixdate="2013-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD on virtual column of partitioned table is not working</summary>
      <description>select * from src where BLOCK__OFFSET__INSIDE__FILE&lt;100;is working, butselect * from srcpart where BLOCK__OFFSET__INSIDE__FILE&lt;100;throws SemanticException. Disabling PPD makes it work.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3953" opendate="2013-1-28 00:00:00" fixdate="2013-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reading of partitioned Avro data fails because of missing properties</summary>
      <description>After HIVE-3833, reading partitioned Avro data fails due to missing properties. The "avro.schema.(url|literal)" properties are not making it all the way to the SerDe. Non-partitioned data can still be read.</description>
      <version>0.11.0,0.12.0,0.11.1</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3969" opendate="2013-2-1 00:00:00" fixdate="2013-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Session state for hive server should be cleaned-up</summary>
      <description>Currently "add jar" command by clients are adding child ClassLoader to worker thread cumulatively, causing various problems.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNumeric.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="3985" opendate="2013-2-5 00:00:00" fixdate="2013-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update new UDAFs introduced for Windowing to work with new Decimal Type</summary>
      <description></description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.java</file>
    </fixedFiles>
  </bug>
  <bug id="3996" opendate="2013-2-7 00:00:00" fixdate="2013-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correctly enforce the memory limit on the multi-table map-join</summary>
      <description>Currently with HIVE-3784, the joins are converted to map-joins based on checks of the table size against the config variable: hive.auto.convert.join.noconditionaltask.size. However, the current implementation will also merge multiple mapjoin operators into a single task regardless of whether the sum of the table sizes will exceed the configured value.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4005" opendate="2013-2-8 00:00:00" fixdate="2013-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column truncation</summary>
      <description>Column truncation allows users to remove data for columns that are no longer useful.This is done by removing the data for the column and setting the length of the column data and related lengths to 0 in the RC file header.RC file was fixed to recognize columns with lengths of zero to be empty and are treated as if the column doesn't exist in the data, a null is returned for every value of that column in every row. This is the same thing that happens when more columns are selected than exist in the file.A new command was added to the CLITRUNCATE TABLE ... PARTITION ... COLUMNS ...This launches a map only job where each mapper rewrites a single file without the unnecessary column data and the adjusted headers. It does not uncompress/deserialize the data so it is much faster than rewriting the data with NULLs.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4015" opendate="2013-2-13 00:00:00" fixdate="2013-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ORC file to the grammar as a file format</summary>
      <description>It would be much more convenient for users if we enable them to use ORC as a file format in the HQL grammar.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4019" opendate="2013-2-13 00:00:00" fixdate="2013-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to create and drop temporary partition function</summary>
      <description>Just like udf/udaf/udtf functions, user should be able to add and drop custom partitioning functions.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="402" opendate="2009-4-9 00:00:00" fixdate="2009-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create regexp_extract udf</summary>
      <description>This will allow users to extract substrings from a string based on a regular expression.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4021" opendate="2013-2-14 00:00:00" fixdate="2013-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL upgrade scripts are creating column with incorrect name</summary>
      <description>I've noticed that PostgreSQL upgrade scripts are creating table PART_COL_STATS and TAB_COL_STATS with column DOUBLE_HIGH_VALUES, however hive (and all other scripts) are expecting column name DOUBLE_HIGH_VALUE (without the "S" at the end).</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.012-HIVE-1362.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="4025" opendate="2013-2-15 00:00:00" fixdate="2013-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add reflect UDF for member method invocation of column</summary>
      <description>There are many useful non-static methods on type of primitive types. But current reflect UDF cannot invoke those. For example,select reflect2(value, "replace", "val", "VALUE") from src;which replaces 'val' part of value column with 'VALUE'</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="4027" opendate="2013-2-16 00:00:00" fixdate="2013-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift alter_table api doesnt validate column type</summary>
      <description>Thrift alter_table api doesnt validate column type so that invalid column type can sneak it.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4067" opendate="2013-2-22 00:00:00" fixdate="2013-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup to HIVE-701: reduce ambiguity in grammar</summary>
      <description>After HIVE-701 the grammar has become much more ambiguous, and the compilation generates a large number of warnings. Making FROM, DISTINCT, PRESERVE, COLUMN, ALL, AND, OR, and NOT reserved keywords again reduces the number of warnings to 134, up from the original 81 warnings but down from the 565 after HIVE-701. Most of the remaining ambiguity is trivial, an example being "KW_ELEM_TYPE | KW_KEY_TYPE | KW_VALUE_TYPE | identifier", and they are all correctly handled by ANTLR.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.nonreserved.keywords.insert.into1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="4068" opendate="2013-2-24 00:00:00" fixdate="2013-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Size of aggregation buffer which uses non-primitive type is not estimated correctly</summary>
      <description>Currently, hive assumes an aggregation buffer which holds a map is occupying just 256 byte (fixed). If it's bigger than that in real, OutOfMemoryError can be thrown (especially for &gt;1k buffer). workaround : set hive.map.aggr.hash.percentmemory=&lt;smaller value than default(0.5)&gt;</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4072" opendate="2013-2-25 00:00:00" fixdate="2013-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive eclipse build path update for string template jar</summary>
      <description>StringTemplate jar version needs to be updated for hive to work with eclipse without user intervention.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4078" opendate="2013-2-27 00:00:00" fixdate="2013-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delay the serialize-deserialize pair in CommonJoinTaskDispatcher</summary>
      <description>CommonJoinProcessor tries to clone a MapredWork while attempting a conversion to a map-join // deep copy a new mapred work from xml InputStream in = new ByteArrayInputStream(xml.getBytes("UTF-8")); MapredWork newWork = Utilities.deserializeMapRedWork(in, physicalContext.getConf());which is a very heavy operation memory wise &amp; cpu-wise.It would be better to do this only if a conditional task is required, resulting in a copy of the task.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4090" opendate="2013-2-27 00:00:00" fixdate="2013-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use of hive.exec.script.allow.partial.consumption can produce partial results</summary>
      <description>When users execute use a transform script with the config hive.exec.script.allow.partial.consumption set to true, it may produce partial results.When this config is set the script may close it's input pipe before its parent operator has finished passing it rows. In the catch block for this exception, the setDone method is called marking the operator as done. However, there's a separate thread running to process rows passed from the script back to Hive via stdout. If this thread is not done processing rows, any rows it forwards after the setDone method is called will not be passed to its children. This leads to partial results.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4092" opendate="2013-2-28 00:00:00" fixdate="2013-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store complete names of tables in column access analyzer</summary>
      <description>Right now the db name is not being stored. We should store the complete name, which includes the db name, as the table access analyzer does.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4096" opendate="2013-3-1 00:00:00" fixdate="2013-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>problem in hive.map.groupby.sorted with distincts</summary>
      <description>set hive.enforce.bucketing = true;set hive.enforce.sorting = true;set hive.exec.reducers.max = 10;set hive.map.groupby.sorted=true;CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='1');&amp;#8211; perform an insert to make sure there are 2 filesINSERT OVERWRITE TABLE T1 PARTITION (ds='1') select key, val from T1 where ds = '1';CREATE TABLE outputTbl1(cnt INT);&amp;#8211; The plan should be converted to a map-side group by, since the&amp;#8211; sorting columns and grouping columns match, and all the bucketing columns&amp;#8211; are part of sorting columnsEXPLAINselect count(distinct key) from T1;select count(distinct key) from T1;explainINSERT OVERWRITE TABLE outputTbl1select count(distinct key) from T1;INSERT OVERWRITE TABLE outputTbl1select count(distinct key) from T1;SELECT * FROM outputTbl1;DROP TABLE T1;The above query gives wrong results</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4097" opendate="2013-3-1 00:00:00" fixdate="2013-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC file doesn&amp;#39;t properly interpret empty hive.io.file.readcolumn.ids</summary>
      <description>Hive assumes that an empty string in hive.io.file.readcolumn.ids means all columns. The ORC reader currently assumes it means no columns.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="413" opendate="2009-4-14 00:00:00" fixdate="2009-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>multi-table insert</summary>
      <description>some problem in multi-table insert if both of them contain grouping keys which are different.have not marked it a blocker, since a workaround exists (issue both inserts separately) - but this if the release is not yet done, we should fix this also.FROM SRCINSERT OVERWRITE TABLE DEST1 SELECT SRC.key, src.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key\, src.valueINSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4130" opendate="2013-3-6 00:00:00" fixdate="2013-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bring the Lead/Lag UDFs interface in line with Lead/Lag UDAFs</summary>
      <description>support a default value arg both amt and defaultValue args can be optional</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
    </fixedFiles>
  </bug>
  <bug id="4155" opendate="2013-3-12 00:00:00" fixdate="2013-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose ORC&amp;#39;s FileDump as a service</summary>
      <description>Expose ORC's FileDump class as a service similar to RC File Cate.g.hive --orcfiledump &lt;path_to_file&gt;Should run FileDump on the file.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="4156" opendate="2013-3-12 00:00:00" fixdate="2013-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>need to add protobuf classes to hive-exec.jar</summary>
      <description>In some queries, the tasks fail when they can't find classes from the protobuf library.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4167" opendate="2013-3-14 00:00:00" fixdate="2013-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive converts bucket map join to SMB join even when tables are not sorted</summary>
      <description>If tables are just bucketed but not sorted, we are generating smb join operator. This results in loss of rows in queries.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
    </fixedFiles>
  </bug>
  <bug id="4176" opendate="2013-3-14 00:00:00" fixdate="2013-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable TestBeeLineDriver in ptest util</summary>
      <description>The test is disabled for ant test, so it should be disabled for ptest as well.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="4178" opendate="2013-3-14 00:00:00" fixdate="2013-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC fails with files with different numbers of columns</summary>
      <description>When CombineHiveInputFormat is used, it's possible that two files with different numbers of files can be included in the same split, in which case Hive will fail at one of several points with an ArrayIndexOutOfBoundsException.This can happen when a partition contains empty files or two partitions are read with different numbers of columns.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
    </fixedFiles>
  </bug>
  <bug id="4189" opendate="2013-3-15 00:00:00" fixdate="2013-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC fails with String column that ends in lots of nulls</summary>
      <description>When ORC attempts to write out a string column that ends in enough nulls to span an index stride, StringTreeWriter's writeStripe method will get an exception from TreeWriter's writeStripe methodColumn has wrong number of index entries found: x expected: yThis is caused by rowIndexValueCount having multiple entries equal to the number of non-null rows in the column, combined with the fact that StringTreeWriter has special logic for constructing its index.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4206" opendate="2013-3-20 00:00:00" fixdate="2013-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort merge join does not work for outer joins for 7 inputs</summary>
      <description></description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4212" opendate="2013-3-21 00:00:00" fixdate="2013-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sort merge join should work for outer joins for more than 8 inputs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="4219" opendate="2013-3-22 00:00:00" fixdate="2013-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>explain dependency does not capture the input table</summary>
      <description>hive&gt; explain dependency select * from srcpart where ds is not null;OK{"input_partitions":[{"partitionName":"default@srcpart@ds=2008-04-08/hr=11"},{"partitionName":"default@srcpart@ds=2008-04-08/hr=12"},{"partitionName":"default@srcpart@ds=2008-04-09/hr=11"},{"partitionName":"default@srcpart@ds=2008-04-09/hr=12"}],"input_tables":[]}input_tables should contain srcpart</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.offline.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.protect.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.16.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.17.part.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.23.import.part.authsuccess.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insertexternal1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="422" opendate="2009-4-15 00:00:00" fixdate="2009-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logo for Hive</summary>
      <description>Greetings fine Hadoop peoples,While working on a few projects here at Cloudera we found ourselves wanting for some sort of icon for both the JobTracker and for Hive. After checking on the project page for Hive (the JobTracker doesn't really have one) and finding that these items have no icons, we rolled up our sleeves and made some. We'd like to contribute these to the project, so if you want 'em, they're all yours.I opened a ticket here for both Hive and JobTracker logos: https://issues.apache.org/jira/browse/HADOOP-5683But Ashish Thusoo suggested I open a ticket here for the hive logo specifically. See attached.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.images.hive-logo.jpg</file>
    </fixedFiles>
  </bug>
  <bug id="423" opendate="2009-4-16 00:00:00" fixdate="2009-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change launch templates to use hive_model.jar</summary>
      <description>the model-jar target now builds hive_model.jar instead of metastore_model.jar. This causes the launch files (used to run tests) to not work anymore in eclipse.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">eclipse-templates.TestTruncate.launchtemplate</file>
      <file type="M">eclipse-templates.TestMTQueries.launchtemplate</file>
      <file type="M">eclipse-templates.TestJdbc.launchtemplate</file>
      <file type="M">eclipse-templates.TestHive.launchtemplate</file>
      <file type="M">eclipse-templates.TestCliDriver.launchtemplate</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4235" opendate="2013-3-26 00:00:00" fixdate="2013-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE IF NOT EXISTS uses inefficient way to check if table exists</summary>
      <description>CREATE TABLE IF NOT EXISTS uses inefficient way to check if table exists.It uses Hive.java's getTablesByPattern(...) to check if table exists. It involves regular expression and eventually database join. Very efficient. It can cause database lock time increase and hurt db performance if a lot of such commands hit database.The suggested approach is to use getTable(...) since we know tablename already</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4236" opendate="2013-3-27 00:00:00" fixdate="2013-1-27 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>JDBC2 HivePreparedStatement does not release resources</summary>
      <description>HivePreparedStatement does not close the associated server-side operation when close() is called. Nor does it call close() on the ResultSet. When execute() is called the current ResultSet is not closed first it is just set to null.Similarly, HiveStatement's close() does not call close() on the ResultSet, it just sets it to null.</description>
      <version>0.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.historical.trunk.RELEASE.NOTES.txt</file>
      <file type="M">hcatalog.historical.trunk.README.txt</file>
      <file type="M">hcatalog.historical.trunk.NOTICE.txt</file>
      <file type="M">hcatalog.historical.trunk.license.thrift-LICENSE.txt</file>
      <file type="M">hcatalog.historical.trunk.license.pig-LICENSE.txt</file>
      <file type="M">hcatalog.historical.trunk.license.hive-LICENSE.txt</file>
      <file type="M">hcatalog.historical.trunk.license.hadoop-LICENSE.txt</file>
      <file type="M">hcatalog.historical.trunk.LICENSE.txt</file>
      <file type="M">hcatalog.historical.trunk.KEYS</file>
      <file type="M">hcatalog.historical.trunk.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4251" opendate="2013-3-29 00:00:00" fixdate="2013-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indices can&amp;#39;t be built on tables whose schema info comes from SerDe</summary>
      <description>Building indices on tables who get the schema information from the deserializer (e.g. Avro backed tables) doesn't work because when the column is checked to exist, the correct API isn't used.hive&gt; describe doctors; OK# col_name data_type comment number int from deserializer first_name string from deserializer last_name string from deserializer Time taken: 0.215 seconds, Fetched: 5 row(s)hive&gt; create index doctors_index on table doctors(number) as 'compact' with deferred rebuild; FAILED: Error in metadata: java.lang.RuntimeException: Check the index columns, they should appear in the table being indexed.FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask</description>
      <version>0.10.0,0.11.0,0.10.1</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="4252" opendate="2013-3-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hiveserver2 string representation of complex types are inconsistent with cli</summary>
      <description>For example, it prints struct as "&amp;#91;null, null, null&amp;#93;" instead of "{\"r\":null,\"s\":null,\"t\":null}"And for maps it is printing it as "{k=v}" instead of {\"k\":\"v\"}</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="4257" opendate="2013-3-29 00:00:00" fixdate="2013-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>java.sql.SQLNonTransientConnectionException on JDBCStatsAggregator</summary>
      <description>java.sql.SQLNonTransientConnectionException occurs on JDBCStatsAggregator after executing dozens of Hive queries periodically, which inserts thousands of rows. It may have a relation with DERBY-5098. To avoid this error, Hive should use a more recent version of Derby(10.6.2.3, 10.7.1.4, 10.8.2.2, 10.9.1.0 or later). Hive 0.11.0-SNAPSHOT uses Derby 10.4.2.0.2013-03-24 15:54:30,487 ERROR jdbc.JDBCStatsAggregator (JDBCStatsAggregator.java:aggregateStats(168)) - Error during publishing aggregation. java.sql.SQLNonTransientConnectionException: No current connection.2013-03-24 15:54:30,487 ERROR jdbc.JDBCStatsAggregator (JDBCStatsAggregator.java:aggregateStats(168)) - Error during publishing aggregation. java.sql.SQLNonTransientConnectionException: No current connection.2013-03-24 15:54:30,487 ERROR jdbc.JDBCStatsAggregator (JDBCStatsAggregator.java:cleanUp(249)) - Error during publishing aggregation. java.sql.SQLNonTransientConnectionException: No current connection.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4258" opendate="2013-3-29 00:00:00" fixdate="2013-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log logical plan tree for debugging</summary>
      <description>Debugging or implementing optimizer, knowing the shape of logical plan helps general progress of it.For example,select count(val) from (select a.key as key, b.value as array_val from src a join array_valued_src b on a.key=b.key) i lateral view explode (array_val) c as val TS[1]-RS[2]-JOIN[4]-SEL[5]-LVF[6]-SEL[7]-LVJ[10]-SEL[11]-GBY[12]-RS[13]-GBY[14]-SEL[15]-FS[16] -SEL[8]-UDTF[9]-LVJ[10]TS[0]-RS[3]-JOIN[4]</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4261" opendate="2013-3-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>union_remove_10 is failing on hadoop2 with assertion (root task with non-empty set of parents)</summary>
      <description>Output seems to indicate that the stage plan is broken.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="4262" opendate="2013-3-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix last_value UDAF behavior</summary>
      <description>When an Order Spec is present in an Over clauselast_value should return the value from the last row that matches the current row's Sort Key.See egs here http://msdn.microsoft.com/en-us/library/hh231517.aspx</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="4270" opendate="2013-4-1 00:00:00" fixdate="2013-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bug in hive.map.groupby.sorted in the presence of multiple input partitions</summary>
      <description>This can lead to wrong results.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4271" opendate="2013-4-1 00:00:00" fixdate="2013-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit precision of decimal type</summary>
      <description>The current decimal implementation does not limit the precision of the numbers. This has a number of drawbacks. A maximum precision would allow us to: Have SerDes/filformats store decimals more efficiently Speed up processing by implementing operations w/o generating java BigDecimals Simplify extending the datatype to allow for decimal(p) and decimal(p,s) Write a more efficient BinarySortable SerDe for sorting/grouping/joiningExact numeric datatype are typically used to represent money, so if the limit is high enough it doesn't really become an issue.A typical representation would pack 9 decimal digits in 4 bytes. So, with 2 longs we can represent 36 digits - which is what I propose as the limit.Final thought: It's easier to restrict this now and have the option to do the things above than to try to do so once people start using the datatype.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.TestStatsSerde.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.MyTestClassSmaller.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazybinary.MyTestClassBigger.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBigDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBigDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBigDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseNumericOp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseNumericUnaryOp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCeil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFloor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPPositive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPosMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRound.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToString.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.BigDecimalWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBigDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBigDecimal.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBigDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBigDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="4272" opendate="2013-4-1 00:00:00" fixdate="2013-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition wise metadata does not work for text files</summary>
      <description>The following test fails:set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;&amp;#8211; This tests that the schema can be changed for binary serde datacreate table partition_test_partitioned(key string, value string)partitioned by (dt string) stored as textfile;insert overwrite table partition_test_partitioned partition(dt='1')select * from src where key = 238;select * from partition_test_partitioned where dt is not null;select key+key, value from partition_test_partitioned where dt is not null;alter table partition_test_partitioned change key key int;select key+key, value from partition_test_partitioned where dt is not null;select * from partition_test_partitioned where dt is not null;It works fine for a RCFile</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4275" opendate="2013-4-2 00:00:00" fixdate="2013-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive does not differentiate scheme and authority in file uris</summary>
      <description>Consider the following set of queries:ALTER TABLE abc ADD PARTITION (x='0') LOCATION 'file:///foo';ALTER TABLE abc ADD PARTITION (x='1') LOCATION '/foo';select count from abc;Even though there are different files under these directories, depending on number of mappers, the count produces a value = num of mappers * num of files in the 2 directories. This is incorrect.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4281" opendate="2013-4-2 00:00:00" fixdate="2013-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hive.map.groupby.sorted.testmode</summary>
      <description>The idea behind this would be to test hive.map.groupby.sorted.Since this is a new feature, it might be a good idea to run it in test mode,where a query property would denote that this query plan would have changed.If a customer wants, they can run those queries offline, compare the resultsfor correctness, and set hive.map.groupby.sorted only if all the results arethe same.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4283" opendate="2013-4-2 00:00:00" fixdate="2013-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized logical expressions.</summary>
      <description>Implement logical expressions that operate on column vectors.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="4289" opendate="2013-4-3 00:00:00" fixdate="2013-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog build fails when behind a firewall</summary>
      <description>A bug in Maven makes it impossible to set a proxy for a Maven Ant POM task (see https://jira.codehaus.org/browse/MANTTASKS-216). Building behind a firewall results in the following error:&amp;#91;artifact:pom&amp;#93; Downloading: org/apache/apache/11/apache-11.pom from repository central at http://repo1.maven.org/maven2&amp;#91;artifact:pom&amp;#93; Transferring 14K from central&amp;#91;artifact:pom&amp;#93; &amp;#91;WARNING&amp;#93; Unable to get resource 'org.apache:apache:pom:11' from repository central (http://repo1.maven.org/maven2): Error transferring file: No route to host&amp;#91;artifact:pom&amp;#93; An error has occurred while processing the Maven artifact tasks.&amp;#91;artifact:pom&amp;#93; Diagnosis:&amp;#91;artifact:pom&amp;#93;&amp;#91;artifact:pom&amp;#93; Unable to initialize POM pom.xml: Cannot find parent: org.apache:apache for project: org.apache.hcatalog:hcatalog:pom:0.11.0-SNAPSHOT for project org.apache.hcatalog:hcatalog:pom:0.11.0-SNAPSHOT&amp;#91;artifact:pom&amp;#93; Unable to download the artifact from any repositoryDespite the error message, Ant/Maven is actually able to retrieve the POM file by using the proxy set for Ant. However, it mysteriously fails when trying to retrieve the checksum, which causes the entire operation to fail. Regardless, a proxy should be set through Maven's settings.xml file. Since this is not possible, the only way to build HCat behind a firewall right now is to manually fetch the POM file and have Maven read it from the cache.Ideally we would fix this in Maven, but given that this issue has been reported for a long time in a number of separate places I think it is more practical to modify the HCatalog build to specify the POM as a dependency, fetching it into the cache so that the artifact:pom task can succeed.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.build-support.ant.deploy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4292" opendate="2013-4-4 00:00:00" fixdate="2013-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hiveserver2 should support -hiveconf commandline parameter</summary>
      <description>hiveserver2 command show allow configuration to be customized using -hiveconf command line argument. This is very useful, for example this will make it easy to run hiveserver2 in local mode.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="4294" opendate="2013-4-4 00:00:00" fixdate="2013-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Single sourced multi query cannot handle lateral view</summary>
      <description>For example,hive&gt; explain from src &gt; select key, C lateral view explode(array(key, value)) A as C;FAILED: ParseException line 3:22 missing EOF at 'view' near 'lateral'</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4308" opendate="2013-4-7 00:00:00" fixdate="2013-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Newly added test TestCliDriver.hiveprofiler_union0 is failing on trunk</summary>
      <description>This only happens while running whole test suite. Failure doesn't manifest if this test is run alone.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.profiler.HiveProfilerStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="432" opendate="2009-4-19 00:00:00" fixdate="2009-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SORT BY is always sending data to only the first reducer even if there are multiple reducers</summary>
      <description>When we generate the ReduceSInkOperator, the partition columns are empty, which means all the rows will get a hash value of 0, and they will all go to the first reducer.In the meanwhile we are fixing this bug, please use "CLUSTER BY" instead of "SORT BY" so that the data will get distributed to multiple reducers.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4320" opendate="2013-4-9 00:00:00" fixdate="2013-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consider extending max limit for precision to 38</summary>
      <description>Max precision of 38 still fits in 128. It changes the way you do math on these numbers though. Need to see if there will be perf implications, but there's a strong case to support 38 (instead of 36) to comply with other DBs. (Oracle, SQL Server, Teradata).</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">data.files.kv8.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="4324" opendate="2013-4-10 00:00:00" fixdate="2013-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC Turn off dictionary encoding when number of distinct keys is greater than threshold</summary>
      <description>Add a configurable threshold so that if the number of distinct values in a string column is greater than that fraction of non-null values, dictionary encoding is turned off.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestFileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OutStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4328" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default bigtable selection policy for sort-merge joins</summary>
      <description>The following parameterhive.auto.convert.sortmerge.join.bigtable.selection.policydictates it.It should not be based on the position of the table, but it should be basedon the size of the table instead.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4334" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ctas test on hadoop 2 has outdated golden file</summary>
      <description>golden file was not updated after last change to test file</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4336" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Selecting from a view, and another view that also selects from that view fails</summary>
      <description>E.g. the following query fails with an NPECREATE VIEW test_view1 AS SELECT * FROM src;CREATE VIEW test_view2 AS SELECT * FROM test_view1;SELECT COUNT FROM test_view1 a JOIN test_view2 b ON a.key = b.key;</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
    </fixedFiles>
  </bug>
  <bug id="4337" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update list bucketing test results</summary>
      <description>A recent change resulted in different output for the list bucketing tests, which run for Hadoop23. The output files were not updated to reflect this.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4352" opendate="2013-4-12 00:00:00" fixdate="2013-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Guava not getting included in build package</summary>
      <description>Since HIVE-4148, Guava is not getting included in the appropriate packages. This manifests as a ClassNotFoundException.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4356" opendate="2013-4-15 00:00:00" fixdate="2013-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove duplicate impersonation parameters for hiveserver2</summary>
      <description>There are two parameters controlling impersonation in hiveserver2. hive.server2.enable.impersonation that controls this in kerberos secure mode, while hive.server2.enable.doAs controls this for unsecure mode.We should have just one for both modes.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4367" opendate="2013-4-16 00:00:00" fixdate="2013-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enhance TRUNCATE syntax to drop data of external table</summary>
      <description>In my use case ,sometimes I have to remove data of external tables to free up storage space of the cluster .So it's necessary for to enhance the syntax like "TRUNCATE TABLE srcpart_truncate PARTITION (dt='201130412') FORCE;"to remove data from EXTERNAL table.And I add a configuration property to enable remove data to Trash &lt;property&gt; &lt;name&gt;hive.truncate.skiptrash&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; if true will remove data to trash, else false drop data immediately &lt;/description&gt;&lt;/property&gt;For example :hive (default)&gt; TRUNCATE TABLE external1 partition (ds='11'); FAILED: Error in semantic analysis: Cannot truncate non-managed table external1hive (default)&gt; TRUNCATE TABLE external1 partition (ds='11') FORCE;&amp;#91;2013-04-16 17:15:52&amp;#93;: Compile Start &amp;#91;2013-04-16 17:15:52&amp;#93;: Compile End&amp;#91;2013-04-16 17:15:52&amp;#93;: OK&amp;#91;2013-04-16 17:15:52&amp;#93;: Time taken: 0.413 secondshive (default)&gt; set hive.truncate.skiptrash;hive.truncate.skiptrash=falsehive (default)&gt; set hive.truncate.skiptrash=true; hive (default)&gt; TRUNCATE TABLE external1 partition (ds='12') FORCE;&amp;#91;2013-04-16 17:16:21&amp;#93;: Compile Start &amp;#91;2013-04-16 17:16:21&amp;#93;: Compile End&amp;#91;2013-04-16 17:16:21&amp;#93;: OK&amp;#91;2013-04-16 17:16:21&amp;#93;: Time taken: 0.143 secondshive (default)&gt; dfs -ls /user/test/.Trash/Current/; Found 1 itemsdrwxr-xr-x -test supergroup 0 2013-04-16 17:06 /user/test/.Trash/Current/ds=11</description>
      <version>0.11.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="437" opendate="2009-4-21 00:00:00" fixdate="2009-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow both table.col and col.field in the grammar</summary>
      <description>There is a bug in the grammar that only allows table.col but not col.field. We should allow both.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.parse.union.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.subq.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join8.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.part1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input9.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input8.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input20.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.cast1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.queries.clientnegative.input.testxpath4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.quote1.q</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4371" opendate="2013-4-17 00:00:00" fixdate="2013-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>some issue with merging join trees</summary>
      <description>navis, I would really appreciate if you can take a look.I am attaching a testcase, for which in the optimizer the join context leftaliases and right aliases do not look correct.</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
    </fixedFiles>
  </bug>
  <bug id="4378" opendate="2013-4-18 00:00:00" fixdate="2013-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Counters hit performance even when not used</summary>
      <description>preprocess/postprocess counters perform a number of computations even when there are no counters to update. Performance runs are captured in: https://issues.apache.org/jira/browse/HIVE-4318</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4379" opendate="2013-4-18 00:00:00" fixdate="2013-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Vectorized Column-Column expressions</summary>
      <description>This covers the expressions involving two columns.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorizedRowGroupGenUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.CodeGen.java</file>
    </fixedFiles>
  </bug>
  <bug id="4380" opendate="2013-4-18 00:00:00" fixdate="2013-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement Vectorized Scalar-Column expressions</summary>
      <description>The expressions with scalar as the first operand.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.CodeGen.java</file>
    </fixedFiles>
  </bug>
  <bug id="4392" opendate="2013-4-22 00:00:00" fixdate="2013-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Illogical InvalidObjectException throwed when use mulit aggregate functions with star columns</summary>
      <description>For Example:hive (default)&gt; create table liza_1 as &gt; select *, sum(key), sum(value) &gt; from new_src;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapred.reduce.tasks=&lt;number&gt;Starting Job = job_201304191025_0003, Tracking URL = http://hd17-vm5:51030/jobdetails.jsp?jobid=job_201304191025_0003Kill Command = /home/zongren/hadoop-current/bin/../bin/hadoop job -kill job_201304191025_0003Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 12013-04-22 11:09:28,017 Stage-1 map = 0%, reduce = 0%2013-04-22 11:09:34,054 Stage-1 map = 0%, reduce = 100%2013-04-22 11:09:37,074 Stage-1 map = 100%, reduce = 100%Ended Job = job_201304191025_0003Moving data to: hdfs://hd17-vm5:9101/user/zongren/hive/liza_1FAILED: Error in metadata: InvalidObjectException(message:liza_1 is not a valid object name)FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskMapReduce Jobs Launched: Job 0: Reduce: 1 HDFS Read: 0 HDFS Write: 12 SUCCESSTotal MapReduce CPU Time Spent: 0 msechive (default)&gt; create table liza_1 as &gt; select *, sum(key), sum(value) &gt; from new_src &gt; group by key, value;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks not specified. Estimated from input data size: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapred.reduce.tasks=&lt;number&gt;Starting Job = job_201304191025_0004, Tracking URL = http://hd17-vm5:51030/jobdetails.jsp?jobid=job_201304191025_0004Kill Command = /home/zongren/hadoop-current/bin/../bin/hadoop job -kill job_201304191025_0004Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 12013-04-22 11:11:58,945 Stage-1 map = 0%, reduce = 0%2013-04-22 11:12:01,964 Stage-1 map = 0%, reduce = 100%2013-04-22 11:12:04,982 Stage-1 map = 100%, reduce = 100%Ended Job = job_201304191025_0004Moving data to: hdfs://hd17-vm5:9101/user/zongren/hive/liza_1FAILED: Error in metadata: InvalidObjectException(message:liza_1 is not a valid object name)FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskMapReduce Jobs Launched: Job 0: Reduce: 1 HDFS Read: 0 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 0 msecBut the following tow Queries work:hive (default)&gt; create table liza_1 as select * from new_src;Total MapReduce jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_201304191025_0006, Tracking URL = http://hd17-vm5:51030/jobdetails.jsp?jobid=job_201304191025_0006Kill Command = /home/zongren/hadoop-current/bin/../bin/hadoop job -kill job_201304191025_0006Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 02013-04-22 11:15:00,681 Stage-1 map = 0%, reduce = 0%2013-04-22 11:15:03,697 Stage-1 map = 100%, reduce = 100%Ended Job = job_201304191025_0006Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hd17-vm5:9101/user/zongren/hive-scratchdir/hive_2013-04-22_11-14-54_632_6709035018023861094/-ext-10001Moving data to: hdfs://hd17-vm5:9101/user/zongren/hive/liza_1Table default.liza_1 stats: &amp;#91;num_partitions: 0, num_files: 0, num_rows: 0, total_size: 0, raw_data_size: 0&amp;#93;MapReduce Jobs Launched: Job 0: HDFS Read: 0 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKTime taken: 9.576 secondshive (default)&gt; create table liza_1 as &gt; select sum (key), sum(value) &gt; from new_test;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapred.reduce.tasks=&lt;number&gt;Starting Job = job_201304191025_0008, Tracking URL = http://hd17-vm5:51030/jobdetails.jsp?jobid=job_201304191025_0008Kill Command = /home/zongren/hadoop-current/bin/../bin/hadoop job -kill job_201304191025_0008Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 12013-04-22 11:22:52,200 Stage-1 map = 0%, reduce = 0%2013-04-22 11:22:55,216 Stage-1 map = 0%, reduce = 100%2013-04-22 11:22:58,234 Stage-1 map = 100%, reduce = 100%Ended Job = job_201304191025_0008Moving data to: hdfs://hd17-vm5:9101/user/zongren/hive/liza_1Table default.liza_1 stats: &amp;#91;num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6, raw_data_size: 0&amp;#93;MapReduce Jobs Launched: Job 0: Reduce: 1 HDFS Read: 0 HDFS Write: 6 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKTime taken: 11.115 secondsIn MySQL :mysql&gt; select * from example ;----------+ id data ----------+ 1 2 2 2 3 3 ----------+3 rows in set (0.00 sec)mysql&gt; select *, sum(id),count(data) from example ;----------------------------+ id data sum(id) count(data) ----------------------------+ 1 2 6 3 ----------------------------+1 row in set (0.03 sec)</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4395" opendate="2013-4-22 00:00:00" fixdate="2013-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TFetchOrientation.FIRST for HiveServer2 FetchResults</summary>
      <description>Currently HiveServer2 only support fetching next row (TFetchOrientation.NEXT). This ticket is to implement support for TFetchOrientation.FIRST that resets the fetch position at the begining of the resultset.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DfsProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="4403" opendate="2013-4-23 00:00:00" fixdate="2013-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running Hive queries on Yarn (MR2) gives warnings related to overriding final parameters</summary>
      <description>While working on BIGTOP-885, I saw that Hive was giving a bunch of warnings related to overriding final parameters in job.conf. This was on a pseudo distributed cluster. FWIW, I didn't see this happen on a fully-distributed cluster. Perhaps, Hive's job.conf is overriding some final parameters it shouldn't.Here is what the warnings looked like:2013-04-19 14:20:32,304 WARN [main] conf.Configuration (Configuration.java:loadProperty(2032)) - file:/tmp/root/hive_2013-04-19_14-20-30_159_5701876916688815815/-local-10002/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.2013-04-19 14:20:32,367 WARN [main] conf.Configuration (Configuration.java:loadProperty(2032)) - file:/tmp/root/hive_2013-04-19_14-20-30_159_5701876916688815815/-local-10002/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.To reproduce, run a query like:CREATE TABLE u_data ( userid INT, movieid INT, rating INT, unixtime STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'STORED AS TEXTFILE;Load some data into u_data, here is some sample data:https://github.com/apache/bigtop/blob/master/bigtop-tests/test-artifacts/hive/src/main/resources/seed_data_files/ml-data/u.dataRun a simple query on that data (on YARN/MR2)INSERT OVERWRITE DIRECTORY '/tmp/count'SELECT COUNT(1) FROM u_data</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4405" opendate="2013-4-23 00:00:00" fixdate="2013-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate vectorized execution plan</summary>
      <description>The execution plan will be transformed into a vectorized plan using vectorized operators.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4406" opendate="2013-4-23 00:00:00" fixdate="2013-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing "/" or "/&lt;dbname&gt;" in hs2 jdbc uri switches mode to embedded mode</summary>
      <description>When the jdbc uri does not have a / or "/default" after the hostname:port, and the principal is specified, it ends up launching HS2 in embedded mode.This is because the parsing of uri in such case does not end up extracting the hostname.eg . "jdbc:hive2://&lt;host&gt;:&lt;port&gt;;principal=&lt;&gt;." results HS2 embedded mode getting used."jdbc:hive2://&lt;host&gt;:&lt;port&gt;/;principal=&lt;&gt;" or "jdbc:hive2://&lt;host&gt;:&lt;port&gt;/default;principal=&lt;&gt;" results in it connecting to the standalone hive server 2.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="4421" opendate="2013-4-25 00:00:00" fixdate="2013-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve memory usage by ORC dictionaries</summary>
      <description>Currently, for tables with many string columns, it is possible to significantly underestimate the memory used by the ORC dictionaries and cause the query to run out of memory in the task.</description>
      <version>None</version>
      <fixedVersion>0.11.0,0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RedBlackTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.PositionedOutputStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OutStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.MemoryManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.java</file>
    </fixedFiles>
  </bug>
  <bug id="4457" opendate="2013-4-30 00:00:00" fixdate="2013-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries not supported by vectorized code path should fall back to non vector path.</summary>
      <description>Queries not supported by vectorized code path should fall back to non vector path.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4459" opendate="2013-4-30 00:00:00" fixdate="2013-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Script hcat is overriding HIVE_CONF_DIR variable</summary>
      <description>Script hcat is currently overriding variable HIVE_CONF_DIR to ${HIVE_HOME}/conf. It would be useful to use the previous content of the variable if it was set by the user.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.bin.hcat</file>
    </fixedFiles>
  </bug>
  <bug id="446" opendate="2009-4-25 00:00:00" fixdate="2009-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement TRUNCATE</summary>
      <description>truncate the data but leave the table and metadata intact.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="4465" opendate="2013-5-1 00:00:00" fixdate="2013-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat e2e tests succeed regardless of exitvalue</summary>
      <description>Currently the webhcat tests that check job status for Pig, Hive, and MR do not check the exit value of the job. So a job can fail and the test will succeed.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="4474" opendate="2013-5-1 00:00:00" fixdate="2013-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column access not tracked properly for partitioned tables</summary>
      <description>The columns recorded as being accessed is incorrect for partitioned tables. The index of accessed columns is a position in the list of non-partition columns, but a list of all columns is being used right now to do the lookup.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.column.access.stats.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4475" opendate="2013-5-2 00:00:00" fixdate="2013-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch RCFile default to LazyBinaryColumnarSerDe</summary>
      <description>For most workloads it seems LazyBinaryColumnarSerDe (binary) will perform better than ColumnarSerDe (text). Not sure why ColumnarSerDe is the default, but my guess is, that's for historical reasons. I suggest switching the default.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.rcfile.default.format.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4477" opendate="2013-5-2 00:00:00" fixdate="2013-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove redundant copy of arithmetic filter unit test testColOpScalarNumericFilterNullAndRepeatingLogic</summary>
      <description>same test got ported to 2 different files</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4500" opendate="2013-5-5 00:00:00" fixdate="2013-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 holding too many file handles of hive_job_log_hive_*.txt files</summary>
      <description>In the hiveserver2 setup used for testing, we see that it has 2444 files open and of them 2152 are /tmp/hive/hive_job_log_hive_*.txt files</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
    </fixedFiles>
  </bug>
  <bug id="4505" opendate="2013-5-6 00:00:00" fixdate="2013-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive can&amp;#39;t load transforms added using &amp;#39;ADD FILE&amp;#39;</summary>
      <description>ADD FILE mangles name of the resource when copying to resource download directory. As a results following doesn't work:ADD FILE test.py;SELECT TRANSFORM (id) USING 'python test.py' AS b FROM tab1;The resource gets added with a different name every time which makes it impossible to use transform in non-interactive mode.This seems to be due to HIVE-3431</description>
      <version>0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4521" opendate="2013-5-8 00:00:00" fixdate="2013-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto join conversion fails in certain cases (empty tables, empty partitions, no partitions)</summary>
      <description>Automatic join conversion for both map joins as well as smb joins fails when tables, files or partitions are empty (see test cases in patch). Error messages include: "Big Table Alias is null" and "Divide by Zero".</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
    </fixedFiles>
  </bug>
  <bug id="4546" opendate="2013-5-13 00:00:00" fixdate="2013-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive CLI leaves behind the per session resource directory on non-interactive invocation</summary>
      <description>As part of HIVE-4505, the resource directory is set to /tmp/${hive.session.id}_resources and suppose to be removed at the end. The CLI fails to remove it when invoked using -f or -e (non-interactive mode)</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4562" opendate="2013-5-15 00:00:00" fixdate="2013-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-3393 brought in Jackson library,and these four jars should be packed into hive-exec.jar</summary>
      <description>Some jars of Hive are required not only by the client but also the server (every Hadoop slave),though we could use 'add jar' command to add all the jars in dis-cache ,but in common way ,we may add these jars in $HADOOP_HOME/lib/ of every salve of the Hadoop Cluster,and need restart all the tasktrackers .For example:When using hive stats, If we use mysql as tmp stats db ,every salve of the Hadoop Cluster should contain mysql-connector-java-****.jar in $HADOOP_HOME/lib/ And for column stats In all slaves $HADOOP_HOME/lib/ should contain:jackson-core-asl-1.8.8.jarjackson-jaxrs-1.8.8.jarjackson-mapper-asl-1.8.8.jarjackson-xc-1.8.8.jarThese jars should be separated from other common client-side-jars .</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4566" opendate="2013-5-15 00:00:00" fixdate="2013-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException if typeinfo and nativesql commands are executed at beeline before a DB connection is established</summary>
      <description>Before a DB connection is established, executing a command such as typeinfo and nativesql results an NPE shown at the console:beeline&gt; !typeinfojava.lang.NullPointerExceptionbeeline&gt; !nativesqljava.lang.NullPointerExceptionInstead, a message, such as "No current connection" should be given, as in case of some other commands, such as dropall.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.src.test.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="4581" opendate="2013-5-21 00:00:00" fixdate="2013-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCat e2e tests broken by changes to Hive&amp;#39;s describe table formatting</summary>
      <description>In Hive 0.11 the default formatting for describe table changed. A number of the HCat e2e tests do describe table and apply regular expressions to the output to make sure the table looks correct. These formatting changes broke those tests.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.hcatalog.tests.hcat.conf</file>
    </fixedFiles>
  </bug>
  <bug id="4583" opendate="2013-5-21 00:00:00" fixdate="2013-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive compile and run with JDK7</summary>
      <description>This is an umbrella ticket to cover all issues related to supporting JDK 7 in HIVE. Many such issues are expected. Of course, JDK 6 needs to continue to work.The major obstacles on the way supporting JDK7 are:1. JDBC component needs to be upgraded because of JDBC interface changes in JDK7.2. DataNucleus needs to be upgraded because the current version doesn't support JDK7.3. Many test failures needs to be fixed. The majority of the failures are caused by JDK7 subtle behaviour change.4. Build needs to be changed to accommodate JDK7 compiler.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hcatalog.pig.TestPigHCatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4586" opendate="2013-5-22 00:00:00" fixdate="2013-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[HCatalog] WebHCat should return 404 error for undefined resource</summary>
      <description></description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hcatalog.templeton.TestWebHCatE2e.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.CatchallExceptionMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="4588" opendate="2013-5-22 00:00:00" fixdate="2013-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support session level hooks for HiveServer2</summary>
      <description>Support session level hooks for HiveSrver2. The configured hooks will get executed at beginning of each new session.This is useful for auditing connections, possibly tuning the session level properties etc.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4601" opendate="2013-5-23 00:00:00" fixdate="2013-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat needs to support proxy users</summary>
      <description>We have a use case where a Gateway would provide unified and controlled access to secure hadoop cluster.The Gateway itself would authenticate to secure WebHDFS, Oozie and Templeton with SPNego.The Gateway would authenticate the end user with http basic and would assert the end user identity as douser argument in the calls to downstream WebHDFS, Oozie and Templeton.This works fine with WebHDFS and Oozie. But, does not work for Templeton as Templeton does not support proxy users.Hence, request to add this improvement to Templeton.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.UgiFactory.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.tool.NotFoundException.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.tool.HDFSStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4609" opendate="2013-5-24 00:00:00" fixdate="2013-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow hive tests to specify an alternative to /tmp</summary>
      <description>It'd be nice to be able to force hive not to use /tmp/. This is particularly useful when using the ptest framework.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4620" opendate="2013-5-28 00:00:00" fixdate="2013-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR temp directory conflicts in case of parallel execution mode</summary>
      <description>In parallel query execution mode, all the parallel running task ends up sharing the same temp/scratch directory. This could lead to file conflicts and temp files getting deleted before the job completion.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="4625" opendate="2013-5-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 should not attempt to get delegation token from metastore if using embedded metastore</summary>
      <description>colored textIn kerberos secure mode, with doas enabled, Hive server2 tries to get delegation token from metastore even if the metastore is being used in embedded mode. To avoid failure in that case, it uses catch block for UnsupportedOperationException thrown that does nothing. But this leads to an error being logged by lower levels and can mislead users into thinking that there is a problem.It should check if delegation token mode is supported with current configuration before calling the function.</description>
      <version>0.11.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="4629" opendate="2013-5-29 00:00:00" fixdate="2013-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 should support an API to retrieve query logs</summary>
      <description>HiveServer2 should support an API to retrieve query logs. This is particularly relevant because HiveServer2 supports async execution but doesn't provide a way to report progress. Providing an API to retrieve query logs will help report progress to the client.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.Operation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.MetadataOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetSchemasOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetFunctionsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetCatalogsOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ICLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.EmbeddedCLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TFetchResultsReq.java</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service.if.TCLIService.thrift</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4638" opendate="2013-5-31 00:00:00" fixdate="2013-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thread local PerfLog can get shared by multiple hiveserver2 sessions</summary>
      <description>The PerfLog is accessed as thread local which can be shared by multiple hiveserver2 session, overwriting query runtime information.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug id="4639" opendate="2013-5-31 00:00:00" fixdate="2013-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add has null flag to ORC internal index</summary>
      <description>It would enable more predicate pushdown if we added a flag to the index entry recording if there were any null values in the column for the 10k rows.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestFileDump.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatistics.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="464" opendate="2009-5-1 00:00:00" fixdate="2009-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add alter partition method to the meastore interface</summary>
      <description>add alter_partition() interface similar to that of alter_table()</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen-php.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4640" opendate="2013-5-31 00:00:00" fixdate="2013-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CommonOrcInputFormat should be the default input format for Orc tables.</summary>
      <description>CommonOrcInputFormat should be the default input format for Orc files, so that default orc format tables work with both vectorized and non-vectorized path.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4642" opendate="2013-5-31 00:00:00" fixdate="2013-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized RLIKE and REGEXP filter expressions</summary>
      <description>See title. I will add more details next week. The goal is (a) make this work correctly and (b) optimize it as well as possible, at least for the common cases.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
    </fixedFiles>
  </bug>
  <bug id="4662" opendate="2013-6-5 00:00:00" fixdate="2013-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>first_value can&amp;#39;t have more than one order by column</summary>
      <description>In the current implementation of the first_value function, it's not allowed to have more than one (1) order by column, as so:select distinct first_value(kastr.DEWNKNR) over ( partition by kastr.DEKTRNR order by kastr.DETRADT, kastr.DEVPDNR )from RTAVP_DRKASTR kastr;Error given:FAILED: SemanticException Range based Window Frame can have only 1 Sort Key</description>
      <version>0.11.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4669" opendate="2013-6-6 00:00:00" fixdate="2013-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make username available to semantic analyzer hooks</summary>
      <description>Make username available to the semantic analyzer hooks.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4683" opendate="2013-6-7 00:00:00" fixdate="2013-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix coverage org.apache.hadoop.hive.cli</summary>
      <description></description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.RCFileCat.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4684" opendate="2013-6-7 00:00:00" fixdate="2013-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query with filter constant on left of "=" and column expression on right does not vectorize</summary>
      <description>select dmachineid from factsqlengineam_vec_orc where 1073 = dmachineid + 1;Does not go down the vectorization path.Output:hive&gt; select dmachineid from factsqlengineam_vec_orc where 1073 = dmachineid + 1;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorValidating if vectorized execution is applicableCannot vectorize the plan: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarEqualLongColumnStarting Job = job_201306061504_0038, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201306061504_0038Kill Command = c:\Hadoop\hadoop-1.1.0-SNAPSHOT\bin\hadoop.cmd job -kill job_201306061504_0038Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 02013-06-07 10:25:30,932 Stage-1 map = 0%, reduce = 0%2013-06-07 10:25:39,953 Stage-1 map = 25%, reduce = 0%2013-06-07 10:25:42,959 Stage-1 map = 49%, reduce = 0%, Cumulative CPU 8.172 sec2013-06-07 10:25:43,962 Stage-1 map = 49%, reduce = 0%, Cumulative CPU 8.172 sec...</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.CodeGen.java</file>
    </fixedFiles>
  </bug>
  <bug id="4692" opendate="2013-6-9 00:00:00" fixdate="2013-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant agg parameters will be replaced by ExprNodeColumnDesc with single-sourced multi-gby cases</summary>
      <description>HIVE-3495 fixed this for single gbys but not for multi-gbys. For example,FROM (select key, cast(key as double) as value from src order by key) aINSERT OVERWRITE TABLE e1 SELECT COUNT(*)INSERT OVERWRITE TABLE e2 SELECT percentile_approx(value, 0.5);FAILED: UDFArgumentTypeException The second argument must be a constant, but double was passed instead.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4707" opendate="2013-6-11 00:00:00" fixdate="2013-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support configurable domain name for HiveServer2 LDAP authentication using Active Directory</summary>
      <description>LDAP providers like Active Directory use a fully qualified user name in user@domain format. For HiveServer2 LDAP auth can be used with active directory by passing the userid in that format. This causes hive authentication module to retrun the username in that mangled format. This prohibits LDAP users to be impersonated over secure hadoop or reported correctly in audit etc.HiveServer2 should support a configurable LDAP domain that is appended to the user name.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4732" opendate="2013-6-13 00:00:00" fixdate="2013-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce or eliminate the expensive Schema equals() check for AvroSerde</summary>
      <description>The AvroSerde spends a significant amount of time checking schema equality. Changing to compare hashcodes (which can be computed once then reused) will improve performance.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.Utils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestSchemaReEncoder.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestGenericAvroRecordWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="474" opendate="2009-5-6 00:00:00" fixdate="2009-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for distinct selection on two or more columns</summary>
      <description>The ability to select distinct several, individual columns as by example: select count(distinct user), count(distinct session) from actions; Currently returns the following failure: FAILED: Error in semantic analysis: line 2:7 DISTINCT on Different Columns not Supported user</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4740" opendate="2013-6-17 00:00:00" fixdate="2013-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-2379 is missing hbase.jar itself</summary>
      <description>I should test that before addressing comments. Sorry.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4761" opendate="2013-6-19 00:00:00" fixdate="2013-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeperHiveLockManage.unlockPrimitive has race condition with threads</summary>
      <description>In unlockPrimitive, we check to see if children exist and if not delete the parent node. If two threads do this at the same time it's possible for two threads to call Zookeeper.delete() on the same node.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="4769" opendate="2013-6-20 00:00:00" fixdate="2013-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized ORC reader does not handle absence of column present stream correctly.</summary>
      <description>If the present stream is not present for a column then by design all the rows are present for that column. Currently ORC vectorized reader does initialize the IsNull vector correctly in this case.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4785" opendate="2013-6-24 00:00:00" fixdate="2013-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement isCaseSensitive for Hive JDBC driver</summary>
      <description>Implement the "boolean isCaseSensitive(int column) throws SQLException" JDBC method.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="4807" opendate="2013-7-2 00:00:00" fixdate="2013-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore hangs</summary>
      <description>Hive metastore hangs (does not accept any new connections) due to a bug in DBCP. The root cause analysis is here https://issues.apache.org/jira/browse/DBCP-398. The fix is to change Hive connection pool to BoneCP which is natively supported by DataNucleus.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4810" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor exec package</summary>
      <description>The exec package contains both operators and classes used to execute the job. Moving the latter into a sub package makes the package slightly more manageable and will make it easier to provide a tez-based implementation.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.loadpart.err.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.reflect.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udfnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.publisher.error.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.publisher.error.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.aggregator.error.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.serde.regex2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.broken.pipe1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.minimr.broken.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.turnoff.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.turnoff.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mapreduce.stack.trace.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.local.mapred.error.cache.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fatal.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part.max.per.node.q.out</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapperContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Throttle.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IndexUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.java</file>
      <file type="M">ql.src.test.results.clientnegative.autolocal1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.cachingprintstream.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.cluster.tasklog.retrieval.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4812" opendate="2013-7-3 00:00:00" fixdate="2013-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logical explain plan</summary>
      <description>In various situations it would have been useful to me to glance at the operator plan before we break it into tasks and apply join, total order sort, etc optimizations.I've added this as an options to explain. "Explain logical &lt;QUERY&gt;" will output the full operator tree (not the stage plans, tasks, AST etc).Again, I don't think this has to even be documented for users, but might be useful to developers.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="4818" opendate="2013-7-7 00:00:00" fixdate="2013-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SequenceId in operator is not thread safe</summary>
      <description>SequenceId , seqId in the operator class is not modified concurrently.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4858" opendate="2013-7-15 00:00:00" fixdate="2013-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort "show grant" result to improve usability and testability</summary>
      <description>Currently Hive outputs the result of "show grant" command in no deterministic order. It outputs the set of each privilege type in the order of whatever returned from DB (DataNucleus). Randomness can arise and tests (depending on the order) can fail, especially in events of library upgrade (DN or JVM upgrade). Sorting the result will avoid the potential randomness and make the output more deterministic, thus not only improving the readability of the output but also making the test more robust.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.authorization.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="4863" opendate="2013-7-16 00:00:00" fixdate="2013-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix parallel order by on hadoop2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4876" opendate="2013-7-17 00:00:00" fixdate="2013-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeling help text do not contain -f and -e parameters</summary>
      <description>I've noticed that beeline do have support for -e and -f parameters, however those are not documented in the help text that can be retrieved by calling beeline with parameter -h.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4878" opendate="2013-7-17 00:00:00" fixdate="2013-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>With Dynamic partitioning, some queries would scan default partition even if query is not using it.</summary>
      <description>With Dynamic partitioning, Hive would scan default partitions in some cases even if query excludes it.As part of partition pruning, predicate is narrowed down to those pieces that involve partition columns only. This predicate is then evaluated with partition values to determine, if scan should include those partitions.But in some cases (like when comparing "_HIVE_DEFAULT_PARTITION_" to numeric data types) expression evaluation would fail and would return NULL instead of true/false. In such cases the partition is added to unknown partitions which is then subsequently scanned.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
    </fixedFiles>
  </bug>
  <bug id="4879" opendate="2013-7-18 00:00:00" fixdate="2013-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window functions that imply order can only be registered at compile time</summary>
      <description>Adding an annotation for impliesOrder</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="4885" opendate="2013-7-18 00:00:00" fixdate="2013-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alternative object serialization for execution plan in hive testing</summary>
      <description>Currently there are a lot of test cases involving in comparing execution plan, such as those in TestParse suite. XmlEncoder is used to serialize the generated plan by hive, and store it in the file for file diff comparison. However, XmlEncoder is tied with Java compiler, whose implementation may change from version to version. Thus, upgrade the compiler can generate a lot of fake test failures. The following is an example of diff generated when running hive with JDK7:Begin query: case_sensitivity.qdiff -a /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/build/ql/test/logs/positive/case_sensitivity.q.out /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/ql/src/test/results/compiler/parse/case_sensitivity.q.outdiff -a -b /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/build/ql/test/logs/positive/case_sensitivity.q.xml /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/ql/src/test/results/compiler/plan/case_sensitivity.q.xml3c3&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MapRedTask" id="MapRedTask0"&gt;---&gt; &lt;object id="MapRedTask0" class="org.apache.hadoop.hive.ql.exec.MapRedTask"&gt; 12c12&lt; &lt;object class="java.util.ArrayList" id="ArrayList0"&gt;---&gt; &lt;object id="ArrayList0" class="java.util.ArrayList"&gt; 14c14&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MoveTask" id="MoveTask0"&gt;---&gt; &lt;object id="MoveTask0" class="org.apache.hadoop.hive.ql.exec.MoveTask"&gt; 18c18&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MoveTask" id="MoveTask1"&gt;---&gt; &lt;object id="MoveTask1" class="org.apache.hadoop.hive.ql.exec.MoveTask"&gt; 22c22&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.StatsTask" id="StatsTask0"&gt;---&gt; &lt;object id="StatsTask0" class="org.apache.hadoop.hive.ql.exec.StatsTask"&gt; 60c60&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MapRedTask" id="MapRedTask1"&gt;---&gt; &lt;object id="MapRedTask1" class="org.apache.hadoop.hive.ql.exec.MapRedTask"&gt; As it can be seen, the only difference is the order of the attributes in the serialized XML doc, yet it brings 50+ test failures in Hive.We need to have a better plan comparison, or object serialization to improve the situation.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4886" opendate="2013-7-18 00:00:00" fixdate="2013-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline code should have apache license headers</summary>
      <description>The beeline jdbc client added as part of hive server2 changes is based on SQLLine. As beeline is modified version of SQLLine and further modifications are also under apache license, the license headers of these files need to be replaced with apache license headers. We already have the license text of SQLLine in LICENSE file .</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.XMLElementOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.XMLAttributeOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.VerticalOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableNameCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SunSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SQLCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Reflector.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ReflectiveCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.OutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.OutputFile.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.IncrementalRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DriverInfo.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnections.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.CommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ColorBuffer.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BufferedRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCommandCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractCommandHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4888" opendate="2013-7-19 00:00:00" fixdate="2013-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>listPartitionsByFilter doesn&amp;#39;t support lt/gt/lte/gte</summary>
      <description>Filter pushdown could be improved. Based on my experiments there's no reasonable way to do it with DN 2.0, due to DN bug in substring and Collection.get(int) not being implemented.With version as low as 2.1 we can use values.get on partition to extract values to compare to. Type compatibility is an issue, but is easy for strings and integral values.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.gen.thrift.gen-rb.serde.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-py.org.apache.hadoop.hive.serde.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-php.org.apache.hadoop.hive.serde.Types.php</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.cpp</file>
      <file type="M">serde.if.serde.thrift</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="4904" opendate="2013-7-22 00:00:00" fixdate="2013-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A little more CP crossing RS boundaries</summary>
      <description>Currently, CP context cannot be propagated over RS except for JOIN/EXT. A little more CP is possible.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4909" opendate="2013-7-22 00:00:00" fixdate="2013-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized ORC reader does not set isRepeating flag correctly when 1s are present is the input stream</summary>
      <description>As the default value for nulls in Vectorization for int types is 1, and as non-null values can also be 1, the isRepeating logic should also check for IsNull flag when determining the isRepeating flag.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.BitFieldReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="4910" opendate="2013-7-22 00:00:00" fixdate="2013-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop 2 archives broken</summary>
      <description>Hadoop 2 archive tests are broken. The issue stems from the fact that har uri construction does not really have a port in the URI when unit tests are run. This means that an invalid uri is constructed resulting in failures.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.archive.excludeHadoop20.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.archive.excludeHadoop20.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="4911" opendate="2013-7-22 00:00:00" fixdate="2013-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable QOP configuration for Hive Server 2 thrift transport</summary>
      <description>The QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed "hive.server2.thrift.sasl.qop". This would give greater control configuring hive server 2 service.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
      <file type="M">shims.src.common-secure.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.KerberosSaslHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4957" opendate="2013-7-30 00:00:00" fixdate="2013-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restrict number of bit vectors, to prevent out of Java heap memory</summary>
      <description>normally increase number of bit vectors will increase calculation accuracy. Let's sayselect compute_stats(a, 40) from test_hive;generally get better accuracy thanselect compute_stats(a, 16) from test_hive;But larger number of bit vectors also cause query run slower. When number of bit vectors over 50, it won't help to increase accuracy anymore. But it still increase memory usage, and crash Hive if number if too huge. Current Hive doesn't prevent user use ridiculous large number of bit vectors in 'compute_stats' query.One exampleselect compute_stats(a, 999999999) from column_eight_types;crashes Hive.2012-12-20 23:21:52,247 Stage-1 map = 0%, reduce = 0%2012-12-20 23:22:11,315 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.29 secMapReduce Total cumulative CPU time: 290 msecEnded Job = job_1354923204155_0777 with errorsError during job, obtaining debugging information...Job Tracking URL: http://cs-10-20-81-171.cloud.cloudera.com:8088/proxy/application_1354923204155_0777/Examining task ID: task_1354923204155_0777_m_000000 (and more) from job job_1354923204155_0777Task with the most failures(4): -----Task ID: task_1354923204155_0777_m_000000URL: http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1354923204155_0777&amp;tipid=task_1354923204155_0777_m_000000-----Diagnostic Messages for this Task:Error: Java heap space</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
    </fixedFiles>
  </bug>
  <bug id="4969" opendate="2013-7-31 00:00:00" fixdate="2013-1-31 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>HCatalog HBaseHCatStorageHandler is not returning all the data</summary>
      <description>Repro steps:1) Create an HCatalog table mapped to HBase table.hcat -e "CREATE TABLE studentHCat(rownum int, name string, age int, gpa float) STORED BY 'org.apache.hcatalog.hbase.HBaseHCatStorageHandler' TBLPROPERTIES('hbase.table.name' ='studentHBase', 'hbase.columns.mapping' = ':key,onecf:name,twocf:age,threecf:gpa')";2) Load the following data from Pig.cat student_data1^Asarah laertes^A23^A2.402^Atom allen^A72^A1.573^Abob ovid^A61^A2.674^Aethan nixon^A38^A2.155^Acalvin robinson^A28^A2.536^Airene ovid^A65^A2.567^Ayuri garcia^A36^A1.658^Acalvin nixon^A41^A1.049^Ajessica davidson^A48^A2.1110^Akatie king^A39^A1.05grunt&gt; A = LOAD 'student_data' AS (rownum:int,name:chararray,age:int,gpa:float);grunt&gt; STORE A INTO 'studentHCat' USING org.apache.hcatalog.pig.HCatStorer();3) Now from HBase do a scan on the studentHBase tablehbase(main):026:0&gt; scan 'studentPig', {LIMIT =&gt; 5}4) From pig access the data in tablegrunt&gt; A = LOAD 'studentHCat' USING org.apache.hcatalog.pig.HCatLoader();grunt&gt; STORE A INTO '/user/root/studentPig';5) Verify the output written in StudentPighadoop fs -cat /user/root/studentPig/part-r-000001 232 723 614 385 286 657 368 419 4810 39The data returned has only two fields (rownum and age).Problem:While reading the data from HBase table, HbaseSnapshotRecordReader gets data row in Result (org.apache.hadoop.hbase.client.Result) object and processes the KeyValue fields in it. After processing, it creates another Result object out of the processed KeyValue array. Problem here is KeyValue array is not sorted. Result object expects the input KeyValue array to have sorted elements. When we call the Result.getValue() it returns no value for some of the fields as it does a binary search on un-ordered array.</description>
      <version>0.11.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4990" opendate="2013-8-2 00:00:00" fixdate="2013-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC seeks fails with non-zero offset or column projection</summary>
      <description>The ORC reader gets exceptions when seeking with non-zero offsets or column projection.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0,0.11.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4996" opendate="2013-8-5 00:00:00" fixdate="2013-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unbalanced calls to openTransaction/commitTransaction</summary>
      <description>when we used hiveserver1 based on hive-0.10.0, we found the Exception thrown.It was:FAILED: Error in metadata: MetaException(message:java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that there are unbalanced calls to openTransaction/commitTransaction)FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskhelp</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRawStoreTxn.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4998" opendate="2013-8-5 00:00:00" fixdate="2013-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support jdbc documented table types in default configuration</summary>
      <description>The jdbc table types supported by hive server2 are not the documented typical types &amp;#91;1&amp;#93; in jdbc, they are hive specific types (MANAGED_TABLE, EXTERNAL_TABLE, VIRTUAL_VIEW). HIVE-4573 added support for the jdbc documented typical types, but the HS2 default configuration is to return the hive types The default configuration should result in the expected jdbc typical behavior.&amp;#91;1&amp;#93; http://docs.oracle.com/javase/6/docs/api/java/sql/DatabaseMetaData.html?is-external=true#getTables(java.lang.String, java.lang.String, java.lang.String, java.lang.String[])</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="500" opendate="2009-5-20 00:00:00" fixdate="2009-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select from newly created table will fail</summary>
      <description>The bug can be reproduced by:create table zshao_int (a int);select 1 from zshao_int;The problem is that the directory for the table does not exist yet, but FileInputFormat.getSplits will fail in case the directory does not exist.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5023" opendate="2013-8-7 00:00:00" fixdate="2013-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive get wrong result when partition has the same path but different schema or authority</summary>
      <description>Hive does not differentiate scheme and authority in file uris which cause wrong result when partition has the same path but different schema or authority. Here is a simple repropartition file path:asv://container1@secondary1.blob.core.windows.net/2013-08-05/00/text1.txtwith content "2013-08-05 00:00:00"asv://container2@secondary1.blob.core.windows.net/2013-08-05/00/text2.txtwith content "2013-08-05 00:00:20"CREATE EXTERNAL TABLE IF NOT EXISTS T1 (t STRING) PARTITIONED BY (ProcessDate STRING, Hour STRING, ClusterName STRING) ROW FORMAT DELIMITED FIELDS TERMINATED by '\t' STORED AS TEXTFILE;ALTER TABLE T1 DROP IF EXISTS PARTITION(processDate='2013-08-05', Hour='00', clusterName ='CLusterA');ALTER TABLE T1 ADD IF NOT EXISTS PARTITION(processDate='2013-08-05', Hour='00', clusterName ='ClusterA') LOCATION 'asv://container1@secondary1.blob.core.windows.net/2013-08-05/00';ALTER TABLE T1 DROP IF EXISTS PARTITION(processDate='2013-08-05', Hour='00', clusterName ='ClusterB');ALTER TABLE T1 ADD IF NOT EXISTS PARTITION(processDate='2013-08-05', Hour='00', clusterName ='ClusterB') LOCATION 'asv://container2@secondary1.blob.core.windows.net/2013-08-05/00';the expect output of the hive querySELECT ClusterName, t FROM T1 WHERE ProcessDate=2013-08-05 AND Hour=00;should beClusterA 2013-08-05 00:00:00ClusterB 2013-08-05 00:00:20However it isClusterA 2013-08-05 00:00:00ClusterA 2013-08-05 00:00:20</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5026" opendate="2013-8-8 00:00:00" fixdate="2013-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-3926 is committed in the state of not rebased to trunk</summary>
      <description>Current trunk build fails.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5027" opendate="2013-8-8 00:00:00" fixdate="2013-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Ivy to 2.3</summary>
      <description>Ivy version 2.1 doesn't include classifiers when creating pom files. Therefore our generated pom's are not correct. Version 2.3 fixes this.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5031" opendate="2013-8-8 00:00:00" fixdate="2013-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] GET job/:jobid to return userargs for a job in addtion to status information</summary>
      <description>It would be nice to also have any user args that were passed into job creation API including job type specific information (e.g. mapreduce libjars)NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.QueueStatusBean.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.queue.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5035" opendate="2013-8-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Hardening parameters for Windows</summary>
      <description>everything pass to pig/hive/hadoop command line will be quoted. That include:mapreducejar:libjarsargdefinemapreducestream:cmdenvdefineargpigargexecutehiveargdefineexecuteNO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5036" opendate="2013-8-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Add cmd script for WebHCat</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.build.xml</file>
      <file type="M">hcatalog.build-support.checkstyle.apache.header.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5047" opendate="2013-8-9 00:00:00" fixdate="2013-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive client filters partitions incorrectly via pushdown in certain cases involving "or"</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
    </fixedFiles>
  </bug>
  <bug id="5049" opendate="2013-8-9 00:00:00" fixdate="2013-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create an ORC test case that has a 0.11 ORC file</summary>
      <description>We should add a test case that includes a 0.11.0 ORC file to ensure compatibility for reading old ORC files is kept correct.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="5058" opendate="2013-8-12 00:00:00" fixdate="2013-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NPE issue with DAG submission in TEZ</summary>
      <description>Submitting dag caused NPE on execution.Multiple issues: Some configs weren't set right Key desc/Table desc weren't set properly parallelism was left at -1NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5060" opendate="2013-8-12 00:00:00" fixdate="2013-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC driver assumes executeStatement is synchronous</summary>
      <description>The JDBC driver seems to assume that ExecuteStatement is a synchronous call when performing updates via executeUpdate, where the following comment on the RPC in the Thrift file indicates otherwise:// ExecuteStatement()//// Execute a statement.// The returned OperationHandle can be used to check on the// status of the statement, and to fetch results once the// statement has finished executing.I understand that Hive's implementation of ExecuteStatement is blocking (see https://issues.apache.org/jira/browse/HIVE-4569), but presumably other implementations of the HiveServer2 API (and I'm talking specifically about Impala here, but others might have a similar concern) should be free to return a pollable OperationHandle per the specification.The JDBC driver's executeUpdate is as follows:public int executeUpdate(String sql) throws SQLException { execute(sql); return 0; }execute(sql) discards the OperationHandle that it gets from the server after determining whether there are results to be fetched.This is problematic for us, because Impala will cancel queries that are running when a session executes, but there's no easy way to be sure that an INSERT statement has completed before terminating a session on the client.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="5066" opendate="2013-8-12 00:00:00" fixdate="2013-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Other code fixes for Windows</summary>
      <description>This is equivalent to HCATALOG-526, but updated to sync with latest trunk.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.TestServer.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5069" opendate="2013-8-13 00:00:00" fixdate="2013-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests on list bucketing are failing again in hadoop2</summary>
      <description>org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_12org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_2org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_4org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_5org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_1org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_query_multiskew_3</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="507" opendate="2009-5-22 00:00:00" fixdate="2009-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>empty files not handled properly</summary>
      <description>file: for the path of the empty file only works in local mode</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5085" opendate="2013-8-14 00:00:00" fixdate="2013-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metatool errors out if HIVE_OPTS is set</summary>
      <description>If HIVE_OPTS were to be set, then the Hive Metatool fails on parsing the commandline arguments. This can happen, for example, if HIVE_AUX_JARS_PATH were to include the HCatalog jar.The result, then, is that it tries to execute the following:exec $HADOOP jar ${HIVE_LIB}/hive-cli-*.jar $CLASS $HIVE_OPTS "$@"which maps to:exec /usr/lib/hadoop/bin/hadoop jar /usr/lib/hive/lib/hive-cli-0.11.0.2.0.5.0-30.jar org.apache.hadoop.hive.metastore.tools.HiveMetaTool -hiveconf hive.aux.jars.path=file:///usr/lib/hcatalog/share/hcatalog/hcatalog-core.jar -listFSRootThen Metatool chokes on seeing the "-hiveconf" parameter.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.metatool.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5102" opendate="2013-8-15 00:00:00" fixdate="2013-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC getSplits should create splits based the stripes</summary>
      <description>Currently ORC inherits getSplits from FileFormat, which basically makes a split per an HDFS block. This can create too little parallelism and would be better done by having getSplits look at the file footer and create splits based on the stripes.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StripeInformation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5103" opendate="2013-8-15 00:00:00" fixdate="2013-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job numbers are incorrectly displayed in Tez</summary>
      <description>We display the number of jobs before we run anything. However, we don't look at Tez tasks yet.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5122" opendate="2013-8-20 00:00:00" fixdate="2013-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add partition for multiple partition ignores locations for non-first partitions</summary>
      <description>http://www.mail-archive.com/user@hive.apache.org/msg09151.htmlWhen multiple partitions are being added in single alter table statement, the location for first partition is being used as the location of all partitions.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5126" opendate="2013-8-20 00:00:00" fixdate="2013-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make vector expressions serializable.</summary>
      <description>We should make all vectorized expressions serializable.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColSubtractDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.VectorUDAFAvg.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.ScalarArithmeticColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.FilterStringScalarCompareColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.FilterStringColumnCompareScalar.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.FilterStringColumnCompareColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.FilterScalarCompareColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.FilterColumnCompareScalar.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.FilterColumnCompareColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.ColumnUnaryMinus.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.ColumnCompareScalar.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.ColumnArithmeticScalar.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.ColumnArithmeticColumn.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarSubtractLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarSubtractDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarMultiplyLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarMultiplyDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarModuloLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarModuloDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarDivideDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarAddLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarAddDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColUnaryMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColNotEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColLessEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColLessDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColGreaterEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColGreaterDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColDivideDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColDivideDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringScalarNotEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringScalarLessStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringScalarLessEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringScalarGreaterStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringScalarGreaterEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringScalarEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColNotEqualStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColNotEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColLessStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColLessStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColLessEqualStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColLessEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColGreaterStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColGreaterStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColGreaterEqualStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColGreaterEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColEqualStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColEqualStringColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarNotEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarLessEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarLessDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarGreaterEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarGreaterDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColNotEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColNotEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColNotEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarNotEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarLessEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarLessDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarGreaterEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarGreaterDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleScalarEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColNotEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColNotEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColNotEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColNotEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColGreaterDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColEqualLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColEqualLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColEqualDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarSubtractLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarSubtractDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarMultiplyLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarMultiplyDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarModuloLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarModuloDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarDivideDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarAddLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleScalarAddDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColUnaryMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColSubtractLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColSubtractLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFAvgLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdSampLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFSumDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFSumLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarPopLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFVarSampLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterNotExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColAddDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColAddDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColAddLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColAddLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColDivideDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColDivideDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColDivideLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColDivideLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColGreaterDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColGreaterEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColLessDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColLessEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColModuloDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColModuloDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColModuloLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColModuloLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColMultiplyDoubleColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColMultiplyDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColMultiplyLongColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColMultiplyLongScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColNotEqualDoubleScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColSubtractDoubleColumn.java</file>
    </fixedFiles>
  </bug>
  <bug id="5129" opendate="2013-8-21 00:00:00" fixdate="2013-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple table insert fails on count(distinct)</summary>
      <description>Hive fails with a class cast exception on queries of the form:from studenttab10kinsert overwrite table multi_insert_2_1select name, avg(age) as avgagegroup by nameinsert overwrite table multi_insert_2_2select name, age, sum(gpa) as sumgpagroup by name, ageinsert overwrite table multi_insert_2_3select name, count(distinct age) as distagegroup by name;</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5131" opendate="2013-8-21 00:00:00" fixdate="2013-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC client&amp;#39;s hive variables are not passed to HS2</summary>
      <description>Related to HIVE-2914. However, HIVE-2914 seems addressing Hive CLI only. JDBC clients suffer the same problem. This was identified in HIVE-4568. I decided it might be better to separate issue from a different issue.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.src.test.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="5132" opendate="2013-8-21 00:00:00" fixdate="2013-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t access to hwi due to "No Java compiler available"</summary>
      <description>I want to use hwi to submit hive queries, but after start hwi successfully, I can't open the web page of it.I noticed that someone also met the same issue in hive-0.10.Reproduce steps:--------------------------1. start hwibin/hive --config $HIVE_CONF_DIR --service hwi2. access to http://&lt;hive_hwi_node&gt;:9999/hwi via browsergot the following error message:HTTP ERROR 500Problem accessing /hwi/. Reason: No Java compiler availableCaused by:java.lang.IllegalStateException: No Java compiler available at org.apache.jasper.JspCompilationContext.createCompiler(JspCompilationContext.java:225) at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:560) at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:299) at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:315) at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:265) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) at org.mortbay.jetty.servlet.Dispatcher.forward(Dispatcher.java:327) at org.mortbay.jetty.servlet.Dispatcher.forward(Dispatcher.java:126) at org.mortbay.jetty.servlet.DefaultServlet.doGet(DefaultServlet.java:503) at javax.servlet.http.HttpServlet.service(HttpServlet.java:707) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5133" opendate="2013-8-21 00:00:00" fixdate="2013-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat jobs that need to access metastore fails in secure mode</summary>
      <description>Webhcat job submission requests result in the pig/hive/mr job being run from a map task that it launches. In secure mode, for the pig/hive/mr job that is run to be authorized to perform actions on metastore, it has to have the delegation tokens from the hive metastore.In case of pig/MR job this is needed if hcatalog is being used in the script/job.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug id="5176" opendate="2013-8-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wincompat : Changes for allowing various path compatibilities with Windows</summary>
      <description>We need to make certain changes across the board to allow us to read/parse windows paths. Some are escaping changes, some are being strict about how we read paths (through URL.encode/decode, etc)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5179" opendate="2013-8-29 00:00:00" fixdate="2013-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wincompat : change script tests from bash to sh</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.script.env.var2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.script.env.var2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.script.env.var1.q</file>
    </fixedFiles>
  </bug>
  <bug id="518" opendate="2009-5-26 00:00:00" fixdate="2009-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test mode in hive</summary>
      <description>It would be good to have a test mode in hive - this will help in checking the validity of a hive drop on a production cluster.The following would be good to have:Testmode --&gt; In testmode, all input tables are sampled (if not already sampled) and all output tables are prefixed by a user supplied name.This way, multiple hive drops can be compared quickly for correctnessNew Options:// whether hive is running in test mode. If yes, it turns on sampling and prefixes the output tablenameset hive.test.mode=true;// if hive is running in test mode, prefixes the output table by this stringset hive.test.mode.prefix=;// if hive is running in test mode and table is not bucketed, sampling frequencyset hive.test.mode.samplefreq=256;// if hive is running in test mode, dont sample the above comma seperated list of tablesset hive.test.mode.nosamplelist=;</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5183" opendate="2013-8-30 00:00:00" fixdate="2013-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez EdgeProperty class has changed</summary>
      <description>Tez has changed the names of its EdgeProperties. Need to update the code to use the new names.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5184" opendate="2013-8-30 00:00:00" fixdate="2013-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load filesystem, ugi, metastore client at tez session startup</summary>
      <description>Make sure the session is ready to go when we connect. That way once the session/connection is open, we're ready to go.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="5187" opendate="2013-8-30 00:00:00" fixdate="2013-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance explain to indicate vectorized execution of operators.</summary>
      <description>Explain should be able to indicate whether an operator will be executed in vectorized mode or not.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5198" opendate="2013-9-3 00:00:00" fixdate="2013-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat returns exitcode 143 (w/o an explanation)</summary>
      <description>The message might look like this:{"statement":"use default; show table extended like xyz;","error":"unable to show table: xyz","exec":{"stdout":"","stderr":"","exitcode":143}} WebHCat has a templeton.exec.timeout property which kills an HCat request (i.e. something like a DDL statement that gets routed to HCat CLI) if it takes longer than this timeout.Since WebHCat does a fork/exec to 'hcat' script, the timeout is implemented as SIGTERM sent to the subprocess. SIGTERM value is 15. So it's reported as 128 + 15 = 143.Error logging/reporting should be improved in this case.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="5209" opendate="2013-9-4 00:00:00" fixdate="2013-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC support for varchar</summary>
      <description>Support returning varchar length in result set metadata</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeDescriptor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Type.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnDescriptor.java</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service.src.gen.thrift.gen-rb.t.c.l.i.service.constants.rb</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service.src.gen.thrift.gen-py.TCLIService.constants.py</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TUnionTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeId.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTypeDesc.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TTableSchema.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStructTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TStatus.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TRowSet.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TRow.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionResp.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TOpenSessionReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TGetTablesReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TExecuteStatementReq.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TColumn.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.constants.h</file>
      <file type="M">service.src.gen.thrift.gen-cpp.TCLIService.constants.cpp</file>
      <file type="M">service.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveBaseResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="5241" opendate="2013-9-7 00:00:00" fixdate="2013-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default log4j log level for WebHCat should be INFO not DEBUG</summary>
      <description>webhcat-logj4.properties</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.HDFSStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.CompleteDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5246" opendate="2013-9-9 00:00:00" fixdate="2013-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local task for map join submitted via oozie job fails on a secure HDFS</summary>
      <description>For a Hive query started by Oozie Hive action, the local task submitted for Mapjoin fails. The HDFS delegation token is not shared properly with the child JVM created for the local task.Oozie creates a delegation token for the Hive action and sets env variable HADOOP_TOKEN_FILE_LOCATION as well as mapreduce.job.credentials.binary config property. However this doesn't get passed down to the child JVM which causes the problem.This is similar issue addressed by HIVE-4343 which address the problem HiveServer2</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5277" opendate="2013-9-12 00:00:00" fixdate="2013-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase handler skips rows with null valued first cells when only row key is selected</summary>
      <description>HBaseStorageHandler skips rows with null valued first cells when only row key is selected.SELECT key, col1, col2 FROM hbase_table;key1 cell1 cell2 key2 NULL cell3SELECT COUNT(key) FROM hbase_table;1HiveHBaseTableInputFormat.getRecordReader makes first cell selected to avoid skipping rows. But when the first cell is null, HBase skips that row.http://hbase.apache.org/book/perf.reading.html 12.9.6. Optimal Loading of Row Keys describes how to deal with this problem.I tried to find an existing issue, but I couldn't. If you find a same issue, please make this issue duplicated.</description>
      <version>0.11.0,0.12.0,0.11.1,0.13.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5278" opendate="2013-9-12 00:00:00" fixdate="2013-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move some string UDFs to GenericUDFs, for better varchar support</summary>
      <description>To better support varchar/char types in string UDFs, select UDFs should be converted to GenericUDFs. This allows the UDF to return the resulting char/varchar length in the type metadata.This work is being split off as a separate task from HIVE-4844. The initial UDFs as part of this work are concat/lower/upper.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5289" opendate="2013-9-13 00:00:00" fixdate="2013-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 should disable checking of libraries during batch exec</summary>
      <description>PTest2 has two phases:1) Build from source2) Exec in parallelduring phase two we don't want ivy making HTTP requests.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug id="5292" opendate="2013-9-13 00:00:00" fixdate="2013-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join on decimal columns fails to return rows</summary>
      <description>Join on matching decimal columns returns 0 rowsTo reproduce (I used beeline):1. create 2 simple identical tables with 2 identical rows: CREATE TABLE SERGDEC(I INT, D DECIMAL) ROW FORMAT DELIMITED FIELDSTERMINATED BY '|';CREATE TABLE SERGDEC2(I INT, D DECIMAL) ROW FORMAT DELIMITED FIELDSTERMINATED BY '|';2. populate tables with identical data:LOAD DATA LOCAL INPATH './decdata' OVERWRITE INTO TABLE SERGDEC ;LOAD DATA LOCAL INPATH './decdata' OVERWRITE INTO TABLE SERGDEC2 ;3. data file decdata contains:10|1111.9820|1234567890.12344. Perform join (returns 0 rows instead of 2):SELECT T1.I, T1.D, T2.D FROM SERGDEC T1 JOIN SERGDEC2 T2 ONT1.D = T2.D ;</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="5294" opendate="2013-9-15 00:00:00" fixdate="2013-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create collect UDF and make evaluator reusable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udaf.collect.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.collect.set.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5295" opendate="2013-9-16 00:00:00" fixdate="2013-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveConnection#configureConnection tries to execute statement even after it is closed</summary>
      <description>HiveConnection#configureConnection tries to execute statement even after it is closed. For remote JDBC client, it tries to set the conf var using 'set foo=bar' by calling HiveStatement.execute for each conf var pair, but closes the statement after the 1st iteration through the conf var pairs.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="5301" opendate="2013-9-17 00:00:00" fixdate="2013-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a schema tool for offline metastore schema upgrade</summary>
      <description>HIVE-3764 is addressing metastore version consistency.Besides it would be helpful to add a tool that can leverage this version information to figure out the required set of upgrade scripts, and execute those against the configured metastore. Now that Hive includes Beeline client, it can be used to execute the scripts.</description>
      <version>0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.014-HIVE-3764.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.014-HIVE-3764.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.014-HIVE-3764.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.014-HIVE-3764.derby.sql</file>
      <file type="M">build.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="5351" opendate="2013-9-24 00:00:00" fixdate="2013-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure-Socket-Layer (SSL) support for HiveServer2</summary>
      <description>HiveServer2 and JDBC driver should support encrypted communication using SSL</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5352" opendate="2013-9-24 00:00:00" fixdate="2013-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cast(&amp;#39;1.0&amp;#39; as int) returns null</summary>
      <description>Casting strings to int/smallint/bigint/tinyint yields null if the string isn't a 'pure' integer. '1.0', '2.4' all return null. I think for those cases the cast should return the truncated int (i.e.: if c is string, cast(c as int) should be the same as cast(cast(c as float) as int).This is in line with the standard and is the same behavior as mysql and oracle. (postgres and sql server throw error, see first answer here: http://social.msdn.microsoft.com/Forums/sqlserver/en-US/af3eff9c-737b-42fe-9016-05da9203a667/oracle-does-understand-cast10-as-int-why-sql-server-does-not)</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestToInteger.java</file>
    </fixedFiles>
  </bug>
  <bug id="5356" opendate="2013-9-25 00:00:00" fixdate="2013-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move arithmatic UDFs to generic UDF implementations</summary>
      <description>Currently, all of the arithmetic operators, such as add/sub/mult/div, are implemented as old-style UDFs and java reflection is used to determine the return type TypeInfos/ObjectInspectors, based on the return type of the evaluate() method chosen for the expression. This works fine for types that don't have type params.Hive decimal type participates in these operations just like int or double. Different from double or int, however, decimal has precision and scale, which cannot be determined by just looking at the return type (decimal) of the UDF evaluate() method, even though the operands have certain precision/scale. With the default of "decimal" without precision/scale, then (10, 0) will be the type params. This is certainly not desirable.To solve this problem, all of the arithmetic operators would need to be implemented as GenericUDFs, which allow returning ObjectInspector during the initialize() method. The object inspectors returned can carry type params, from which the "exact" return type can be determined.It's worth mentioning that, for user UDF implemented in non-generic way, if the return type of the chosen evaluate() method is decimal, the return type actually has (10,0) as precision/scale, which might not be desirable. This needs to be documented.This JIRA will cover minus, plus, divide, multiply, mod, and pmod, to limit the scope of review. The remaining ones will be covered under HIVE-5706.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.java</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseNumericOp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMinus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPMultiply.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPPlus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPosMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFOPDivide.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFOPMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFPosMod.java</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.arithmetic.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5439" opendate="2013-10-4 00:00:00" fixdate="2013-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set input edge map for map join operator in Tez</summary>
      <description>Need to remember which logical inputs to read from when constructing the hashtables for a map join. NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5459" opendate="2013-10-5 00:00:00" fixdate="2013-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add --version option to hive script</summary>
      <description>Hive jars already contain all the build information, similar to hadoop. This was added as part of HiveServer2 feature.We are still missing the command line wrapper to extract that information</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="5460" opendate="2013-10-6 00:00:00" fixdate="2013-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>invalid offsets in lag lead should return an exception (per ISO-SQL)</summary>
      <description>ISO-SQL 2011 defines how lag and lead should behave when invalid offsets are provided to the functions.i.e. select tint.rnum,tint.cint, lag( tint.cint, -100 ) over ( order by tint.rnum) from tint tint Instead of a meaningful error (as other vendors will emit) you get Error: Query returned non-zero code: 2, cause: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTaskSQLState: 08S01ErrorCode: 2</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="5472" opendate="2013-10-6 00:00:00" fixdate="2013-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support a simple scalar which returns the current timestamp</summary>
      <description>ISO-SQL has two forms of functionslocal and current timestamp where the former is a TIMESTAMP WITHOUT TIMEZONE and the latter with TIME ZONEselect cast ( unix_timestamp() as timestamp ) from Timplement a function which computes LOCAL TIMESTAMP which would be the current timestamp for the users session time zone.</description>
      <version>0.11.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5474" opendate="2013-10-6 00:00:00" fixdate="2013-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop table hangs when concurrency=true</summary>
      <description>This is seen in hive 0.12 branch sequential test run. TestThriftHttpCLIService.testExecuteStatementhttps://builds.apache.org/job/Hive-branch-0.12-hadoop1/13/testReport/org.apache.hive.service.cli.thrift/TestThriftHttpCLIService/testExecuteStatement/stderr has "FAILED: Error in acquiring locks: Locks on the underlyingobjects cannot be acquired. retry after some time"</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5486" opendate="2013-10-8 00:00:00" fixdate="2013-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 should create base scratch directories at startup</summary>
      <description>With impersonation enabled, the same base directory is used by all sessions/queries. For a new deployment, this directory gets created on first invocation by the user running that session. This would cause directory permission conflict for other users.HiveServer2 should create the base scratch dirs if it doesn't exist.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5506" opendate="2013-10-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive SPLIT function does not return array correctly</summary>
      <description>Hello all, I think I have outlined a bug in the hive split function:Summary: When calling split on a string of data, it will only return all array items if the the last array item has a value. For example, if I have a string of text delimited by tab with 7 columns, and the first four are filled, but the last three are blank, split will only return a 4 position array. If any number of "middle" columns are empty, but the last item still has a value, then it will return the proper number of columns. This was tested in Hive 0.9 and hive 0.11. Data:(Note \t represents a tab char, \x09 the line endings should be \n (UNIX style) not sure what email will do to them). Basically my data is 7 lines of data with the first 7 letters separated by tab. On some lines I've left out certain letters, but kept the number of tabs exactly the same. input.txta\tb\tc\td\te\tf\tga\tb\tc\td\te\t\tga\tb\t\td\t\tf\tg\t\t\td\te\tf\tga\tb\tc\td\t\t\ta\t\t\t\te\tf\tga\t\t\td\t\t\tgI then created a table with one column from that data:DROP TABLE tmp_jo_tab_test;CREATE table tmp_jo_tab_test (message_line STRING)STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '/tmp/input.txt'OVERWRITE INTO TABLE tmp_jo_tab_test;Ok just to validate I created a python counting script:#!/usr/bin/pythonimport sysfor line in sys.stdin: line = line&amp;#91;0:-1&amp;#93; out = line.split("\t") print len(out)The output there is : $ cat input.txt |./cnt_tabs.py7777777Based on that information, split on tab should return me 7 for each line as well:hive -e "select size(split(message_line, 't')) from tmp_jo_tab_test;"7777477However it does not. It would appear that the line where only the first four letters are filled in(and blank is passed in on the last three) only returns 4 splits, where there should technically be 7, 4 for letters included, and three blanks. a\tb\tc\td\t\t\t</description>
      <version>0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="5531" opendate="2013-10-13 00:00:00" fixdate="2013-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiverserver2 doesn&amp;#39;t honor command line argument when initializing log4j</summary>
      <description>The reason is in the main function of hiveserver2, the log4j is initialized before processing the arguments to the function</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ServerOptionsProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="5535" opendate="2013-10-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Webhcat e2e test JOBS_2 fail due to permission when hdfs umask setting is 022</summary>
      <description>Complaining no permission to output directory "/tmp/templeton_test_out/$runid". This is because /tmp/templeton_test_out/runid is created with umask 022 with user "test.other.user.name" (the userid of the first test in the group JOBS_1). Other user cannot write to it (JOBS_2, which run as userid "test.user.name")NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="5536" opendate="2013-10-14 00:00:00" fixdate="2013-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect Operation Name is passed to hookcontext</summary>
      <description>HS2 passes incorrect operation name to hookcontext.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.hooks.TestHs2Hooks.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.java</file>
    </fixedFiles>
  </bug>
  <bug id="5545" opendate="2013-10-15 00:00:00" fixdate="2013-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatRecord getInteger method returns String when used on Partition columns of type INT</summary>
      <description>HCatRecord getInteger method returns String when used on Partition columns of type INT.java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer</description>
      <version>0.11.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatRecordReader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5548" opendate="2013-10-15 00:00:00" fixdate="2013-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests under common directory don&amp;#39;t run as part of &amp;#39;ant test&amp;#39;</summary>
      <description>All tests, including TestHiveVarcharl, under /common don't get run as part of "ant test". As part of decimal precision/scale support, more tests will be added, including TestHiveDecimal.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.build.xml</file>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5550" opendate="2013-10-15 00:00:00" fixdate="2013-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import fails for tables created with default text, sequence and orc file formats using HCatalog API</summary>
      <description>A table was created using HCatalog API with out specifying the file format, it defaults to:fileFormat=TextFile, inputformat=org.apache.hadoop.mapred.TextInputFormat, outputformat=org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormatBut, when hive fetches the table from the metastore, it strangely replaces the output format with org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormatand the comparison between source and target table fails.The code in org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer#checkTable does a string comparison of classes and fails. // check IF/OF/Serde String existingifc = table.getInputFormatClass().getName(); String importedifc = tableDesc.getInputFormat(); String existingofc = table.getOutputFormatClass().getName(); String importedofc = tableDesc.getOutputFormat(); if ((!existingifc.equals(importedifc)) || (!existingofc.equals(importedofc))) { throw new SemanticException( ErrorMsg.INCOMPATIBLE_SCHEMA .getMsg(" Table inputformat/outputformats do not match")); }This only affects tables with text and sequence file formats but not rc or orc.</description>
      <version>0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hcatalog.api.HCatCreateTableDesc.java</file>
    </fixedFiles>
  </bug>
  <bug id="5551" opendate="2013-10-15 00:00:00" fixdate="2013-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create perf logger statements for orc init/split creation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="5553" opendate="2013-10-15 00:00:00" fixdate="2013-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix vertex start logging for tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5561" opendate="2013-10-16 00:00:00" fixdate="2013-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear work map for container reuse on tez</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5595" opendate="2013-10-19 00:00:00" fixdate="2013-1-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized SMB JOIN</summary>
      <description>Vectorized implementation of SMB Map Join.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="567" opendate="2009-6-19 00:00:00" fixdate="2009-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc: integrate hive with pentaho report designer</summary>
      <description>Instead of trying to get a complete implementation of jdbc, its probably more useful to pick reporting/analytics software out there and implement the jdbc methods necessary to get them working. This jira is a first attempt at this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5670" opendate="2013-10-28 00:00:00" fixdate="2013-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>annoying ZK exceptions are annoying</summary>
      <description>when I run tests locally (or on cluster IIRC) there are bunch of ZK-related exceptions in Hive log, such as2013-10-28 09:50:50,851 ERROR zookeeper.ClientCnxn (ClientCnxn.java:processEvent(523)) - Error while calling watcher java.lang.NullPointerException at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497) 2013-10-28 09:51:05,747 DEBUG server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1024)) - ignoring exception during input shutdownjava.net.SocketException: Socket is not connected at sun.nio.ch.SocketChannelImpl.shutdown(Native Method) at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:633) at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360) at org.apache.zookeeper.server.NIOServerCnxn.closeSock(NIOServerCnxn.java:1020) at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:977) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:347) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:224) at java.lang.Thread.run(Thread.java:680)They are annoying when you look for actual problems in logs.Those on DEBUG level should be silenced via log levels for ZK classes by default. Not sure what to do with ERROR level one(s?), I'd need to look if they can be silenced/logged as DEBUG on hive side, or maybe file a bug for ZK...</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.log4j.properties</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">data.conf.hive-log4j.properties</file>
      <file type="M">common.src.test.resources.hive-log4j-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j-test.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5849" opendate="2013-11-19 00:00:00" fixdate="2013-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the stats of operators based on heuristics in the absence of any column statistics</summary>
      <description>In the absence of any column statistics, operators will simply use the statistics from its parents. It is useful to apply some heuristics to update basic statistics (number of rows and data size) in the absence of any column statistics. This will be worst case scenario.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby.q</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5889" opendate="2013-11-26 00:00:00" fixdate="2013-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add counter based stats aggregator for tez</summary>
      <description>In tez counters are retrieved with different API. Need to add a separate implementation of the recently added counter based stats aggregator.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug id="5946" opendate="2013-12-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6024" opendate="2013-12-12 00:00:00" fixdate="2013-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data local inpath unnecessarily creates a copy task</summary>
      <description>Load data command creates an additional copy task only when its loading from local It doesn't create this additional copy task while loading from DFS though.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
    </fixedFiles>
  </bug>
  <bug id="61" opendate="2008-11-13 00:00:00" fixdate="2008-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implment ORDER BY</summary>
      <description>ORDER BY is in the query language reference but currently is a no-op. We should make it an op.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6174" opendate="2014-1-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline "set varible" doesn&amp;#39;t show the value of the variable as Hive CLI</summary>
      <description>Currently it displays nothing.0: jdbc:hive2://&gt; set env:TERM; 0: jdbc:hive2://&gt; In contrast, Hive CLI displays the value of the variable.hive&gt; set env:TERM; env:TERM=xterm</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="62" opendate="2008-11-13 00:00:00" fixdate="2008-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to describe nested types</summary>
      <description>Add ability to describe nested types.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6200" opendate="2014-1-15 00:00:00" fixdate="2014-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive custom SerDe cannot load DLL added by "ADD FILE" command</summary>
      <description>When custom SerDe need to load a DLL file added using "ADD FILE" command in HIVE, the loading fail with exception like "java.lang.UnsatisfiedLinkError:C:\tmp\admin2_6996@headnode0_201401100431_resources\hello.dll: Access is denied". The reason is when FileSystem creating local copy of the file, the permission of local file is set to default as "666". DLL file need "execute" permission to be loaded successfully.Similar scenario also happens when Hadoop localize files in distributed cache. The solution in Hadoop is to add "execute" permission to the file after localizationl.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="6221" opendate="2014-1-17 00:00:00" fixdate="2014-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize stats based queries in presence of filter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6338" opendate="2014-1-30 00:00:00" fixdate="2014-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception handling in createDefaultDb() in Metastore</summary>
      <description>There is a suggestion on HIVE-5959 comment list on possible improvements.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="6342" opendate="2014-1-30 00:00:00" fixdate="2014-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive drop partitions should use standard expr filter instead of some custom class</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.filter.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.drop.partition.filter.failure2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6345" opendate="2014-1-31 00:00:00" fixdate="2014-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DECIMAL support to vectorized JOIN operators</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.UnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6389" opendate="2014-2-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.</summary>
      <description>RCFile tables that use the LazyBinaryColumnarSerDe don't seem to handle look-ups into map-columns when the value of the column is null.When an RCFile table is created with LazyBinaryColumnarSerDe (as is default in 0.12), and queried as follows:select mymap['1024'] from mytable;and if the mymap column has nulls, then one is treated to the following guttural utterance:2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":null,"mymap":null,"isnull":null} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41) at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524) ... 10 moreA patch is on the way, but the short of it is that the LazyBinaryMapOI needs to return nulls if either the map or the lookup-key is null.This is handled correctly for Text data, and for RCFiles using ColumnarSerDe.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="645" opendate="2009-7-16 00:00:00" fixdate="2009-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A UDF that can export data to JDBC databases.</summary>
      <description>A UDF that can export data to JDBC databases.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6486" opendate="2014-2-22 00:00:00" fixdate="2014-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support secure Subject.doAs() in HiveServer2 JDBC client.</summary>
      <description>HIVE-5155 addresses the problem of kerberos authentication in multi-user middleware server using proxy user. In this mode the principal used by the middle ware server has privileges to impersonate selected users in Hive/Hadoop. This enhancement is to support Subject.doAs() authentication in Hive JDBC layer so that the end users Kerberos Subject is passed through in the middle ware server. With this improvement there won't be any additional setup in the server to grant proxy privileges to some users and there won't be need to specify a proxy user in the JDBC client. This version should also be more secure since it won't require principals with the privileges to impersonate other users in Hive/Hadoop setup.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.KerberosSaslHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="6546" opendate="2014-3-4 00:00:00" fixdate="2014-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat job submission for pig with -useHCatalog argument fails on Windows</summary>
      <description>On a one-box windows setup, do the following from a powershell prompt:cmd /c curl.exe -s ` -d user.name=hadoop ` -d arg=-useHCatalog ` -d execute="emp = load '/data/emp/emp_0.dat'; dump emp;" ` -d statusdir="/tmp/webhcat.output01" ` 'http://localhost:50111/templeton/v1/pig' -vThe job fails with error code 7, but it should run. I traced this down to the following. In the job configuration for the TempletonJobController, we have templeton.args set tocmd,/c,call,C:\\hadoop\\\\pig-0.11.0.1.3.0.0-0846/bin/pig.cmd,-D_WEBHCAT_TOKEN_FILE_LOCATION_="-useHCatalog",-execute,"emp = load '/data/emp/emp_0.dat'; dump emp;"Notice the = sign before "-useHCatalog". I think this should be a comma.The bad string D_WEBHCAT_TOKEN_FILE_LOCATION_="-useHCatalog" gets created in org.apache.hadoop.util.GenericOptionsParser.preProcessForWindows().It happens at line 434: } else { if (i &lt; args.length - 1) { prop += "=" + args[++i]; // RIGHT HERE! at iterations i = 37, 38 } }Bug is here: if (prop != null) { if (prop.contains("=")) { // -D__WEBHCAT_TOKEN_FILE_LOCATION__ does not contain equal, so else branch is run and appends ="-useHCatalog", // everything good } else { if (i &lt; args.length - 1) { prop += "=" + args[++i]; } } newArgs.add(prop); }One possible fix is to change the string constant org.apache.hcatalog.templeton.tool.TempletonControllerJob.TOKEN_FILE_ARG_PLACEHOLDER to have an "=" sign in it. Or, preProcessForWindows() itself could be changed.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6561" opendate="2014-3-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should accept -i option to Initializing a SQL file</summary>
      <description>Hive CLI has -i option. From Hive CLI help:... -i &lt;filename&gt; Initialization SQL file...However, Beeline has no such option:xzhang@xzlt:~/apa/hive3$ ./packaging/target/apache-hive-0.14.0-SNAPSHOT-bin/apache-hive-0.14.0-SNAPSHOT-bin/bin/beeline -u jdbc:hive2:// -i hive.rc...Connected to: Apache Hive (version 0.14.0-SNAPSHOT)Driver: Hive JDBC (version 0.14.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ-i (No such file or directory)Property "url" is requiredBeeline version 0.14.0-SNAPSHOT by Apache Hive...</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="6591" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Importing a table containing hidden dirs fails</summary>
      <description>hidden files should be ignored while exporting</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6609" opendate="2014-3-10 00:00:00" fixdate="2014-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doing Ctrl-C on hive cli doesn&amp;#39;t kill running MR jobs on hadoop-2</summary>
      <description>This is because url based job killing which we use doesn't work on hadoop2. We need to use java api.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="6633" opendate="2014-3-12 00:00:00" fixdate="2014-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pig -useHCatalog with embedded metastore fails to pass command line args to metastore</summary>
      <description>This fails because the embedded metastore can't connect to the database because the command line -D arguments passed to pig are not getting passed to the metastore when the embedded metastore is created. Using hive.metastore.uris set to the empty string causes creation of an embedded metastore.pig -useHCatalog "-Dhive.metastore.uris=" "-Djavax.jdo.option.ConnectionPassword=AzureSQLDBXYZ"The goal is to allow a pig job submitted via WebHCat to specify a metastore to use via job arguments. That is not working because it is not possible to pass Djavax.jdo.option.ConnectionPassword and other necessary arguments to the embedded metastore.</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hcatalog.pig.PigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hcatalog.pig.HCatLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="6681" opendate="2014-3-16 00:00:00" fixdate="2014-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table sometimes shows "from deserializer" for column comments</summary>
      <description></description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.context.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.varchar.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.varchar.nested.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.counter.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.counter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.tblproperty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.date2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.vectorization.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatdir.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lb.fs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.import.exported.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.ignore.protection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.pretty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.custom.input.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.nested.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.colserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.bincolserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats2.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.stats.empty.partition.q.out</file>
      <file type="M">itests.test-serde.src.main.java.org.apache.hadoop.hive.serde2.TestSerDe.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.2columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidcolname.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidtype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.partialscan.autogether.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="6689" opendate="2014-3-17 00:00:00" fixdate="2014-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to not display partition columns separately in describe table output</summary>
      <description>In ancient Hive partition columns were not displayed differently, in newer version they are displayed differently. This has resulted in backward incompatible change for upgrade scenarios.</description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6694" opendate="2014-3-18 00:00:00" fixdate="2014-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should provide a way to execute shell command as Hive CLI does</summary>
      <description>Hive CLI allows a user to execute a shell command using ! notation. For instance, !cat myfile.txt. Being able to execute shell command may be important for some users. As a replacement, however, Beeline provides no such capability, possibly because ! notation is reserved for SQLLine commands. It's possible to provide this using a slightly syntactic variation such as !sh cat myfilie.txt.</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="6711" opendate="2014-3-20 00:00:00" fixdate="2014-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC maps uses getMapSize() from MapOI which is unreliable</summary>
      <description>HIVE-6707 had issues with map size. getMapSize() of LazyMap and LazyBinaryMap does not deserialize the keys and count the number of unique keys. Since getMapSize() may return non-distinct count of keys, the length of maps stored using ORC's map tree writer will not be in sync with actual map size. As a result of this RLE reader will try to read beyond the disk range expecting more map entries and will throw exception.Stack trace will look like:Caused by: java.io.EOFException: Read past end of RLE integer from compressed stream Stream for column 2 kind DATA position: 22059699 length: 22059699 range: 0 offset: 22359014 limit: 22359014 range 0 = 0 to 22059699 uncompressed: 53370 to 53370 at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54) at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:301) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringDictionaryTreeReader.next(RecordReaderImpl.java:1572) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringTreeReader.next(RecordReaderImpl.java:1330) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$MapTreeReader.next(RecordReaderImpl.java:2041) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1772) at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2963) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:121)</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6758" opendate="2014-3-26 00:00:00" fixdate="2014-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesn&amp;#39;t work with -e option when started in background</summary>
      <description>In hive CLI you could easily integrate its use into a script and back ground the process like this: hive -e "some query" &amp;Beeline does not run when you do the same even with the -f switch.</description>
      <version>0.11.0,0.14.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  <bug id="6759" opendate="2014-3-26 00:00:00" fixdate="2014-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix reading partial ORC files while they are being written</summary>
      <description>HDFS with the hflush ensures the bytes are visible, but doesn't update the file length on the NameNode. Currently the Orc reader will only read up to the length on the NameNode. If the user specified a length from a flush_length file, the Orc reader should trust it to be right.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="6843" opendate="2014-4-4 00:00:00" fixdate="2014-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>INSTR for UTF-8 returns incorrect position</summary>
      <description></description>
      <version>0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestGenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="6903" opendate="2014-4-14 00:00:00" fixdate="2014-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of hive.metastore.execute.setugi to true</summary>
      <description>Since its introduction in HIVE-2616 I havent seen any bug reported for it, only grief from users who expect system to work as if this is true by default.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="70" opendate="2008-11-18 00:00:00" fixdate="2008-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>provide option to limit standard error output by user scripts</summary>
      <description>runaway user scripts emitting every row to standard error overwhelm our log partitions. We need to provide an option to limit standard error size per task.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="700" opendate="2009-7-28 00:00:00" fixdate="2009-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test error by adding "DROP FUNCTION"</summary>
      <description>Since we added "Show Functions" in HIVE-580, test results will depend on what temporary functions are added to the system.We should add the capability of "DROP FUNCTION", and do that at the end of those "create function" tests to make sure the "show functions" results are deterministic.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.udaf.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.genericudf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FunctionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7000" opendate="2014-5-1 00:00:00" fixdate="2014-8-1 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Several issues with javadoc generation</summary>
      <description>1.Ran 'mvn javadoc:javadoc -Phadoop-2'. Encountered several issues Generated classes are included in the javadoc generation fails in the top level hcatalog folder because its src folder contains no java files.Patch attached to fix these issues.2.Tried mvn javadoc:aggregate -Phadoop-2 cannot get an aggregated javadoc for all of hive tried setting 'aggregate' parameter to true. Didn't workThere are several questions in StackOverflow about multiple project javadoc. Seems like this is broken.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7071" opendate="2014-5-15 00:00:00" fixdate="2014-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use custom Tez split generator to support schema evolution</summary>
      <description>Right now we're falling back to combinehivefileinputformat and switch of am side grouping when there's different schemata in a single vertex. We need to handle this in a custom initializer so we can still group on the AM.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug id="7169" opendate="2014-6-2 00:00:00" fixdate="2014-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in Http Mode should have a configurable IdleMaxTime timeout</summary>
      <description>Currently, in HiveServer2 we use Jetty Server to start the Http Server. The connector used for this Thrift Http Cli Service has maximum idle time as the default timeout as mentioned in http://grepcode.com/file/repo1.maven.org/maven2/org.eclipse.jetty/jetty-server/7.0.0.v20091005/org/eclipse/jetty/server/AbstractConnector.java#AbstractConnector.0_maxIdleTime.This should be manually configurable using connector.setMaxIdleTime(maxIdleTime);</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7255" opendate="2014-6-19 00:00:00" fixdate="2014-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow partial partition spec in analyze command</summary>
      <description>So that stats collection can happen for multiple partitions through one statement.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.invalid.values.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.incorrect.num.keys.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.incorrect.num.keys.q</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.dp.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="7429" opendate="2014-7-16 00:00:00" fixdate="2014-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set replication for archive called before file exists</summary>
      <description>The call to set replication is called prior to uploading the archive file to hdfs, which does not throw an error, but the replication never gets set.This has a significant impact on large jobs (especially hash joins) due to too many tasks hitting the data nodes.</description>
      <version>0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="743" opendate="2009-8-8 00:00:00" fixdate="2009-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>let user specify serde for custom scripts</summary>
      <description>Splitting up https://issues.apache.org/jira/browse/HIVE-708 into this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7634" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Configuration.getPassword() if available to eliminate passwords from hive-site.xml</summary>
      <description>HADOOP-10607 provides a Configuration.getPassword() API that allows passwords to be retrieved from a configured credential provider, while also being able to fall back to the HiveConf setting if no provider is set up. Hive should use this API for versions of Hadoop that support this API. This would give users the ability to remove the passwords from their Hive configuration files.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7848" opendate="2014-8-22 00:00:00" fixdate="2014-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refresh SparkContext when spark configuration changes [Spark Branch]</summary>
      <description>Recreate the spark client if spark configurations are updated (through set command).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="8006" opendate="2014-9-5 00:00:00" fixdate="2014-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO Trunk Merge: Test fail that includes Table Sample, rows(), query hints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8428" opendate="2014-10-10 00:00:00" fixdate="2014-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PCR doesnt remove filters involving casts</summary>
      <description>e.g.,select key,value from srcpart where hr = cast(11 as double);</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.pcr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.vectorization.ppd.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.decimal.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="8429" opendate="2014-10-10 00:00:00" fixdate="2014-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add records in/out counters</summary>
      <description>We don't do counters for input/output records right now. That would help for debugging though (if it can be done with minimal overhead).</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="8435" opendate="2014-10-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add identity project remover optimization</summary>
      <description></description>
      <version>0.9.0,0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="8436" opendate="2014-10-12 00:00:00" fixdate="2014-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify SparkWork to split works with multiple child works [Spark Branch]</summary>
      <description>Based on the design doc, we need to split the operator tree of a work in SparkWork if the work is connected to multiple child works. The way splitting the operator tree is performed by cloning the original work and removing unwanted branches in the operator tree. Please refer to the design doc for details.This process should be done right before we generate SparkPlan. We should have a utility method that takes the orignal SparkWork and return a modified SparkWork.This process should also keep the information about the original work and its clones. Such information will be needed during SparkPlan generation (HIVE-8437).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkTableScanProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMultiInsertionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMergeTaskProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8732" opendate="2014-11-4 00:00:00" fixdate="2014-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC string statistics are not merged correctly</summary>
      <description>Currently ORC's string statistics do not merge correctly causing incorrect maximum values.</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="8746" opendate="2014-11-5 00:00:00" fixdate="2014-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC timestamp columns are sensitive to daylight savings time</summary>
      <description>Hive uses Java's Timestamp class to manipulate timestamp columns. Unfortunately the textual parsing in Timestamp is done in local time and the internal storage is in UTC.ORC mostly side steps this issue by storing the difference between the time and a base time also in local and storing that difference in the file. Reading the file between timezones will mostly work correctly "2014-01-01 12:34:56" will read correctly in every timezone.However, when moving between timezones with different daylight saving it creates trouble. In particular, moving from a computer in PST to UTC will read "2014-06-06 12:34:56" as "2014-06-06 11:34:56".</description>
      <version>0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.static.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.resources.orc-file-has-null.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-dictionary-threshold.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter2.out</file>
      <file type="M">ql.src.test.resources.orc-file-dump-bloomfilter.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="8847" opendate="2014-11-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bugs in jenkins scripts</summary>
      <description>1) Incorrect help message in process_jira function2) Spark builds do not work3) Build "profiles" (which map to a properties file) are hard coded4) A JIRA is required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="9593" opendate="2015-2-5 00:00:00" fixdate="2015-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC Reader should ignore unknown metadata streams</summary>
      <description>ORC readers should ignore metadata streams which are non-essential additions to the main data streams.This will include additional indices, histograms or anything we add as an optional stream.</description>
      <version>0.11.0,0.12.0,0.13.1,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
