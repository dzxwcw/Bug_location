<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="10568" opendate="2015-5-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="11731" opendate="2015-9-4 00:00:00" fixdate="2015-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude hbase-metastore in itests for hadoop-1</summary>
      <description>This is a follow up of HIVE-11694. We need to further exclude hbase-metastore for hadoop-1 in itests.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.HBaseIntegrationTests.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14001" opendate="2016-6-13 00:00:00" fixdate="2016-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline doesn&amp;#39;t give out an error when takes either "-e" or "-f" in command instead of both</summary>
      <description>When providing both arguments there should be an error message</description>
      <version>0.10.0,2.0.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="2055" opendate="2011-3-15 00:00:00" fixdate="2011-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should add HBase classpath dependencies when available</summary>
      <description>Created an external table in hive , which points to the HBase table. When tried to query a column using the column name in select clause got the following exception : ( java.lang.ClassNotFoundException: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat), errorCode:12, SQLState:42000)</description>
      <version>0.10.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="20550" opendate="2018-9-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch WebHCat to use beeline to submit Hive queries</summary>
      <description>Since hive cli is deprecated, we shall switch WebHCat to use beeline instead.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug id="2101" opendate="2011-4-8 00:00:00" fixdate="2011-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin sometimes gives wrong results if there is a filter in the on condition</summary>
      <description>"SELECT / * + mapjoin(src1, src2) * / * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key &lt; 10 AND src2.key &gt; 10) JOIN src src3 ON (src2.key = src3.key AND src3.key &lt; 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;" will give wrong results in today's hive</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23020" opendate="2020-3-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid using _files for replication data copy during incremental run</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CommitTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.CopyUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestMetaStoreEventListenerInRepl.java</file>
    </fixedFiles>
  </bug>
  <bug id="23023" opendate="2020-3-13 00:00:00" fixdate="2020-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR compaction ignores column schema evolution</summary>
      <description>Repro:create table compaction_error(i int) partitioned by (`part1` string) stored as orc TBLPROPERTIES ('transactional'='true');insert into table compaction_error values (1, 'aa');ALTER TABLE compaction_error ADD COLUMNS (newcol string);insert into table compaction_error values (2, 2000, 'aa');alter table compaction_error partition (part1='aa') compact 'minor'; --or majordata row will look like:1, NULL, 'aa'2, NULL, 'aa'</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
    </fixedFiles>
  </bug>
  <bug id="2519" opendate="2011-10-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema</summary>
      <description>Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:insert overwrite ... partition (p2="...", p1);which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="255" opendate="2009-1-28 00:00:00" fixdate="2009-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>User name is not propagated correctly when a hive server or a web client is issuing hive queries.</summary>
      <description>Hive should use the parameter set in hadoop.job.ugi config value instead of user.name which gets overwritten during job submission. Otherwise clients have to set both user.name and hadoop.job.ugi which is clunky.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2552" opendate="2011-11-4 00:00:00" fixdate="2011-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Omit incomplete Postgres upgrade scripts from release tarball</summary>
      <description>The Postgres metastore upgrade scripts are not officially supported, and at this point are incomplete. We should not include them in the release artifacts.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2553" opendate="2011-11-5 00:00:00" fixdate="2011-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use hashing instead of list traversal for IN operator for primitive types</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2732" opendate="2012-1-21 00:00:00" fixdate="2012-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce Sink deduplication fails if the child reduce sink is followed by a join</summary>
      <description>set hive.optimize.reducededuplication=true;set hive.auto.convert.join=true;explain select * from (select * from src distribute by key sort by key) a join src b on a.key = b.key;fails with the following exceptionjava.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.SelectOperator cannot be cast to org.apache.hadoop.hive.ql.exec.ReduceSinkOperator at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertMapJoin(MapJoinProcessor.java:313) at org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(MapJoinProcessor.java:226) at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver$CommonJoinTaskDispatcher.processCurrentTask(CommonJoinResolver.java:174) at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver$CommonJoinTaskDispatcher.dispatch(CommonJoinResolver.java:287) at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111) at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194) at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139) at org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(CommonJoinResolver.java:68) at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:72) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapRedTasks(SemanticAnalyzer.java:7019) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7312) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:243) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:889) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)If hive.auto.convert.join is set to false, it produces an incorrect plan where the two halves of the join are processed in two separate map reduce tasks, and the reducers of these two tasks both contain the join operator resulting in an exception.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  <bug id="2905" opendate="2012-3-26 00:00:00" fixdate="2012-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Desc table can&amp;#39;t show non-ascii comments</summary>
      <description>When desc a table with command line or hive jdbc way, the table's comment can't be read.1. I have updated javax.jdo.option.ConnectionURL parameter in hive-site.xml file. jdbc:mysql://...:3306/hive?characterEncoding=UTF-82. In mysql database, the comment field of COLUMNS table can be read normally.</description>
      <version>0.7.0,0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2911" opendate="2012-3-28 00:00:00" fixdate="2012-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move global .hiverc file</summary>
      <description>Currently, the .hiverc files are loaded from:$HIVE_HOME/bin/.hiverc~/.hivercIt seems more ops-friendly to have it in the config directory.$HIVE_HOME/bin/.hiverc &lt;- for backwards compatibility$HIVE_CONF_DIR/.hiverc~/.hiverc</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2979" opendate="2012-4-24 00:00:00" fixdate="2012-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement INCLUDE_HADOOP_MAJOR_VERSION test macro</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.split.sample.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.corrupt.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.split.sample.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample.islocalmode.hook.q</file>
      <file type="M">ql.src.test.queries.clientpositive.combine2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.archive.corrupt.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2990" opendate="2012-4-30 00:00:00" fixdate="2012-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hadoop-source Ivy resolvers and Ant targets</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="30" opendate="2008-10-24 00:00:00" fixdate="2008-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive web interface</summary>
      <description>Hive needs a web interface. The initial checkin should have: simple schema browsing query submission query history (similar to MySQL's SHOW PROCESSLIST)A suggested feature: the ability to have a query notify the user when it's completed.Edward Capriolo has expressed some interest in driving this process.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3002" opendate="2012-5-4 00:00:00" fixdate="2012-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2986</summary>
      <description>Given the amount of push back, reverting this patch pending further changes/review seems like a good idea.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.stats.HiveStatsMetricsPublisher.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.Triple.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.SuggestionPrintingHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.StartFinishHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.SplitSizeHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.SMCStatsDBHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.SmcConfigHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.SmcConfigDriverRunHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.SampleConcurrencyHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.ReplicationHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.RegressionTestHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.QueryPlanHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.QueryDroppedPartitionsHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.PyRulesHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.Pair.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.LineageHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.JobTrackerHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.JobStatsHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.HookUtils.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.HiveConfigLoggingHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.FifoPoolHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.FbUpdateInputAccessTimeHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.ExternalInputsHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.CreateTableChangeDFSHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.ConnectionUrlFactory.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.conf.FBHiveConf.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.ConfUrlFactory.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.CheckRetentionsHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.CheckArchivedDataHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.BaseReplicationHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.AuditLocalModeHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.AuditJoinHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.ArchiverHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.AlterTableRestrictHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.ql.hooks.AbstractSmcConfigHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.metastore.hooks.StatsManager.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.metastore.hooks.MysqlSmcHook.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.metastore.hooks.FbhiveAlterHandler.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.metastore.hooks.CounterMetaStoreEndFunctionListener.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.metastore.hooks.AuditMetaStoreEventListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="3018" opendate="2012-5-12 00:00:00" fixdate="2012-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the new header for RC Files introduced in HIVE-2711 optional</summary>
      <description>HIVE-2711 introduced a new header for RC files. This breaks a number of things, for example, copying data from a cluster running a post-HIVE-2711 version of Hive to a cluster running a pre-HIVE-2711 version of Hive.Make this optional.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3051" opendate="2012-5-24 00:00:00" fixdate="2012-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC cannot find metadata for tables/columns containing uppercase character</summary>
      <description>create table TEST_TABLE ( ... );...ResultSet rs = databaseMetaData.getColumns(null, null, "TEST_TABLE", null); // emptyTrivial, but hive shell or thrift client accepts above use-case by converting strings to lower case. This should be consistent with JDBC.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="3056" opendate="2012-5-25 00:00:00" fixdate="2012-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a new metastore tool to bulk update location field in Db/Table/Partition records</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3076" opendate="2012-6-1 00:00:00" fixdate="2012-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop partition does not work for non-partition columns</summary>
      <description>There is still a problem in case there is a mixture of string and non-string partition columns.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3079" opendate="2012-6-1 00:00:00" fixdate="2012-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2989</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.tablelink.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.table.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.create.tablelink.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure5.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TableType.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableIdentifier.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.010-HIVE-2989.mysql.sql</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="3086" opendate="2012-6-5 00:00:00" fixdate="2012-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skewed Join Optimization</summary>
      <description>During a join operation, if one of the columns has a skewed key, it can cause that particular reducer to become the bottleneck. The following feature will address it:https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3090" opendate="2012-6-6 00:00:00" fixdate="2012-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timestamp type values not having nano-second part breaks row</summary>
      <description>Timestamp values are reading additional one byte if nano-sec part is zero, breaking following columns. &gt;create table timestamp_1 (t timestamp, key string, value string);&gt;insert overwrite table timestamp_1 select cast('2011-01-01 01:01:01' as timestamp), key, value from src limit 5;&gt;select t,key,value from timestamp_1;2011-01-01 01:01:01 2382011-01-01 01:01:01 862011-01-01 01:01:01 3112011-01-01 01:01:01 272011-01-01 01:01:01 165&gt;select t,key,value from timestamp_1 distribute by t;2011-01-01 01:01:01 2011-01-01 01:01:01 2011-01-01 01:01:01 2011-01-01 01:01:01 2011-01-01 01:01:01</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
    </fixedFiles>
  </bug>
  <bug id="3092" opendate="2012-6-6 00:00:00" fixdate="2012-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive tests should load Hive classes from build directory, not Ivy cache</summary>
      <description>As discussed in HIVE-895, currently the tests pull in jars for other components rather from Ivy rather than using the built classes and jars in the build directory (bit.ly/LzndQU). This means that absent a very-clean, one is testing against a previous version of the code and cross-component tests are invalid.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3120" opendate="2012-6-12 00:00:00" fixdate="2012-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make copyLocal work for parallel tests</summary>
      <description>It would be very useful if I can test a local patch using theparallel test framework.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="3127" opendate="2012-6-13 00:00:00" fixdate="2012-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass hconf values as XML instead of command line arguments to child JVM</summary>
      <description>The maximum length of the DOS command string is 8191 characters (in Windows latest versions http://support.microsoft.com/kb/830473). This limit will be exceeded easily when it appends individual –hconf values to the command string. To work around this problem, Write all changed hconf values to a temp file and pass the temp file path to the child jvm to read and initialize the -hconf parameters from file.</description>
      <version>0.9.0,0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3128" opendate="2012-6-13 00:00:00" fixdate="2012-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use commons-compress instead of forking tar process</summary>
      <description>TAR tool doesn’t exist by default on windows systems so use the CAB files on windows</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">common.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3134" opendate="2012-6-13 00:00:00" fixdate="2012-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop table/index/database can result in orphaned locations</summary>
      <description>Today when a managed table has a partition with a location which is not a subdirectory of the table's location, when the table is dropped the partition's data is not deleted from HDFS, resulting in an orphaned directory (the data exists but nothing points to it).The same applies to dropping a database with cascade and a table has a location outside the database.I think it is safe to assume managed tables/partitions own the directories they point to, so we should clean these up.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3135" opendate="2012-6-14 00:00:00" fixdate="2012-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option in ptest to run on a single machine</summary>
      <description>There is no need for any sudo in that case</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="3146" opendate="2012-6-15 00:00:00" fixdate="2012-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support external hive tables whose data are stored in Azure blob store/Azure Storage Volumes (ASV)</summary>
      <description>Support external hive tables whose data are stored in Azure blob store/Azure Storage Volumes (ASV)</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3152" opendate="2012-6-18 00:00:00" fixdate="2012-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow certain character patterns in partition names</summary>
      <description>New event listener to allow metastore to reject a partition name if it contains undesired character patterns such as unicode and commas.Match pattern is implemented as a regular expressionModifies append_partition to call a new MetaStorePreventListener implementation, PreAppendPartitionEvent.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MetaDataExportListener.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreCreateTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.ListenerEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AddPartitionEvent.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3202" opendate="2012-6-27 00:00:00" fixdate="2012-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hive command for resetting hive confs</summary>
      <description>For the purpose of optimization we set various configs per query. It's worthy but all those configs should be reset every time for next query.Just simple reset command would make it less painful.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3210" opendate="2012-6-28 00:00:00" fixdate="2012-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Bucketed mapjoin on partitioned table which has two or more partitions</summary>
      <description>Bucketed mapjoin on multiple partition seemed to have no reason to be prohibited and even safer than doing simple mapjoin.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PrunedPartitionList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3219" opendate="2012-6-30 00:00:00" fixdate="2012-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BucketizedHiveInputFormat should be automatically used with SMBJoin</summary>
      <description>For SMBJoin, BucketizedHiveInputFormat should(could?) be used with set command.set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;But this affects all of the MR taks in a query, which might not be intended. With some configuration, it should be automatically configured only for MR tasks which contains SMBJoin.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3247" opendate="2012-7-9 00:00:00" fixdate="2012-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sorted by order of table not respected</summary>
      <description>When a table a sorted by a column or columns, and data is inserted with hive.enforce.sorting=true, regardless of whether the metadata says the table is sorted in ascending or descending order, the data will be sorted in ascending order.e.g.create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS;insert overwrite table table_desc select key, value from src;insert overwrite table table_asc select key, value from src;select * from table_desc;...96 val_9697 val_9797 val_9798 val_9898 val_98select * from table_asc;...96 val_9697 val_9797 val_9798 val_9898 val_98</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3262" opendate="2012-7-16 00:00:00" fixdate="2012-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketed mapjoin silently ignores mapjoin hint</summary>
      <description>If the bucketed mapjoin is not performed, it is silently ignored.Atleast under strict mode, it should lead to an error.Would wait for HIVE-3210 before working on this.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3267" opendate="2012-7-18 00:00:00" fixdate="2012-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>escaped columns in cluster/distribute/order/sort by are not working</summary>
      <description>The following query:select `key`, value from src cluster by `key`, value;fails</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3268" opendate="2012-7-18 00:00:00" fixdate="2012-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>expressions in cluster by are not working</summary>
      <description>The following query fails:select key+key, value from src cluster by key+key, value;</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3273" opendate="2012-7-18 00:00:00" fixdate="2012-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add avro jars into hive execution classpath</summary>
      <description>avro*.jar should be added to hive execution classpath</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3277" opendate="2012-7-19 00:00:00" fixdate="2012-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Metastore audit logging for non-secure connections</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3289" opendate="2012-7-23 00:00:00" fixdate="2012-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sort merge join may not work silently</summary>
      <description>The user does not know, if the sort-merge join is working or not.create table table_asc(key int, value string) CLUSTERED BY (key) SORTED BY (key asc) INTO 1 BUCKETS STORED AS RCFILE; create table table_desc(key int, value string) CLUSTERED BY (key) SORTED BY (key desc) INTO 1 BUCKETS STORED AS RCFILE; set hive.enforce.sorting = true;insert overwrite table table_asc select key, value from src; insert overwrite table table_desc select key, value from src;set hive.optimize.bucketmapjoin = true;set hive.optimize.bucketmapjoin.sortedmerge = true;set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;explain select /+mapjoin(a)/ * from table_asc a join table_desc b on a.key = b.key;select /+mapjoin(a)/ * from table_asc a join table_desc b on a.key = b.key;explainselect /+mapjoin(b)/ * from table_asc a join table_desc b on a.key = b.key;select /+mapjoin(b)/ * from table_asc a join table_desc b on a.key = b.key;In the above test, the sort-merge join is not obeyed as expected.If you user explicitly asked for sort-merge join, and it is not beingobeyed, the operation should fail.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3297" opendate="2012-7-25 00:00:00" fixdate="2012-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>change hive.auto.convert.join&amp;#39;s default value to true</summary>
      <description>For unit tests also, this parameter should be set to true.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3300" opendate="2012-7-25 00:00:00" fixdate="2012-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LOAD DATA INPATH fails if a hdfs file with same name is added to table</summary>
      <description>If we are loading data from local fs to hive tables using 'LOAD DATA LOCAL INPATH' and if a file with the same name exists in the table's location then the new file will be suffixed by *_copy_1.But if we do the 'LOAD DATA INPATH' for a file in hdfs then there is no rename happening but just a move task is getting triggered. Since a file with same name exists in same hdfs location, hadoop fs move operation throws an error.hive&gt; LOAD DATA INPATH '/userdata/bejoy/site.txt' INTO TABLE test.site;Loading data to table test.siteFailed with exception nullFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTaskhive&gt;</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3301" opendate="2012-7-26 00:00:00" fixdate="2012-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix quote printing bug in mapreduce_stack_trace.q testcase failure when running hive on hadoop23</summary>
      <description>When running hive on hadoop0.23, mapreduce_stack_trace.q is failing due to quote printing bug:quote is printed as: '"', instead of "Seems not able to state the bug clearly in html:quote is printed as 'address sign' + 'quot' + semicolonnot the expected 'quote sign'</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="3304" opendate="2012-7-26 00:00:00" fixdate="2012-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sort merge join should work if both the tables are sorted in descending order</summary>
      <description>Currently, sort merge join only works if both the tables are sorted inascending order</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3308" opendate="2012-7-27 00:00:00" fixdate="2012-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mixing avro and snappy gives null values</summary>
      <description>On default hive uses LazySimpleSerDe for output.When I now enable compression and "select count from avrotable" the output is a file with the .avro extension but this then will display null values since the file is in reality not an avro file but a file created by LazySimpleSerDe using compression so should be a .snappy file.This causes any job (exception select * from avrotable is that not truly a job) to show null values.If you use any serde other then avro you can temporarily fix this by setting "set hive.output.file.extension=.snappy" and it will correctly work again but this won't work on avro since it overwrites the hive.output.file.extension during initializing.When you dump the query result into a table with "create table bla as" you can rename the .avro file into .snappy and the "select from bla" will also magiacally work again.Input and Ouput serdes don't always match so when I use avro as an input format it should not set the hive.output.file.extension.Onces it's set all queries will use it and fail making the connection useless to reuse.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="3310" opendate="2012-7-27 00:00:00" fixdate="2012-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Regression] TestMTQueries test is failing on trunk</summary>
      <description>Hudson reported https://builds.apache.org/job/Hive-trunk-h0.21/1571/ this as a regression. Previous build was clean https://builds.apache.org/job/Hive-trunk-h0.21/1570/</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3315" opendate="2012-7-28 00:00:00" fixdate="2012-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagates filters which are on the join condition transitively</summary>
      <description>explain select src1.key from src src1 join src src2 on src1.key=src2.key and src1.key &lt; 100;In this case, filter on join condition src1.key &lt; 100 can be propagated transitively to src2 by src2.key &lt; 100.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.join.nullsafe.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3323" opendate="2012-7-31 00:00:00" fixdate="2012-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enum to string conversions</summary>
      <description>When using serde-reported schemas with the ThriftDeserializer, Enum fields are presented as struct&lt;value:int&gt;Many users expect to work with the string values, which is both easier and more meaningful as the string value communicates what is represented.Hive should provide a mechanism to optionally convert enum values to strings.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3393" opendate="2012-8-17 00:00:00" fixdate="2012-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_json_object and json_tuple should use Jackson library</summary>
      <description>The Jackson library's JSON parsers have been shown to be significantly faster that json.org's. The library is already included, so I can't think of a reason not to use it.There's also the potential for further improvements in replacing many of the try catch blocks with if statements.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFJson.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
    </fixedFiles>
  </bug>
  <bug id="3395" opendate="2012-8-17 00:00:00" fixdate="2012-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>0.23 compatibility: shim job.tracker.address</summary>
      <description>In essence mapred.job.tracker references need to be replaced with yarn.resourcemanager.address else job submission fails.</description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="3396" opendate="2012-8-17 00:00:00" fixdate="2012-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.stats.reliable config causes FileSinkOperator to fail when writing empty file</summary>
      <description>With the configs hive.stats.reliable and hive.stats.autogather are set to true, and using either the HBase or JDBC Stats Publishers, if a FileSinkOperator does not receive any rows, and hence collects no stats, it will throw an exception.Related, if hive.stats.reliable is set to false it will still log a warning which seems unnecessary.Repro:create table tmptable(key string, value string) partitioned by (part string);set hive.stats.autogather=true;set hive.stats.reliable=true;insert overwrite table tmptable partition (part = '1') select * from src where key = 'no_such_value';</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStatsPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="340" opendate="2009-3-11 00:00:00" fixdate="2009-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hive] null pointer exception with nulls in map-side aggregation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3401" opendate="2012-8-22 00:00:00" fixdate="2012-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Diversify grammar for split sampling</summary>
      <description>Current split sampling only supports grammar like TABLESAMPLE(n PERCENT). But some users wants to specify just the size of input. It can be easily calculated with a few commands but it seemed good to support more grammars something like TABLESAMPLE(500M).</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.split.sample.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.split.sample.wrong.format.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.split.sample.out.of.range.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.split.sample.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SplitSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3403" opendate="2012-8-22 00:00:00" fixdate="2012-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>user should not specify mapjoin to perform sort-merge bucketed join</summary>
      <description>Currently, in order to perform a sort merge bucketed join, the user needsto set hive.optimize.bucketmapjoin.sortedmerge to true, and also specify the mapjoin hint.The user should not specify any hints.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientnegative.smb.mapjoin.14.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SMBJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3405" opendate="2012-8-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="3411" opendate="2012-8-28 00:00:00" fixdate="2012-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter predicates on outer join overlapped on single alias is not handled properly</summary>
      <description>Currently, join predicates on outer join are evaluated in join operator (or HashSink for MapJoin) and the result value is tagged to end of each values(as a boolean), which is used for joining values. But when predicates are overlapped on single alias, all the predicates are evaluated with AND conjunction, which makes invalid result. For example with table a with values,100 40100 50100 60Query below has overlapped predicates on alias b, which is making all the values on b are tagged with true(filtered)select * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60);NULL NULL 100 40 NULL NULLNULL NULL 100 50 NULL NULLNULL NULL 100 60 NULL NULL-- Join predicateJoin Operator condition map: Right Outer Join0 to 1 Left Outer Join1 to 2 condition expressions: 0 {VALUE._col0} {VALUE._col1} 1 {VALUE._col0} {VALUE._col1} 2 {VALUE._col0} {VALUE._col1} filter predicates: 0 1 {(VALUE._col1 = 50)} {(VALUE._col1 = 60)} 2 but this should be NULL NULL 100 40 NULL NULL100 50 100 50 NULL NULLNULL NULL 100 60 100 60</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3429" opendate="2012-9-5 00:00:00" fixdate="2012-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket map join involving table with more than 1 partition column causes FileNotFoundException</summary>
      <description>Running a bucket map join exception on a table with more than one partition results in an exception is below. This is because the partition spec is added to the file name, which unintentionally, produces a new subdirectory. &amp;#91;junit&amp;#93; java.io.FileNotFoundException: /Users/kevinwilfong/Documents/hive_driver_start/build/ql/scratchdir/local/hive_2012-09-04_18-35-38_679_3765928822897237252/local-10002/HashTable-Stage-1/MapJoin-b-21(ds=2008-04-08 (No such file or directory) &amp;#91;junit&amp;#93; at java.io.FileInputStream.open(Native Method) &amp;#91;junit&amp;#93; at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:120) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.common.CompressionUtils.tar(CompressionUtils.java:59) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:398) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:135) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1326) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1112) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.Driver.run(Driver.java:945) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:712) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7(TestMinimrCliDriver.java:288) &amp;#91;junit&amp;#93; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) &amp;#91;junit&amp;#93; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) &amp;#91;junit&amp;#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) &amp;#91;junit&amp;#93; at java.lang.reflect.Method.invoke(Method.java:597) &amp;#91;junit&amp;#93; at junit.framework.TestCase.runTest(TestCase.java:168) &amp;#91;junit&amp;#93; at junit.framework.TestCase.runBare(TestCase.java:134) &amp;#91;junit&amp;#93; at junit.framework.TestResult$1.protect(TestResult.java:110) &amp;#91;junit&amp;#93; at junit.framework.TestResult.runProtected(TestResult.java:128) &amp;#91;junit&amp;#93; at junit.framework.TestResult.run(TestResult.java:113) &amp;#91;junit&amp;#93; at junit.framework.TestCase.run(TestCase.java:124) &amp;#91;junit&amp;#93; at junit.framework.TestSuite.runTest(TestSuite.java:232) &amp;#91;junit&amp;#93; at junit.framework.TestSuite.run(TestSuite.java:227) &amp;#91;junit&amp;#93; at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518) &amp;#91;junit&amp;#93; at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052) &amp;#91;junit&amp;#93; at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906) &amp;#91;junit&amp;#93; java.lang.IllegalArgumentException: Can not create a Path from an empty string &amp;#91;junit&amp;#93; at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82) &amp;#91;junit&amp;#93; at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:90) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.Utilities.getHiveJobID(Utilities.java:381) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:194) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:472) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:137) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:135) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1326) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1112) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.Driver.run(Driver.java:945) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:712) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin7(TestMinimrCliDriver.java:288) &amp;#91;junit&amp;#93; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) &amp;#91;junit&amp;#93; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) &amp;#91;junit&amp;#93; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) &amp;#91;junit&amp;#93; at java.lang.reflect.Method.invoke(Method.java:597) &amp;#91;junit&amp;#93; at junit.framework.TestCase.runTest(TestCase.java:168) &amp;#91;junit&amp;#93; at junit.framework.TestCase.runBare(TestCase.java:134) &amp;#91;junit&amp;#93; at junit.framework.TestResult$1.protect(TestResult.java:110) &amp;#91;junit&amp;#93; at junit.framework.TestResult.runProtected(TestResult.java:128) &amp;#91;junit&amp;#93; at junit.framework.TestResult.run(TestResult.java:113) &amp;#91;junit&amp;#93; at junit.framework.TestCase.run(TestCase.java:124) &amp;#91;junit&amp;#93; at junit.framework.TestSuite.runTest(TestSuite.java:232) &amp;#91;junit&amp;#93; at junit.framework.TestSuite.run(TestSuite.java:227) &amp;#91;junit&amp;#93; at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518) &amp;#91;junit&amp;#93; at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052) &amp;#91;junit&amp;#93; at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906) &amp;#91;junit&amp;#93; junit.framework.AssertionFailedError: Client Execution failed with error code = 1</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3431" opendate="2012-9-5 00:00:00" fixdate="2012-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid race conditions while downloading resources from non-local filesystem</summary>
      <description>"add resource &lt;remote-uri&gt;" command downloads the resource file to location specified by conf "hive.downloaded.resources.dir" in local file system. But when the command above is executed concurrently to hive-server for same file, some client fails by VM crash, which is caused by overwritten file by other requests.So there should be a configuration to provide per request location for add resource command, something like "set hiveconf:hive.downloaded.resources.dir=temporary"</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="3432" opendate="2012-9-5 00:00:00" fixdate="2012-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>perform a map-only group by if grouping key matches the sorting properties of the table</summary>
      <description>There should be an option to use bucketizedinputformat and use map-only group by. There would be no need to perform a map-side aggregation.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3435" opendate="2012-9-5 00:00:00" fixdate="2012-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get pdk pluginTest passed when triggered from both builtin tests and pdk tests on hadoop23</summary>
      <description>Hive pdk pluginTest is running twice in unit testing, one is triggered from running builtin tests, another is triggered from running pdk tests.HIVE-3413 fixed pdk pluginTest on hadoop23 when triggered from running builtin tests. While, when triggered from running pdk tests directly on hadoop23, it is failing:Testcase: SELECT tp_rot13('Mixed Up!') FROM onerow; took 6.426 secFAILEDexpected:&lt;[]Zvkrq Hc!&gt; but was:&lt;[2012-09-04 18:13:01,668 WARN &amp;#91;main&amp;#93; conf.HiveConf (HiveConf.java:&lt;clinit&gt;(73)) - hive-site.xml not found on CLASSPATH]Zvkrq Hc!&gt;junit.framework.ComparisonFailure: expected:&lt;[]Zvkrq Hc!&gt; but was:&lt;[2012-09-04 18:13:01,668 WARN &amp;#91;main&amp;#93; conf.HiveConf (HiveConf.java:&lt;clinit&gt;(73)) - hive-site.xml not found on CLASSPATH]Zvkrq Hc!&gt;</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pdk.test-plugin.test.conf.log4j.properties</file>
      <file type="M">pdk.scripts.build-plugin.xml</file>
      <file type="M">pdk.ivy.xml</file>
      <file type="M">pdk.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3436" opendate="2012-9-5 00:00:00" fixdate="2012-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Difference in exception string from native method causes script_pipe.q to fail on windows</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3440" opendate="2012-9-6 00:00:00" fixdate="2012-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix pdk PluginTest failing on trunk-h0.21</summary>
      <description>Get the failure when running on hadoop21, triggered directly from pdk(when triggered from builtin, pdk test is passed).Here is the execution log:2012-09-06 13:46:05,646 WARN mapred.LocalJobRunner (LocalJobRunner.java:run(256)) - job_local_0001java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:354) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88) ... 5 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34) ... 10 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88) ... 13 moreCaused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121) ... 18 moreCaused by: java.lang.NoClassDefFoundError: org/codehaus/jackson/map/ObjectMapper at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.&lt;clinit&gt;(GenericUDTFJSONTuple.java:54) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:532) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:113) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDTF(FunctionRegistry.java:545) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDTF(FunctionRegistry.java:539) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.&lt;clinit&gt;(FunctionRegistry.java:472) at org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(DefaultUDFMethodResolver.java:59) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:154) at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:98) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:137) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:898) at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:924) at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:358) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:434) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:390) at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:166) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:358) at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:441) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:358) at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98) ... 18 moreCaused by: java.lang.ClassNotFoundException: org.codehaus.jackson.map.ObjectMapper at java.net.URLClassLoader$1.run(URLClassLoader.java:217) at java.security.AccessController.doPrivileged(Native Method)</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3443" opendate="2012-9-7 00:00:00" fixdate="2012-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metatool should take serde_param_key from the user to allow for changes to avro serde&amp;#39;s schema url key</summary>
      <description>Hive Metatool should take serde_param_key from the user to allow for chanes to avro serde's schema url key. In the past "avro.schema.url" key used to be called "schema.url".</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaTool.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3459" opendate="2012-9-13 00:00:00" fixdate="2012-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition queries producing no partitions fail with hive.stats.reliable=true</summary>
      <description>Dynamic partition inserts which result in no partitions (either because the input is empty or all input rows are filtered out) will fail because stats cannot be collected if hive.stats.reliable=true.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3464" opendate="2012-9-14 00:00:00" fixdate="2012-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging join tree may reorder joins which could be invalid</summary>
      <description>Currently, hive merges join tree from right to left regardless of join types, which may introduce join reordering. For example,select * from a join a b on a.key=b.key join a c on b.key=c.key join a d on a.key=d.key; Hive tries to merge join tree in a-d=b-d, a-d=a-b, b-c=a-b order and a-d=a-b and b-c=a-b will be merged. Final join tree is "a-(bdc)".With this, ab-d join will be executed prior to ab-c. But if join type of -c and -d is different, this is not valid.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.17.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3471" opendate="2012-9-17 00:00:00" fixdate="2012-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement grouping sets in hive</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3477" opendate="2012-9-18 00:00:00" fixdate="2012-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate data possible with speculative execution for dynamic partitions</summary>
      <description>Consider a query like:insert overwrite T partition (ds)select * from(mapreduce-subq1 union allmapreduce-subq2)x;Once, mapreduce-subq1 and mapreduce-subq2 are done, the task for the unionis invoked. At the end of the union task, jobClose is invoked.Note that there are 2 tablescan operators. The tree is something like:TABLESCAN1 &amp;#8211; \ UNION &amp;#8211; SELECT &amp;#8211; FILESINK /TABLESCAN2 &amp;#8211;In the current setup, jobClose will be invoked twice for FileSink.In case of speculative execution, it is possible that data is still isbeing written to tmp Dir. after jobClose is finished once. The correct fix would be to make sure that jobClose is only invoked once.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3478" opendate="2012-9-18 00:00:00" fixdate="2012-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the specialized logic to handle the file schemas in windows vs unix from build.xml</summary>
      <description>After more testing, I realized that this special check can be removed by changing the “” with “\\\” to work on both platforms</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3479" opendate="2012-9-18 00:00:00" fixdate="2012-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug fix: Return the child JVM exit code to the parent process to handle the error conditions</summary>
      <description>It is a bug in the script and noticed it while fixing some of the Negative CLI test failures</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.hadoop.cmd</file>
    </fixedFiles>
  </bug>
  <bug id="3480" opendate="2012-9-18 00:00:00" fixdate="2012-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&lt;Resource leak&gt;: Fix the file handle leaks in Symbolic &amp; Symlink related input formats.</summary>
      <description>Noticed these file handle leaks while fixing the Symlink related unit test failures on Windows.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SymbolicInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="3486" opendate="2012-9-19 00:00:00" fixdate="2012-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS in database with location on non-default name node fails</summary>
      <description>If a database has a location which is on a different name node than the default database's location, CTAS queries run in that database will fail.This is because the intermediate location which is where the final FileSinkOperator writes to is determined based on the scheme and authority of the value of hive.metastore.warehouse.dir instead of the table's database's location.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3494" opendate="2012-9-20 00:00:00" fixdate="2012-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some of the JDBC test cases are failing on Windows because of the longer class path.</summary>
      <description>If the class path size is more than 8K then we can’t set the environment variable so some of the test cases are failing on Windows. Remove the duplicate JAR entries from the class path to reduce the chance of exceeding the 8K limit.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="350" opendate="2009-3-16 00:00:00" fixdate="2009-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Hive] wrong order in explain plan</summary>
      <description>In case of multiple aggregations, the explain plan might be wrong -the order of aggregations since AbParseInfo maintains the information in a hashmap, which does the guarantee the results to be returned in order</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3512" opendate="2012-9-28 00:00:00" fixdate="2012-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log client IP address with command in metastore&amp;#39;s startFunction method</summary>
      <description>We have the client IP address for metastore commands available in the HMSHandler. It would make determining the source of commands (reads in particular) much easier if the IP address was logged with the command.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3514" opendate="2012-9-28 00:00:00" fixdate="2012-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Partition Pruner so that logic can be reused.</summary>
      <description>Partition Pruner has logic reusable like1. walk through operator tree2. walk through operation tree3. create pruning predicateThe first candidate is list bucketing pruner.Some consideration:1. refactor for general use case not just list bucketing2. avoid over-refactor by focusing on pieces targeted for reuse</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3523" opendate="2012-10-3 00:00:00" fixdate="2012-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive info logging is broken</summary>
      <description>Hive Info logging is broken on trunk. hive -hiveconf hive.root.logger=INFO,console doesn't print the output of LOG.info statements to the console.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3525" opendate="2012-10-3 00:00:00" fixdate="2012-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro Maps with Nullable Values fail with NPE</summary>
      <description>When working against current trunk@1393794, using a backing Avro schema that has a Map field with nullable values causes a NPE on deserialization when the map contains a null value.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3529" opendate="2012-10-4 00:00:00" fixdate="2012-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect partition bucket/sort metadata when overwriting partition with different metadata from table</summary>
      <description>If you have a partition with bucket/sort metadata set, then you alter the table to have different bucket/sort metadata, and insert overwrite the partition with hive.enforce.bucketing=true and/or hive.enforce.sorting=true, the partition data will be bucketed/sorted by the table's metadata, but the partition will have the same metadata.This could result in wrong results.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3531" opendate="2012-10-4 00:00:00" fixdate="2012-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simple lock manager for dedicated hive server</summary>
      <description>In many cases, we uses hive server as a sole proxy for executing all the queries. For that, current default lock manager based on zookeeper seemed a little heavy. Simple in-memory lock manager could be enough.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
    </fixedFiles>
  </bug>
  <bug id="3536" opendate="2012-10-5 00:00:00" fixdate="2012-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Output of sort merge join is no longer bucketed</summary>
      <description>I don't know if this was a feature or a happy coincidence, but before HIVE-3230, the output of a sort merge join on two partitions would be bucketed, even if hive.enforce.bucketing was set to false. This could potentially save a reduce phase when inserting into a bucketed table.This would be good to have back.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="3537" opendate="2012-10-5 00:00:00" fixdate="2012-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>release locks at the end of move tasks</summary>
      <description>Look at HIVE-3106 for details.In order to make sure that concurrency is not an issue for multi-table inserts, the current option is to introduce a dependency task, which therebydelays the creation of all partitions. It would be desirable to release thelocks for the outputs as soon as the move task is completed. That way, formulti-table inserts, the concurrency can be enabled without delaying any table.Currently, the movetask contains a input/output, but they do not seem to bepopulated correctly.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="354" opendate="2009-3-18 00:00:00" fixdate="2009-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hive] udf needed for getting length of a string</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3550" opendate="2012-10-8 00:00:00" fixdate="2012-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Early skipping for limit operator at reduce stage</summary>
      <description>Queries with limit clause whose final result is from reduce stage could not be early exited like map stage. But it seemed possible to just skip remaining lines. It can be accomplished by adding just two lines. Is there any reason we should not to do that?select key from src order by key limit 10;1. currentExtractOperator: 3 forwarded 500 rowsLimitOperator: 4 forwarded 10 rows2. early skippingExtractOperator: 3 forwarded 12 rowsLimitOperator: 4 forwarded 10 rows</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3552" opendate="2012-10-8 00:00:00" fixdate="2012-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-3552 performant manner for performing cubes/rollups/grouping sets for a high number of grouping set keys</summary>
      <description>This is a follow up for HIVE-3433.Had a offline discussion with Sambavi - she pointed out a scenario where theimplementation in HIVE-3433 will not scale. Assume that the user is performinga cube on many columns, say '8' columns. So, each row would generate 256 rowsfor the hash table, which may kill the current group by implementation.A better implementation would be to add an additional mr job - in the first mr job perform the group by assuming there was no cube. Add another mr job, whereyou would perform the cube. The assumption is that the group by would have decreased the output data significantly, and the rows would appear in the order ofgrouping keys which has a higher probability of hitting the hash table.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3562" opendate="2012-10-10 00:00:00" fixdate="2012-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some limit can be pushed down to map stage</summary>
      <description>Queries with limit clause (with reasonable number), for exampleselect * from src order by key limit 10;makes operator tree, TS-SEL-RS-EXT-LIMIT-FSBut LIMIT can be partially calculated in RS, reducing size of shuffling.TS-SEL-RS(TOP-N)-EXT-LIMIT-FS</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExtractOperator.java</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3571" opendate="2012-10-12 00:00:00" fixdate="2012-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a way to run a small unit quickly</summary>
      <description>A simple unit test:ant test -Dtestcase=TestCliDriver -Dqfile=groupby2.qtakes a long time.There should be a quick way to achieve that for debugging.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3573" opendate="2012-10-12 00:00:00" fixdate="2012-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-3268</summary>
      <description>This patch introduces some code which can breaks distribute/order/cluster/sort by. We should revert this code until it can be fixed (HIVE-3572).</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.expr.sortby1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.expr.orderby1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.expr.distributeby.sortby.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.expr.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.expr.clusterby1.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.expr.sortby1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.expr.orderby1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.expr.distributeby.sortby.1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.expr.distributeby1.q</file>
      <file type="M">ql.src.test.queries.clientnegative.expr.clusterby1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3603" opendate="2012-10-21 00:00:00" fixdate="2012-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable client-side caching for scans on HBase</summary>
      <description>HBaseHandler sets up a TableInputFormat MR job against HBase to read data in. The underlying implementation (in HBaseHandler.java) makes an RPC call per row-key, which makes it very inefficient. Need to specify a client side cache size on the scan.Note that HBase currently only supports num-rows based caching (no way to specify a memory limit). Created HBASE-6770 to address this.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="3613" opendate="2012-10-24 00:00:00" fixdate="2012-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement grouping_id function</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.VirtualColumn.java</file>
    </fixedFiles>
  </bug>
  <bug id="3616" opendate="2012-10-25 00:00:00" fixdate="2012-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Paths consistently</summary>
      <description>Currently, we interchangeably use Path, Uri and Strings in various parts of codebases. This may results in subtle bugs. We should consistently use Path in the codebase.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug id="3628" opendate="2012-10-27 00:00:00" fixdate="2012-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a way to use counters in Hive through UDF</summary>
      <description>Currently it is not possible to generate counters through UDF. We should support this. Pig currently allows this.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="3631" opendate="2012-10-29 00:00:00" fixdate="2012-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>script_pipe.q fails when using JDK7</summary>
      <description>Hive Runtime Error while closing operators: Hit error while closing ..The MR job fails on this test. Unfortunately, the exception is not all that helpful.I tracked this down to a class which attempts to close a stream that is already closed. Broken pipe exceptions are caught and not propagated further, but stream closed exception are not caught.</description>
      <version>0.9.1,0.10.0,0.11.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3632" opendate="2012-10-29 00:00:00" fixdate="2012-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade datanucleus to support JDK7</summary>
      <description>I found serious problems with datanucleus code when using JDK7, resulting in some sort of exception being thrown when datanucleus code is entered.I tried source=1.7, target=1.7 with JDK7 as well as source=1.6, target=1.6 with JDK7 and there was no visible difference in that the same unit tests failed.I tried upgrading datanucleus to 3.0.1, as per HIVE-2084.patch, which did not fix the failing tests.I tried upgrading datanucleus to 3.1-release, as per the advise of http://www.datanucleus.org/servlet/jira/browse/NUCENHANCER-86, which suggests using ASMv4 will allow datanucleus to work with JDK7. I was not successful with this either.I tried upgrading datanucleus to 3.1.2. I was not successful with this either.Regarding datanucleus support for JDK7+, there is the following JIRAhttp://www.datanucleus.org/servlet/jira/browse/NUCENHANCER-81which suggests that they don't plan to actively support JDK7+ bytecode any time soon.I also tested the following JVM parameters found onhttp://veerasundar.com/blog/2012/01/java-lang-verifyerror-expecting-a-stackmap-frame-at-branch-target-jdk-7/with no success either.This will become a more serious problem as people move to newer JVMs. If there are other who have solved this issue, please post how this was done. Otherwise, it is a topic that I would like to raise for discussion.Test Properties:CLEAR LIBRARY CACHE</description>
      <version>0.9.1,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3633" opendate="2012-10-29 00:00:00" fixdate="2012-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>sort-merge join does not work with sub-queries</summary>
      <description>Consider the following query:create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 6 BUCKETS STORED AS TEXTFILE;create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 6 BUCKETS STORED AS TEXTFILE;&amp;#8211; load the above tablesset hive.optimize.bucketmapjoin = true;set hive.optimize.bucketmapjoin.sortedmerge = true;set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;explainselect count from(select /+mapjoin(a)/ a.key as key1, b.key as key2, a.value as value1, b.value as value2from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key)subq;The above query does not use sort-merge join. This would be very useful as we automatically convert the queries to use sorting and bucketing properties for join.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SMBJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3640" opendate="2012-10-30 00:00:00" fixdate="2012-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reducer allocation is incorrect if enforce bucketing and mapred.reduce.tasks are both set</summary>
      <description>When I enforce bucketing and fix the number of reducers via mapred.reduce.tasks Hive ignores my input and instead takes the largest value &lt;= hive.exec.reducers.max that is also an even divisor of num_buckets. In other words, if I set 1024 buckets and set mapred.reduce.tasks=1024 I'll get. . . 256 reducers. If I set 1997 buckets and set mapred.reduce.tasks=1997 I'll get. . . 1 reducer. This is totally crazy, and it's far, far crazier when the data inputs get large. In the latter case the bucketing job will almost certainly fail because we'll most likely try to stuff several TB of input through a single reducer. We'll also drastically reduce the effectiveness of bucketing, since the buckets themselves will be larger.If the user sets mapred.reduce.tasks in a query that inserts into a bucketed table we should either accept that value or raise an exception if it's invalid relative to the number of buckets. We should absolutely NOT override the user's direction and fall back on automatically allocating reducers based on some obscure logic dictated by completely different setting. I have yet to encounter a single person who expected this the first time, so it's clearly a bug.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3646" opendate="2012-11-1 00:00:00" fixdate="2012-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;IGNORE PROTECTION&amp;#39; predicate for dropping partitions</summary>
      <description>There are cases where it is desirable to move partitions between clusters. Having to undo protection and then re-protect tables in order to delete partitions from a source are multi-step and can leave us in a failed open state where partition and table metadata is dirty. By implementing an 'rm -rf'-like functionality, we can perform these operations atomically.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3647" opendate="2012-11-1 00:00:00" fixdate="2012-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>map-side groupby wrongly due to HIVE-3432</summary>
      <description>There seems to be a bug due to HIVE-3432.We are converting the group by to a map side group by after only looking atsorting columns. This can give wrong results if the data is sorted andbucketed by different columns.Add some tests for that scenario, verify and fix any issues.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.skew.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3651" opendate="2012-11-1 00:00:00" fixdate="2012-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketmapjoin?.q tests fail with hadoop 0.23</summary>
      <description>The hive.log show error in MR job -Task failed!Task ID: Stage-1The job log has following error -2012-11-01 15:51:20,253 WARN mapred.LocalJobRunner (LocalJobRunner.java:run(479)) - job_local_0001java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory) at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="3658" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to generate the Hbase related unit tests using velocity templates on Windows</summary>
      <description>Requires to escape the “\” on windows to make it compile. So make sure to use the escaped path in the VM templates instead of actual path.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug id="366" opendate="2009-3-25 00:00:00" fixdate="2009-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hive] testparse depends on a value of a static field</summary>
      <description>TestParse depends on the value of "id" which depends on the number of tests run before that</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3661" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the Windows specific “=” related swizzle path changes from Proxy FileSystems</summary>
      <description>Because of this special conversion, Some other unit tests are failing on Windows. After some other investigation, We noticed that “=” is a valid character that can be included in the Windows paths. So I am reverting back “=” related changes from the swizzle path.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="3662" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestHiveServer: testScratchDirShouldClearWhileStartup is failing on Windows</summary>
      <description>Test case is attempting to delete the ScratchDir but it is failing on Windows because one of the subfolders (local scratchdir) in use. So change the location of the local scratch directory.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3663" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to display the MR Job file path on Windows in case of MR job failures.</summary>
      <description>Because of this bunch of CLI negative tests are failing on windows.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="3664" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid to create a symlink for hive-contrib.jar file in dist\lib folder.</summary>
      <description>It forces us to enumerate all the jars except this jar on Windows instead of directly referencing the “dist\lib&amp;#42;.jar” folder in the class path.</description>
      <version>0.9.1,0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3665" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow URIs without port to be specified in metatool</summary>
      <description>Metatool should accept input URIs where one URI contains a port and the other doesn't. While metatool today accepts input URIs without the port when both the input URIs (oldLoc and newLoc) don't contain the port, we should make the tool a little more flexible to allow for the case where one URI contains a valid port and the other input URI doesn't. This makes more sense when transitioning to HA and a user chooses to specify the port as part of the oldLoc, but the port doesn't mean much for the newLoc.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="3673" opendate="2012-11-5 00:00:00" fixdate="2012-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort merge join not used when join columns have different names</summary>
      <description>If two tables are joined on columns with different names, the sort merge join optimization is not applied. E.g.SELECT /*+ MAPJOIN(b) */ * FROM t1 a JOIN t2 b ON a.key = b.value;This will not use sort merge join even if t1 and t2 are bucketed and sorted by key, value respectively.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3679" opendate="2012-11-6 00:00:00" fixdate="2012-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unescape partition names returned by show partitions</summary>
      <description>The partition names returned by the SHOW PARTITIONS call in Hive contain escape sequences from the metastore.ex) SHOW PARTITIONS XXX;returns partition names like...ts=2012-02-02+17%3A12%3A43...where %3A is ':'Hive should unescape escape sequences like that before returning the partition names unless it is configured to return the URL encoded partition name.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0,0.11.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3686" opendate="2012-11-7 00:00:00" fixdate="2012-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix compile errors introduced by the interaction of HIVE-1362 and HIVE-3524</summary>
      <description>HIVE-3524 changed the signature of endFunction in HiveMetastore.java and was committed some hours before HIVE-1362. The change in signature broke the build after HIVE-1362 which still contained the old signature was committed.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3696" opendate="2012-11-9 00:00:00" fixdate="2012-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-3483 which causes performance regression</summary>
      <description>HIVE-3483 causes performance regression.We'd like to revert it first and find another solution for it later.This issue is to track revert and HIVE-3693 tracks subsequent solution.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="3704" opendate="2012-11-12 00:00:00" fixdate="2012-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>name of some metastore scripts are not per convention</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.9.0-to-0.10.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.010-HIVE-3649.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.010-HIVE-3649.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.9.0-to-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.010-HIVE-3649.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.9.0-to-0.10.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.010-HIVE-3649.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="3705" opendate="2012-11-13 00:00:00" fixdate="2012-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding authorization capability to the metastore</summary>
      <description>In an environment where multiple clients access a single metastore, and we want to evolve hive security to a point where it's no longer simply preventing users from shooting their own foot, we need to be able to authorize metastore calls as well, instead of simply performing every metastore api call that's made.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3708" opendate="2012-11-13 00:00:00" fixdate="2012-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add mapreduce workflow information to job configuration</summary>
      <description>Adding workflow properties to the job configuration would enable logging and analysis of workflows in addition to individual MapReduce jobs. Suggested properties include a workflow ID, workflow name, adjacency list connecting nodes in the workflow, and the name of the current node in the workflow.mapreduce.workflow.id - a unique ID for the workflow, ideally prepended with the application namee.g. hive_&lt;hiveQueryId&gt;mapreduce.workflow.name - a name for the workflow, to distinguish this workflow from other workflows and to group different runs of the same workflowe.g. hive query stringmapreduce.workflow.adjacency - an adjacency list for the workflow graph, encoded as mapreduce.workflow.adjacency.&lt;source node&gt; = &lt;comma-separated list of target nodes&gt;mapreduce.workflow.node.name - the name of the node corresponding to this MapReduce job in the workflow adjacency list</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3709" opendate="2012-11-14 00:00:00" fixdate="2012-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stop storing default ConfVars in temp file</summary>
      <description>To work around issues with Hadoop's Configuration object, specifically it's addResource(InputStream), default configurations are written to a temp file (I think HIVE-2362 introduced this).This, however, introduces the problem that once that file is deleted from /tmp the client crashes. This is particularly problematic for long running services like the metastore server.Writing a custom InputStream to deal with the problems in the Configuration object should provide a work around, which does not introduce a time bomb into Hive.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3713" opendate="2012-11-15 00:00:00" fixdate="2012-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore: Sporadic unit test failures</summary>
      <description>For instance: https://builds.apache.org/job/Hive-trunk-h0.21/1792/testReport/org.apache.hadoop.hive.metastore/Found the following issues:testListener: Assumes that a certain tmp database hasn't been created yet, but doesn't enforce ittestSynchronized: Assumes that there's only one database, but doesn't enforce the facttestDatabaseLocation: Fails if the user running the tests is root and doesn't clean up after itself.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3714" opendate="2012-11-16 00:00:00" fixdate="2012-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Patch: Hive&amp;#39;s ivy internal resolvers need to use sourceforge for sqlline</summary>
      <description>While building hive with an internal resolver, ivy fails to resolve sqlline, which needs to be picked up fromhttp://sourceforge.net/projects/sqlline/files/sqlline/1.0.2/sqlline-1_0_2.jar/downloadant package -Dresolvers=internalfails with[ivy:resolve] ==== datanucleus-repo: tried[ivy:resolve] -- artifact sqlline#sqlline#1.0.2;1_0_2!sqlline.jar:[ivy:resolve] http://www.datanucleus.org/downloads/maven2/sqlline/sqlline/1_0_2/sqlline-1_0_2.jar[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::[ivy:resolve] :: UNRESOLVED DEPENDENCIES ::[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::[ivy:resolve] :: sqlline#sqlline#1.0.2;1_0_2: not found[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::The attached patch adds sourceforge to the internal resolver list so that if the default sqlline version (&amp; a hadoop snapshot) is used, the build does not fail.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3718" opendate="2012-11-17 00:00:00" fixdate="2012-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add check to determine whether partition can be dropped at Semantic Analysis time</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.nodrop.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3721" opendate="2012-11-19 00:00:00" fixdate="2012-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER TABLE ADD PARTS should check for valid partition spec and throw a SemanticException if part spec is not valid</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3722" opendate="2012-11-19 00:00:00" fixdate="2012-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create index fails on CLI using remote metastore</summary>
      <description>If the CLI uses a remote metastore and the user attempts to create an index without a comment, it will fail with a NPE.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="3724" opendate="2012-11-19 00:00:00" fixdate="2012-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore tests use hardcoded ports</summary>
      <description>Several of the metastore tests use hardcoded ports for remote metastore Thrift servers. This is causing transient failures in Jenkins, e.g. https://builds.apache.org/job/Hive-trunk-h0.21/1804/A few tests already dynamically determine free ports, and this logic can be shared.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="3728" opendate="2012-11-21 00:00:00" fixdate="2012-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>make optimizing multi-group by configurable</summary>
      <description>This was done as part of https://issues.apache.org/jira/browse/HIVE-609.This should be configurable.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3735" opendate="2012-11-22 00:00:00" fixdate="2012-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest doesn&amp;#39;t work due to hive snapshot version upgrade to 11</summary>
      <description>PTest fails. Error::::::::::::::::::::::::::::::::::::::::::::::&amp;#91;ivy:resolve&amp;#93; :: UNRESOLVED DEPENDENCIES ::&amp;#91;ivy:resolve&amp;#93; ::::::::::::::::::::::::::::::::::::::::::::::&amp;#91;ivy:resolve&amp;#93; :: org.apache.hive#hive-builtins;0.11.0-SNAPSHOT: not found&amp;#91;ivy:resolve&amp;#93; ::::::::::::::::::::::::::::::::::::::::::::::</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0,0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="3764" opendate="2012-12-3 00:00:00" fixdate="2012-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support metastore version consistency check</summary>
      <description>Today there's no version/compatibility information stored in hive metastore. Also the datanucleus configuration property to automatically create missing tables is enabled by default. If you happen to start an older or newer hive or don't run the correct upgrade scripts during migration, the metastore would end up corrupted. The autoCreate schema is not always sufficient to upgrade metastore when migrating to newer release. It's not supported with all databases. Besides the migration often involves altering existing table, changing or moving data etc.Hence it's very useful to have some consistency check to make sure that hive is using correct metastore and for production systems the schema is not automatically by running hive.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.11.0-to-0.12.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.12.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.11.0-to-0.12.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.12.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.11.0-to-0.12.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.12.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.11.0-to-0.12.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.10.0-to-0.11.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.12.0.derby.sql</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3789" opendate="2012-12-11 00:00:00" fixdate="2012-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Patch HIVE-3648 causing the majority of unit tests to fail on branch 0.9</summary>
      <description>Rolling back to before this patch shows that the unit tests are passing, after the patch, the majority of the unit tests are failing.</description>
      <version>0.9.0,0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="3792" opendate="2012-12-12 00:00:00" fixdate="2012-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive pom file has missing conf and scope mapping for compile configuration.</summary>
      <description>hive-0.10.0 pom file has missing conf and scope mapping for compile configuration.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3807" opendate="2012-12-15 00:00:00" fixdate="2012-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive authorization should use short username when Kerberos authentication</summary>
      <description>Currently when authentication method is Kerberos,Hive authorization uses user full name as privilege principal, for example, it uses john@EXAMPLE.COM instead of john.It should use the short name instead. The benefits:1. Be consistent. Hadoop, HBase and etc they all use short name in related ACLs or authorizations. For Hive authorization works well with them, this should be.2. Be convenient. It's very inconvenient to use the lengthy Kerberos principal name when grant or revoke privileges via Hive CLI.</description>
      <version>0.9.0,0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3814" opendate="2012-12-18 00:00:00" fixdate="2012-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot drop partitions on table when using Oracle metastore</summary>
      <description>Create a table with a partition. Try to drop the partition or the table containing the partition. Following error is seen:FAILED: Error in metadata: MetaException(message:javax.jdo.JDODataStoreException: Error executing JDOQL query "SELECT 'org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics' AS NUCLEUS_TYPE,THIS.AVG_COL_LEN,THIS."COLUMN_NAME",THIS.COLUMN_TYPE,THIS.DB_NAME,THIS.DOUBLE_HIGH_VALUE,THIS.DOUBLE_LOW_VALUE,THIS.LAST_ANALYZED,THIS.LONG_HIGH_VALUE,THIS.LONG_LOW_VALUE,THIS.MAX_COL_LEN,THIS.NUM_DISTINCTS,THIS.NUM_FALSES,THIS.NUM_NULLS,THIS.NUM_TRUES,THIS.PARTITION_NAME,THIS."TABLE_NAME",THIS.CS_ID FROM PART_COL_STATS THIS LEFT OUTER JOIN PARTITIONS THIS_PARTITION_PARTITION_NAME ON THIS.PART_ID = THIS_PARTITION_PARTITION_NAME.PART_ID WHERE THIS_PARTITION_PARTITION_NAME.PART_NAME = ? AND THIS.DB_NAME = ? AND THIS."TABLE_NAME" = ?" : ORA-00904: "THIS"."PARTITION_NAME": invalid identifierThe problem here is that the column "PARTITION_NAME" that the query is referring to in table "PART_COL_STATS" is non-existent. Looking at the hive schema scripts for mysql &amp; derby, this should be "PARTITION_NAME". Postgres also suffers from the same problem.</description>
      <version>0.10.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.10.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.012-HIVE-1362.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.012-HIVE-1362.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug id="3815" opendate="2012-12-18 00:00:00" fixdate="2012-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive table rename fails if filesystem cache is disabled</summary>
      <description>If fs.&lt;filesyste&gt;.impl.disable.cache (eg fs.hdfs.impl.disable.cache) is set to true, then table rename fails.The exception that gets thrown (though not logged!) is Caused by: InvalidOperationException(message:table new location hdfs://host1:8020/apps/hive/warehouse/t2 is on a different file system than the old location hdfs://host1:8020/apps/hive/warehouse/t1. This operation is not supported) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_result$alter_table_resultStandardScheme.read(ThriftHiveMetastore.java:28825) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_result$alter_table_resultStandardScheme.read(ThriftHiveMetastore.java:28811) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$alter_table_result.read(ThriftHiveMetastore.java:28753) at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table(ThriftHiveMetastore.java:977) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.alter_table(ThriftHiveMetastore.java:962) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:208) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:74) at $Proxy7.alter_table(Unknown Source) at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:373) ... 18 more</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="382" opendate="2009-4-1 00:00:00" fixdate="2009-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>for hash aggr, purge the hash table as you go along</summary>
      <description>for hash aggr, purge the hash table as you go along</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3820" opendate="2012-12-19 00:00:00" fixdate="2012-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consider creating a literal like "D" or "BD" for representing Decimal type constants</summary>
      <description>When the HIVE-2693 gets committed, users are going to see this behavior:hive&gt; select cast(3.14 as decimal) from decimal_3 limit 1;3.140000000000000124344978758017532527446746826171875That's intuitively incorrect but is the case because 3.14 (double) is being converted to BigDecimal because of which there is a precision mismatch.We should consider creating a new literal for expressing constants of Decimal type as Gunther suggested in HIVE-2693.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBigDecimalObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3824" opendate="2012-12-20 00:00:00" fixdate="2012-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bug if different serdes are used for different partitions</summary>
      <description>Consider the following testcase:create table tst5 (key string, value string) partitioned by (ds string) stored as rcfile;insert overwrite table tst5 partition (ds='1') select * from src; insert overwrite table tst5 partition (ds='2') select * from src; insert overwrite table tst5 partition (ds='3') select * from src; alter table tst5 stored as sequencefile; insert overwrite table tst5 partition (ds='4') select * from src; insert overwrite table tst5 partition (ds='5') select * from src; insert overwrite table tst5 partition (ds='6') select * from src; alter table tst5 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'; insert overwrite table tst5 partition (ds='7') select * from src; insert overwrite table tst5 partition (ds='8') select * from src; insert overwrite table tst5 partition (ds='9') select * from src; The following query works fine: select key + key, value from tst5 where ((ds = '4') or (ds = '1')); since both the partitions use ColumnarSerDeBut the following query fails:select key + key, value from tst5 where ((ds = '4') or (ds = '1') or (ds='7'));since different serdes are used.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="3832" opendate="2012-12-23 00:00:00" fixdate="2012-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite doesn&amp;#39;t create a dir if the skewed column position doesnt match</summary>
      <description>If skewed column doesn't match the position in table column, insert overwrite doesn't create sub-dir but put all into default directory.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ListBucketingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3833" opendate="2012-12-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>object inspectors should be initialized based on partition metadata</summary>
      <description>Currently, different partitions can be picked up for the same input split based on theserdes' etc. And, we dont allow to change the schema for LazyColumnarBinarySerDe.Instead of that, different partitions should be part of the same split, only if thepartition schemas exactly match. The operator tree object inspectors should be basedon the partition schema. That would give greater flexibility and also help using binary serde with rcfile</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.NullStructSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ObjectPair.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestPartition.java</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.wise.fileformat8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.wise.fileformat9.q</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="3840" opendate="2012-12-27 00:00:00" fixdate="2012-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive cli null representation in output is inconsistent</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.java</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.columnarserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.nullable.fields.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="3842" opendate="2012-12-28 00:00:00" fixdate="2012-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove redundant test codes</summary>
      <description>Currently hive writes same test code again and again for each test, making test class huge (50k line for ql).</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestParse.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug id="3846" opendate="2012-12-29 00:00:00" fixdate="2012-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter view rename NPEs with authorization on.</summary>
      <description>Click to add description</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="3849" opendate="2012-12-31 00:00:00" fixdate="2012-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aliased column in where clause for multi-groupby single reducer cannot be resolved</summary>
      <description>Verifying HIVE-3847, I've found an exception is thrown before meeting the error situation described in it. Something like, FAILED: SemanticException &amp;#91;Error 10025&amp;#93;: Line 40:6 Expression not in GROUP BY key 'crit5'</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.mutli.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.mutli.insert.common.distinct.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3875" opendate="2013-1-9 00:00:00" fixdate="2013-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>negative value for hive.stats.ndv.error should be disallowed</summary>
      <description>Currently, if a negative value is specified for hive.stats.ndv.error in hive-site.xml, it is treated as 0. We should instead throw an exception.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3908" opendate="2013-1-17 00:00:00" fixdate="2013-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create view statement&amp;#39;s outputs contains the view and a temporary dir.</summary>
      <description>It should only contain the view</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3913" opendate="2013-1-18 00:00:00" fixdate="2013-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Possible deadlock in ZK lock manager</summary>
      <description>ZK Hive lock manager can get into a state when the connection is closed, but no reconnection is attempted.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3951" opendate="2013-1-27 00:00:00" fixdate="2013-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Decimal type columns in Regex Serde</summary>
      <description>Decimal type in Hive was recently added by HIVE-2693. We should allow users to create tables with decimal type columns when using Regex Serde. HIVE-3004 did something similar for other primitive types.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
      <file type="M">ql.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.serde.regex.q</file>
    </fixedFiles>
  </bug>
  <bug id="3958" opendate="2013-1-29 00:00:00" fixdate="2013-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support partial scan for analyze command - RCFile</summary>
      <description>analyze commands allows us to collect statistics on existing tables/partitions. It works great but might be slow since it scans all files.There are 2 ways to speed it up:1. collect stats without file scan. It may not collect all stats but good and fast enough for use case. HIVE-3917 addresses it2. collect stats via partial file scan. It doesn't scan all content of files but part of it to get file metadata. some examples are https://cwiki.apache.org/Hive/rcfilecat.html for RCFile, ORC ( HIVE-3874 ) and HFile of Hbase (Edit: That link should be https://cwiki.apache.org/confluence/display/Hive/RCFileCat.)This jira is targeted to address the #2. More specifically RCFile format.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.StatsWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3978" opendate="2013-2-2 00:00:00" fixdate="2013-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE_AUX_JARS_PATH should have : instead of , as separator since it gets appended to HADOOP_CLASSPATH</summary>
      <description>The following code gets executed only in case of cygwin.HIVE_AUX_JARS_PATH=`echo $HIVE_AUX_JARS_PATH | sed 's/,/:/g'`But since HIVE_AUX_JARS_PATH gets added to HADOOP_CLASSPATH, the comma should get replaced by : for all cases.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug id="3990" opendate="2013-2-6 00:00:00" fixdate="2013-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide input threshold for direct-fetcher (HIVE-2925)</summary>
      <description>As a followup of HIVE-2925, add input threshold for fetch task conversion.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4004" opendate="2013-2-8 00:00:00" fixdate="2013-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect status for AddPartition metastore event if RawStore commit fails</summary>
      <description>For ADD PARTITION operations, the AddPartitionEvent does not care if the RawStore commit succeeded or not. This means that an AddPartitionEvent with status=true is fired even if the the actual ADD PARTITION operation failed. This will confuse any AddPartitionEvent listeners.Other MetastoreListenerEvents like CreateTableEvent correctly incorporate the status of the RawStore commit. Only AddPartitionEvent has this problem.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="4009" opendate="2013-2-11 00:00:00" fixdate="2013-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI Tests fail randomly due to MapReduce LocalJobRunner race condition</summary>
      <description>Hadoop has a race condition MAPREDUCE-5001 which causes tests to fail randomly when using LocalJobRunner.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="4015" opendate="2013-2-13 00:00:00" fixdate="2013-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ORC file to the grammar as a file format</summary>
      <description>It would be much more convenient for users if we enable them to use ORC as a file format in the HQL grammar.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4039" opendate="2013-2-19 00:00:00" fixdate="2013-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive compiler sometimes fails in semantic analysis / optimisation stage when boolean variable appears in WHERE clause.</summary>
      <description>Hive compiler fails with a NullPointerException in semantic analysis / optimisation stage when a boolean variable appears in the WHERE clause in some cases. A minimal query to generate this error is here:SELECT 1FROM (SELECT TRUE AS flagFROM dim_one_row:measurementsystems) aWHERE flag;On the other hand, the following query is perfectly fine:SELECT 1FROM (SELECT TRUE AS flagFROM dim_one_row:measurementsystems) aWHERE flag=TRUE;</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="404" opendate="2009-4-10 00:00:00" fixdate="2009-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problems in "SELECT * FROM t SORT BY col1 LIMIT 100"</summary>
      <description>Unless the user specify "set mapred.reduce.tasks=1;", he will see unexpected results with the query of "SELECT * FROM t SORT BY col1 LIMIT 100"Basically, in the first map-reduce job, each reducer will get sorted data and only keep the first 100. In the second map-reduce job, we will distribute and sort the data randomly, before feeding into a single reducer that outputs the first 100.In short, the query will output 100 random records in N * 100 top records from each of the reducer in the first map-reduce job.This is contradicting to what people expects.We should propagate the SORT BY columns to the second map-reduce job.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4042" opendate="2013-2-20 00:00:00" fixdate="2013-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ignore mapjoin hint</summary>
      <description>After HIVE-3784, in a production environment, it can become difficult todeploy since a lot of production queries can break.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4056" opendate="2013-2-22 00:00:00" fixdate="2013-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend rcfilecat to support (un)compressed size and no. of row</summary>
      <description>rcfilecat supports data and metadata:https://cwiki.apache.org/Hive/rcfilecat.htmlIn metadata, it supports column statistics.It will be natural to extend metadata support to 1. no. of rows 2. uncompressed size for the file3. compressed size for the file</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFile.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.RCFileCat.java</file>
    </fixedFiles>
  </bug>
  <bug id="4120" opendate="2013-3-5 00:00:00" fixdate="2013-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement decimal encoding for ORC</summary>
      <description>Currently, ORC does not have an encoder for decimal.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestSerializationUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.SerializationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug id="4122" opendate="2013-3-5 00:00:00" fixdate="2013-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries fail if timestamp data not in expected format</summary>
      <description>Queries will fail if timestamp data not in expected format. The expected behavior is to return NULL for these invalid values.# Not all timestamps in correct format:echo "1999-10-101999-10-10 90:10:100000-01-01 00:00:00" &gt; table.datahive -e "create table timestamp_tbl (t timestamp)"hadoop fs -put ./table.data HIVE_WAREHOUSE_DIR/timestamp_tbl/hive -e "select t from timestamp_tbl"Execution failed with exit status: 213/03/05 09:47:05 ERROR exec.Task: Execution failed with exit status: 2Obtaining error information13/03/05 09:47:05 ERROR exec.Task: Obtaining error informationTask failed!Task ID: Stage-1Logs:13/03/05 09:47:05 ERROR exec.Task: Task failed!Task ID: Stage-1Logs:</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
    </fixedFiles>
  </bug>
  <bug id="4138" opendate="2013-3-7 00:00:00" fixdate="2013-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC&amp;#39;s union object inspector returns a type name that isn&amp;#39;t parseable by TypeInfoUtils</summary>
      <description>Currently the typename returned by ORC's union object inspector isn't parseable by TypeInfoUtils. The format needs to be union&lt;type1,type2,type3&gt;.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcStruct.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcUnion.java</file>
    </fixedFiles>
  </bug>
  <bug id="4139" opendate="2013-3-7 00:00:00" fixdate="2013-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniDFS shim does not work for hadoop 2</summary>
      <description>There's an incompatibility between hadoop 1 &amp; 2 wrt to the MiniDfsCluster class. That causes the hadoop 2 line Minimr tests to fail with a "MethodNotFound" exception.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4149" opendate="2013-3-11 00:00:00" fixdate="2013-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong results big outer joins with array of ints</summary>
      <description>Consider the following query:create table tinyA(a bigint, b bigint) stored as textfile;create table tinyB(a bigint, bList array&lt;int&gt;) stored as textfile;load data local inpath '../data/files/tiny_a' into table tinyA;load data local inpath '../data/files/tiny_b' into table tinyB;select * from tinyA;select * from tinyB;select tinyB.a, tinyB.bList from tinyB full outer join tinyA on tinyB.a = tinyA.a;The results are wrong</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyArray.java</file>
    </fixedFiles>
  </bug>
  <bug id="4191" opendate="2013-3-15 00:00:00" fixdate="2013-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>describe table output always prints as if formatted keyword is specified</summary>
      <description>With the change in HIVE-3140, describe table output prints like the format expected from "describe formatted table". ie, the headers are included and there is padding with space for the fields. This is a non backward compatible change, we should discuss if this change in the formatting of output should remain. This has impact on hiveserver2, it has been relying on the old format, and with this change it prints additional headers and fields with space padding.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.14.managed.location.over.existing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.13.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.12.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.11.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.10.external.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.08.nonpart.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.03.nonpart.over.compat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.01.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.00.nonpart.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.ignore.protection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.2columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidcolname.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidtype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.filter.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.hiveconf.validation0.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.hiveconf.validation1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.change.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.evolved.schemas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.joins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.sanity.test.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.error.message.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.bincolserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.colserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="4209" opendate="2013-3-21 00:00:00" fixdate="2013-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cache evaluation result of deterministic expression and reuse it</summary>
      <description>For example, select key from src where key + 1 &gt; 100 AND key + 1 &lt; 200 limit 3;key + 1 need not to be evaluated twice.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeNullEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="421" opendate="2009-4-15 00:00:00" fixdate="2009-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>union followed by multi-table insert does not work properly</summary>
      <description>Like jira 413, multi-table inserts has some problems with unions.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby9.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4251" opendate="2013-3-29 00:00:00" fixdate="2013-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indices can&amp;#39;t be built on tables whose schema info comes from SerDe</summary>
      <description>Building indices on tables who get the schema information from the deserializer (e.g. Avro backed tables) doesn't work because when the column is checked to exist, the correct API isn't used.hive&gt; describe doctors; OK# col_name data_type comment number int from deserializer first_name string from deserializer last_name string from deserializer Time taken: 0.215 seconds, Fetched: 5 row(s)hive&gt; create index doctors_index on table doctors(number) as 'compact' with deferred rebuild; FAILED: Error in metadata: java.lang.RuntimeException: Check the index columns, they should appear in the table being indexed.FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask</description>
      <version>0.10.0,0.11.0,0.10.1</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug id="4268" opendate="2013-3-30 00:00:00" fixdate="2013-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should support the -f option</summary>
      <description>Beeline should support the -f option (pass in a script to execute) for compatibility with the Hive CLI.</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="4296" opendate="2013-4-5 00:00:00" fixdate="2013-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ant thriftif fails on hcatalog</summary>
      <description></description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="4302" opendate="2013-4-5 00:00:00" fixdate="2013-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix how RowSchema and RowResolver are set on ReduceSinkOp that precedes PTFOp</summary>
      <description>Currently the RowSchema and RowResolver for the ReduceSinkOp just point to the 'input' Op's structures. This causes issues when input Op's structures are changed during Optimization. See Jira 2340 of a problem.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4303" opendate="2013-4-5 00:00:00" fixdate="2013-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>2 empty java files in hcatalog</summary>
      <description>Two empty java files came in from hcatalog.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistPreEventHook.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.PartitionNameWhitelistPreEventListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="4310" opendate="2013-4-8 00:00:00" fixdate="2013-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize count(distinct) with hive.map.groupby.sorted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.8.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4364" opendate="2013-4-16 00:00:00" fixdate="2013-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline always exits with 0 status, should exit with non-zero status on error</summary>
      <description>beeline should exit with non-zero status on error so that executors such as a shell script or Oozie can detect failure.</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="4365" opendate="2013-4-16 00:00:00" fixdate="2013-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong result in left semi join</summary>
      <description>wrong result in left semi join while hive.optimize.ppd=truefor example:1、create table create table t1(c1 int,c2 int, c3 int, c4 int, c5 double,c6 int,c7 string) row format DELIMITED FIELDS TERMINATED BY '|'; create table t2(c1 int) ;2、load dataload data local inpath '/home/test/t1.txt' OVERWRITE into table t1;load data local inpath '/home/test/t2.txt' OVERWRITE into table t2;t1 data:1|3|10003|52|781.96|555|2012031|3|10003|39|782.96|555|2012031|3|10003|87|783.96|555|2012032|5|10004|24|789.96|555|2012032|5|10004|58|788.96|555|201203t2 data:5553、excute Queryselect t1.c1,t1.c2,t1.c3,t1.c4,t1.c5,t1.c6,t1.c7 from t1 left semi join t2 on t1.c6 = t2.c1 and t1.c1 = '1' and t1.c7 = '201203' ; can got result.select t1.c1,t1.c2,t1.c3,t1.c4,t1.c5,t1.c6,t1.c7 from t1 left semi join t2 on t1.c6 = t2.c1 where t1.c1 = '1' and t1.c7 = '201203' ; can't got result.</description>
      <version>0.9.0,0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.semijoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4398" opendate="2013-4-22 00:00:00" fixdate="2013-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 Resource leak: operation handles not cleaned when originating session is closed</summary>
      <description>In HS2 closing of sessions doesn't lead to closing of all the operation handles that the session had opened. This JIRA is meant to address this issue.</description>
      <version>0.10.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="4403" opendate="2013-4-23 00:00:00" fixdate="2013-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Running Hive queries on Yarn (MR2) gives warnings related to overriding final parameters</summary>
      <description>While working on BIGTOP-885, I saw that Hive was giving a bunch of warnings related to overriding final parameters in job.conf. This was on a pseudo distributed cluster. FWIW, I didn't see this happen on a fully-distributed cluster. Perhaps, Hive's job.conf is overriding some final parameters it shouldn't.Here is what the warnings looked like:2013-04-19 14:20:32,304 WARN [main] conf.Configuration (Configuration.java:loadProperty(2032)) - file:/tmp/root/hive_2013-04-19_14-20-30_159_5701876916688815815/-local-10002/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.2013-04-19 14:20:32,367 WARN [main] conf.Configuration (Configuration.java:loadProperty(2032)) - file:/tmp/root/hive_2013-04-19_14-20-30_159_5701876916688815815/-local-10002/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.To reproduce, run a query like:CREATE TABLE u_data ( userid INT, movieid INT, rating INT, unixtime STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'STORED AS TEXTFILE;Load some data into u_data, here is some sample data:https://github.com/apache/bigtop/blob/master/bigtop-tests/test-artifacts/hive/src/main/resources/seed_data_files/ml-data/u.dataRun a simple query on that data (on YARN/MR2)INSERT OVERWRITE DIRECTORY '/tmp/count'SELECT COUNT(1) FROM u_data</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4405" opendate="2013-4-23 00:00:00" fixdate="2013-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate vectorized execution plan</summary>
      <description>The execution plan will be transformed into a vectorized plan using vectorized operators.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4406" opendate="2013-4-23 00:00:00" fixdate="2013-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing "/" or "/&lt;dbname&gt;" in hs2 jdbc uri switches mode to embedded mode</summary>
      <description>When the jdbc uri does not have a / or "/default" after the hostname:port, and the principal is specified, it ends up launching HS2 in embedded mode.This is because the parsing of uri in such case does not end up extracting the hostname.eg . "jdbc:hive2://&lt;host&gt;:&lt;port&gt;;principal=&lt;&gt;." results HS2 embedded mode getting used."jdbc:hive2://&lt;host&gt;:&lt;port&gt;/;principal=&lt;&gt;" or "jdbc:hive2://&lt;host&gt;:&lt;port&gt;/default;principal=&lt;&gt;" results in it connecting to the standalone hive server 2.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug id="4409" opendate="2013-4-24 00:00:00" fixdate="2013-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent incompatible column type changes</summary>
      <description>If a user changes the type of an existing column of a partitioned table to an incompatible type, subsequent accesses of old partitions will result in a ClassCastException (see example below). We should prevent the user from making incompatible type changes. This feature will be controlled by a new config parameter.Example:CREATE TABLE test_table123 (a INT, b MAP&lt;STRING, STRING&gt;) PARTITIONED BY (ds STRING) STORED AS SEQUENCEFILE;INSERT OVERWRITE TABLE test_table123 PARTITION(ds="foo1") SELECT 1, MAP("a1", "b1") FROM src LIMIT 1;SELECT * from test_table123 WHERE ds="foo1";ALTER TABLE test_table123 REPLACE COLUMNS (a INT, b STRING);SELECT * from test_table123 WHERE ds="foo1";The last SELECT fails with the following exception:Failed with exception java.io.IOException:java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspectorjava.io.IOException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1406) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348) at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:790) at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:124) at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_class_cast(TestCliDriver.java:108)The ALTER TABLE statement is blocked if you set the following parameter, introduced int the fix to this JIRA:SET hive.metastore.disallow.incompatible.col.type.changes=true;</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4477" opendate="2013-5-2 00:00:00" fixdate="2013-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove redundant copy of arithmetic filter unit test testColOpScalarNumericFilterNullAndRepeatingLogic</summary>
      <description>same test got ported to 2 different files</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4487" opendate="2013-5-3 00:00:00" fixdate="2013-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive does not set explicit permissions on hive.exec.scratchdir</summary>
      <description>The hive.exec.scratchdir defaults to /tmp/hive-${user.name}, but when Hive creates this directory it doesn't set any explicit permission on it. This means if you have the default HDFS umask setting of 022, then these directories end up being world readable. These permissions also get applied to the staging directories and their files, thus leaving inter-stage data world readable.This can cause a potential leak of data especially when operating on a Kerberos enabled cluster. Hive should probably default these directories to only be readable by the owner.</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4489" opendate="2013-5-3 00:00:00" fixdate="2013-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline always return the same error message twice</summary>
      <description>Beeline always returns the same error message twice. for example, if I try to create a table a2 which already exists, it prints out two exact same messages and it is not quite user friendly.beeline&gt; !connect jdbc:hive2://localhost:10000 scott tiger org.apache.hive.jdbc.HiveDriverConnecting to jdbc:hive2://localhost:10000Connected to: Hive (version 0.10.0)Driver: Hive (version 0.10.0-cdh4.2.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://localhost:10000&gt; create table a2 (value int);Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask (state=08S01,code=1)Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask (state=08S01,code=1)</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="4547" opendate="2013-5-13 00:00:00" fixdate="2013-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A complex create view statement fails with new Antlr 3.4</summary>
      <description>A complex create view statement with CAST in join condition fails with IllegalArgumentException error. This is exposed by the Antlr 3.4 upgrade (HIVE-2439). The same statement works fine with Hive 0.9</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4550" opendate="2013-5-13 00:00:00" fixdate="2013-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>local_mapred_error_cache fails on some hadoop versions</summary>
      <description>I've tested it manually on the upcoming 1.3 version (branch 1).We do mask job_* ids, but not job_local* ids. The fix is to extend this to both.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.local.mapred.error.cache.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4554" opendate="2013-5-14 00:00:00" fixdate="2013-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed to create a table from existing file if file path has spaces</summary>
      <description>To reproduce the problem,1. Create a table, say, person_age (name STRING, age INT).2. Create a file whose name has a space in it, say, "data set.txt".3. Try to load the date in the file to the table.The following error can be seen in the console:hive&gt; LOAD DATA INPATH '/home/xzhang/temp/data set.txt' INTO TABLE person_age;Loading data to table default.person_ageFailed with exception Wrong file format. Please check the file's format.FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTaskNote: the error message is confusing.</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4562" opendate="2013-5-15 00:00:00" fixdate="2013-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-3393 brought in Jackson library,and these four jars should be packed into hive-exec.jar</summary>
      <description>Some jars of Hive are required not only by the client but also the server (every Hadoop slave),though we could use 'add jar' command to add all the jars in dis-cache ,but in common way ,we may add these jars in $HADOOP_HOME/lib/ of every salve of the Hadoop Cluster,and need restart all the tasktrackers .For example:When using hive stats, If we use mysql as tmp stats db ,every salve of the Hadoop Cluster should contain mysql-connector-java-****.jar in $HADOOP_HOME/lib/ And for column stats In all slaves $HADOOP_HOME/lib/ should contain:jackson-core-asl-1.8.8.jarjackson-jaxrs-1.8.8.jarjackson-mapper-asl-1.8.8.jarjackson-xc-1.8.8.jarThese jars should be separated from other common client-side-jars .</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4568" opendate="2013-5-15 00:00:00" fixdate="2013-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline needs to support resolving variables</summary>
      <description>Previous Hive CLI allows user to specify hive variables at the command line using option "--hivevar". In user's script, reference to a hive variable will be substituted with the value of the variable. In such way, user can parameterize his/her script and invoke the script with different hive variable values. The following script is one usage:hive --hivevar INPUT=/user/jenkins/oozie.1371538916178/examples/input-data/table --hivevar OUTPUT=/user/jenkins/oozie.1371538916178/examples/output-data/hive -f script.qscript.q makes use of hive variables:CREATE EXTERNAL TABLE test (a INT) STORED AS TEXTFILE LOCATION '${INPUT}';INSERT OVERWRITE DIRECTORY '${OUTPUT}' SELECT * FROM test;However, after upgrade to hiveserver2 and beeline, this functionality is missing. Beeline doesn't take --hivevar option, and any hive variable isn't passed to server so it cannot be used for substitution.This JIRA is to address this issue, providing a backward compatible behavior at Beeline.</description>
      <version>0.10.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.src.test.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="4573" opendate="2013-5-17 00:00:00" fixdate="2013-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support alternate table types for HiveServer2</summary>
      <description>The getTables jdbc function no longer returns information when using normal JDBC table types like TABLE or VIEW. You must now use a more specific type such as MANAGED_TABLE or VIRTUAL_VIEW. An example application that will fail to return results against 0.10 is below, works without issue in 0.9. In my 0.10 test I used HS2. import java.sql.SQLException;import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import java.sql.DriverManager;import org.apache.hive.jdbc.HiveDriver;import java.sql.DatabaseMetaData;public class TestGet { private static String driverName = "org.apache.hive.jdbc.HiveDriver"; /** * @param args * @throws SQLException */ public static void main(String[] args) throws SQLException { try { Class.forName(driverName); } catch (ClassNotFoundException e) { // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); } Connection con = DriverManager.getConnection("jdbc:hive2://hostname:10000/default"); DatabaseMetaData dbmd = con.getMetaData(); String[] types = {"TABLE"}; ResultSet rs = dbmd.getTables(null, null, "%", types); while (rs.next()) { System.out.println(rs.getString("TABLE_NAME")); } } }}</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTableTypesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4618" opendate="2013-5-27 00:00:00" fixdate="2013-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>show create table creating unusable DDL when field delimiter is \001</summary>
      <description>When including a "fields terminated by" in the create statement. If the delimiter is preceded by a \001, hive turns this into \u0001 which is correct. However it then gives you a ddl that does not work because the parser changes the \u0001 into u0001. Example: hive&gt; create table j1 (a string) row format delimited fields terminated by '\001';hive&gt; show create table j1;CREATE TABLE j1( a string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION 'hdfs://forza-1.cloud.rtp.cloudera.com:8020/user/hive/warehouse/j1'TBLPROPERTIES ( 'transient_lastDdlTime'='1369664999')hive&gt; desc formatted j1;…shortened to save spaceStorage Desc Params: field.delim \u0001 serialization.format \u0001hive&gt; drop table j1;hive&gt; CREATE TABLE j1( &gt; a string) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY '\u0001' &gt; STORED AS INPUTFORMAT &gt; 'org.apache.hadoop.mapred.TextInputFormat' &gt; OUTPUTFORMAT &gt; 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' &gt; LOCATION &gt; 'hdfs://forza-1.cloud.rtp.cloudera.com:8020/user/hive/warehouse/j1' &gt; TBLPROPERTIES ( &gt; 'transient_lastDdlTime'='1369664999');hive&gt; desc formatted j1;…shortened to save spaceStorage Desc Params: field.delim u0001 serialization.format u0001</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4646" opendate="2013-6-3 00:00:00" fixdate="2013-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>skewjoin.q is failing in hadoop2</summary>
      <description>https://issues.apache.org/jira/browse/HDFS-538 changed to throw exception instead of returning null for not-existing path. But skew resolver depends on old behavior.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">hcatalog.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4669" opendate="2013-6-6 00:00:00" fixdate="2013-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make username available to semantic analyzer hooks</summary>
      <description>Make username available to the semantic analyzer hooks.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4679" opendate="2013-6-6 00:00:00" fixdate="2013-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat can deadlock Hadoop if the number of concurrently running tasks if higher or equal than the number of mappers</summary>
      <description>o In the current Templeton design, each time a Job is submitted thru the REST API (it can be Pig/Hive or MR job), it will consume one Hadoop map slot. Given that the number of map slots is finite in the cluster (16 node cluster will have 32 map slots), in some circumstances, a user can deadlock the cluster if Templeton job submission pipeline takes over all map slots (Templeton map tasks will wait for the actual underlying jobs to complete, what will never happen, given that Hadoop has no free map slots to schedule new tasks).o HCat queries use a different mechanism and do not contribute to the deadlock.</description>
      <version>0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4683" opendate="2013-6-7 00:00:00" fixdate="2013-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix coverage org.apache.hadoop.hive.cli</summary>
      <description></description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestCliDriverMethods.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.RCFileCat.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4684" opendate="2013-6-7 00:00:00" fixdate="2013-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query with filter constant on left of "=" and column expression on right does not vectorize</summary>
      <description>select dmachineid from factsqlengineam_vec_orc where 1073 = dmachineid + 1;Does not go down the vectorization path.Output:hive&gt; select dmachineid from factsqlengineam_vec_orc where 1073 = dmachineid + 1;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorValidating if vectorized execution is applicableCannot vectorize the plan: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongScalarEqualLongColumnStarting Job = job_201306061504_0038, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201306061504_0038Kill Command = c:\Hadoop\hadoop-1.1.0-SNAPSHOT\bin\hadoop.cmd job -kill job_201306061504_0038Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 02013-06-07 10:25:30,932 Stage-1 map = 0%, reduce = 0%2013-06-07 10:25:39,953 Stage-1 map = 25%, reduce = 0%2013-06-07 10:25:42,959 Stage-1 map = 49%, reduce = 0%, Cumulative CPU 8.172 sec2013-06-07 10:25:43,962 Stage-1 map = 49%, reduce = 0%, Cumulative CPU 8.172 sec...</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.templates.CodeGen.java</file>
    </fixedFiles>
  </bug>
  <bug id="4766" opendate="2013-6-20 00:00:00" fixdate="2013-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support HS2 client login timeout when the thrift thread max# is reached</summary>
      <description>HiveServer2 client (beeline) hangs in login if the thrift max thread# has been reached. It is because the server crashes due to a defect in currently used thrift 0.9.0. When hive is upgraded to use a new version of Thrift (say thrift 1.0), HS2 should support client login timeout instead of current hanging.</description>
      <version>0.10.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4785" opendate="2013-6-24 00:00:00" fixdate="2013-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement isCaseSensitive for Hive JDBC driver</summary>
      <description>Implement the "boolean isCaseSensitive(int column) throws SQLException" JDBC method.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveResultSetMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="48" opendate="2008-9-7 00:00:00" fixdate="2008-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support JDBC connections for interoperability between Hive and RDBMS</summary>
      <description>In many DW and BI systems, the data are stored in RDBMS for now such as oracle, mysql, postgresql ... for reporting, charting and etc.It would be useful to be able to import data from RDBMS and export data to RDBMS using JDBC connections.If Hive support JDBC connections, It wll be much easier to use 3rd party DW/BI tools.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4807" opendate="2013-7-2 00:00:00" fixdate="2013-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore hangs</summary>
      <description>Hive metastore hangs (does not accept any new connections) due to a bug in DBCP. The root cause analysis is here https://issues.apache.org/jira/browse/DBCP-398. The fix is to change Hive connection pool to BoneCP which is natively supported by DataNucleus.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4818" opendate="2013-7-7 00:00:00" fixdate="2013-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SequenceId in operator is not thread safe</summary>
      <description>SequenceId , seqId in the operator class is not modified concurrently.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug id="4858" opendate="2013-7-15 00:00:00" fixdate="2013-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sort "show grant" result to improve usability and testability</summary>
      <description>Currently Hive outputs the result of "show grant" command in no deterministic order. It outputs the set of each privilege type in the order of whatever returned from DB (DataNucleus). Randomness can arise and tests (depending on the order) can fail, especially in events of library upgrade (DN or JVM upgrade). Sorting the result will avoid the potential randomness and make the output more deterministic, thus not only improving the readability of the output but also making the test more robust.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.rename.partition.authorization.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="4885" opendate="2013-7-18 00:00:00" fixdate="2013-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alternative object serialization for execution plan in hive testing</summary>
      <description>Currently there are a lot of test cases involving in comparing execution plan, such as those in TestParse suite. XmlEncoder is used to serialize the generated plan by hive, and store it in the file for file diff comparison. However, XmlEncoder is tied with Java compiler, whose implementation may change from version to version. Thus, upgrade the compiler can generate a lot of fake test failures. The following is an example of diff generated when running hive with JDK7:Begin query: case_sensitivity.qdiff -a /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/build/ql/test/logs/positive/case_sensitivity.q.out /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/ql/src/test/results/compiler/parse/case_sensitivity.q.outdiff -a -b /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/build/ql/test/logs/positive/case_sensitivity.q.xml /data/4/hive-local/a2307.halxg.cloudera.com-hiveptest-2/cdh-source/ql/src/test/results/compiler/plan/case_sensitivity.q.xml3c3&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MapRedTask" id="MapRedTask0"&gt;---&gt; &lt;object id="MapRedTask0" class="org.apache.hadoop.hive.ql.exec.MapRedTask"&gt; 12c12&lt; &lt;object class="java.util.ArrayList" id="ArrayList0"&gt;---&gt; &lt;object id="ArrayList0" class="java.util.ArrayList"&gt; 14c14&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MoveTask" id="MoveTask0"&gt;---&gt; &lt;object id="MoveTask0" class="org.apache.hadoop.hive.ql.exec.MoveTask"&gt; 18c18&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MoveTask" id="MoveTask1"&gt;---&gt; &lt;object id="MoveTask1" class="org.apache.hadoop.hive.ql.exec.MoveTask"&gt; 22c22&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.StatsTask" id="StatsTask0"&gt;---&gt; &lt;object id="StatsTask0" class="org.apache.hadoop.hive.ql.exec.StatsTask"&gt; 60c60&lt; &lt;object class="org.apache.hadoop.hive.ql.exec.MapRedTask" id="MapRedTask1"&gt;---&gt; &lt;object id="MapRedTask1" class="org.apache.hadoop.hive.ql.exec.MapRedTask"&gt; As it can be seen, the only difference is the order of the attributes in the serialized XML doc, yet it brings 50+ test failures in Hive.We need to have a better plan comparison, or object serialization to improve the situation.</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="4886" opendate="2013-7-18 00:00:00" fixdate="2013-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline code should have apache license headers</summary>
      <description>The beeline jdbc client added as part of hive server2 changes is based on SQLLine. As beeline is modified version of SQLLine and further modifications are also under apache license, the license headers of these files need to be replaced with apache license headers. We already have the license text of SQLLine in LICENSE file .</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.XMLElementOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.XMLAttributeOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.VerticalOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.TableNameCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SunSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SQLCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Reflector.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ReflectiveCommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.OutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.OutputFile.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.IncrementalRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DriverInfo.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnections.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.DatabaseConnection.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.CommandHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.ColorBuffer.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BufferedRows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineSignalHandler.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCommandCompletor.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractOutputFormat.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.AbstractCommandHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="4888" opendate="2013-7-19 00:00:00" fixdate="2013-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>listPartitionsByFilter doesn&amp;#39;t support lt/gt/lte/gte</summary>
      <description>Filter pushdown could be improved. Based on my experiments there's no reasonable way to do it with DN 2.0, due to DN bug in substring and Collection.get(int) not being implemented.With version as low as 2.1 we can use values.get on partition to extract values to compare to. Type compatibility is an issue, but is easy for strings and integral values.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.gen.thrift.gen-rb.serde.constants.rb</file>
      <file type="M">serde.src.gen.thrift.gen-py.org.apache.hadoop.hive.serde.constants.py</file>
      <file type="M">serde.src.gen.thrift.gen-php.org.apache.hadoop.hive.serde.Types.php</file>
      <file type="M">serde.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.serde.serdeConstants.java</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.h</file>
      <file type="M">serde.src.gen.thrift.gen-cpp.serde.constants.cpp</file>
      <file type="M">serde.if.serde.thrift</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug id="4996" opendate="2013-8-5 00:00:00" fixdate="2013-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unbalanced calls to openTransaction/commitTransaction</summary>
      <description>when we used hiveserver1 based on hive-0.10.0, we found the Exception thrown.It was:FAILED: Error in metadata: MetaException(message:java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that there are unbalanced calls to openTransaction/commitTransaction)FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskhelp</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRawStoreTxn.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5132" opendate="2013-8-21 00:00:00" fixdate="2013-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t access to hwi due to "No Java compiler available"</summary>
      <description>I want to use hwi to submit hive queries, but after start hwi successfully, I can't open the web page of it.I noticed that someone also met the same issue in hive-0.10.Reproduce steps:--------------------------1. start hwibin/hive --config $HIVE_CONF_DIR --service hwi2. access to http://&lt;hive_hwi_node&gt;:9999/hwi via browsergot the following error message:HTTP ERROR 500Problem accessing /hwi/. Reason: No Java compiler availableCaused by:java.lang.IllegalStateException: No Java compiler available at org.apache.jasper.JspCompilationContext.createCompiler(JspCompilationContext.java:225) at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:560) at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:299) at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:315) at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:265) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) at org.mortbay.jetty.servlet.Dispatcher.forward(Dispatcher.java:327) at org.mortbay.jetty.servlet.Dispatcher.forward(Dispatcher.java:126) at org.mortbay.jetty.servlet.DefaultServlet.doGet(DefaultServlet.java:503) at javax.servlet.http.HttpServlet.service(HttpServlet.java:707) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5176" opendate="2013-8-29 00:00:00" fixdate="2013-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wincompat : Changes for allowing various path compatibilities with Windows</summary>
      <description>We need to make certain changes across the board to allow us to read/parse windows paths. Some are escaping changes, some are being strict about how we read paths (through URL.encode/decode, etc)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="5179" opendate="2013-8-29 00:00:00" fixdate="2013-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wincompat : change script tests from bash to sh</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.script.env.var2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.script.env.var2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.script.env.var1.q</file>
    </fixedFiles>
  </bug>
  <bug id="518" opendate="2009-5-26 00:00:00" fixdate="2009-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test mode in hive</summary>
      <description>It would be good to have a test mode in hive - this will help in checking the validity of a hive drop on a production cluster.The following would be good to have:Testmode --&gt; In testmode, all input tables are sampled (if not already sampled) and all output tables are prefixed by a user supplied name.This way, multiple hive drops can be compared quickly for correctnessNew Options:// whether hive is running in test mode. If yes, it turns on sampling and prefixes the output tablenameset hive.test.mode=true;// if hive is running in test mode, prefixes the output table by this stringset hive.test.mode.prefix=;// if hive is running in test mode and table is not bucketed, sampling frequencyset hive.test.mode.samplefreq=256;// if hive is running in test mode, dont sample the above comma seperated list of tablesset hive.test.mode.nosamplelist=;</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableSample.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5183" opendate="2013-8-30 00:00:00" fixdate="2013-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez EdgeProperty class has changed</summary>
      <description>Tez has changed the names of its EdgeProperties. Need to update the code to use the new names.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="5184" opendate="2013-8-30 00:00:00" fixdate="2013-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load filesystem, ugi, metastore client at tez session startup</summary>
      <description>Make sure the session is ready to go when we connect. That way once the session/connection is open, we're ready to go.NO PRECOMMIT TESTS (this is wip for the tez branch)</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug id="5187" opendate="2013-8-30 00:00:00" fixdate="2013-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance explain to indicate vectorized execution of operators.</summary>
      <description>Explain should be able to indicate whether an operator will be executed in vectorized mode or not.</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5223" opendate="2013-9-5 00:00:00" fixdate="2013-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>explain doesn&amp;#39;t show serde used for table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.struct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sign.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.radians.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.printf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.PI.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.notequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.named.struct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.java.method.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.in.file.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.format.number.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.find.in.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.E.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.degrees.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.cast.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.variable.sub.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.processor.namespaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.result.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.macro.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.ints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.keyword.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.sortby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.orderby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.clusterby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes4.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.add.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.arraymapstruct.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.format.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.row.sequence.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.serde2.TestSerDe.java</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sortmerge.mapjoin.mismatch.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alias.casted.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5246" opendate="2013-9-9 00:00:00" fixdate="2013-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Local task for map join submitted via oozie job fails on a secure HDFS</summary>
      <description>For a Hive query started by Oozie Hive action, the local task submitted for Mapjoin fails. The HDFS delegation token is not shared properly with the child JVM created for the local task.Oozie creates a delegation token for the Hive action and sets env variable HADOOP_TOKEN_FILE_LOCATION as well as mapreduce.job.credentials.binary config property. However this doesn't get passed down to the child JVM which causes the problem.This is similar issue addressed by HIVE-4343 which address the problem HiveServer2</description>
      <version>0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="5354" opendate="2013-9-25 00:00:00" fixdate="2013-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal precision/scale support in ORC file</summary>
      <description>A subtask of HIVE-3976.</description>
      <version>0.10.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.org.apache.hadoop.hive.ql.io.orc.orc.proto.proto</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.java</file>
      <file type="M">ql.src.gen.protobuf.gen-java.org.apache.hadoop.hive.ql.io.orc.OrcProto.java</file>
    </fixedFiles>
  </bug>
  <bug id="5355" opendate="2013-9-25 00:00:00" fixdate="2013-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC support for decimal precision/scale</summary>
      <description>A subtask of HIVE-3976.</description>
      <version>0.10.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeQualifiers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TProtocolVersion.java</file>
      <file type="M">service.src.gen.thrift.gen-javabean.org.apache.hive.service.cli.thrift.TCLIServiceConstants.java</file>
      <file type="M">service.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumnAttributes.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="5506" opendate="2013-10-9 00:00:00" fixdate="2013-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive SPLIT function does not return array correctly</summary>
      <description>Hello all, I think I have outlined a bug in the hive split function:Summary: When calling split on a string of data, it will only return all array items if the the last array item has a value. For example, if I have a string of text delimited by tab with 7 columns, and the first four are filled, but the last three are blank, split will only return a 4 position array. If any number of "middle" columns are empty, but the last item still has a value, then it will return the proper number of columns. This was tested in Hive 0.9 and hive 0.11. Data:(Note \t represents a tab char, \x09 the line endings should be \n (UNIX style) not sure what email will do to them). Basically my data is 7 lines of data with the first 7 letters separated by tab. On some lines I've left out certain letters, but kept the number of tabs exactly the same. input.txta\tb\tc\td\te\tf\tga\tb\tc\td\te\t\tga\tb\t\td\t\tf\tg\t\t\td\te\tf\tga\tb\tc\td\t\t\ta\t\t\t\te\tf\tga\t\t\td\t\t\tgI then created a table with one column from that data:DROP TABLE tmp_jo_tab_test;CREATE table tmp_jo_tab_test (message_line STRING)STORED AS TEXTFILE;LOAD DATA LOCAL INPATH '/tmp/input.txt'OVERWRITE INTO TABLE tmp_jo_tab_test;Ok just to validate I created a python counting script:#!/usr/bin/pythonimport sysfor line in sys.stdin: line = line&amp;#91;0:-1&amp;#93; out = line.split("\t") print len(out)The output there is : $ cat input.txt |./cnt_tabs.py7777777Based on that information, split on tab should return me 7 for each line as well:hive -e "select size(split(message_line, 't')) from tmp_jo_tab_test;"7777477However it does not. It would appear that the line where only the first four letters are filled in(and blank is passed in on the last three) only returns 4 splits, where there should technically be 7, 4 for letters included, and three blanks. a\tb\tc\td\t\t\t</description>
      <version>0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.java</file>
    </fixedFiles>
  </bug>
  <bug id="5525" opendate="2013-10-12 00:00:00" fixdate="2013-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized query failing for partitioned tables.</summary>
      <description>Caused by: java.lang.NullPointerExceptionat org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getInputColumnIndex(VectorizationContext.java:154)at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorBinaryComparisonFilterExpression(VectorizationContext.java:1163)at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:436)at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:274)at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.&lt;init&gt;(VectorFilterOperator.java:50)</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="5574" opendate="2013-10-17 00:00:00" fixdate="2013-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary newline at the end of message of ParserException</summary>
      <description>Error messages in ParserException is ended with newline, which is a little annoying.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.quoted.string.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.macro.reserved.word.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.uniquejoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.columns2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.charliteral.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.window.boundaries2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.window.boundaries.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.PartitionBySortBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.DistributeByOrderBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.multiple.part.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.archive.partspec3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidtype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.2columns.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseException.java</file>
    </fixedFiles>
  </bug>
  <bug id="5595" opendate="2013-10-19 00:00:00" fixdate="2013-1-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Implement vectorized SMB JOIN</summary>
      <description>Vectorized implementation of SMB Map Join.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="5608" opendate="2013-10-22 00:00:00" fixdate="2013-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>tez AM should be able to serialize orc footer in splits</summary>
      <description>This will avoid seeks to end of file from each split in each task to find the footer metadata.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="567" opendate="2009-6-19 00:00:00" fixdate="2009-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc: integrate hive with pentaho report designer</summary>
      <description>Instead of trying to get a complete implementation of jdbc, its probably more useful to pick reporting/analytics software out there and implement the jdbc methods necessary to get them working. This jira is a first attempt at this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5670" opendate="2013-10-28 00:00:00" fixdate="2013-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>annoying ZK exceptions are annoying</summary>
      <description>when I run tests locally (or on cluster IIRC) there are bunch of ZK-related exceptions in Hive log, such as2013-10-28 09:50:50,851 ERROR zookeeper.ClientCnxn (ClientCnxn.java:processEvent(523)) - Error while calling watcher java.lang.NullPointerException at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497) 2013-10-28 09:51:05,747 DEBUG server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1024)) - ignoring exception during input shutdownjava.net.SocketException: Socket is not connected at sun.nio.ch.SocketChannelImpl.shutdown(Native Method) at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:633) at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360) at org.apache.zookeeper.server.NIOServerCnxn.closeSock(NIOServerCnxn.java:1020) at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:977) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:347) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:224) at java.lang.Thread.run(Thread.java:680)They are annoying when you look for actual problems in logs.Those on DEBUG level should be silenced via log levels for ZK classes by default. Not sure what to do with ERROR level one(s?), I'd need to look if they can be silenced/logged as DEBUG on hive side, or maybe file a bug for ZK...</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.log4j.properties</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">data.conf.hive-log4j.properties</file>
      <file type="M">common.src.test.resources.hive-log4j-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j-test.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="5755" opendate="2013-11-6 00:00:00" fixdate="2013-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hadoop2 execution environment Milestone 1</summary>
      <description>It looks like the hadoop2 execution environment isn't exactly correct post mavenization.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">hwi.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5756" opendate="2013-11-6 00:00:00" fixdate="2013-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorization support for IF conditional expression for long, double, timestamp, boolean and string inputs</summary>
      <description>Implement full, end-to-end support for IF in vectorized mode, including new VectorExpression class(es), VectorizationContext translation to a VectorExpression, and unit tests for these, as well as end-to-end ad hoc testing. An end-to-end .q test is recommended but optional.This is high priority because IF is the most popular conditional expression.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="5849" opendate="2013-11-19 00:00:00" fixdate="2013-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the stats of operators based on heuristics in the absence of any column statistics</summary>
      <description>In the absence of any column statistics, operators will simply use the statistics from its parents. It is useful to apply some heuristics to update basic statistics (number of rows and data size) in the absence of any column statistics. This will be worst case scenario.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby.q</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="5946" opendate="2013-12-4 00:00:00" fixdate="2013-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="6024" opendate="2013-12-12 00:00:00" fixdate="2013-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data local inpath unnecessarily creates a copy task</summary>
      <description>Load data command creates an additional copy task only when its loading from local It doesn't create this additional copy task while loading from DFS though.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
    </fixedFiles>
  </bug>
  <bug id="6027" opendate="2013-12-12 00:00:00" fixdate="2013-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>non-vectorized log10 has rounding issue</summary>
      <description>In HIVE-6010, I found that vectorized and non-vectorized log10 may produce different results in the last digit of the mantissa (e.g. 7 vs 8). It turns out that vectorized one uses Math.log10, but non-vectorized uses log/log(10). Both should use Math.log10.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog10.java</file>
    </fixedFiles>
  </bug>
  <bug id="6055" opendate="2013-12-18 00:00:00" fixdate="2013-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup aisle tez</summary>
      <description>Some of the past merges have led to some dead code. Need to remove this from the tez branch.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="61" opendate="2008-11-13 00:00:00" fixdate="2008-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implment ORDER BY</summary>
      <description>ORDER BY is in the query language reference but currently is a no-op. We should make it an op.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6174" opendate="2014-1-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline "set varible" doesn&amp;#39;t show the value of the variable as Hive CLI</summary>
      <description>Currently it displays nothing.0: jdbc:hive2://&gt; set env:TERM; 0: jdbc:hive2://&gt; In contrast, Hive CLI displays the value of the variable.hive&gt; set env:TERM; env:TERM=xterm</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
    </fixedFiles>
  </bug>
  <bug id="619" opendate="2009-7-9 00:00:00" fixdate="2009-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the error messages for missing/incorrect UDF/UDAF class</summary>
      <description>While creating a (temporary) function, if the underlying class does not exist - the current error message is very cryptic.It should be something like "Class xxxx not found" or "Class xxxx does not implement UDF, GenericUDF, or UDAF"</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.udf.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.genericudf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6231" opendate="2014-1-20 00:00:00" fixdate="2014-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when switching to Tez execution mode after session has been initialized</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="6298" opendate="2014-1-24 00:00:00" fixdate="2014-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add config flag to turn off fetching partition stats</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="631" opendate="2009-7-13 00:00:00" fixdate="2009-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestParse dies</summary>
      <description>TestParse is dying - This is probably not working for a long time, and needs to be fixed</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.OpParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6338" opendate="2014-1-30 00:00:00" fixdate="2014-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception handling in createDefaultDb() in Metastore</summary>
      <description>There is a suggestion on HIVE-5959 comment list on possible improvements.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="6342" opendate="2014-1-30 00:00:00" fixdate="2014-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive drop partitions should use standard expr filter instead of some custom class</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.filter.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.drop.partition.filter.failure2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6345" opendate="2014-1-31 00:00:00" fixdate="2014-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DECIMAL support to vectorized JOIN operators</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.UnsignedInt128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  <bug id="6389" opendate="2014-2-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.</summary>
      <description>RCFile tables that use the LazyBinaryColumnarSerDe don't seem to handle look-ups into map-columns when the value of the column is null.When an RCFile table is created with LazyBinaryColumnarSerDe (as is default in 0.12), and queried as follows:select mymap['1024'] from mytable;and if the mymap column has nulls, then one is treated to the following guttural utterance:2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":null,"mymap":null,"isnull":null} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41) at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524) ... 10 moreA patch is on the way, but the short of it is that the LazyBinaryMapOI needs to return nulls if either the map or the lookup-key is null.This is handled correctly for Text data, and for RCFiles using ColumnarSerDe.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug id="6561" opendate="2014-3-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should accept -i option to Initializing a SQL file</summary>
      <description>Hive CLI has -i option. From Hive CLI help:... -i &lt;filename&gt; Initialization SQL file...However, Beeline has no such option:xzhang@xzlt:~/apa/hive3$ ./packaging/target/apache-hive-0.14.0-SNAPSHOT-bin/apache-hive-0.14.0-SNAPSHOT-bin/bin/beeline -u jdbc:hive2:// -i hive.rc...Connected to: Apache Hive (version 0.14.0-SNAPSHOT)Driver: Hive JDBC (version 0.14.0-SNAPSHOT)Transaction isolation: TRANSACTION_REPEATABLE_READ-i (No such file or directory)Property "url" is requiredBeeline version 0.14.0-SNAPSHOT by Apache Hive...</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="6591" opendate="2014-3-7 00:00:00" fixdate="2014-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Importing a table containing hidden dirs fails</summary>
      <description>hidden files should be ignored while exporting</description>
      <version>0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="6683" opendate="2014-3-17 00:00:00" fixdate="2014-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline does not accept comments at end of line</summary>
      <description>Beeline fails to read queries where lines have comments at the end. This works in the embedded Hive CLI.Example:SELECT1 &amp;#8211; this is a comment about this valueFROMtable;Error: Error while processing statement: FAILED: ParseException line 1:36 mismatched input '&lt;EOF&gt;' expecting FROM near '1' in from clause (state=42000,code=40000)</description>
      <version>0.10.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug id="6684" opendate="2014-3-17 00:00:00" fixdate="2014-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline does not accept comments that are preceded by spaces</summary>
      <description>Beeline throws an error if single-line comments are indented with spaces. This works in the embedded Hive CLI.For example:SELECT &amp;#8211; this is the field we want fieldFROM table;Error: Error while processing statement: FAILED: ParseException line 1:71 cannot recognize input near '&lt;EOF&gt;' '&lt;EOF&gt;' '&lt;EOF&gt;' in select clause (state=42000,code=40000)</description>
      <version>0.10.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="6903" opendate="2014-4-14 00:00:00" fixdate="2014-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of hive.metastore.execute.setugi to true</summary>
      <description>Since its introduction in HIVE-2616 I havent seen any bug reported for it, only grief from users who expect system to work as if this is true by default.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7045" opendate="2014-5-12 00:00:00" fixdate="2014-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results in multi-table insert aggregating without group by clause</summary>
      <description>This happens whenever there are more than 1 reducers.The scenario :CREATE TABLE t1 (a int, b int);CREATE TABLE t2 (cnt int) PARTITIONED BY (var_name string);insert into table t1 select 1,1 from asd limit 1;insert into table t1 select 2,2 from asd limit 1;t1 contains :1 12 2from t1insert overwrite table t2 partition(var_name='a') select count(a) cnt insert overwrite table t2 partition(var_name='b') select count(b) cnt ;select * from t2;returns : 2 a2 bas expected.Setting the number of reducers higher than 1 :set mapred.reduce.tasks=2;from t1insert overwrite table t2 partition(var_name='a') select count(a) cntinsert overwrite table t2 partition(var_name='b') select count(b) cnt;select * from t2;1 a1 a1 b1 bWrong results.This happens when ever t1 is big enough to automatically generate more than 1 reducers and without specifying it directly.adding "group by 1" in the end of each insert solves the problem :from t1insert overwrite table t2 partition(var_name='a') select count(a) cnt group by 1insert overwrite table t2 partition(var_name='b') select count(b) cnt group by 1;generates : 2 a2 bThis should work without the group by...The number of rows for each partition will be the amount of reducers.Each reducer calculated a sub total of the count.</description>
      <version>0.10.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7140" opendate="2014-5-28 00:00:00" fixdate="2014-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump default hive.metastore.client.socket.timeout to 5 minutes</summary>
      <description>The issue is that OOTB clients often face timeouts when using HMS since many operations in the HMS completes are long running (e.g. many operations on a table with many partitions). A few supporting pieces of information: The default value of hive.metastore.client.socket.timeout is 20 seconds. Since the timeout is client only, the server happy continues doing the requested work Clients retry after a small delay to perform the requested work again, often while the server is still trying to complete the original request A few tests have actually increased this value in order to pass reliably.</description>
      <version>0.10.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7143" opendate="2014-5-29 00:00:00" fixdate="2014-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Streaming support in Windowing mode for more UDAFs (min/max, lead/lag, fval/lval)</summary>
      <description>Provided implementations for Streaming for the above fns.Min/Max based on Alg by Daniel Lemire: http://www.archipel.uqam.ca/309/1/webmaximinalgo.pdf</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEnhancer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
    </fixedFiles>
  </bug>
  <bug id="7169" opendate="2014-6-2 00:00:00" fixdate="2014-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in Http Mode should have a configurable IdleMaxTime timeout</summary>
      <description>Currently, in HiveServer2 we use Jetty Server to start the Http Server. The connector used for this Thrift Http Cli Service has maximum idle time as the default timeout as mentioned in http://grepcode.com/file/repo1.maven.org/maven2/org.eclipse.jetty/jetty-server/7.0.0.v20091005/org/eclipse/jetty/server/AbstractConnector.java#AbstractConnector.0_maxIdleTime.This should be manually configurable using connector.setMaxIdleTime(maxIdleTime);</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7193" opendate="2014-6-6 00:00:00" fixdate="2014-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should support additional LDAP authentication parameters</summary>
      <description>Currently hive has only following authenticator parameters for LDAP authentication for hiveserver2:&lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;LDAP&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.ldap.url&lt;/name&gt; &lt;value&gt;ldap://our_ldap_address&lt;/value&gt; &lt;/property&gt; We need to include other LDAP properties as part of hive-LDAP authentication like below:a group search base -&gt; dc=domain,dc=com a group search filter -&gt; member={0} a user search base -&gt; dc=domain,dc=com a user search filter -&gt; sAMAAccountName={0} a list of valid user groups -&gt; group1,group2,group3</description>
      <version>0.10.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="7194" opendate="2014-6-7 00:00:00" fixdate="2014-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>authorization_ctas.q failing on trunk</summary>
      <description>Need to update .q.out file</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7255" opendate="2014-6-19 00:00:00" fixdate="2014-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow partial partition spec in analyze command</summary>
      <description>So that stats collection can happen for multiple partitions through one statement.</description>
      <version>0.10.0,0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.invalid.values.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.incorrect.num.keys.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.incorrect.num.keys.q</file>
      <file type="M">ql.src.test.queries.clientnegative.columnstats.partlvl.dp.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="7450" opendate="2014-7-19 00:00:00" fixdate="2014-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Database should inherit perms of warehouse dir</summary>
      <description>One more ask: the database directory should inherit permission and extended ACL's of the hive warehouse directory.As table dirs are inheriting it, theres no reason that database dirs shouldn't inherit it.Behavior is governed by "hive.warehouse.subdir.inherit.perms" flag.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="7451" opendate="2014-7-19 00:00:00" fixdate="2014-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pass function name in create/drop function to authorization api</summary>
      <description>If function names are passed to the authorization api for create/drop function calls, then authorization decisions can be made based on the function names as well.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.fail.nogrant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.udaf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.using.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.logic.java.boolean.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.context.aware.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.compare.java.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.sum.list.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.register.tblfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compile.processor.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.admin.almighty2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.test.error.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.nonexistent.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.local.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.function.does.not.implement.udf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.temp.table.authorize.create.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.native.udf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.udf.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.unknown.genericudf.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.udaf.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.function.nonudf.class.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.function.nonexistent.class.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorize.create.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.truncate.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.show.parts.nosel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.rolehierarchy.privs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.tab.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.alter.tab.serdeprop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.alter.tab.rename.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insert.noselectpriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insert.noinspriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.insertoverwrite.nodel.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.invalid.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.udtf.explode2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.add.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.arraymapstruct.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.example.format.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.row.sequence.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udtf.explode2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udtf.output.on.close.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.Entity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.addjar.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.addpartition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.alter.db.owner.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.alter.db.owner.default.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.compile.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.createview.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.func2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.create.macro1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.deletejar.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.desc.table.nosel.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.dfs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.droppartition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.allpriv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.fail1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="7453" opendate="2014-7-19 00:00:00" fixdate="2014-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Partition Pruning enhancements 1</summary>
      <description>1. Handle type casts2. Handle Literal Conversion for Partition Pruning expressions</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7457" opendate="2014-7-21 00:00:00" fixdate="2014-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor HCatalog Pig Adapter test clean up</summary>
      <description>Minor cleanup to the HCatalog Pig Adapter tests in preparation for HIVE-7420: Run through Hive Eclipse formatter. Convert JUnit 3-style tests to follow JUnit 4 conventions.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestPigHCatUtil.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestOrcHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerMulti.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestE2EScenarios.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.MockLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="7459" opendate="2014-7-21 00:00:00" fixdate="2014-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="7468" opendate="2014-7-22 00:00:00" fixdate="2014-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:UDF translation needs to use Hive UDF name</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7580" opendate="2014-7-31 00:00:00" fixdate="2014-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support dynamic partitioning [Spark Branch]</summary>
      <description>My understanding is that we don't need to do anything special for this. However, this needs to be verified and tested.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="7592" opendate="2014-8-1 00:00:00" fixdate="2014-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>List Jars or Files are not supported by Beeline</summary>
      <description>Through adding jars or files are supported by Beeline, List jars or Files are still not supported.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7634" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Configuration.getPassword() if available to eliminate passwords from hive-site.xml</summary>
      <description>HADOOP-10607 provides a Configuration.getPassword() API that allows passwords to be retrieved from a configured credential provider, while also being able to fall back to the HiveConf setting if no provider is set up. Hive should use this API for versions of Hadoop that support this API. This would give users the ability to remove the passwords from their Hive configuration files.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8435" opendate="2014-10-12 00:00:00" fixdate="2014-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add identity project remover optimization</summary>
      <description></description>
      <version>0.9.0,0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="8436" opendate="2014-10-12 00:00:00" fixdate="2014-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify SparkWork to split works with multiple child works [Spark Branch]</summary>
      <description>Based on the design doc, we need to split the operator tree of a work in SparkWork if the work is connected to multiple child works. The way splitting the operator tree is performed by cloning the original work and removing unwanted branches in the operator tree. Please refer to the design doc for details.This process should be done right before we generate SparkPlan. We should have a utility method that takes the orignal SparkWork and return a modified SparkWork.This process should also keep the information about the original work and its clones. Such information will be needed during SparkPlan generation (HIVE-8437).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkTableScanProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMultiInsertionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMergeTaskProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8522" opendate="2014-10-20 00:00:00" fixdate="2014-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Update Calcite Version to 0.9.2-incubating-SNAPSHOT</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8572" opendate="2014-10-23 00:00:00" fixdate="2014-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable more vectorization tests [Spark Branch]</summary>
      <description>There are some vectorization tests not enabled for the spark branch. I gave them a try and most of them work fine.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
</bugrepository>
