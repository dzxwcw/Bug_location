<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="15135" opendate="2016-11-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an llap mode which fails if queries cannot run in llap</summary>
      <description>ALL currently ends up launching new containers for queries which cannot run in llap.There should be a mode where these queries don't run.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="15136" opendate="2016-11-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: allow slider placement policy configuration during install</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug id="16413" opendate="2017-4-10 00:00:00" fixdate="2017-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table as select does not check ownership of the location</summary>
      <description>1. following statement failed: create table foo(id int) location 'hdfs:///tmp/foo';Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=userx, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/tmp/foo]] (state=42000,code=40000)2. but when use create table as select, it successed:0: jdbc:hive2://189.39.151.44:21066/&gt; create table foo location 'hdfs:///tmp/foo' as select * from xxx2;INFO : Number of reduce tasks is set to 0 since there's no reduce operatorINFO : number of splits:1INFO : Submitting tokens for job: job_1491449632882_0094INFO : Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:haclusterINFO : The url to track the job: https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Starting Job = job_1491449632882_0094, Tracking URL = https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Kill Command = /opt/hive-1.3.0/bin/..//../hadoop/bin/hadoop job -kill job_1491449632882_0094INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0INFO : 2017-04-10 09:44:49,185 Stage-1 map = 0%, reduce = 0%INFO : 2017-04-10 09:44:57,202 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.98 secINFO : MapReduce Total cumulative CPU time: 1 seconds 980 msecINFO : Ended Job = job_1491449632882_0094INFO : Stage-3 is selected by condition resolver.INFO : Stage-2 is filtered out by condition resolver.INFO : Stage-4 is filtered out by condition resolver.INFO : Moving data to directory hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001 from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10003INFO : Moving data to directory hdfs:/tmp/foo from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001No rows affected (26.969 seconds)3. and the table location is hdfs://hacluster/tmp/foo :0: jdbc:hive2://189.39.151.44:21066/&gt; desc formatted foo;+-------------------------------+-------------------------------------------------------+-----------------------+--+| col_name | data_type | comment |+-------------------------------+-------------------------------------------------------+-----------------------+--+| # col_name | data_type | comment || | NULL | NULL || id | int | || | NULL | NULL || # Detailed Table Information | NULL | NULL || Database: | default | NULL || Owner: | userx | NULL || CreateTime: | Mon Apr 10 09:44:59 CST 2017 | NULL || LastAccessTime: | UNKNOWN | NULL || Protect Mode: | None | NULL || Retention: | 0 | NULL || Location: | hdfs://hacluster/tmp/foo | NULL || Table Type: | MANAGED_TABLE | NULL || Table Parameters: | NULL | NULL || | COLUMN_STATS_ACCURATE | false || | numFiles | 1 || | numRows | -1 || | rawDataSize | -1 || | totalSize | 56 || | transient_lastDdlTime | 1491788699 || | NULL | NULL || # Storage Information | NULL | NULL || SerDe Library: | org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe | NULL || InputFormat: | org.apache.hadoop.hive.ql.io.RCFileInputFormat | NULL || OutputFormat: | org.apache.hadoop.hive.ql.io.RCFileOutputFormat | NULL || Compressed: | No | NULL || Num Buckets: | -1 | NULL || Bucket Columns: | [] | NULL || Sort Columns: | [] | NULL || Storage Desc Params: | NULL | NULL || | serialization.format | 1 |+-------------------------------+-------------------------------------------------------+-----------------------+--+</description>
      <version>1.2.2,1.3.0,2.1.1</version>
      <fixedVersion>1.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug id="16425" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: unload old hashtables before reloadHashTable</summary>
      <description>@Override protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException { // The super method will reload a hash table partition of one of the small tables. // Currently, for native vector map join it will only be one small table. super.reloadHashTable(pos, partitionId); MapJoinTableContainer smallTable = spilledMapJoinTables[pos]; vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable); needHashTableSetup = true; LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName()); if (isLogDebugEnabled) { LOG.debug(CLASS_NAME + " reloadHashTable!"); } }The super call causes an OOM because of existing memory usage by vectorMapJoinHashTable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16429" opendate="2017-4-12 00:00:00" fixdate="2017-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</summary>
      <description>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="17107" opendate="2017-7-17 00:00:00" fixdate="2017-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Yetus to 0.5.0</summary>
      <description>Yetus 0.5.0 is released, and it contains our fixes.We should upgrade and remove our extra patched files.CC: kgyrtkirk</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.yetus-wrapper.sh</file>
      <file type="M">dev-support.maven.YETUS-506.sh</file>
      <file type="M">dev-support.findbugs.YETUS-471.sh</file>
      <file type="M">dev-support.checkstyle.YETUS-484.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17167" opendate="2017-7-25 00:00:00" fixdate="2017-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore specific configuration tool</summary>
      <description>As part of making the metastore a separately releasable module we need configuration tools that are specific to that module. It cannot use or extend HiveConf as that is in hive common. But it must take a HiveConf object and be able to operate on it.The best way to achieve this is using Hadoop's Configuration object (which HiveConf extends) together with enums and static methods.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17168" opendate="2017-7-25 00:00:00" fixdate="2017-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for stand alone metastore</summary>
      <description>We need to create a separate maven module for the stand alone metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17241" opendate="2017-8-3 00:00:00" fixdate="2017-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change metastore classes to not use the shims</summary>
      <description>As part of moving the metastore into a standalone package, it will no longer have access to the shims. This means we need to either copy them or access the underlying Hadoop operations directly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TUGIBasedProcessor.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.StorageBasedMetastoreTestBase.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
    </fixedFiles>
  </bug>
  <bug id="1731" opendate="2010-10-19 00:00:00" fixdate="2010-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve miscellaneous error messages</summary>
      <description>This is a place for accumulating error message improvements so that we can update a bunch in batch.</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.function.param2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.dot.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.insert.wrong.number.columns.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.in.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.not.bool.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.function.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.index.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby.key.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.nospec.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.noof.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sample.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="17682" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: IF stmt produces wrong results</summary>
      <description>A query using with a vectorized IF(condition, thenExpr, elseExpr) function can produce wrong results.</description>
      <version>1.2.2,2.3.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17684" opendate="2017-10-3 00:00:00" fixdate="2017-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS memory issues with MapJoinMemoryExhaustionHandler</summary>
      <description>We have seen a number of memory issues due the HashSinkOperator use of the MapJoinMemoryExhaustionHandler. This handler is meant to detect scenarios where the small table is taking too much space in memory, in which case a MapJoinMemoryExhaustionError is thrown.The configs to control this logic are:hive.mapjoin.localtask.max.memory.usage (default 0.90)hive.mapjoin.followby.gby.localtask.max.memory.usage (default 0.55)The handler works by using the MemoryMXBean and uses the following logic to estimate how much memory the HashMap is consuming: MemoryMXBean#getHeapMemoryUsage().getUsed() / MemoryMXBean#getHeapMemoryUsage().getMax()The issue is that MemoryMXBean#getHeapMemoryUsage().getUsed() can be inaccurate. The value returned by this method returns all reachable and unreachable memory on the heap, so there may be a bunch of garbage data, and the JVM just hasn't taken the time to reclaim it all. This can lead to intermittent failures of this check even though a simple GC would have reclaimed enough space for the process to continue working.We should re-think the usage of MapJoinMemoryExhaustionHandler for HoS. In Hive-on-MR this probably made sense to use because every Hive task was run in a dedicated container, so a Hive Task could assume it created most of the data on the heap. However, in Hive-on-Spark there can be multiple Hive Tasks running in a single executor, each doing different things.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="17988" opendate="2017-11-6 00:00:00" fixdate="2017-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace patch utility usage with git apply in ptest</summary>
      <description>It would be great to replace the standard diff util because git can do a 3-way merge - which in most cases successfull.This could reduce the ptest results which are erroring out because of build failure.error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:7003Falling back to three-way merge...Applied patch to 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java' cleanly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="17994" opendate="2017-11-7 00:00:00" fixdate="2017-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Serialization bottlenecked on irrelevant hashmap lookup</summary>
      <description>On machines with slower NUMA, the hashmap lookup for TypeInfo::getPrimitiveCategory is the slowest part of the vectorized serialization loops. The static object references run hot with the NUMA access speeds penalizing half the threads.This lookup is done for every column, for every row - though vectorization enforces that this type cannot change at all.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
    </fixedFiles>
  </bug>
  <bug id="17996" opendate="2017-11-7 00:00:00" fixdate="2017-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ASF headers</summary>
      <description>Yetus check reports some ASF header related issues in Hive code. Let's fix them up.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.resources.log4j2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumEmptyIsZeroAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="18415" opendate="2018-1-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower "Updating Partition Stats" Logging Level</summary>
      <description>org.apache.hadoop.hive.metastore.utils.MetaStoreUtilsLOG.warn("Updating partition stats fast for: " + part.getTableName());...LOG.warn("Updated size to " + params.get(StatsSetupConst.TOTAL_SIZE));This logging produces many lines of WARN log messages in my log file and it's not clear to me what the issue is here. Why is this a warning and how should I respond to address this warning?DEBUG is probably more appropriate for a utility class. Please lower.</description>
      <version>1.2.2,2.2.0,2.3.2,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="18919" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove separate keytab setting for ZK in LLAP</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="1892" opendate="2011-1-6 00:00:00" fixdate="2011-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>show functions also returns internal operators</summary>
      <description>show functions: returns bigint etc. in its outputs, which are not valid external functions</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="18920" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Initialize the Janino providers ahead of 1st query</summary>
      <description>Hive Calcite metadata providers are compiled when the 1st query comes in.If a second query arrives before the 1st one has built a metadata provider, it will also try to do the same thing, because the cache is not populated yet.With 1024 concurrent users, it takes 6 minutes for the 1st query to finish fighting all the other queries which are trying to load that cache.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19077" opendate="2018-3-29 00:00:00" fixdate="2018-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle duplicate ptests requests standing in queue at the same time</summary>
      <description>I've been keeping on eye on our PreCommit-HIVE-Build job, and what I noticed that sometimes huge queues can build up, that contain jira's more than once. (Yesterday I've seen a queue of 40, having 31 distinct jiras..)Simple scenario is that I upload a patch, it gets queued for ptest (already long queue), and 3 hours later I will update it, re-upload and re-queue. Now the current ptest infra seems to be smart enough to always deal with the latest patch, so what will happen is that the same patch will be tested 2 times (with ~3 hours) diff, most probably with same result.I propose we do some deduplication - if ptest starts running the request for Jira X, then it can take a look on the current queue, and see if X is there again. If so, it can skip for now, it will be picked up later anyway.In practice this means that if you reconsider your patch and update it, your original place in the queue will be gone (like as a penalty for changing it), but overall it saves resources for the whole community.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestTestCheckPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.TestCheckPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.JenkinsQueueUtil.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.PTestClient.java</file>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19079" opendate="2018-3-29 00:00:00" fixdate="2018-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add extended query string to Spark job description</summary>
      <description>As of HIVE-16601, we place a shortened version of the query into the Spark job description. We should look into adding a longer version of the query. It seems that the Spark Web UI has a nice feature where long job descriptions will be truncated with a ..., but when you double click on the ... it expands to show the rest of the string.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestHiveSparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="19768" opendate="2018-6-1 00:00:00" fixdate="2018-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Utility to convert tables to conform to Hive strict managed tables mode</summary>
      <description>Create a utility that can check existing hive tables and convert them if necessary to conform to strict managed tables mode. Managed non-transactional ORC tables will be converted to full transactional tables Managed non-transactional tables of other types will be converted to insert-only transactional tables Tables with non-native storage/schema will be converted to external tables.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20402" opendate="2018-8-16 00:00:00" fixdate="2018-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ITest needs explicit dependency on hbase-common test-jar</summary>
      <description>itest currently relies on hbase-common test jar for HBaseMiniCluster, but pulls in this dependency transitively via a bug in hbase-procedure's own dependency tree. This was fixed in HBase 2.1, so Hive should prepare for that by explicitly declaring the dependency.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20772" opendate="2018-10-18 00:00:00" fixdate="2018-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>record per-task CPU counters in LLAP</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.counters.LlapIOCounters.java</file>
    </fixedFiles>
  </bug>
  <bug id="21050" opendate="2018-12-17 00:00:00" fixdate="2018-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Parquet LogicalTypes</summary>
      <description>&amp;#91;WIP until Parquet community releases version 1.11.0&amp;#93;The new Parquet version (1.11.0) uses LogicalTypes instead of OriginalTypes. These are backwards-compatible with OriginalTypes.Thanks to kuczoram for her work on this patch.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestMapStructures.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestArrayCompatibility.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.HiveParquetSchemaTestUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedListColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.BaseVectorizedColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22053" opendate="2019-7-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Function name is not normalized when creating function</summary>
      <description>If a function is created with a name containing upper case characters, we get NoSuchObjectException when trying to get that function.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestFunctions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
